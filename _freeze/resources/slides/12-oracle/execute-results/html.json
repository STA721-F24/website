{
  "hash": "fde64a7f83d367c1476a351e3bd56c17",
  "result": {
    "engine": "knitr",
    "markdown": "---\nsubtitle: \"STA 721: Lecture 12\"\ntitle: \"Optimal Shrinkage/Selection and Oracle Properties\"\nauthor: \"Merlise Clyde (clyde@duke.edu)\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    smaller: true\n    logo: ../../img/icon.png\n    footer: <https://sta721-F24.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"   \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\nnumber-sections: false\nfilters:\n  - custom-numbered-blocks  \ncustom-numbered-blocks:\n  groups:\n    thmlike:\n      colors: [948bde, 584eab]\n      boxstyle: foldbox.simple\n      collapse: false\n      listin: [mathstuff]\n    todos: default  \n  classes:\n    Theorem:\n      group: thmlike\n      numbered: false\n    Lemma:\n      group: thmlike\n      numbered: false\n    Corollary:\n      group: thmlike\n      numbered: false\n    Proposition:\n      group: thmlike\n      numbered: false      \n    Proof:\n      group: thmlike\n      numbered: false\n      collapse: false   \n    Exercise:\n      group: thmlike\n      numbered: false\n    Definition:\n      group: thmlike\n      numbered: false\n      colors: [d999d3, a01793]\n    Feature: \n       numbered: false\n    TODO:\n      label: \"To do\"\n      colors: [e7b1b4, 8c3236]\n      group: todos\n      listin: [stilltodo]\n    DONE:\n      label: \"Done\"\n      colors: [cce7b1, 86b754]  \n      group: todos  \n---\n\n\n\n\n\n## Outline\n\n\\usepackage{xcolor}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator{\\sgn}{sgn}\n\\newcommand{\\e}{\\mathbf{e}}\n\\newcommand{\\Mb}{\\mathbf{M}}\n\\renewcommand{\\P}{\\mathbf{P}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\R}{\\textsf{R}}\n\\newcommand{\\mat}[1] {\\mathbf{#1}}\n\\newcommand{\\pen}{\\textsf{pen}}\n\\newcommand{\\E}{\\textsf{E}}\n\\newcommand{\\SE}{\\textsf{SE}}\n\\newcommand{\\SSE}{\\textsf{SSE}}\n\\newcommand{\\RSS}{\\textsf{RSS}}\n\\newcommand{\\FSS}{\\textsf{FSS}}\n\\renewcommand{\\SS}{\\textsf{SS}}\n\\newcommand{\\MSE}{\\textsf{MSE}}\n\\newcommand{\\SSR}{\\textsf{SSR}}\n\\newcommand{\\Be}{\\textsf{Beta}}\n\\newcommand{\\St}{\\textsf{St}}\n\\newcommand{\\Ca}{\\textsf{C}}\n\\newcommand{\\Lv}{\\textsf{LÃ©vy}}\n\\newcommand{\\Exp}{\\textsf{Exp}}\n\\newcommand{\\GDP}{\\textsf{GDP}}\n\\newcommand{\\NcSt}{\\textsf{NcSt}}\n\\newcommand{\\Bin}{\\textsf{Bin}}\n\\newcommand{\\NB}{\\textsf{NegBin}}\n\\newcommand{\\Mult}{\\textsf{MultNom}}\n\\renewcommand{\\NG}{\\textsf{NG}}\n\\newcommand{\\N}{\\textsf{N}}\n\\newcommand{\\Ber}{\\textsf{Ber}}\n\\newcommand{\\Poi}{\\textsf{Poi}}\n\\newcommand{\\Gam}{\\textsf{Gamma}}\n\\newcommand{\\BB}{\\textsf{BB}}\n\\newcommand{\\BF}{\\textsf{BF}}\n\\newcommand{\\Gm}{\\textsf{G}}\n\\newcommand{\\Un}{\\textsf{Unif}}\n\\newcommand{\\Ex}{\\textsf{Exp}}\n\\newcommand{\\DE}{\\textsf{DE}}\n\\newcommand{\\tr}{\\textsf{tr}}\n\\newcommand{\\cF}{{\\cal{F}}}\n\\newcommand{\\cL}{{\\cal{L}}}\n\\newcommand{\\cI}{{\\cal{I}}}\n\\newcommand{\\cB}{{\\cal{B}}}\n\\newcommand{\\cP}{{\\cal{P}}}\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\pperp}{\\mathrel{{\\rlap{$\\,\\perp$}\\perp\\,\\,}}}\n\\newcommand{\\OFP}{(\\Omega,\\cF, \\P)}\n\\newcommand{\\eps}{\\boldsymbol{\\epsilon}}\n\\def\\ehat{\\hat{\\boldsymbol{\\epsilon}}}\n\\newcommand{\\Psib}{\\boldsymbol{\\Psi}}\n\\newcommand{\\Omegab}{\\boldsymbol{\\Omega}}\n\\newcommand{\\1}{\\mathbf{1}_n}\n\\newcommand{\\gap}{\\vspace{8mm}}\n\\newcommand{\\ind}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}}\n\\newcommand{\\iid}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}}\n\\newcommand{\\simiid}{\\ensuremath{\\mathrel{\\mathop{\\sim}\\limits^{\\rm\niid}}}}\n\\newcommand{\\eqindis}{\\mathrel{\\mathop{=}\\limits^{\\rm D}}}\n\\newcommand{\\SSZ}{S_{zz}}\n\\newcommand{\\SZW}{S_{zw}}\n\\newcommand{\\Var}{\\textsf{Var}}\n\\newcommand{\\corr}{\\textsf{corr}}\n\\newcommand{\\cov}{\\textsf{cov}}\n\\newcommand{\\diag}{\\textsf{diag}}\n\\newcommand{\\var}{\\textsf{var}}\n\\newcommand{\\Cov}{\\textsf{Cov}}\n\\newcommand{\\Sam}{{\\cal S}}\n\\newcommand{\\VS}{{\\cal V}}\n\\def\\H{\\mathbf{H}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\tY}{\\tilde{\\mathbf{Y}}}\n\\newcommand{\\Yhat}{\\hat{\\mathbf{Y}}}\n\\newcommand{\\yhat}{\\hat{\\mathbf{y}}}\n\\newcommand{\\Yobs}{\\mathbf{Y}_{{\\cal S}}}\n\\newcommand{\\barYobs}{\\bar{Y}_{{\\cal S}}}\n\\newcommand{\\barYmiss}{\\bar{Y}_{{\\cal S}^c}}\n\\def\\bv{\\mathbf{b}}\n\\def\\X{\\mathbf{X}}\n\\def\\XtX{\\X^T\\X}\n\\def\\tX{\\tilde{\\mathbf{X}}}\n\\def\\x{\\mathbf{x}}\n\\def\\xbar{\\bar{\\mathbf{x}}}\n\\def\\Xbar{\\bar{\\mathbf{X}}}\n\\def\\Xg{\\mathbf{X}_{\\boldsymbol{\\gamma}}}\n\\def\\Ybar{\\bar{\\Y}}\n\\def\\ybar{\\bar{y}}\n\\def\\y{\\mathbf{y}}\n\\def\\Yf{\\mathbf{Y_f}}\n\\def\\W{\\mathbf{W}}\n\\def\\L{\\mathbf{L}}\n\\def\\m{\\mathbf{m}}\n\\def\\n{\\mathbf{n}}\n\\def\\w{\\mathbf{w}}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\z{\\mathbf{z}}\n\\def\\v{\\mathbf{v}}\n\\def\\u{\\mathbf{u}}\n\n\\def\\zero{\\mathbf{0}}\n\\def\\one{\\mathbf{1}}\n\\newcommand{\\taub}{\\boldsymbol{\\tau}}\n\\newcommand{\\betav}{\\boldsymbol{\\beta}}\n\\newcommand{\\alphav}{\\boldsymbol{\\alpha}}\n\\newcommand{\\bfomega}{\\boldsymbol{\\omega}}\n\\newcommand{\\bfchi}{\\boldsymbol{\\chi}}\n\\newcommand{\\bfLambda}{\\boldsymbol{\\Lambda}}\n\\newcommand{\\Lmea}{{\\cal L}} \n\\newcommand{\\scale}{\\bflambda} \n\\newcommand{\\Scale}{\\bfLambda} \n\\newcommand{\\bfscale}{\\bflambda} \n\\newcommand{\\mean}{\\bfchi}\n\\newcommand{\\loc}{\\bfchi}\n\\newcommand{\\bfmean}{\\bfchi}\n\\newcommand{\\bfx}{\\x}\n\\renewcommand{\\k}{g}\n\\newcommand{\\Gen}{{\\cal G}}\n\\newcommand{\\Levy}{L{\\'e}vy}\n\\def\\bfChi    {\\boldsymbol{\\Chi}}\n\\def\\bfOmega  {\\boldsymbol{\\Omega}}\n\\newcommand{\\Po}{\\mbox{\\sf P}}\n\\newcommand{\\A}{\\mathbf{A}}\n\\def\\a{\\mathbf{a}}\n\\def\\K{\\mathbf{K}}\n\\newcommand{\\B}{\\mathbf{B}}\n\\def\\b{\\boldsymbol{\\beta}}\n\\def\\bhat{\\hat{\\boldsymbol{\\beta}}}\n\\def\\btilde{\\tilde{\\boldsymbol{\\beta}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\bg{\\boldsymbol{\\beta_\\gamma}}\n\\def\\bgnot{\\boldsymbol{\\beta_{(-\\gamma)}}}\n\\def\\mub{\\boldsymbol{\\mu}}\n\\def\\tmub{\\tilde{\\boldsymbol{\\mu}}}\n\\def\\muhat{\\hat{\\boldsymbol{\\mu}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\tk{\\boldsymbol{\\theta}_k}\n\\def\\tj{\\boldsymbol{\\theta}_j}\n\\def\\Mk{\\boldsymbol{{\\cal M}}_k}\n\\def\\M{\\boldsymbol{{\\cal M}}}\n\\def\\Mj{\\boldsymbol{{\\cal M}}_j}\n\\def\\Mi{\\boldsymbol{{\\cal M}}_i}\n\\def\\Mg{{\\boldsymbol{{\\cal M}_\\gamma}}}\n\\def\\Mnull{\\boldsymbol{{\\cal M}}_{N}}\n\\def\\gMPM{\\boldsymbol{\\gamma}_{\\text{MPM}}}\n\\def\\gHPM{\\boldsymbol{\\gamma}_{\\text{HPM}}}\n\\def\\Mfull{\\boldsymbol{{\\cal M}}_{F}}\n\\def\\NS{\\boldsymbol{{\\cal N}}}\n\\def\\tg{\\boldsymbol{\\theta}_{\\boldsymbol{\\gamma}}}\n\\def\\g{\\boldsymbol{\\gamma}}\n\\def\\eg{\\boldsymbol{\\eta}_{\\boldsymbol{\\gamma}}}\n\\def\\G{\\mathbf{G}}\n\\def\\cM{\\cal M}\n\\def\\D{\\boldsymbol{\\Delta}}\n\\def\\Dbf{\\mathbf{D}}\n\\def \\shat{{\\hat{\\sigma}}^2}\n\\def\\uv{\\mathbf{u}}\n\\def\\l {\\lambda}\n\\def\\d{\\delta}\n\\def\\deltab{\\mathbf{\\delta}}\n\\def\\Sigmab{\\boldsymbol{\\Sigma}}\n\\def\\Phib{\\boldsymbol{\\Phi}}\n\\def\\Lambdab{\\boldsymbol{\\Lambda}}\n\\def\\lambdab{\\boldsymbol{\\lambda}}\n\\def\\Mg{{\\cal M}_\\gamma}\n\\def\\S{{\\cal{S}}}\n\\def\\Sbf{{\\mathbf{S}}}\n\\def\\qg{p_{\\boldsymbol{\\gamma}}}\n\\def\\pg{p_{\\boldsymbol{\\gamma}}}\n\\def\\T{\\boldsymbol{\\Theta}}\n\\def\\Tb{\\boldsymbol{\\Theta}}\n\\def\\t{\\mathbf{t}}\n\n\n\n- Bounded Influence and Posterior Mean\n- Shrinkage properties and nonconcave penalties\n- conditions for optimal shrinkage and selection\n. . .\n\n- Readings (see reading link)\n\n\n  - Tibshirani (JRSS B 1996)\n  - [Carvalho, Polson & Scott (Biometrika 2010)](https://www.jstor.org/stable/25734098)\n  - [Armagan, Dunson & Lee (Statistica Sinica 2013)](https://www.jstor.org/stable/24310517)\n  - [Fan & Li (JASA 2001)](https://www.jstor.org/stable/3085904)\n\n\n\n\n## Horseshoe Priors\n  Carvalho, Polson & Scott (2010) propose an alternative shrinkage prior \n\\begin{align*}\n\\b \\mid \\phi & \\sim \\N(\\zero_p, \\frac{\\diag(\\tau^2)}{ \\phi\n    }) \\\\\n\\tau \\mid \\lambda & \\iid C^+(0, \\lambda) \\\\ \n\\lambda & \\sim \\Ca^+(0, 1/\\phi) \\\\\np(\\alpha, \\phi) & \\propto 1/\\phi\n\\end{align*}\n\n- $C^+(0, \\lambda)$ is the half-Cauchy distribution with scale $\\lambda$ \n$$\np(\\tau \\mid \\lambda) = \\frac{2}{\\pi} \\frac{\\lambda}{\\lambda^2 + \\tau_j^2}\n$$\n- $\\Ca^+(0, 1/\\phi)$ is the half-Cauchy distribution with scale $1/\\phi$\n\n## Special Case: Orthonormal Regression \n\n:::: {.columns}\n::: {.column width=\"60%\"}\nIn the case $\\lambda = \\phi = 1$ and with $\\X^t\\X = \\I$, $\\Y^* =\n\\X^T\\Y$ \\pause\n\\begin{align*}\nE[\\beta_i \\mid \\Y] & = \\E_{\\kappa_i \\mid \\Y}[ \\E_{\\beta_i \\mid \\kappa_i, \\Y}[\\beta_i \\mid \\Y] \\\\\n& = \\int_0^1 (1 - \\kappa_i) y^*_i p(\\kappa_i \\mid \\Y)\n\\ d\\kappa_i \\\\\n& = (1 - \\E[\\kappa \\mid y^*_i]) y^*_i\n\\end{align*}\nwhere $\\kappa_i = 1/(1 + \\tau_i^2)$ is the shrinkage factor (like in James-Stein)\n\n- Half-Cauchy prior induces a Beta(1/2, 1/2) distribution on $\\kappa_i$\na priori  (change of variables)\n\n- marginal prior (after integrating out )\n:::\n\n::: {.column width=\"40%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](12-oracle_files/figure-revealjs/unnamed-chunk-1-1.png){fig-align='center' width=5in height=5in}\n:::\n:::\n\n\n\n:::\n::::\n\n## Bounded Influence ($\\XtX = \\I$)\n\n:::: {.columns}\n::: {.column width=\"60%}\n- Posterior mean of $\\beta_i$  may be written as \n$$E[\\beta_i \\mid y^*_i] = y^*_i + \\frac{d} {d y} \\log m(y^*_i)$$\nwhere $m(y)$ is the predictive density $y^*_i$ under the prior (known $\\lambda$) \n\n- Bounded Influence of the prior (in this setting) means that $$\\lim_{|y| \\to \\infty} \\frac{d}{dy} \\log m(y) = c$$\n\n-  For HS $\\lim_{|y| \\to \\infty} \\frac{d}{dy} \\log m(y) = 0$\n\n-  $\\lim_{|y_i^*| \\to \\infty} E[\\beta_i \\mid y^*_i) \\to y^*_i$ (the MLE)\n\n- unbiasedness for large $|y_i^*|$\n:::\n\n::: {.column width=\"40%\"}\n![](img/shrinkage.png)\n\n- DE has bounded influence, but bound does not decay to zero in tails so the posterior mean does not shrink to the MLE (bounded away)\n:::\n::::\n\n::: footer\n:::\n\n## Comparison\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n- Diabetes data (from the `lars` package)\n\n- 64 predictors: 10 main effects, 2-way interactions and quadratic \nterms \n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n- sample size of 442\n\n- split into training and test sets\n\n- compare MSE for out-of-sample prediction\nusing OLS, lasso and horseshoe priors\n\n- Root MSE for prediction for left out data based on 25 different random splits with 100 test cases\n\n- both Lasso and Horseshoe much better than OLS\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](12-oracle_files/figure-revealjs/diabetes-plot-mse-1.png){fig-align='center' width=7in height=6in}\n:::\n:::\n\n\n\n\n:::\n\n::::\n\n## Duality for Modal Estimators\n- Model $Y = \\X \\b + \\eps$ with $\\XtX = \\I_p$ and $\\bhat = \\X^T \\Y \\equiv \\Y^*$ (take $\\sigma^2 = 1$)\n\n-   Penalized Least Squares \n$$\n\\bhat_j^\\lambda = \\argmin_{\\b} \\ \\frac{1}{2}\\| \\Y - \\X \\bhat \\|^2 + \\frac{1}{2} \\sum_j(\\beta_j -\\hat{\\beta}_j)^2 + \\sum_j \\pen_\\lambda(\\beta_j)\n$$ \n\n-  Bayes posterior mode (conditional) with prior $p(\\b \\mid \\lambda) = \\prod_j p(\\beta_j \\mid \\lambda)$\n\\begin{align*}\n\\bhat_j^\\lambda & =\\argmax_{\\b} -\\frac{1}{2}  \\| \\Y - \\X \\bhat \\|^2 - \\frac{1}{2} \\sum_j(\\beta_j -\\hat{\\beta}_j)^2 + \\sum_j \\log(p(\\beta_j \\mid \\lambda)) \\\\\n& = \\argmin_{\\b}  \\frac{1}{2}\\| \\Y - \\X \\bhat \\|^2 + \\frac{1}{2} \\sum_j(\\beta_j -\\hat{\\beta}_j)^2  -  \\sum_j\\log(p(\\beta_j \\mid \\lambda)) \\\\\n& = \\argmin_{\\b}  \\frac{1}{2}\\| \\Y - \\X \\bhat \\|^2 + \\frac{1}{2} \\sum_j(\\beta_j -\\hat{\\beta}_j)^2  + \\sum_j\\pen_\\lambda(\\beta_j) \\\\\n\\end{align*} \n\n\n::: footer\n:::\n\n## Properties for Modal Estimates\nFan \\& Li (JASA 2001) discuss variable\nselection via nonconcave penalties and oracle properties  in the context of penalized likelihoods in this setting\n\n- with duality of the negative log prior as their penalty we can extend to Bayesian modal estimates where the prior is a function of $|\\beta_j|$\n$$\\frac 1 2 \\sum(\\beta_i - y_i^*)^2 + \\frac 1 2 \\sum_j(\\beta_j - \\hat{\\beta}_j)^2 +  \\sum_j \\pen_\\lambda(|\\beta_j|)$$ \n\n-  Requirements on penality \n   -  Unbiasedness: The resulting estimator is nearly unbiased when the true unknown parameter is large (avoid unnecessary modeling bias).\n   -  Sparsity: thresholding rule sets small coefficients to 0 (avoid model complexity)\n   -  Continuity:  continuous in the data $\\hat{\\beta}_j = y_i^*$ (avoid instability in model prediction)\n\n## Conditions for Unbiasedness\n\nTo find the optimal estimator take derivative of  $\\frac 1 2 \\sum_j(\\beta_j - \\hat{\\beta}_j)^2 +  \\sum_j \\pen_\\lambda(|\\beta_j|)$ componentwise and set to zero\n\n- Derivative is \n\\begin{align*}\n\\frac{d}{d\\,\\beta_j} \\left\\{\\frac 1 2 (\\beta_j - \\hat{\\beta}_j)^2 +   \\pen_\\lambda(|\\beta_j|)\\right\\}\n& = (\\beta_j - \\hat{\\beta}_j) + \\sgn(\\beta_j)\\pen^\\prime_\\lambda(|\\beta_j|) \\\\\n& = \\sgn(\\beta_j)\\left\\{|\\beta_j| + \\pen^\\prime_\\lambda(|\\beta_j|) \\right\\} - \\hat{\\beta}_j\n\\end{align*}\n\n- setting derivative to zero gives $\\hat{\\beta}_j = \\sgn(\\beta_j)\\left\\{|\\beta_j| + \\pen^\\prime_\\lambda(|\\beta_j|) \\right\\}$\n- if $\\lim_{|\\beta_j| \\to \\infty} \\pen^\\prime_\\lambda(|\\beta|) = 0$\n   then \n  $\\hat{\\beta}_j = \\sgn(\\beta_j) |\\beta_j| = \\beta_j$\n- for large   $|\\beta_j|$, $|\\hat{\\beta}_j|$ is large with high probability \n- as MLE is unbiased, the optimal estimator is approximately unbiased for large $|\\beta_j|$ \n\n\n\n## Conditions for Thresholding & Continuity\n\n:::: {.columns}\n::: {.column width=\"60%\"}\nAs sufficient condition for a thresholding rule $\\bhat_j^\\lambda = 0$ is if \n$$0 < \\min \\left\\{ |\\beta_j| + \\pen^\\prime_\\lambda(|\\beta_j|)\\right\\}$$ \n\n- if $|\\hat{\\beta}_j| < \\min \\left\\{ |\\beta_j| + \\pen^\\prime_\\lambda(|\\beta_j|) \\right\\}$\nthen the derivative is positive for all positive $\\beta_j$ and negative for all negative $\\beta_j$ so $\\hat{\\beta}_j^\\lambda = 0$ is a local minimum\n\n\n- if $|\\hat{\\beta}_j| > \\min \\left\\{ |\\beta_j| + \\pen^\\prime_\\lambda(|\\beta_j|) \\right\\}$ multiple crossings (local roots)\n\n- a sufficient and necessary condition for continuity is that the  minimum of $|\\beta_j| + \\pen^\\prime_\\lambda(|\\beta_j|)$ is obtained at zero\n\n:::\n::: {.column width=\"40%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](12-oracle_files/figure-revealjs/threshold-condition-1.png){fig-align='center' width=4.5in height=5in}\n:::\n:::\n\n\n\n:::\n\n::::\n\n\n## Example: Gaussian Prior \n\n:::: {.columns}\n::: {.column width=\"60%\"}\n \n- Prior $\\N(0, 1/\\lambda^2)$\n- Penalty: $\\pen_\\lambda(|\\beta_j|) = \\frac{1}{2} \\lambda |\\beta_j|^2$\n- Unbiasedness: for large $|\\beta_j|$?\n  - Derivative of $\\pen_\\lambda(|\\beta_j|) = \\lambda \\beta_j = \\sgn(\\beta_j) \\lambda |\\beta_j|$\n  - does not go to zero as $|\\beta_j| \\to \\infty$ \n  - No!  (bias towards zero)\n\n- not a thresholding rule as \n$$\\min \\left\\{ |\\beta_j| + \\pen^\\prime_\\lambda(|\\beta_j|)\\right\\}  = (1 + \\lambda)|\\beta_j|$$\nis zero  \n\n- is continuous as minimum is at zero\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](12-oracle_files/figure-revealjs/ridge-derivative-1.png){fig-align='center' width=5in height=5in}\n:::\n:::\n\n\n\n:::\n::::\n\n## Example: Lasso Prior\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n- Penalty: $\\pen_\\lambda(|\\beta_j|) =  \\lambda |\\beta_j|$\n- Unbiasedness: for large $|\\beta_j|$?\n  - Derivative of $\\pen_\\lambda(|\\beta_j|) = \\lambda \\sgn(\\beta_j)$\n  - does not go to zero as $|\\beta_j| \\to \\infty$ \n  - No!  (bias towards zero)\n\n- Is a thresholding rule as \n$$\\min \\left\\{ |\\beta_j| + \\pen^\\prime_\\lambda(|\\beta_j|)\\right\\}  = (|\\beta_j| +  \\lambda) > 0 $$\n\n- is continuous as minimum is at $\\beta_j = 0$\n:::\n\n::: {.column width=\"40%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](12-oracle_files/figure-revealjs/lasso-condition-1.png){fig-align='center' width=3.5in height=7in}\n:::\n:::\n\n\n:::\n::::\n\n## Generalized Double Pareto Prior\n\nThe Generalized Double Pareto of Armagan, Dunson & Lee (2013)  \nhas a prior density for $\\beta_j$ of the form\n$$\np(\\beta_j \\mid \\xi, \\alpha) = \\frac{1}{2 \\xi} \\left(1 + \\frac{\\beta_j}{\\alpha \\xi}\\right)^{-(1 + \\alpha)}  \n$$\n\n- express as $\\beta_j \\mid \\xi, \\alpha \\sim \\textsf{GDP}(\\xi, \\alpha)$\n- Scale mixtures of Normals representation\n\\begin{align*}\n \\beta \\mid \\tau_j & \\sim \\N(0, \\tau_j) \\\\\n \\tau_j \\mid \\lambda_j & \\sim \\Exp(\\lambda_j^2/2) \\\\\n  \\lambda_j & \\sim \\Gam(\\alpha, \\eta) \\\\\n  \\beta_j & \\sim \\textsf{GDP}(\\xi = \\eta/\\alpha, \\alpha)\n\\end{align*}\n\n- is this a thresholding rule? unbiasedness? continuity?\n- for all parameters or are there restrictions?\n\n::: footer\n:::\n\n## Choice of Penalty/Prior and  Conditions\n\n- Ridge:  none\n- Lasso: does not satisfy conditions for unbiasedness\n- GDP: Can show that Generalized Double Pareto does for some choices of hyperparameters\n- Horseshoe: need marginal distribution of $\\beta_j$ for penalty \n   - marginal generally not available in closed form\n   - can show for a special case where there is an analytic expression for the marginal density ($\\lambda = \\phi = 1$)\n$$p(\\beta) = k \\exp(\\beta^2/2) E_1(\\beta^2/2)$$\n- where $E_n(x) = \\int_1^\\infty \\frac{e^{-xt}}{t^n} dt$ for $n = 1, 2, \\ldots$\n- $E_n^\\prime(x) = -E_{n-1}(x)$ for $n = 1, 2, \\ldots$\n\n\n\n## Shrinkage Estimators\n\nThe literature on shrinkage estimators (with or without selection) is extensive \n\n- Ridge\n- Lasso\n- Elastic Net (Zou & Hastie 2005)\n- SCAD (Fan & Li 2001)\n- Generalized Double Pareto Prior (Armagan, Dunson & Lee 2013)\n- Spike-and-Slab Lasso (Rockova & George 2018)\n\n. . . \n\nFor Bayes, choice of estimator\n\n- posterior mean (easy via MCMC)\n- posterior mode (optimization)\n- posterior median (via MCMC)\n\n## Selection and Uncertainty\n\n- Prior/Posterior do not put any probability on the event $\\beta_j = 0$\n\n- Uncertainty that the coefficient is zero?\n\n- Selection solved as a post-analysis decision problem \n- Selection part of model uncertainty \n  - add prior probability that $\\beta_j = 0$ \n  - combine with decision problem",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}