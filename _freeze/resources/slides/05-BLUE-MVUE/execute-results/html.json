{
  "hash": "a5655e1e33934983d2454cb969dc2717",
  "result": {
    "engine": "knitr",
    "markdown": "---\nsubtitle: \"STA 721: Lecture 5\"\ntitle: \"Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs\"\nauthor: \"Merlise Clyde (clyde@duke.edu)\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    smaller: true\n    logo: ../../img/icon.png\n    footer: <https://sta721-F24.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"   \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\nnumber-sections: false\nfilters:\n  - custom-numbered-blocks  \ncustom-numbered-blocks:\n  groups:\n    thmlike:\n      colors: [948bde, 584eab]\n      boxstyle: foldbox.simple\n      collapse: false\n      listin: [mathstuff]\n    todos: default  \n  classes:\n    Theorem:\n      group: thmlike\n      numbered: false\n    Lemma:\n      group: thmlike\n      numbered: false\n    Corollary:\n      group: thmlike\n      numbered: false\n    Proposition:\n      group: thmlike\n      numbered: false      \n    Proof:\n      group: thmlike\n      numbered: false\n      collapse: true  \n    Exercise:\n      group: thmlike\n      numbered: false\n    Definition:\n      group: thmlike\n      numbered: false\n      colors: [d999d3, a01793]\n    Feature: \n       numbered: false\n    TODO:\n      label: \"To do\"\n      colors: [e7b1b4, 8c3236]\n      group: todos\n      listin: [stilltodo]\n    DONE:\n      label: \"Done\"\n      colors: [cce7b1, 86b754]  \n      group: todos  \n---\n\n\n\n\n\n## Outline\n\n\\usepackage{xcolor}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator{\\sgn}{sgn}\n\\newcommand{\\e}{\\mathbf{e}}\n\\newcommand{\\Mb}{\\mathbf{M}}\n\\renewcommand{\\P}{\\mathbf{P}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\R}{\\textsf{R}}\n\\newcommand{\\mat}[1] {\\mathbf{#1}}\n\\newcommand{\\E}{\\textsf{E}}\n\\newcommand{\\SE}{\\textsf{SE}}\n\\newcommand{\\SSE}{\\textsf{SSE}}\n\\newcommand{\\RSS}{\\textsf{RSS}}\n\\newcommand{\\FSS}{\\textsf{FSS}}\n\\renewcommand{\\SS}{\\textsf{SS}}\n\\newcommand{\\MSE}{\\textsf{MSE}}\n\\newcommand{\\SSR}{\\textsf{SSR}}\n\\newcommand{\\Be}{\\textsf{Beta}}\n\\newcommand{\\St}{\\textsf{St}}\n\\newcommand{\\Ca}{\\textsf{C}}\n\\newcommand{\\Lv}{\\textsf{LÃ©vy}}\n\\newcommand{\\Exp}{\\textsf{Exp}}\n\\newcommand{\\GDP}{\\textsf{GDP}}\n\\newcommand{\\NcSt}{\\textsf{NcSt}}\n\\newcommand{\\Bin}{\\textsf{Bin}}\n\\newcommand{\\NB}{\\textsf{NegBin}}\n\\newcommand{\\Mult}{\\textsf{MultNom}}\n\\renewcommand{\\NG}{\\textsf{NG}}\n\\newcommand{\\N}{\\textsf{N}}\n\\newcommand{\\Ber}{\\textsf{Ber}}\n\\newcommand{\\Poi}{\\textsf{Poi}}\n\\newcommand{\\Gam}{\\textsf{Gamma}}\n\\newcommand{\\BB}{\\textsf{BB}}\n\\newcommand{\\BF}{\\textsf{BF}}\n\\newcommand{\\Gm}{\\textsf{G}}\n\\newcommand{\\Un}{\\textsf{Unif}}\n\\newcommand{\\Ex}{\\textsf{Exp}}\n\\newcommand{\\DE}{\\textsf{DE}}\n\\newcommand{\\tr}{\\textsf{tr}}\n\\newcommand{\\cF}{{\\cal{F}}}\n\\newcommand{\\cL}{{\\cal{L}}}\n\\newcommand{\\cI}{{\\cal{I}}}\n\\newcommand{\\cB}{{\\cal{B}}}\n\\newcommand{\\cP}{{\\cal{P}}}\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\pperp}{\\mathrel{{\\rlap{$\\,\\perp$}\\perp\\,\\,}}}\n\\newcommand{\\OFP}{(\\Omega,\\cF, \\P)}\n\\newcommand{\\eps}{\\boldsymbol{\\epsilon}}\n\\newcommand{\\Psib}{\\boldsymbol{\\Psi}}\n\\newcommand{\\1}{\\mathbf{1}_n}\n\\newcommand{\\gap}{\\vspace{8mm}}\n\\newcommand{\\ind}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}}\n\\newcommand{\\iid}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}}\n\\newcommand{\\simiid}{\\ensuremath{\\mathrel{\\mathop{\\sim}\\limits^{\\rm\niid}}}}\n\\newcommand{\\eqindis}{\\mathrel{\\mathop{=}\\limits^{\\rm D}}}\n\\newcommand{\\SSZ}{S_{zz}}\n\\newcommand{\\SZW}{S_{zw}}\n\\newcommand{\\Var}{\\textsf{Var}}\n\\newcommand{\\corr}{\\textsf{corr}}\n\\newcommand{\\cov}{\\textsf{cov}}\n\\newcommand{\\diag}{\\textsf{diag}}\n\\newcommand{\\var}{\\textsf{var}}\n\\newcommand{\\Cov}{\\textsf{Cov}}\n\\newcommand{\\Sam}{{\\cal S}}\n\\newcommand{\\VS}{{\\cal V}}\n\\def\\H{\\mathbf{H}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\tY}{\\tilde{\\mathbf{Y}}}\n\\newcommand{\\Yhat}{\\hat{\\mathbf{Y}}}\n\\newcommand{\\yhat}{\\hat{\\mathbf{y}}}\n\\newcommand{\\Yobs}{\\mathbf{Y}_{{\\cal S}}}\n\\newcommand{\\barYobs}{\\bar{Y}_{{\\cal S}}}\n\\newcommand{\\barYmiss}{\\bar{Y}_{{\\cal S}^c}}\n\\def\\bv{\\mathbf{b}}\n\\def\\X{\\mathbf{X}}\n\\def\\XtX{\\X^T\\X}\n\\def\\tX{\\tilde{\\mathbf{X}}}\n\\def\\x{\\mathbf{x}}\n\\def\\xbar{\\bar{\\mathbf{x}}}\n\\def\\Xbar{\\bar{\\mathbf{X}}}\n\\def\\Xg{\\mathbf{X}_{\\boldsymbol{\\gamma}}}\n\\def\\Ybar{\\bar{\\Y}}\n\\def\\ybar{\\bar{y}}\n\\def\\y{\\mathbf{y}}\n\\def\\Yf{\\mathbf{Y_f}}\n\\def\\W{\\mathbf{W}}\n\\def\\L{\\mathbf{L}}\n\\def\\m{\\mathbf{m}}\n\\def\\n{\\mathbf{n}}\n\\def\\w{\\mathbf{w}}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\z{\\mathbf{z}}\n\\def\\v{\\mathbf{v}}\n\\def\\u{\\mathbf{u}}\n\n\\def\\zero{\\mathbf{0}}\n\\def\\one{\\mathbf{1}}\n\\newcommand{\\taub}{\\boldsymbol{\\tau}}\n\\newcommand{\\betav}{\\boldsymbol{\\beta}}\n\\newcommand{\\alphav}{\\boldsymbol{\\alpha}}\n\\newcommand{\\bfomega}{\\boldsymbol{\\omega}}\n\\newcommand{\\bfchi}{\\boldsymbol{\\chi}}\n\\newcommand{\\bfLambda}{\\boldsymbol{\\Lambda}}\n\\newcommand{\\Lmea}{{\\cal L}} \n\\newcommand{\\scale}{\\bflambda} \n\\newcommand{\\Scale}{\\bfLambda} \n\\newcommand{\\bfscale}{\\bflambda} \n\\newcommand{\\mean}{\\bfchi}\n\\newcommand{\\loc}{\\bfchi}\n\\newcommand{\\bfmean}{\\bfchi}\n\\newcommand{\\bfx}{\\x}\n\\renewcommand{\\k}{g}\n\\newcommand{\\Gen}{{\\cal G}}\n\\newcommand{\\Levy}{L{\\'e}vy}\n\\def\\bfChi    {\\boldsymbol{\\Chi}}\n\\def\\bfOmega  {\\boldsymbol{\\Omega}}\n\\newcommand{\\Po}{\\mbox{\\sf P}}\n\\newcommand{\\A}{\\mathbf{A}}\n\\def\\a{\\mathbf{a}}\n\\def\\K{\\mathbf{K}}\n\\newcommand{\\B}{\\mathbf{B}}\n\\def\\b{\\boldsymbol{\\beta}}\n\\def\\bhat{\\hat{\\boldsymbol{\\beta}}}\n\\def\\btilde{\\tilde{\\boldsymbol{\\beta}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\bg{\\boldsymbol{\\beta_\\gamma}}\n\\def\\bgnot{\\boldsymbol{\\beta_{(-\\gamma)}}}\n\\def\\mub{\\boldsymbol{\\mu}}\n\\def\\tmub{\\tilde{\\boldsymbol{\\mu}}}\n\\def\\muhat{\\hat{\\boldsymbol{\\mu}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\tk{\\boldsymbol{\\theta}_k}\n\\def\\tj{\\boldsymbol{\\theta}_j}\n\\def\\Mk{\\boldsymbol{{\\cal M}}_k}\n\\def\\M{\\boldsymbol{{\\cal M}}}\n\\def\\Mj{\\boldsymbol{{\\cal M}}_j}\n\\def\\Mi{\\boldsymbol{{\\cal M}}_i}\n\\def\\Mg{{\\boldsymbol{{\\cal M}_\\gamma}}}\n\\def\\Mnull{\\boldsymbol{{\\cal M}}_{N}}\n\\def\\gMPM{\\boldsymbol{\\gamma}_{\\text{MPM}}}\n\\def\\gHPM{\\boldsymbol{\\gamma}_{\\text{HPM}}}\n\\def\\Mfull{\\boldsymbol{{\\cal M}}_{F}}\n\\def\\NS{\\boldsymbol{{\\cal N}}}\n\\def\\tg{\\boldsymbol{\\theta}_{\\boldsymbol{\\gamma}}}\n\\def\\g{\\boldsymbol{\\gamma}}\n\\def\\eg{\\boldsymbol{\\eta}_{\\boldsymbol{\\gamma}}}\n\\def\\G{\\mathbf{G}}\n\\def\\cM{\\cal M}\n\\def\\D{\\boldsymbol{\\Delta}}\n\\def\\Dbf{\\mathbf{D}}\n\\def \\shat{{\\hat{\\sigma}}^2}\n\\def\\uv{\\mathbf{u}}\n\\def\\l {\\lambda}\n\\def\\d{\\delta}\n\\def\\deltab{\\mathbf{\\delta}}\n\\def\\Sigmab{\\boldsymbol{\\Sigma}}\n\\def\\Phib{\\boldsymbol{\\Phi}}\n\\def\\Lambdab{\\boldsymbol{\\Lambda}}\n\\def\\lambdab{\\boldsymbol{\\lambda}}\n\\def\\Mg{{\\cal M}_\\gamma}\n\\def\\S{{\\cal{S}}}\n\\def\\Sbf{{\\mathbf{S}}}\n\\def\\qg{p_{\\boldsymbol{\\gamma}}}\n\\def\\pg{p_{\\boldsymbol{\\gamma}}}\n\\def\\T{\\boldsymbol{\\Theta}}\n\\def\\Tb{\\boldsymbol{\\Theta}}\n\\def\\t{\\mathbf{t}}\n\n\n\n- Gauss-Markov Theorem for non-full rank $\\X$ (recap)\n- Best Linear Unbiased Estimators for Prediction \n- MVUE\n- Discussion of recent papers on Best Unbiased Estimators beyond linearity\n\n\n\n. . .\n\nReadings: \n\n- Christensen Chapter 2 (Appendix B as needed)\n \n- Seber & Lee Chapter 3\n \n- For the curious: \n\n  - Andersen (1962) [Least squares and best unbiased estimates](https://www.jstor.org/stable/2237657) \n  \n  - Hansen (2022)  [A modern gauss-markov theorem](https://users.ssc.wisc.edu/~bhansen/papers/ecnmt_2022.pdf)\n\n  -  [What Estimators are Unbiased for Linear Models](https://arxiv.org/pdf/2212.14185) (2023) and references within\n\n\n## Identifiability\n::: {.Definition .unnumbered}\n## Identifiable\n$\\b$ and $\\sigma^2$ are identifiable if the distribution of $\\Y$, $f_\\Y(\\y;\n\\b_1, \\sigma^2_1) = f_\\Y(\\y;\n\\b_2, \\sigma^2_2)$ implies that $(\\b_1, \\sigma^2_1)^T =  (\\b_2, \\sigma^2_2)^T$\n:::\n\n- For linear models, equivalent definition is that $\\b$ is identifiable\nif for any $\\b_1$ and $\\b_2$, $\\mu(\\b_1)  = \\mu(\\b_2)$ or $\\X\\b_1 =\\X\\b_2$ implies that\n$\\b_1 = \\b_2$.  \n\n- If $r(\\X) = p$ then $\\b$ is identifiable\n \n- If $\\X$ is not full rank, there exists $\\b_1 \\neq \\b_2$, but $\\X\\b_1 =\n\\X\\b_2$ and hence $\\b$ is not identifiable!\n\n- identifiable linear functions of $\\b$, $\\Lambdab^T\\b$ that have an unbiased estimator are historically referred to as **estimable** in linear models.\n\n## BLUE of $\\Lambdab^T \\b$\n\nIf $\\Lambdab^T= \\B\\X$ for some matrix $\\B$ (or $\\Lambdab = \\X^T\\B$ then\n\n- $\\E[\\B\\P\\Y] = \\E[\\B\\X\\bhat] = \\E[\\Lambdab^T \\bhat] = \\Lambdab^T\\b$ \n- identifiable as it is a function of $\\mub$, linear and unbiased\n- The unique OLS estimate of $\\Lambdab^T\\b$ is $\\Lambdab^T\\bhat$\n- $\\B\\P\\Y = \\Lambdab^T\\bhat$ is the BLUE of $\\Lambdab^T\\b$\n\\begin{align*}\n & \\E[\\|\\B\\P\\Y - \\B\\mub\\|^2]  \\le \\E[\\|\\A\\Y - \\B\\mub\\|^2] \\\\\n \\Leftrightarrow & \\\\\n& \\E[\\|\\Lambdab^T\\bhat - \\Lambdab^T\\b)\\|^2]  \\le \\E[\\|\\L^T\\btilde - \\Lambdab^T\\b\\|^2]\n\\end{align*}\nfor LUE $\\A\\Y = \\L^T\\btilde$ of $\\Lambdab^T\\b$\n\n\n## Non-Identifiable Example \nOne-way ANOVA model \n$$\\mu_{ij} = \\mu + \\tau_j \\qquad \\mub = (\n    \\mu_{11}, \\ldots,\\mu_{n_1 1},\\mu_{12},\\ldots, \\mu_{n_2,2},\\ldots, \\mu_{1J},\n\\ldots,\n\\mu_{n_J J})^T $$ \n\n- Let $\\b_{1} = (\\mu, \\tau_1, \\ldots, \\tau_J)^T$ \n- Let $\\b_{2} = (\\mu - 42, \\tau_1 + 42, \\ldots, \\tau_J + 42)^T$ \n- Then $\\mub_{1} = \\mub_{2}$ even though  $\\b_1 \\neq \\b_2$ \n- $\\b$ is not identifiable \n- yet  $\\mub$ is identifiable, where $\\mub = \\X \\b$  (a linear\n  combination of $\\b$)\n\n## LUEs of Individual $\\beta_j$\n::: {.Proposition}\n## Christensen 2.1.6\nFor $\\mub = \\X \\b = \\sum_j \\X_j \\beta_j$\n$\\beta_j$ is **not identifiable**  if and only if there exists $\\alpha_j$\nsuch that $\\X_j = \\sum_{i \\neq j} \\X_i \\alpha_i$\n:::\n\n. . .\n\nOne-way Anova Model: $Y_{ij} = \\mu + \\tau_j + \\epsilon_{ij}$\n$$\\mub =  \\left[\n    \\begin{array}{lllll}\n\\one_{n_1} & \\one_{n_1} & \\zero_{n_1} &  \\ldots & \\zero_{n_1} \\\\\n\\one_{n_2} & \\zero_{n_2} & \\one_{n_2} &  \\ldots & \\zero_{n_2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\one_{n_J} & \\zero_{n_J} & \\zero_{n_J} &  \\ldots & \\one_{n_J} \\\\\n    \\end{array} \\right]\n \\left(   \\begin{array}{l}\n      \\mu \\\\\n      \\tau_1 \\\\\n   \\tau_2 \\\\\n \\vdots \\\\\n\\tau_J\n    \\end{array} \\right)\n$$\n\n- Are any parameters $\\mu$ or $\\tau_j$ identifiable?\n\n## Examples of $\\lambdab$ of Interest:\n- A $j$th element of $\\b$: $\\lambdab = (0, 0, \\ldots,1, 0, \\ldots, 0)^T$,  $$\\lambdab^T\\b = \\beta_j$$\n\n- Difference between two treatements: $\\tau_1 - \\tau_2$:  $\\lambdab = (0, 1, -1, \\ldots, 0, \\ldots, 0)^T$,  $$\\lambdab^T\\b = \\tau_1 - \\tau_2$$\n\n- Estimation at observed $\\x_i$: $\\lambdab = \\x_i$  $$\\mu_i = \\x_i^T \\b$$ \n- Estimation or prediction at a new point $\\x_*$:\n$\\lambdab = \\x_*$, $$\\mu_* = \\x_*^T \\b$$\n\n## Another Non-Full Rank Example\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx1 = -4:4\nx2 = c(-2, 1, -1, 2, 0, 2, -1, 1, -2)\nx3 = 3*x1  -2*x2\nx4 = x2 - x1 + 4\nY = 1+x1+x2+x3+x4 + c(-.5,.5,.5,-.5,0,.5,-.5,-.5,.5)\ndev.set = data.frame(Y, x1, x2, x3, x4)\n\n# Order 1\nlm1234 = lm(Y ~ x1 + x2 + x3 + x4, data=dev.set)\nround(coefficients(lm1234), 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)          x1          x2          x3          x4 \n          5           3           0          NA          NA \n```\n\n\n:::\n\n```{.r .cell-code}\n# Order 2\nlm3412 = lm(Y ~ x3 + x4 + x1 + x2, data = dev.set)\nround(coefficients(lm3412), 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)          x3          x4          x1          x2 \n        -19           3           6          NA          NA \n```\n\n\n:::\n:::\n\n\n\n## In Sample Predictions\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncbind(dev.set, predict(lm1234), predict(lm3412))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Y x1 x2  x3 x4 predict(lm1234) predict(lm3412)\n1 -7.5 -4 -2  -8  6              -7              -7\n2 -3.5 -3  1 -11  8              -4              -4\n3 -0.5 -2 -1  -4  5              -1              -1\n4  1.5 -1  2  -7  7               2               2\n5  5.0  0  0   0  4               5               5\n6  8.5  1  2  -1  5               8               8\n7 10.5  2 -1   8  1              11              11\n8 13.5  3  1   7  2              14              14\n9 17.5  4 -2  16 -2              17              17\n```\n\n\n:::\n:::\n\n\n\n\n- Both models agree for estimating the mean at the observed $\\X$ points!\n\n## Out of Sample\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nout = data.frame(test.set,\n      Y1234=predict(lm1234, new=test.set),\n      Y3412=predict(lm3412, new=test.set))\nout\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  x1 x2 x3 x4 Y1234 Y3412\n1  3  1  7  2    14    14\n2  6  2 14  4    23    47\n3  6  2 14  0    23    23\n4  0  0  0  4     5     5\n5  0  0  0  0     5   -19\n6  1  2  3  4     8    14\n```\n\n\n:::\n:::\n\n\n\n- Agreement for cases 1, 3, and 4 only!  \n\n- Can we determine that without finding the predictions and comparing?\n\n- Conditions for general $\\Lambdab$ or $\\lambdab$ without findingn $\\B$ ($\\b^T$)? \n\n## Conditions for LUE of $\\lambdab$\n\n- GM requires that $\\lambdab^T = \\bv^T\\X \\Leftrightarrow \\lambdab = \\X^T \\bv$   therefore $\\lambdab \\in C(\\X^T)$ \n- Suppose we have an arbitrary $\\lambdab = \\lambdab_* + \\u$, where $\\lambdab_*  \\in C(\\X^T)$ and $\\u \\in C(\\X^T)^\\perp$ (orthogonal complement)\n- Let  $\\P_{\\X^T}$ denote an orthogonal projection onto $C(\\X^T)$ then $\\I - \\P_{\\X^T}$ is an orthogonal  projection  onto $C(\\X^T)^\\perp$ \n\n- $(\\I - \\P_{\\X^T})\\lambdab = (\\I - \\P_{\\X^T})\\lambdab_* + (\\I - \\P_{\\X^T})\\u = \\zero_p + \\u$ \n- so if $\\lambdab \\in C(\\X^T)$ we will have $(\\I - \\P_{\\X^T})\\lambdab = \\zero_p$!  (or $\\P_{\\X^T} \\lambdab = \\lambdab$)\n\n- Note this is really just a generalization of Proposition 2.1.6 in Christensen  that $\\beta_j$ is **not** identifiable iff there exist scalars such that $\\X_j = \\sum_{i \\neq j} \\X_i \\alpha_i$\n\n---\n\n::: {.Exercise}\na) Is $\\P_{\\X^T} = (\\X^T\\X)(\\X^T\\X)^{-}$ a projection onto $C(\\X^T)$?\nb) is the expression for $\\P_{\\X^T}$ unique?\nc) Is $\\P_{\\X^T}$ an orthogonal projection in general?\nd) Is $\\P_{\\X^T}$ using the Moore-Penrose generalized inverse an orthogonal projection?\n:::\n\n## Prediction Example Again\n\nFor prediction at a new $\\x_*$, this is implemented in the `R` package `estimability`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrequire(\"estimability\" )\ncbind(epredict(lm1234, test.set), epredict(lm3412, test.set))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  [,1] [,2]\n1   14   14\n2   NA   NA\n3   23   23\n4    5    5\n5   NA   NA\n6   NA   NA\n```\n\n\n:::\n:::\n\n\n\nRows 2, 5, and 6  do not have a unique best linear unbiased estimator, $\\x_*^T \\b$\n\n\n\n## MVUE: Minimum Variance Unbiased Estimators\n- Gauss-Markov Theorem says that OLS has minimum variance in the\n    class of all Linear Unbiased estimators for $\\E[\\eps] = \\zero_n$ and $\\Cov[\\eps] = \\sigma^2 \\I_n$\n- Requires just first and second moments \n- Additional assumption of normality and full rank,  OLS of $\\b$ is the same as MLEs and have\n  minimum variance out of **ALL**  unbiased estimators (MVUE); not\n  just linear estimators  (section 2.5 in Christensen)\n- requires Complete Sufficient Statististics and Rao-Blackwell Theorem - next semester in STA732)\n\n- so Best Unbiased Estimators (BUE) not just BLUE!\n\n## What about ?\n\n- are there nonlinear estimators that are better than OLS under the assumptions ?\n\n- [Anderson (1962)](https://www.jstor.org/stable/2237657) showed OLS is not generally the MVUE with $\\E[\\eps] = \\zero_n$ and $\\Cov[\\eps] = \\sigma^2 \\I_n$ \n\n- pointed out that linear-plus-quadratic (LPQ) estimators can outperform the OLS estimator for certain error distributions.\n\n\n- Other assumptions on $\\Cov[\\eps] = \\Sigmab$?  \n    - Generalized Least Squares are BLUE (not necessarily equivalent to OLS)\n\n- more recently  [Hansen (2022)](https://users.ssc.wisc.edu/~bhansen/papers/ecnmt_2022.pdf) concludes that OLS is BUE over the broader class of  linear models with $\\Cov[\\eps]$  finite and $\\E[\\eps] = \\zero_n$\n\n- lively ongoing debate! - see [What Estimators are Unbiased for Linear Models](https://arxiv.org/pdf/2212.14185) (2023) and references within\n\n\n\n\n## Next Up\n\n- GLS under assumptions $\\E[\\eps] = \\zero_n$ and $\\Cov[\\eps] = \\Sigmab$\n\n- Oblique projections and orthogonality with other inner products on $\\bbR^n$\n\n- MLEs in Multivariate Normal setting \n\n- Gauss-Markov\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}