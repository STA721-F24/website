{
  "hash": "28c9a8c5db98f3da5d30a7adc013db47",
  "result": {
    "engine": "knitr",
    "markdown": "---\nsubtitle: \"STA 721: Lecture 8\"\ntitle: \"Bayesian Estimation in Linear Models\"\nauthor: \"Merlise Clyde (clyde@duke.edu)\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    smaller: true\n    logo: ../../img/icon.png\n    footer: <https://sta721-F24.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"   \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\nnumber-sections: false\nfilters:\n  - custom-numbered-blocks  \ncustom-numbered-blocks:\n  groups:\n    thmlike:\n      colors: [948bde, 584eab]\n      boxstyle: foldbox.simple\n      collapse: false\n      listin: [mathstuff]\n    todos: default  \n  classes:\n    Theorem:\n      group: thmlike\n      numbered: false\n    Lemma:\n      group: thmlike\n      numbered: false\n    Corollary:\n      group: thmlike\n      numbered: false\n    Proposition:\n      group: thmlike\n      numbered: false      \n    Proof:\n      group: thmlike\n      numbered: false\n      collapse: false   \n    Exercise:\n      group: thmlike\n      numbered: false\n    Definition:\n      group: thmlike\n      numbered: false\n      colors: [d999d3, a01793]\n    Feature: \n       numbered: false\n    TODO:\n      label: \"To do\"\n      colors: [e7b1b4, 8c3236]\n      group: todos\n      listin: [stilltodo]\n    DONE:\n      label: \"Done\"\n      colors: [cce7b1, 86b754]  \n      group: todos  \n---\n\n\n\n\n\n## Outline\n\n\\usepackage{xcolor}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator{\\sgn}{sgn}\n\\newcommand{\\e}{\\mathbf{e}}\n\\newcommand{\\Mb}{\\mathbf{M}}\n\\renewcommand{\\P}{\\mathbf{P}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\R}{\\textsf{R}}\n\\newcommand{\\mat}[1] {\\mathbf{#1}}\n\\newcommand{\\E}{\\textsf{E}}\n\\newcommand{\\SE}{\\textsf{SE}}\n\\newcommand{\\SSE}{\\textsf{SSE}}\n\\newcommand{\\RSS}{\\textsf{RSS}}\n\\newcommand{\\FSS}{\\textsf{FSS}}\n\\renewcommand{\\SS}{\\textsf{SS}}\n\\newcommand{\\MSE}{\\textsf{MSE}}\n\\newcommand{\\SSR}{\\textsf{SSR}}\n\\newcommand{\\Be}{\\textsf{Beta}}\n\\newcommand{\\St}{\\textsf{St}}\n\\newcommand{\\Ca}{\\textsf{C}}\n\\newcommand{\\Lv}{\\textsf{LÃ©vy}}\n\\newcommand{\\Exp}{\\textsf{Exp}}\n\\newcommand{\\GDP}{\\textsf{GDP}}\n\\newcommand{\\NcSt}{\\textsf{NcSt}}\n\\newcommand{\\Bin}{\\textsf{Bin}}\n\\newcommand{\\NB}{\\textsf{NegBin}}\n\\newcommand{\\Mult}{\\textsf{MultNom}}\n\\renewcommand{\\NG}{\\textsf{NG}}\n\\newcommand{\\N}{\\textsf{N}}\n\\newcommand{\\Ber}{\\textsf{Ber}}\n\\newcommand{\\Poi}{\\textsf{Poi}}\n\\newcommand{\\Gam}{\\textsf{Gamma}}\n\\newcommand{\\BB}{\\textsf{BB}}\n\\newcommand{\\BF}{\\textsf{BF}}\n\\newcommand{\\Gm}{\\textsf{G}}\n\\newcommand{\\Un}{\\textsf{Unif}}\n\\newcommand{\\Ex}{\\textsf{Exp}}\n\\newcommand{\\DE}{\\textsf{DE}}\n\\newcommand{\\tr}{\\textsf{tr}}\n\\newcommand{\\cF}{{\\cal{F}}}\n\\newcommand{\\cL}{{\\cal{L}}}\n\\newcommand{\\cI}{{\\cal{I}}}\n\\newcommand{\\cB}{{\\cal{B}}}\n\\newcommand{\\cP}{{\\cal{P}}}\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\pperp}{\\mathrel{{\\rlap{$\\,\\perp$}\\perp\\,\\,}}}\n\\newcommand{\\OFP}{(\\Omega,\\cF, \\P)}\n\\newcommand{\\eps}{\\boldsymbol{\\epsilon}}\n\\def\\ehat{\\hat{\\boldsymbol{\\epsilon}}}\n\\newcommand{\\Psib}{\\boldsymbol{\\Psi}}\n\\newcommand{\\Omegab}{\\boldsymbol{\\Omega}}\n\\newcommand{\\1}{\\mathbf{1}_n}\n\\newcommand{\\gap}{\\vspace{8mm}}\n\\newcommand{\\ind}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}}\n\\newcommand{\\iid}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}}\n\\newcommand{\\simiid}{\\ensuremath{\\mathrel{\\mathop{\\sim}\\limits^{\\rm\niid}}}}\n\\newcommand{\\eqindis}{\\mathrel{\\mathop{=}\\limits^{\\rm D}}}\n\\newcommand{\\SSZ}{S_{zz}}\n\\newcommand{\\SZW}{S_{zw}}\n\\newcommand{\\Var}{\\textsf{Var}}\n\\newcommand{\\corr}{\\textsf{corr}}\n\\newcommand{\\cov}{\\textsf{cov}}\n\\newcommand{\\diag}{\\textsf{diag}}\n\\newcommand{\\var}{\\textsf{var}}\n\\newcommand{\\Cov}{\\textsf{Cov}}\n\\newcommand{\\Sam}{{\\cal S}}\n\\newcommand{\\VS}{{\\cal V}}\n\\def\\H{\\mathbf{H}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\tY}{\\tilde{\\mathbf{Y}}}\n\\newcommand{\\Yhat}{\\hat{\\mathbf{Y}}}\n\\newcommand{\\yhat}{\\hat{\\mathbf{y}}}\n\\newcommand{\\Yobs}{\\mathbf{Y}_{{\\cal S}}}\n\\newcommand{\\barYobs}{\\bar{Y}_{{\\cal S}}}\n\\newcommand{\\barYmiss}{\\bar{Y}_{{\\cal S}^c}}\n\\def\\bv{\\mathbf{b}}\n\\def\\X{\\mathbf{X}}\n\\def\\XtX{\\X^T\\X}\n\\def\\tX{\\tilde{\\mathbf{X}}}\n\\def\\x{\\mathbf{x}}\n\\def\\xbar{\\bar{\\mathbf{x}}}\n\\def\\Xbar{\\bar{\\mathbf{X}}}\n\\def\\Xg{\\mathbf{X}_{\\boldsymbol{\\gamma}}}\n\\def\\Ybar{\\bar{\\Y}}\n\\def\\ybar{\\bar{y}}\n\\def\\y{\\mathbf{y}}\n\\def\\Yf{\\mathbf{Y_f}}\n\\def\\W{\\mathbf{W}}\n\\def\\L{\\mathbf{L}}\n\\def\\m{\\mathbf{m}}\n\\def\\n{\\mathbf{n}}\n\\def\\w{\\mathbf{w}}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\z{\\mathbf{z}}\n\\def\\v{\\mathbf{v}}\n\\def\\u{\\mathbf{u}}\n\n\\def\\zero{\\mathbf{0}}\n\\def\\one{\\mathbf{1}}\n\\newcommand{\\taub}{\\boldsymbol{\\tau}}\n\\newcommand{\\betav}{\\boldsymbol{\\beta}}\n\\newcommand{\\alphav}{\\boldsymbol{\\alpha}}\n\\newcommand{\\bfomega}{\\boldsymbol{\\omega}}\n\\newcommand{\\bfchi}{\\boldsymbol{\\chi}}\n\\newcommand{\\bfLambda}{\\boldsymbol{\\Lambda}}\n\\newcommand{\\Lmea}{{\\cal L}} \n\\newcommand{\\scale}{\\bflambda} \n\\newcommand{\\Scale}{\\bfLambda} \n\\newcommand{\\bfscale}{\\bflambda} \n\\newcommand{\\mean}{\\bfchi}\n\\newcommand{\\loc}{\\bfchi}\n\\newcommand{\\bfmean}{\\bfchi}\n\\newcommand{\\bfx}{\\x}\n\\renewcommand{\\k}{g}\n\\newcommand{\\Gen}{{\\cal G}}\n\\newcommand{\\Levy}{L{\\'e}vy}\n\\def\\bfChi    {\\boldsymbol{\\Chi}}\n\\def\\bfOmega  {\\boldsymbol{\\Omega}}\n\\newcommand{\\Po}{\\mbox{\\sf P}}\n\\newcommand{\\A}{\\mathbf{A}}\n\\def\\a{\\mathbf{a}}\n\\def\\K{\\mathbf{K}}\n\\newcommand{\\B}{\\mathbf{B}}\n\\def\\b{\\boldsymbol{\\beta}}\n\\def\\bhat{\\hat{\\boldsymbol{\\beta}}}\n\\def\\btilde{\\tilde{\\boldsymbol{\\beta}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\bg{\\boldsymbol{\\beta_\\gamma}}\n\\def\\bgnot{\\boldsymbol{\\beta_{(-\\gamma)}}}\n\\def\\mub{\\boldsymbol{\\mu}}\n\\def\\tmub{\\tilde{\\boldsymbol{\\mu}}}\n\\def\\muhat{\\hat{\\boldsymbol{\\mu}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\tk{\\boldsymbol{\\theta}_k}\n\\def\\tj{\\boldsymbol{\\theta}_j}\n\\def\\Mk{\\boldsymbol{{\\cal M}}_k}\n\\def\\M{\\boldsymbol{{\\cal M}}}\n\\def\\Mj{\\boldsymbol{{\\cal M}}_j}\n\\def\\Mi{\\boldsymbol{{\\cal M}}_i}\n\\def\\Mg{{\\boldsymbol{{\\cal M}_\\gamma}}}\n\\def\\Mnull{\\boldsymbol{{\\cal M}}_{N}}\n\\def\\gMPM{\\boldsymbol{\\gamma}_{\\text{MPM}}}\n\\def\\gHPM{\\boldsymbol{\\gamma}_{\\text{HPM}}}\n\\def\\Mfull{\\boldsymbol{{\\cal M}}_{F}}\n\\def\\NS{\\boldsymbol{{\\cal N}}}\n\\def\\tg{\\boldsymbol{\\theta}_{\\boldsymbol{\\gamma}}}\n\\def\\g{\\boldsymbol{\\gamma}}\n\\def\\eg{\\boldsymbol{\\eta}_{\\boldsymbol{\\gamma}}}\n\\def\\G{\\mathbf{G}}\n\\def\\cM{\\cal M}\n\\def\\D{\\boldsymbol{\\Delta}}\n\\def\\Dbf{\\mathbf{D}}\n\\def \\shat{{\\hat{\\sigma}}^2}\n\\def\\uv{\\mathbf{u}}\n\\def\\l {\\lambda}\n\\def\\d{\\delta}\n\\def\\deltab{\\mathbf{\\delta}}\n\\def\\Sigmab{\\boldsymbol{\\Sigma}}\n\\def\\Phib{\\boldsymbol{\\Phi}}\n\\def\\Lambdab{\\boldsymbol{\\Lambda}}\n\\def\\lambdab{\\boldsymbol{\\lambda}}\n\\def\\Mg{{\\cal M}_\\gamma}\n\\def\\S{{\\cal{S}}}\n\\def\\Sbf{{\\mathbf{S}}}\n\\def\\qg{p_{\\boldsymbol{\\gamma}}}\n\\def\\pg{p_{\\boldsymbol{\\gamma}}}\n\\def\\T{\\boldsymbol{\\Theta}}\n\\def\\Tb{\\boldsymbol{\\Theta}}\n\\def\\t{\\mathbf{t}}\n\n\n\n\n\n\n\n. . .\n\nReadings:\n\n  -   Christensen Chapter 2.9 and Chapter 15\n  -   Seber & Lee Chapter 3.12\n\n\n## Bayes Estimation\nModel $\\Y = \\X \\b + \\eps$  with $\\eps \\sim \\N(\\zero_n , \\sigma^2\n  \\I_n)$ \nis equivalent to\n$$\n\\Y \\sim \\N(\\X \\b, \\I_n/\\phi)\n$$\n\n- $\\phi = 1/\\sigma^2$ is the **precision** of the data.\n\n- we might expect $\\b$ to be close to some vector $\\bv_0$\n\n- represent this _a priori_ with a **Prior Distribution** for $\\b$, e.g.\n$$\\b \\sim \\N(\\bv_0, \\Phib_0^{-1})$$\n- $\\bv_0$ is the prior mean and $\\Phib_0$ is the **prior precision** of $\\b$ that captures how close $\\b$ is to $\\bv_0$\n\n- Similarly, we could represent prior uncertainty about $\\sigma$, $\\sigma^2$ or equivalently $\\phi$ with a probability distribution\n\n- for now treat $\\phi$ as fixed\n\n## Bayesian Inference\n\n- once we see data $\\Y$, Bayesian inference proceeds by updating prior beliefs\n\n- represented by the **posterior distribution** of $\\b$ which is the \nconditional distribution of $\\b$ given the data $\\Y$ (and $\\phi$ for now)\n\n- Posterior $p(\\b \\mid \\Y, \\phi)$\n$$p(\\b \\mid \\Y) = \\frac{p(\\Y \\mid \\b, \\phi) p(\\b \\mid \\phi)}{c}$$\n- $c$ is a constant so that the posterior density integrates to $1$\n$$c = \\int_{\\bbR^p} p(\\Y \\mid \\b, \\phi) p(\\b \\mid \\phi) d\\, \\b \\equiv p(\\Y)$$\n- since $c$ is a constant that doesn't depend on $\\b$ just ignore\n\n- work with density up to constant of proportionality\n\n## Posterior Density\nPosterior for $\\b$ is \n$p(\\b \\mid \\Y) \\propto p(\\Y \\mid \\b, \\phi) p(\\b \\mid \\phi)$\n\n- Likelihood for $\\b$ is proportional to $p(\\Y \\mid \\b, \\phi$)\n\n. . .\n\n\\begin{align*} p(\\Y \\mid \\b, \\phi) & = (2 \\pi)^{-n/2} |\\I_n / \\phi |^{-1/2} \n\\exp\\left\\{-\\frac{1}{2} \\left( (\\Y - \\X\\b)^T \\phi \\I_n (\\Y - \\X\\b) \\right) \\right\\} \\\\\n& \\propto \\exp\\left\\{-\\frac{1}{2} \\left( \\phi \\Y^T\\Y - 2 \\b^T \\phi \\X^T\\Y + \\phi \\b \\XtX \\b \\right) \\right\\} \n\\end{align*}\n\n- similarly expand prior\n\\begin{align*}\np(\\b \\mid \\phi) & = (2 \\pi)^{-p/2} |\\Phib_0^{-1}|^{-1/2} \n\\exp\\left\\{-\\frac{1}{2} \\left( (\\b - \\bv_0)^T \\Phib_0 (\\b - \\bv_0) \\right) \\right\\} \\\\\n & \\propto \\exp\\left\\{-\\frac{1}{2} \\left(  \\bv_0^T \\Phib_0\\bv_0 - 2 \\b^T\\Phib_0 \\bv_0 + \\b \\Phib_0 \\b \\right) \\right\\} \n\\end{align*}\n\n## Posterior Steps\n\n- Expand quadratics and regroup terms \n\\begin{align*}\np(\\b \\mid \\Y, \\phi) \n & \\propto e^{\\left\\{-\\frac{1}{2} \\left( \\phi \\b \\XtX \\b + \\b \\Phib_0 \\b   - 2(\\phi \\b^T\\X^T\\Y + \\b^T\\Phib_0 \\bv_0)  + \\phi \\Y^T\\Y + \\bv_0^T \\Phib_0\\bv_0 \\right) \\right\\} } \\\\\n &  \\propto \\exp\\left\\{-\\frac{1}{2} \\left( \\b ( \\phi\\XtX  + \\Phib_0) \\b   - 2 \\b^T(\\phi \\X^T\\Y + \\Phib_0 \\bv_0)  \\right) \\right\\}  \n\\end{align*}\n\n. . .\n\nKernel of a Multivariate Normal\n\n\n- Read off posterior precision from Quadratic in $\\b$ \n- Read off posterior precision $\\times$ posterior mean from Linear term in $\\b$  \n- will need to complete the quadratic in the posterior mean$^{\\dagger}$\n\n::: footer\n$\\dagger$ necessary to keep track of all terms for $\\phi$ when we do not condition on $\\phi$\n:::\n\n## Posterior Precision and Covariance\n$$ p(\\b \\mid \\Y, \\phi)  \\propto \\exp\\left\\{-\\frac{1}{2} \\left( \\b ( \\phi\\XtX  + \\Phib_0) \\b   - 2 \\b^T(\\phi \\X^T\\Y + \\Phib_0 \\bv_0)  \\right) \\right\\}\n$$\n\n- Posterior Precision \n$$\\Phib_n \\equiv \\phi\\XtX  + \\Phib_0$$\n\n- sum of data precision and prior precision\n- posterior Covariance \n$$\\Cov[\\b \\mid \\Y, \\phi] = \\Phib_n^{-1} = (\\phi\\XtX  + \\Phib_0)^{-1}$$\n- if $\\Phib_0$ is full rank, then $\\Cov[\\b \\mid \\Y, \\phi]$ is full rank even if $\\XtX$ is not\n\n\n## Posterior Mean Updating \n\\begin{align*} p(\\b \\mid \\Y, \\phi) & \\propto \\exp\\left\\{\\frac{1}{2} \\left( \\b ( \\phi\\XtX  + \\Phib_0) \\b   - 2 \\b^T(\\phi \\X^T\\Y + \\Phib_0 \\bv_0)  \\right) \\right\\} \\\\\n & \\propto \\exp\\left\\{\\frac{1}{2} \\left( \\b ( \\phi\\XtX  + \\Phib_0) \\b   - 2 \\b^T\\Phib_n \\Phib_n^{-1}(\\phi \\X^T\\Y + \\Phib_0 \\bv_0)  \\right) \\right\\} \\\\\n\\end{align*}\n\n- posterior mean $\\bv_n$\n\\begin{align*}\n\\bv_n & \\equiv \\Phib_n^{-1} (\\phi \\X^T\\Y + \\Phib_0 \\bv_0 ) \\\\\n      & = (\\phi\\XtX  + \\Phib_0)^{-1}\\left(\\phi (\\XtX) (\\XtX)^{-1} \\X^T\\Y + \\Phib_0 \\bv_0 \\right) \\\\\n      & = (\\phi\\XtX  + \\Phib_0)^{-1} \\left( \\phi (\\XtX) \\bhat + \\Phib_0 \\bv_0 \\right)\n\\end{align*}\n\n- a precision weighted linear combination of MLE and prior mean\n\n- first expression useful if $\\X$ is not full rank!\n\n## Notes \n\nPosterior is a Multivariate Normal\n$p(\\b \\mid \\Y, \\phi) \\sim \\N(\\bv_n, \\Phib_n^{-1})$\n\n- posterior mean: $\\bv_n  =  \\Phib_n^{-1} (\\phi \\X^T\\Y + \\Phib_0 \\bv_0 )$\n\n- posterior precision: $\\Phib_n = \\phi\\XtX  + \\Phib_0$\n\n-  the posterior precision (inverse posterior variance) is the sum of the prior precision and the data precision.\n- the posterior mean is a linear combination of MLE/OLS and prior mean\n- if the prior precision $\\Phib_n$ is very large compared to the data precision $\\phi \\XtX$, the posterior mean will be close to the prior mean $\\bv_0$.\n-  if the prior precision $\\Phib_n$ is very small compared to the data precision  $\\phi \\XtX$, the posterior mean will be close to the MLE/OLS estimator.\n- data precision will generally be increasing with sample size\n\n## Bayes Estimators\n\nA Bayes estimator is a potential value of $\\b$ that is obtained from the posterior distribution in some principled way. \n\n- Standard estimators include\n  - the posterior mean estimator, which is the minimizer of the Bayes risk under squared error loss\n  - the maximum a posteriori (MAP) estimator, the value $\\b$ that maximizes the posterior density (or log posterior density)\n  \n- The first estimator is based on principles from classical decision theory, whereas the second can be related to penalized likelihood estimation. \n\n- in the case of linear regression they turn out to be the same estimator!\n\n## Bayes Estimator under Squared Error Loss\n\n- the Frequentist Risk $R(\\beta, \\delta) \\equiv \\E_{\\Y \\mid \\b}[\\| \\delta(\\Y)â \\b\\|^2]$ is the expected loss of decision $\\delta$ for a given $\\b$\n\n. . .\n\n::: {.Definition}\n## Bayes Rule and Bayes Risk\nThe Bayes rule under squared error loss is the function of $\\Y$, $\\delta^*(\\Y)$, that minimizes the **Bayes risk** $B(p_\\b, \\delta)$ \n$$\\delta^*(\\Y) =  \\arg \\min_{\\delta \\in \\cal{D}} B(p_\\b, \\delta)$$\n\n$$B(p_\\b, \\delta) = \\E_\\b R(\\b, \\delta) = \\E_{\\b} \\E_{\\Y \\mid \\b}[\\| \\delta(\\Y)â \\b\\|^2]$$\nwhere the expectation is with respect to the prior distribution, $p_\\b$, over $\\b$  and the conditional distribution of $\\Y$ given $\\b$\n:::\n\n\n## Bayes Estimators \n::: {.Definition}\n## Bayes Action\nThe Bayes Action is the action $a \\in {\\cal{A}}$ that minimizes the posterior expected loss:\n$$ \\delta_B^*(\\Y) = \\arg \\min_{\\delta \\in \\cal{D}} E_{\\b \\mid \\Y} [\\| \\delta â \\b\\|^2] \n$$\n::: \n\n- can show that the Bayes action that minimizes the posterior expected loss is the posterior mean $\\b_n = (\\phi \\XtX + \\Phib_0)^{-1}(\\phi \\X^T\\Y + \\Phib_0 \\bv_0$ and is also the Bayes rule.\n\n- different values of $\\bv_0$ and $\\Phib_0$ will lead to different Bayes estimators as will different prior distributions besides the Normal\n\n- take $\\bv_0 = \\zero$; Bayes estimators are often referred to as shrinkage estimators\n$$\\b_n = \\left( \\X^T\\X + \\Phib_0/\\phi  \\right)^{-1}  \\X^T\\Y$$\nas they shrink the MLE/OLS estimator towards $\\zero$\n\n::: footer\n:::\n\n## Prior Choice\n\nOne of the most common priors for the normal linear model is the **g-prior** of Zellner (1986) where $\\Phib_0 = \\frac{\\phi}{g} \\XtX$\n$$\\b \\mid \\phi, g \\sim \\N(\\zero, g/\\phi (\\XtX)^{-1})$$\n\n. . .\n\n\\begin{align*}\n\\bv_n & = \\left( \\X^T\\X + \\frac{\\phi}{g} \\frac{\\XtX}{\\phi} \\right)^{-1} \\X^T\\Y \\\\\n  & = \\left( \\X^T\\X + \\frac{1}{g} \\XtX \\right)^{-1} \\X^T\\Y \\\\\n  & = \\left( \\frac{1 +g}{g} \\XtX \\right)^{-1} \\X^T\\Y \\\\\n  & = \\frac{g}{1+g} \\bhat\n\\end{align*}\n\n- $g$ controls the amount of shrinkage where all of the MLEs are shrunk to zero by the same fraction $g/(1+g)$\n\n::: footer\n:::\n\n## Another Common Choice\n\n- another common choice is the independent prior\n$$\\b \\mid \\phi \\sim \\N(\\zero, \\Phib_0^{-1})$$\nwhere $\\Phib_0 = \\phi \\kappa \\I_b$ for some $\\kappa> 0$ \n\n- the posterior mean is \n\\begin{align*}\n\\b_n & = (\\X^T\\X + \\kappa \\I)^{-1} \\X^T\\Y \\\\\n     & =  (\\X^T\\X + \\kappa \\I)^{-1} \\XtX \\bhat\n\\end{align*}     \n\n- this is also a shrinkage estimator but the amount of shrinkage is different for the different components of $\\bv_n$ depending on the eigenvalues of $\\XtX$\n\n- easiest to see this via an orthogonal rotation of the model\n\n## Rotated Regression\n- Use the singular value decomposition of $\\X = \\U \\Lambdab\\V^T$ and multiply thru by $\\U^T$ to get the rotated model\n\\begin{align*}\n\\U^T \\Y & =  \\Lambdab \\V^T\\b + \\U^T\\eps \\\\\n\\tY & = \\Lambdab \\alphav + \\tilde{\\eps} \n\\end{align*}\nwhere $\\alphav = \\V^T\\b$ and $\\tilde{\\eps} = \\U^T\\eps$\n- the induced prior is still $\\alphav \\mid \\phi \\sim \\N(\\zero, (\\phi \\kappa)^{-1} \\I)$\n- the posterior mean of $\\alphav$ is\n\\begin{align*}\n\\a & =  (\\Lambdab^2 + \\kappa \\I)^{-1} \\Lambdab^2 \\hat{\\alphav}\\\\\na_j & = \\frac{\\lambda_j^2}{\\lambda_j^2 + \\kappa} \\hat{\\alpha}_j\n\\end{align*}\n\n- sets to zero the components of the OLS solution where eigenvalues are zero!\n\n::: footer\n:::\n\n## Connections to Frequentist Estimators\n- The posterior mean under this independent prior is the same as the classic **ridge regression** estimator of Hoerl and \n\n\n- the variance of $\\hat{\\alpha}_j$ is $\\sigma^2/\\lambda_j^2$ while the variance of $a_j$ is $\\sigma^2/(\\lambda_j^2 + \\kappa)$\n\n- clearly components of $\\alphav$ with small eigenvalues will have large variances\n\n- ridge regression keeps those components from \"blowing up\" by shrinking them towards zero and having a finite variance\n\n- rotate back to get the ridge estimator for $\\b$, $\\bhat_R = \\V \\a$\n\n- ridge regression applies a high degree of shrinkage to the âpartsâ (linear combinations) of $\\b$ that have high variability, and a low degree of shrinkage to the parts that are well-estimated.\n\n- turns out there always exists a value of $\\kappa$ that will improve over OLS!  \n- Unfortunately no closed form solution except in orthogonal regression and then it depends on the unknown $\\|\\b\\|^2$!\n\n## Next Class\n\n\n- Frequentist risk of Bayes estimators\n- Bayes and penalized loss functions\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}