{
  "hash": "4ffdd174e49e6e29c901fda05b3c0122",
  "result": {
    "engine": "knitr",
    "markdown": "---\nsubtitle: \"STA 721: Lecture 8\"\ntitle: \"Bayesian Estimation in Linear Models\"\nauthor: \"Merlise Clyde (clyde@duke.edu)\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    smaller: true\n    logo: ../../img/icon.png\n    footer: <https://sta721-F24.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"   \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\nnumber-sections: false\nfilters:\n  - custom-numbered-blocks  \ncustom-numbered-blocks:\n  groups:\n    thmlike:\n      colors: [948bde, 584eab]\n      boxstyle: foldbox.simple\n      collapse: false\n      listin: [mathstuff]\n    todos: default  \n  classes:\n    Theorem:\n      group: thmlike\n      numbered: false\n    Lemma:\n      group: thmlike\n      numbered: false\n    Corollary:\n      group: thmlike\n      numbered: false\n    Proposition:\n      group: thmlike\n      numbered: false      \n    Proof:\n      group: thmlike\n      numbered: false\n      collapse: false   \n    Exercise:\n      group: thmlike\n      numbered: false\n    Definition:\n      group: thmlike\n      numbered: false\n      colors: [d999d3, a01793]\n    Feature: \n       numbered: false\n    TODO:\n      label: \"To do\"\n      colors: [e7b1b4, 8c3236]\n      group: todos\n      listin: [stilltodo]\n    DONE:\n      label: \"Done\"\n      colors: [cce7b1, 86b754]  \n      group: todos  \n---\n\n\n\n\n\n## Outline\n\n\\usepackage{xcolor}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator{\\sgn}{sgn}\n\\newcommand{\\e}{\\mathbf{e}}\n\\newcommand{\\Mb}{\\mathbf{M}}\n\\renewcommand{\\P}{\\mathbf{P}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\R}{\\textsf{R}}\n\\newcommand{\\mat}[1] {\\mathbf{#1}}\n\\newcommand{\\E}{\\textsf{E}}\n\\newcommand{\\SE}{\\textsf{SE}}\n\\newcommand{\\SSE}{\\textsf{SSE}}\n\\newcommand{\\RSS}{\\textsf{RSS}}\n\\newcommand{\\FSS}{\\textsf{FSS}}\n\\renewcommand{\\SS}{\\textsf{SS}}\n\\newcommand{\\MSE}{\\textsf{MSE}}\n\\newcommand{\\SSR}{\\textsf{SSR}}\n\\newcommand{\\Be}{\\textsf{Beta}}\n\\newcommand{\\St}{\\textsf{St}}\n\\newcommand{\\Ca}{\\textsf{C}}\n\\newcommand{\\Lv}{\\textsf{LÃ©vy}}\n\\newcommand{\\Exp}{\\textsf{Exp}}\n\\newcommand{\\GDP}{\\textsf{GDP}}\n\\newcommand{\\NcSt}{\\textsf{NcSt}}\n\\newcommand{\\Bin}{\\textsf{Bin}}\n\\newcommand{\\NB}{\\textsf{NegBin}}\n\\newcommand{\\Mult}{\\textsf{MultNom}}\n\\renewcommand{\\NG}{\\textsf{NG}}\n\\newcommand{\\N}{\\textsf{N}}\n\\newcommand{\\Ber}{\\textsf{Ber}}\n\\newcommand{\\Poi}{\\textsf{Poi}}\n\\newcommand{\\Gam}{\\textsf{Gamma}}\n\\newcommand{\\BB}{\\textsf{BB}}\n\\newcommand{\\BF}{\\textsf{BF}}\n\\newcommand{\\Gm}{\\textsf{G}}\n\\newcommand{\\Un}{\\textsf{Unif}}\n\\newcommand{\\Ex}{\\textsf{Exp}}\n\\newcommand{\\DE}{\\textsf{DE}}\n\\newcommand{\\tr}{\\textsf{tr}}\n\\newcommand{\\cF}{{\\cal{F}}}\n\\newcommand{\\cL}{{\\cal{L}}}\n\\newcommand{\\cI}{{\\cal{I}}}\n\\newcommand{\\cB}{{\\cal{B}}}\n\\newcommand{\\cP}{{\\cal{P}}}\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\pperp}{\\mathrel{{\\rlap{$\\,\\perp$}\\perp\\,\\,}}}\n\\newcommand{\\OFP}{(\\Omega,\\cF, \\P)}\n\\newcommand{\\eps}{\\boldsymbol{\\epsilon}}\n\\def\\ehat{\\hat{\\boldsymbol{\\epsilon}}}\n\\newcommand{\\Psib}{\\boldsymbol{\\Psi}}\n\\newcommand{\\Omegab}{\\boldsymbol{\\Omega}}\n\\newcommand{\\1}{\\mathbf{1}_n}\n\\newcommand{\\gap}{\\vspace{8mm}}\n\\newcommand{\\ind}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}}\n\\newcommand{\\iid}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}}\n\\newcommand{\\simiid}{\\ensuremath{\\mathrel{\\mathop{\\sim}\\limits^{\\rm\niid}}}}\n\\newcommand{\\eqindis}{\\mathrel{\\mathop{=}\\limits^{\\rm D}}}\n\\newcommand{\\SSZ}{S_{zz}}\n\\newcommand{\\SZW}{S_{zw}}\n\\newcommand{\\Var}{\\textsf{Var}}\n\\newcommand{\\corr}{\\textsf{corr}}\n\\newcommand{\\cov}{\\textsf{cov}}\n\\newcommand{\\diag}{\\textsf{diag}}\n\\newcommand{\\var}{\\textsf{var}}\n\\newcommand{\\Cov}{\\textsf{Cov}}\n\\newcommand{\\Sam}{{\\cal S}}\n\\newcommand{\\VS}{{\\cal V}}\n\\def\\H{\\mathbf{H}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\tY}{\\tilde{\\mathbf{Y}}}\n\\newcommand{\\Yhat}{\\hat{\\mathbf{Y}}}\n\\newcommand{\\yhat}{\\hat{\\mathbf{y}}}\n\\newcommand{\\Yobs}{\\mathbf{Y}_{{\\cal S}}}\n\\newcommand{\\barYobs}{\\bar{Y}_{{\\cal S}}}\n\\newcommand{\\barYmiss}{\\bar{Y}_{{\\cal S}^c}}\n\\def\\bv{\\mathbf{b}}\n\\def\\X{\\mathbf{X}}\n\\def\\XtX{\\X^T\\X}\n\\def\\tX{\\tilde{\\mathbf{X}}}\n\\def\\x{\\mathbf{x}}\n\\def\\xbar{\\bar{\\mathbf{x}}}\n\\def\\Xbar{\\bar{\\mathbf{X}}}\n\\def\\Xg{\\mathbf{X}_{\\boldsymbol{\\gamma}}}\n\\def\\Ybar{\\bar{\\Y}}\n\\def\\ybar{\\bar{y}}\n\\def\\y{\\mathbf{y}}\n\\def\\Yf{\\mathbf{Y_f}}\n\\def\\W{\\mathbf{W}}\n\\def\\L{\\mathbf{L}}\n\\def\\m{\\mathbf{m}}\n\\def\\n{\\mathbf{n}}\n\\def\\w{\\mathbf{w}}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\z{\\mathbf{z}}\n\\def\\v{\\mathbf{v}}\n\\def\\u{\\mathbf{u}}\n\n\\def\\zero{\\mathbf{0}}\n\\def\\one{\\mathbf{1}}\n\\newcommand{\\taub}{\\boldsymbol{\\tau}}\n\\newcommand{\\betav}{\\boldsymbol{\\beta}}\n\\newcommand{\\alphav}{\\boldsymbol{\\alpha}}\n\\newcommand{\\bfomega}{\\boldsymbol{\\omega}}\n\\newcommand{\\bfchi}{\\boldsymbol{\\chi}}\n\\newcommand{\\bfLambda}{\\boldsymbol{\\Lambda}}\n\\newcommand{\\Lmea}{{\\cal L}} \n\\newcommand{\\scale}{\\bflambda} \n\\newcommand{\\Scale}{\\bfLambda} \n\\newcommand{\\bfscale}{\\bflambda} \n\\newcommand{\\mean}{\\bfchi}\n\\newcommand{\\loc}{\\bfchi}\n\\newcommand{\\bfmean}{\\bfchi}\n\\newcommand{\\bfx}{\\x}\n\\renewcommand{\\k}{g}\n\\newcommand{\\Gen}{{\\cal G}}\n\\newcommand{\\Levy}{L{\\'e}vy}\n\\def\\bfChi    {\\boldsymbol{\\Chi}}\n\\def\\bfOmega  {\\boldsymbol{\\Omega}}\n\\newcommand{\\Po}{\\mbox{\\sf P}}\n\\newcommand{\\A}{\\mathbf{A}}\n\\def\\a{\\mathbf{a}}\n\\def\\K{\\mathbf{K}}\n\\newcommand{\\B}{\\mathbf{B}}\n\\def\\b{\\boldsymbol{\\beta}}\n\\def\\bhat{\\hat{\\boldsymbol{\\beta}}}\n\\def\\btilde{\\tilde{\\boldsymbol{\\beta}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\bg{\\boldsymbol{\\beta_\\gamma}}\n\\def\\bgnot{\\boldsymbol{\\beta_{(-\\gamma)}}}\n\\def\\mub{\\boldsymbol{\\mu}}\n\\def\\tmub{\\tilde{\\boldsymbol{\\mu}}}\n\\def\\muhat{\\hat{\\boldsymbol{\\mu}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\tk{\\boldsymbol{\\theta}_k}\n\\def\\tj{\\boldsymbol{\\theta}_j}\n\\def\\Mk{\\boldsymbol{{\\cal M}}_k}\n\\def\\M{\\boldsymbol{{\\cal M}}}\n\\def\\Mj{\\boldsymbol{{\\cal M}}_j}\n\\def\\Mi{\\boldsymbol{{\\cal M}}_i}\n\\def\\Mg{{\\boldsymbol{{\\cal M}_\\gamma}}}\n\\def\\Mnull{\\boldsymbol{{\\cal M}}_{N}}\n\\def\\gMPM{\\boldsymbol{\\gamma}_{\\text{MPM}}}\n\\def\\gHPM{\\boldsymbol{\\gamma}_{\\text{HPM}}}\n\\def\\Mfull{\\boldsymbol{{\\cal M}}_{F}}\n\\def\\NS{\\boldsymbol{{\\cal N}}}\n\\def\\tg{\\boldsymbol{\\theta}_{\\boldsymbol{\\gamma}}}\n\\def\\g{\\boldsymbol{\\gamma}}\n\\def\\eg{\\boldsymbol{\\eta}_{\\boldsymbol{\\gamma}}}\n\\def\\G{\\mathbf{G}}\n\\def\\cM{\\cal M}\n\\def\\D{\\boldsymbol{\\Delta}}\n\\def\\Dbf{\\mathbf{D}}\n\\def \\shat{{\\hat{\\sigma}}^2}\n\\def\\uv{\\mathbf{u}}\n\\def\\l {\\lambda}\n\\def\\d{\\delta}\n\\def\\deltab{\\mathbf{\\delta}}\n\\def\\Sigmab{\\boldsymbol{\\Sigma}}\n\\def\\Phib{\\boldsymbol{\\Phi}}\n\\def\\Lambdab{\\boldsymbol{\\Lambda}}\n\\def\\lambdab{\\boldsymbol{\\lambda}}\n\\def\\Mg{{\\cal M}_\\gamma}\n\\def\\S{{\\cal{S}}}\n\\def\\Sbf{{\\mathbf{S}}}\n\\def\\qg{p_{\\boldsymbol{\\gamma}}}\n\\def\\pg{p_{\\boldsymbol{\\gamma}}}\n\\def\\T{\\boldsymbol{\\Theta}}\n\\def\\Tb{\\boldsymbol{\\Theta}}\n\\def\\t{\\mathbf{t}}\n\n\n\n- Frequentist Risk of Bayes estimators\n- Bayes and Penalized Loss Functions\n- Generalized Ridge Regression\n- Hierarchical Bayes and Other Penalties\n\n\n. . .\n\nReadings:\n\n  -   Christensen Chapter 2.9 and Chapter 15\n  -   Seber & Lee Chapter 10.7.3 and Chapter 12\n\n\n## Frequentist Risk of Bayes Estimators\n\nQuadratic loss for estimating  $\\b$ using estimator $\\a$\n$$ L(\\b, \\a) =  ( \\b - \\a)^T(\\b -\\a)$$ \n\n- Consider our expected loss (before we see the data) of taking an\n``action'' $\\a$ (i.e. reporting $\\a$ as the estimate of $\\b$)\n$$ \\E_{\\Y \\mid \\b}[L(\\b, \\a)] =  \\E_{\\Y \\mid \\b}[( \\b - \\a)^T(\\b -\\a)]$$\nwhere the expectation is over the data $\\Y$ given the true value of $\\b$.\n\n## Expectation of Quadratic Forms\n\n::: {.Theorem}\n## Christensen Thm 1.3.2\nIf $\\W$ is a random variable with mean $\\mub$ and covariance matrix $\\Sigmab$ then \n$$\\E[\\W^T\\A\\W] = \\tr(\\A\\Sigmab) + \\mub^T\\A\\mub$$\n:::\n\n. . .\n\n::: {.Proof}\n\\begin{eqnarray*}\n(\\W - \\mub )^T\\A(\\W - \\mub) & = & \\W^T\\A\\W - 2\\mub^T\\A\\W + \\mub^T\\A\\mub \\\\\n\\E[(\\W - \\mub )^T\\A(\\W - \\mub)] & = & \\E[\\W^T\\A\\W ]  - 2 \\mub^T\\A\\E[\\W] + \\mub^T\\A\\mub \n\\end{eqnarray*}\n\nRearranging we have \n$$\\E[\\W^T\\A\\W] =  \\E[(\\W - \\mub )^T\\A(\\W - \\mub)] + \\mub^T\\A\\mub $$\n:::\n\n---\n\n::: {.Proof}\n## continued\nRecall\n\\begin{align*}\n\\E[(\\W - \\mub )^T\\A(\\W - \\mub)] & = \\E[\\tr((\\W - \\mub)\\A(\\W - \\mub)^T)] \\\\\n& = \\tr(\\E[\\A(\\W - \\mub)(\\W - \\mub)^T]) \\\\\n& = \\tr(\\A\\E[(\\W - \\mub)(\\W - \\mub)^T]) \\\\\n& = \\tr(\\A\\Sigmab)\n\\end{align*}\nTherefore the expectation is \n$$\\E[\\W^T\\A\\W] = \\tr(\\A\\Sigmab) + \\mub^T\\A\\mub$$\n:::\n\n- Use Theorem to Explore Frequentist Risk of using a Bayesian estimator \n$$\\E_\\Y[( \\b - \\a)^T(\\b -\\a)$$\ncompared to the OLS estimator $\\bhat$.\n\n## Steps to Evaluate Frequentist Risk\n\n-   MSE:  $\\E_\\Y[( \\b - \\a)^T(\\b -\\a) = \\tr(\\Sigmab_\\a) + (\\b - \\E_{\\Y \\mid \\b}[\\a])^T(\\b - \\E_{\\Y \\mid \\b}[\\a])$\n- Bias of $\\a$: $\\E_{\\Y \\mid \\b}[\\a - \\b] = \\E_{\\Y \\mid \\b}[\\a] - \\b$\n- Covariance of $\\a$: $\\Cov_{\\Y \\mid \\b}[\\a - \\E[\\a]$\n- Multivariate analog of MSE = Bias$^2$ + Variance in the univariate case \n\n\n##   Mean Square Error of OLS Estimator\n-   MSE of OLS $\\E_\\Y[( \\b - \\bhat)^T(\\b -\\bhat)$\n- OLS is unbiased os mean of  $\\b - \\bhat$ is $\\zero_p$ \n- covariance is $\\Cov[\\b - \\bhat] = \\sigma^2 (\\X^T\\X)^{-1}$\n\\begin{eqnarray*}\n\\MSE(\\b) \\equiv \\E_\\Y[( \\b - \\bhat)^T(\\b -\\bhat) & = &\\sigma^2\n  \\tr[(\\X^T\\X)^{-1}]  \\\\\n  & = & \\sigma^2 \\tr \\U \\Lambda^{-1} \\U^T \\\\\n  & = & \\sigma^2 \\sum_{j=1}^p \\lambda_j^{-1}\n  \\end{eqnarray*}\nwhere $\\lambda_j$ are eigenvalues of $\\X^T\\X$.\n\n- If smallest $\\lambda_j \\to 0$ then MSE  $\\to \\infty$\n\n\n\n\n##   Mean Square Error using the $g$-prior\n\n- posterior mean is $\\bhat_g = \\frac{g}{1+g} \\bhat$ (minimizes Bayes risk under squared error loss)\n- bias of  $\\bhat_g$: \n\\begin{align*} \n\\E_{\\Y \\mid \\b}[\\b - \\bhat_g] & = \\b\\left(1 - \\frac{g}{1+g}\\right)  = \\frac{1}{1+g} \\b\n\\end{align*}\n\n- covariance of $\\bhat_g$: $\\Cov(\\bhat_g) = \\frac{g^2}{(1+g)^2} \\sigma^2 (\\X^T\\X)^{-1}$\n\n- MSE of $\\bhat_g$: \n\\begin{align*}\n\\MSE(\\b) = \\frac{g^2}{(1+g)^2} \\sigma^2 \\tr(\\X^T\\X)^{-1} + \\frac{1}{(1+g)^2} \\|\\b\\|^2 \\\\\n= \\frac{1}{(1+g)^2} \\left( g^2 \\sigma^2 \\sum_{j=1}^p\\lambda_j^{-1} + \\|\\b\\|^2 \\right)\n\\end{align*}\n\n## Can Bayes Estimators have smaller MSE than OLS?\n\n-  MSE of OLS is $\\E_\\Y[( \\b - \\bhat)^T(\\b -\\bhat) = \\sigma^2\n  \\tr[(\\X^T\\X)^{-1}]$ (OLS has minimum MSE under squared error loss out of all _unbiased_ estimators)\n\n- MSE of $g$-prior estimator is  \n$$\\MSE_g(\\b) = \\frac{1}{(1+g)^2} \\left( g^2 \\sigma^2 \\tr[(\\X^T\\X)^{-1}] + \\|\\b\\|^2 \\right)$$\n- for fixed $\\b$, what values of $g$ is the MSE of $\\bhat_g$ lower than that of $\\bhat$?\n\n- for fixed $g$, what values of $\\b$ is the MSE of $\\bhat_g$ lower than that of $\\bhat$?\n\n- is there a value of $g$  that minimizes the MSE of $\\bhat_g$?\n\n-  what is the MSE of $\\bhat_g$ under the \"optimal\" $g$?\n- is the MSE of $\\bhat_g$ using the \"optimal\" $g$ always lower than that of $\\bhat$?\n\n##   Mean Square Error under Ridge Priors\n\n- MSE  with OLS and $g$-prior estimators depend on the eigenvalues of $\\X^T\\X$ and can be infinite if the smallest eigenvalue is zero.\n\n- Ridge regression estimator $\\bhat_\\kappa = (\\X^T\\X +  \\kappa \\I_p)^{-1} \\X^T\\Y$ has finite MSE for all $\\kappa > 0$.  ($k = 0$ is OLS)\n \n - MSE of Ridge estimator \n $\\E_{\\Y \\mid \\b}[( \\b - \\bhat_\\kappa)^T(\\b -\\bhat_\\kappa) = \\E[(\\alphav - \\a)^T(\\alphav - \\a)]$\n \n - bias of $a_j = \\frac{\\lambda_j}{\\lambda_j + \\kappa} \\hat{\\alpha}_j$ is $\\frac{\\kappa}{\\lambda_j + \\kappa} {\\alpha}_j$\n \n - variance $a_j = \\sigma^2 \\frac{\\lambda_j^2}{(\\lambda_j + \\kappa)^2}$\n$$\\MSE_R = \\sigma^2 \\sum_{j=1}^p \\frac{\\lambda_j^2}{(\\lambda_j + \\kappa)^2} + \\sum_{j=1}^p \\frac{\\kappa^2}{(\\lambda_j + \\kappa)^2} \\alpha_j^2$$\n\n- can show that the deriviate of the $\\MSE_R$ with respect to $\\kappa$ is negative at $k = 0$ so that there exists a $\\kappa$ so the MSE of the Ridge estimator is always less than that of OLS.\n\n## Penalized Regression\n\n- Ridge regression is a special case of penalized regression where the penalty is $\\kappa \\|\\b\\|^2$ for some $\\kappa > 0$.  (let $\\kappa^* = \\kappa/\\phi$)\n\n- posterior mode maximizes the posterior density or log posterior density\n\\begin{align*}\n\\bhat_R = \\arg \\max_{\\b} \\cal{L}(\\b) & = \\log p(\\b \\mid \\Y) \\propto \\log p(\\Y \\mid \\b) + \\log p(\\b) \\\\\n & = -\\frac{\\phi}{2} \\|\\Y - \\X\\b\\|^2 - \\frac{\\kappa}{2} \\|\\b\\|^2 \\\\\n & = -\\frac{\\phi}{2} \\left( \\|\\Y - \\X\\b\\|^2 + \\kappa^* \\|\\b\\|^2 \\right)\n\\end{align*}\n\n\n- maximizing the posterior mode is equivalent to minimizing the penalized loss function \n\\begin{align*} \n\\bhat_R & =  \\arg \\max_{\\b} -\\left(\\|\\Y - \\X\\b\\|^2 + \\kappa^* \\|\\b\\|^2 \\right) \\\\\n& = \\arg \\min_{\\b} \\left(\\|\\Y - \\X\\b\\|^2 + \\kappa^* \\|\\b\\|^2 \\right) \n\\end{align*}\n\n::: footer\n:::\n\n## Scaling and Centering\nNote:  usually use Ridge regression after centering and scaling the columns of $\\X$ so that the penalty is the same for all variables.\n$\\Y_c = (\\I - \\P_1) \\Y$  and $X_c$ the centered and standardized  $\\X$ matrix\n\n- alternatively as a prior, we are assuming that the $\\b_j$ are iid $\\N(0, \\kappa^*)$ so that the prior for $\\b$ is $\\N(\\zero_p, \\kappa^* \\I_p)$\n\n- if the units/scales of the variables are different, then the variance or penality should be different for each variable.\n\n- standardizing the $\\X$ so that $\\X_c^T\\X_c$ is a constant times the correlation matrix of $\\X$  ensures that all $\\b$'s have the same scale\n\n- centering the data forces the intercept to be 0 (so no shrinkage or penality) \n\n## Alternative Motivation\n\n- If  $\\bhat$ is unconstrained  expect high variance with nearly\n    singular $\\X_c$ \\pause\n - Control how large coefficients may grow \n    $$\\arg \\min_{\\b} (\\Y_c - \\X_c \\b)^T (\\Y_c - \\X_c\\b)$$\n    subject to\n    $$ \\sum \\beta_j^2 \\le t$$ \n- Equivalent Quadratic Programming Problem\n    $$\\bhat_{R} = \\arg \\min_{\\b} \\| \\Y_c - \\X_c \\b\\|^2 + \\kappa^* \\|\\b\\|^2$$ \n\n- different approaches to selecting $\\kappa^*$ from frequentist ane     Bayesian perspectives\n\n## Plot of Constrained Problem\n\n![](ridge-penalized-loss.png) \n\n\n## Generalized Ridge Regression\n\n- rather than a common penalty for all variables, consider a different penalty for each variable\n\n- as a prior, we are assuming that the $\\b_j$ are iid $\\N(0, \\frac{\\kappa_j}{\\phi})$ so that the prior for $\\b$ is $\\N(\\zero_p, \\phi^{-1} \\Kb)$\n\n- hard enough to choose a single penalty, how to choose $p$ penalties?\n\n- place independent priors on each of the $\\kappa_j$'s\n- a hierarchical Bayes model\n\n- if we can integrate out the $\\kappa_j$'s we have a new prior for $\\beta_j$\n\n- this leads to a new penalty!\n\n- examples include the Lasso (Double Exponential Prior) and Double Pareto Priors\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}