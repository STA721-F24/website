---
title: "Linear Mixed Effects Models"
author: "Merlise Clyde"
subtitle: "STA721: Lecture 24"
institute: "Duke University"
format: 
  revealjs:
    theme: [simple, custom.scss]
    slide-number: true
    incremental: true
    scrollable: false
    controls: true
    fragments: true
    preview-links: auto
    smaller: true
    logo: ../../img/icon.png
    footer: <https://sta721-F24.github.io/website/>
    chalkboard: 
      boardmarker-width: 1
      chalk-width: 2
      chalk-effect: 0
    embed-resources: false
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"    
editor: 
  markdown: 
    wrap: 72
execute: 
  echo: false
number-sections: false
filters:
  - custom-numbered-blocks  
custom-numbered-blocks:
  groups:
    thmlike:
      colors: [948bde, 584eab]
      boxstyle: foldbox.simple
      collapse: false
      listin: [mathstuff]
    todos: default  
  classes:
    Theorem:
      group: thmlike
    Corollary:
      group: thmlike
    Conjecture:
      group: thmlike
      collapse: true  
    Definition:
      group: thmlike
      colors: [d999d3, a01793]
    Feature: default
    TODO:
      label: "To do"
      colors: [e7b1b4, 8c3236]
      group: todos
      listin: [stilltodo]
    DONE:
      label: "Done"
      colors: [cce7b1, 86b754]  
      group: todos  
---


```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warm = 1,
  width=72
  )
# Set dpi and height for images
library(knitr)
knitr::opts_chunk$set(fig.height = 2.65, dpi = 300,fig.align='center',fig.show='hold',size='footnotesize', small.mar=TRUE) 
# For nonsese...
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
```

## Random Effects Regression
{{< include macros.qmd >}}

- Easy to extend from random means by groups to random group level coefficients:
$$\begin{align*}Y_{ij} & = \tb^T_j \x_{ij}+ \epsilon_{ij} \\
\epsilon_{ij}   & \iid  \N(0, \sigma^2) 
\end{align*}
$$
- $\tb_j$ is a $p \times 1$ vector regression coefficients for group $j$
- $\x_{ij}$ is a $p \times 1$ vector of predictors for group $j$

- If we view the groups as exchangeable, describe across group heterogeneity by
$$\tb_j \iid \N(\b, \Sigmab)$$
- $\b$, $\Sigmab$ and $\sigma^2$ are population parameters to be estimated.

- Designed to accommodate correlated data due to nested/hierarchical structure/repeated measurements: 
students w/in schools; patients w/in hospitals; additional covariates

 
## Linear Mixed Effects Models

- We can write $\tb = \b + \alphav_j$ with $\alphav_j \iid \N(\zero, \Sigmab)$

- Substituting, we can rewrite model
$$\begin{align*}Y_{ij} & = \b^T \x_{ij}+ \alphav_j^T \x_{ij} + \epsilon_{ij}, \qquad
\epsilon_{ij}  \overset{iid}{\sim}  \N(0, \sigma^2) \\
\alphav_j & \overset{iid}{\sim} \N(\zero_p, \Sigmab)
\end{align*}$$


- Fixed effects contribution $\b$ is constant across groups 

 

- Random effects are $\alphav_j$ as they vary across groups  

- called **mixed effects** as we have both fixed and random effects in the regression model

## More General Model
- No reason for the fixed effects and random effect covariates to be the same
$$\begin{align*}Y_{ij} & = \b^T \x_{ij}+ \alphav_j^T \z_{ij} + \epsilon_{ij}, \qquad
\epsilon_{ij}  \iid  \N(0, \sigma^2) \\
\alphav_j & {\iid} \N(\zero_q, \Sigmab)
\end{align*}$$

- dimension of $\x_{ij}$ $p \times 1$

- dimension of $\z_{ij}$ $q \times 1$

- may or may not be overlapping

- $\x_{ij}$ could include predictors that are constant across all $i$ in group $j$. (can't estimate if they are in $\z_{ij}$)


- features of school $j$ that vary across schools but are constant within a school
 

## Marginal Distribution of Data

- Express observed data as vectors for each group $j$:  $(\Y_j, \X_j, \Z_j)$ where  $\Y_j$ is $n_j \times 1$, $\X_j$ is $n_j \times d$ and $\Z_j$ is $n_j \times q$;
 

- Group Specific Model (1):
$$\begin{align}\Y_j  & = \X_j \b + \Z_j \alphav_j + \eps_j, \qquad
\eps_j  \sim \N(\zero_{n_j}, \sigma^2 \I_{n_j})\\
\alphav_j & \iid \N(\zero_p, \Sigmab)
\end{align}$$

- Population Mean $\E[\Y_j] = \E[\X_j \b + \Z_j \alphav_j + \eps_j] = \X_j \b$

- Covariance $\Cov[\Y_j] = \Var[\X_j \b + \Z_j \alphav_j + \eps_j] = \Z_j \Sigmab \Z_j^T + \sigma^2 \I_{n_j}$
$$\Y_j \mid  \b, \Sigmab, \sigma^2 \ind \N(\X_j \b, \Z_j \Sigmab \Z_j^T + \sigma^2 \I_{n_j})$$
- write out as a big regression model stacking $\Y$
$\X$ and $\Z$ and $\eps = (\eps_1, \ldots, \eps_J)$
$$\Y = \X\b + \Z \alphav + \eps$$



## GLS Estimation 


Marginal Model
$$\Y \mid  \b, \Sigmab, \sigma^2 \ind \N(\X \b, \Z \Sigmab \Z^T + \sigma^2 \I_{n})$$

- Define covariance of $\Y$  to be $\V = \Z \Sigmab \Z^T + \sigma^2 \I_{n}$

- Use GLS conditional on $\Sigmab, \sigma^2$ to estimate $\b$:
$$\b = (\X^T \V^{-1} \X)^{-1} \X^T \V^{-1} \Y$$
- since $\V$ has unknown parameters, typical practice (non-Bayes) is to use an estimate of $\V$, and replace $\V$ by $\hat{\V}$. (MLE, Methods of Moments, REML)

- frequentist random effects models arose from analysis of variance models so generally
some simplification in $\Sigmab$!

## One Way Anova Random Effects Model

- Consider Balance data so that $n_1 = n_2 = \cdots = n_J = r$ and $n = rJ$
- design matrix $\X = \one_n$ 
- covariance for random effects is $\Sigmab = \sigma^2_{\alpha} \I_J$
- matrix $\Z$ is $n \times J$ 
$$\Z = \begin{pmatrix} \one_r & \zero & \cdots & \zero \\
\zero & \one_r & \cdots & \zero \\
\vdots & \vdots & \ddots & \vdots \\
\zero & \zero & \cdots & \one_r
\end{pmatrix}$$

- Covariance 
$$\V =  \sigma^2 \I_{n} + \Z \Sigmab \Z^T = \sigma^2 \I_{n} + \sigma^2_{\alpha} \Z \Z^T = \sigma^2 \I_{n} + \sigma^2_{\alpha} r\P_\Z$$

## MLEs for One-Way Random Effects Model

- Model $\Y = \one_n \beta + \eps$ with $\eps \sim \N(\zero, \V)$
- Since $C(\V\X) \subset C(\X)$, the GLS of $\b$ is the same as the OLS of $\beta$ in this case
$$\hat{\beta} = \ybar_{..} = \sum_{j=1}^J \sum_{i=1}^r y_{ij}/n$$

- We need the determinant and inverse of $\V$ to get the MLEs for $\sigma^2$ and $\sigma^2_{\alpha}$
- note that $\V$ is block diagonal with blocks 
$\sigma^2 \I_r + \sigma^2_{\alpha} r \P_{\one_r}$ (use eigenvalues based on svd of $\P_{\one_r}$ and $\I_r$)
- determinant of $\V$ is the product of determinants of blocks 
$\sigma{^{2}}^n (1 + r \sigma^2_{\alpha}/\sigma^2)^J$


- find inverse of $\V$ via Woodbury identity (or svd of projections/eigenvalues)
$$\V^{-1} = \frac{1}{\sigma^2} \left( \I_n - \frac{r \sigma^2_{\alpha}}
{\sigma^2 + r \sigma^2_{\alpha}}\P_{\Z} \right)$$

## Log likelihood 

- plug in $\hat{\beta}$
\begin{align*}
\log L(\sigma^2, \sigma^2_{\alpha}) & = - \frac{1}{2}\log{|V|} - \frac{1}{2}
(\Y - \one_n \ybar)^T\V^{-1} (\Y - \one_n\ybar)\\
& = - \frac{1}{2}\log{|V|} - \frac{1}{2}
\Y^T(\I - \P_{\one_n})\V^{-1} (\I - \P_{\one_n})\Y \\
& = - \frac{J(r - 1)}{2}\log{\sigma^2} - \frac{J}{2} \log(\sigma^2 + r \sigma^2_{\alpha}) \\
& \ - \frac{1}{2 \sigma^2} \left( \Y^T(\I - \P_{\one_n})( \I_n - \frac{r \sigma^2_{\alpha}}
{\sigma^2 + r \sigma^2_{\alpha}}\P_{\Z})(\I - \P_{\one_n})\Y \right) \\
& = - \frac{J(r - 1)}{2}\log{\sigma^2} - \frac{J}{2} \log(\sigma^2 + r \sigma^2_{\alpha}) \\
& \ -  \frac{1}{2\sigma^2} \left( \Y^T(\I - \P_{\one_n})\left(\frac{ \sigma^2\I_n + r \sigma^2_{\alpha} (\I_n - \P_{\Z})}
{\sigma^2 + r \sigma^2_{\alpha}}\right)(\I - \P_{\one_n})\Y \right)
\end{align*}

## MLEs

- Simplify using Properties of Projections; ie $(\I_n - \P_{\one_n})(\I_n - \P_{\Z})$ to rewrite in terms of familiar $\SSE = \Y^T(\I - \P_{\Z})\Y$ and $\SST = \Y^T(\P_{\Z} - \P_{\one_n})\Y$ based on the fixed effects one-way anova model

- take derivatives and solve for MLEs (some alegebra involved!)


- MLE of $\sigma^2$ is $\hat{\sigma}^2 = \MSE = \SSE/(n - J)$

- MLE of $\sigma^2_{\alpha}$ is 
$$\hat{\sigma}^2_{\alpha} = \frac{\frac{\SST}{J} - \MSE}{n}$$
but this is true only if $\MSE < \SST/J$ otherwise the mode is on the boundary and  $\hat{\sigma}^2_{\alpha} = 0$ 

## Comments

For the One-Way model (and HW) we can find MLEs in closed form - but several approaches to simplify the algebra

- steps outlined here (via the stacked approach - more general)

- treating the response as a matrix and using the matrix normal distribution with the 
  mean function and covariance via Kronecker transformations (lab)
  
  - extends to other balanced ANOVA models
  
- simplify the problem based on summary statistics - i.e.  the distributions in terms of $\SSE$. (Gamma) and the sample means  (Normal) and integrate out random effects  (Approach in Box & Hill for Bayesian solution)

  - easiest imho for the one-way model

. . .

For more general problems we may need iterative methods to find MLEs (alternating between conditional MLE of $\b$ and MLE of $\Sigmab$)  (Gauss-Siedel optimization)

## Best Linear Prediction

Given a linear model with $\E[Y^*] = \X \b$ with or without correlation structure, we can *predict* a new observation $Y^*$ at $\x$ as $\x^T \bhat$ where $\bhat$ is the OLS or GLS of $\b$.

- but if $Y^*$ and $\Y$ are correlated we can do better!

. . .

::: {.Theorem  .unnumbered}
## Christensen 6.3.4; Sec 12.2
Let $\Y$ and $Y^*$ be random variables with the following moments
\begin{align*}
\E[\Y]  =  \X \b & \quad & \E[\Y^*]  = \x^T\b \\
\Var[\Y]  =  \V  & \quad & \Cov[\Y, Y^*]  = \psi 
\end{align*}

Then the best linear predictor of $Y^*$ given $\Y$ is 
$$\E[Y^* \mid \Y] = \x^T \bhat + \delta(\Y - \X \bhat)$$ 
where $\delta = \V^{-1}\psi$



:::

## Best Linear Unbiased Prediction

To go from BLPs to BLUPs we need to estimate the unknown parameters in the model $\b$

- replacing $\b$ by $\bhat$ in the BLP gives the Best Linear Unbiased Predictor (BLUP) of $Y^*$ given $\Y$ (see Christensen 12.2 for details and proof)

- if the $y_i$ are uncorrelated, the BLUP is the same as the BLUE, $\x^T \bhat$

- what about other linear combinations?  $\lambdab^T\alphav$?
- $\E[\lambdab^T\alphav] = 0$ so we can let $Y^* = \lambdab^T\alphav$ with $\x = 0$
\begin{align*} 
\psi & = \Cov[\Y, Y^*] = \Cov(\Z\alphav + \eps, \lambdab^T\alphav) \\
     & = \Cov(\Z\alphav, \lambdab^T\alphav) = \Z\Sigmab\lambdab
\end{align*}

- the BLUP of $\lambdab^T\alphav$ given $\Y$ is 
$$\delta_*^T(\Y - \X\bhat)$$
where $\delta_* = \V^{-1}\psi = \V^{-1}\Z\Sigmab\lambdab$ 

::: {.footer}
:::

## Mixed Model Equations via Bayes Rule

The *mixed model equations* are the normal equations for the mixed effects model and provide both BLUEs and BLUPs

- Consider the model
\begin{align*}
\Y & \sim \N(\W\thetab, \sigma^2 \I_n) \\
\W & = [\X, \Z] \\
\thetab & = [\b^T, \alphav^T] 
\end{align*}

- estimate $\thetab$ using Bayes with the prior $\thetab \sim \N(\zero, \Omega)$
where 
$\Omega = \begin{pmatrix} \I_p  /\kappa & \zero \\ \zero & \Sigmab \end{pmatrix}$
- posterior mean of $\thetab$
$$\hat{\thetab} =  \begin{pmatrix} \bhat \\ \hat{\alphav} \end{pmatrix} =
\begin{pmatrix} \XtX/\sigma^2 + \kappa \I_p & \X^T\Z/\sigma^2  \\
\Z^T\X/\sigma^2   &   \Z^T\Z + \Sigmab^{-1} \end{pmatrix}^{-1} 
\begin{pmatrix} \X^T\Y/\sigma^2 \\ \Z^T\Y/\sigma^2 \end{pmatrix}$$

## BLUEs and BLUPs via Bayes Rule

- take the limiting prior with $\kappa \rightarrow 0$ and $\Sigmab \rightarrow \zero$ to get the mixed model equations

-  The BLUE of $\b$ and BLUP of $\alphav$ satisfy the limiting form of the posterior mean of $\thetab$
$$\hat{\thetab} =  \begin{pmatrix} \bhat \\ \hat{\alphav} \end{pmatrix} =
\begin{pmatrix} \XtX/\sigma^2  & \X^T\Z/\sigma^2  \\
\Z^T\X/\sigma^2   &   \Z^T\Z + \Sigmab^{-1} \end{pmatrix}^{-1} 
\begin{pmatrix} \X^T\Y/\sigma^2 \\ \Z^T\Y/\sigma^2 \end{pmatrix}$$

- see Christensen Sec 12.3 for details

- the mixed model equations have computational advantages over the usual GLS espression for $\b$ as it avoids inverting $\V$ $n \times n$ and instead we are inverting $p + q$ matrix!

- related to spatial kriging  and Gaussian Process Regression


## Other Questions   

- How do you decide what is a random effect or fixed effect?

- Design structure is often important


- What if the means are not normal?  Extensions to Generalized Linear Models

- what if random effects are not normal? (Mixtures of Normals,  Bayes...)

- more examples in Case Studies next semester! 

- for more in depth treatment take STA 610 


