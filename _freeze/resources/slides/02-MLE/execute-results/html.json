{
  "hash": "8d0e788b70acccbe416e7056310213cb",
  "result": {
    "engine": "knitr",
    "markdown": "---\nsubtitle: \"STA 721: Lecture 2\"\ntitle: \"Maximum Likelihood Estimation\"\nauthor: \"Merlise Clyde (clyde@duke.edu)\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    smaller: true\n    logo: ../../img/icon.png\n    footer: <https://sta721-F24.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"   \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\nnumber-sections: false\nfilters:\n  - custom-numbered-blocks  \ncustom-numbered-blocks:\n  groups:\n    thmlike:\n      colors: [948bde, 584eab]\n      boxstyle: foldbox.simple\n      collapse: false\n      listin: [mathstuff]\n    todos: default  \n  classes:\n    Theorem:\n      group: thmlike\n    Corollary:\n      group: thmlike\n    Conjecture:\n      group: thmlike\n      collapse: true  \n    Definition:\n      group: thmlike\n      colors: [d999d3, a01793]\n    Feature: \n       numbered: false\n    TODO:\n      label: \"To do\"\n      colors: [e7b1b4, 8c3236]\n      group: todos\n      listin: [stilltodo]\n    DONE:\n      label: \"Done\"\n      colors: [cce7b1, 86b754]  \n      group: todos  \n---\n\n\n\n\n\n## Outline\n\n\\usepackage{xcolor}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator{\\sgn}{sgn}\n\\newcommand{\\e}{\\mathbf{e}}\n\\newcommand{\\Mb}{\\mathbf{M}}\n\\renewcommand{\\P}{\\mathbf{P}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\R}{\\textsf{R}}\n\\newcommand{\\mat}[1] {\\mathbf{#1}}\n\\newcommand{\\E}{\\textsf{E}}\n\\newcommand{\\SE}{\\textsf{SE}}\n\\newcommand{\\SSE}{\\textsf{SSE}}\n\\newcommand{\\RSS}{\\textsf{RSS}}\n\\newcommand{\\FSS}{\\textsf{FSS}}\n\\renewcommand{\\SS}{\\textsf{SS}}\n\\newcommand{\\MSE}{\\textsf{MSE}}\n\\newcommand{\\SSR}{\\textsf{SSR}}\n\\newcommand{\\Be}{\\textsf{Beta}}\n\\newcommand{\\St}{\\textsf{St}}\n\\newcommand{\\Ca}{\\textsf{C}}\n\\newcommand{\\Lv}{\\textsf{LÃ©vy}}\n\\newcommand{\\Exp}{\\textsf{Exp}}\n\\newcommand{\\GDP}{\\textsf{GDP}}\n\\newcommand{\\NcSt}{\\textsf{NcSt}}\n\\newcommand{\\Bin}{\\textsf{Bin}}\n\\newcommand{\\NB}{\\textsf{NegBin}}\n\\newcommand{\\Mult}{\\textsf{MultNom}}\n\\renewcommand{\\NG}{\\textsf{NG}}\n\\newcommand{\\N}{\\textsf{N}}\n\\newcommand{\\Ber}{\\textsf{Ber}}\n\\newcommand{\\Poi}{\\textsf{Poi}}\n\\newcommand{\\Gam}{\\textsf{Gamma}}\n\\newcommand{\\BB}{\\textsf{BB}}\n\\newcommand{\\BF}{\\textsf{BF}}\n\\newcommand{\\Gm}{\\textsf{G}}\n\\newcommand{\\Un}{\\textsf{Unif}}\n\\newcommand{\\Ex}{\\textsf{Exp}}\n\\newcommand{\\DE}{\\textsf{DE}}\n\\newcommand{\\tr}{\\textsf{tr}}\n\\newcommand{\\cF}{{\\cal{F}}}\n\\newcommand{\\cL}{{\\cal{L}}}\n\\newcommand{\\cI}{{\\cal{I}}}\n\\newcommand{\\cB}{{\\cal{B}}}\n\\newcommand{\\cP}{{\\cal{P}}}\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\pperp}{\\mathrel{{\\rlap{$\\,\\perp$}\\perp\\,\\,}}}\n\\newcommand{\\OFP}{(\\Omega,\\cF, \\P)}\n\\newcommand{\\eps}{\\boldsymbol{\\epsilon}}\n\\newcommand{\\Psib}{\\boldsymbol{\\Psi}}\n\\newcommand{\\1}{\\mathbf{1}_n}\n\\newcommand{\\gap}{\\vspace{8mm}}\n\\newcommand{\\ind}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}}\n\\newcommand{\\iid}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}}\n\\newcommand{\\simiid}{\\ensuremath{\\mathrel{\\mathop{\\sim}\\limits^{\\rm\niid}}}}\n\\newcommand{\\eqindis}{\\mathrel{\\mathop{=}\\limits^{\\rm D}}}\n\\newcommand{\\SSZ}{S_{zz}}\n\\newcommand{\\SZW}{S_{zw}}\n\\newcommand{\\Var}{\\textsf{Var}}\n\\newcommand{\\corr}{\\textsf{corr}}\n\\newcommand{\\cov}{\\textsf{cov}}\n\\newcommand{\\diag}{\\textsf{diag}}\n\\newcommand{\\var}{\\textsf{var}}\n\\newcommand{\\Cov}{\\textsf{Cov}}\n\\newcommand{\\Sam}{{\\cal S}}\n\\newcommand{\\VS}{{\\cal V}}\n\\def\\H{\\mathbf{H}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\tY}{\\tilde{\\mathbf{Y}}}\n\\newcommand{\\Yhat}{\\hat{\\mathbf{Y}}}\n\\newcommand{\\Yobs}{\\mathbf{Y}_{{\\cal S}}}\n\\newcommand{\\barYobs}{\\bar{Y}_{{\\cal S}}}\n\\newcommand{\\barYmiss}{\\bar{Y}_{{\\cal S}^c}}\n\\def\\bv{\\mathbf{b}}\n\\def\\X{\\mathbf{X}}\n\\def\\tX{\\tilde{\\mathbf{X}}}\n\\def\\x{\\mathbf{x}}\n\\def\\xbar{\\bar{\\mathbf{x}}}\n\\def\\Xbar{\\bar{\\mathbf{X}}}\n\\def\\Xg{\\mathbf{X}_{\\boldsymbol{\\gamma}}}\n\\def\\Ybar{\\bar{\\Y}}\n\\def\\ybar{\\bar{y}}\n\\def\\y{\\mathbf{y}}\n\\def\\Yf{\\mathbf{Y_f}}\n\\def\\W{\\mathbf{W}}\n\\def\\L{\\mathbf{L}}\n\\def\\m{\\mathbf{m}}\n\\def\\n{\\mathbf{n}}\n\\def\\w{\\mathbf{w}}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\z{\\mathbf{z}}\n\\def\\v{\\mathbf{v}}\n\\def\\u{\\mathbf{u}}\n\n\\def\\zero{\\mathbf{0}}\n\\def\\one{\\mathbf{1}}\n\\newcommand{\\taub}{\\boldsymbol{\\tau}}\n\\newcommand{\\betav}{\\boldsymbol{\\beta}}\n\\newcommand{\\alphav}{\\boldsymbol{\\alpha}}\n\\newcommand{\\bfomega}{\\boldsymbol{\\omega}}\n\\newcommand{\\bfchi}{\\boldsymbol{\\chi}}\n\\newcommand{\\bfLambda}{\\boldsymbol{\\Lambda}}\n\\newcommand{\\Lmea}{{\\cal L}} \n\\newcommand{\\scale}{\\bflambda} \n\\newcommand{\\Scale}{\\bfLambda} \n\\newcommand{\\bfscale}{\\bflambda} \n\\newcommand{\\mean}{\\bfchi}\n\\newcommand{\\loc}{\\bfchi}\n\\newcommand{\\bfmean}{\\bfchi}\n\\newcommand{\\bfx}{\\x}\n\\renewcommand{\\k}{g}\n\\newcommand{\\Gen}{{\\cal G}}\n\\newcommand{\\Levy}{L{\\'e}vy}\n\\def\\bfChi    {\\boldsymbol{\\Chi}}\n\\def\\bfOmega  {\\boldsymbol{\\Omega}}\n\\newcommand{\\Po}{\\mbox{\\sf P}}\n\\newcommand{\\A}{\\mathbf{A}}\n\\def\\a{\\mathbf{a}}\n\\def\\K{\\mathbf{K}}\n\\newcommand{\\B}{\\mathbf{B}}\n\\def\\b{\\boldsymbol{\\beta}}\n\\def\\bhat{\\hat{\\boldsymbol{\\beta}}}\n\\def\\btilde{\\tilde{\\boldsymbol{\\beta}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\bg{\\boldsymbol{\\beta_\\gamma}}\n\\def\\bgnot{\\boldsymbol{\\beta_{(-\\gamma)}}}\n\\def\\mub{\\boldsymbol{\\mu}}\n\\def\\tmub{\\tilde{\\boldsymbol{\\mu}}}\n\\def\\muhat{\\hat{\\boldsymbol{\\mu}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\tk{\\boldsymbol{\\theta}_k}\n\\def\\tj{\\boldsymbol{\\theta}_j}\n\\def\\Mk{\\boldsymbol{{\\cal M}}_k}\n\\def\\M{\\boldsymbol{{\\cal M}}}\n\\def\\Mj{\\boldsymbol{{\\cal M}}_j}\n\\def\\Mi{\\boldsymbol{{\\cal M}}_i}\n\\def\\Mg{{\\boldsymbol{{\\cal M}_\\gamma}}}\n\\def\\Mnull{\\boldsymbol{{\\cal M}}_{N}}\n\\def\\gMPM{\\boldsymbol{\\gamma}_{\\text{MPM}}}\n\\def\\gHPM{\\boldsymbol{\\gamma}_{\\text{HPM}}}\n\\def\\Mfull{\\boldsymbol{{\\cal M}}_{F}}\n\\def\\tg{\\boldsymbol{\\theta}_{\\boldsymbol{\\gamma}}}\n\\def\\g{\\boldsymbol{\\gamma}}\n\\def\\eg{\\boldsymbol{\\eta}_{\\boldsymbol{\\gamma}}}\n\\def\\G{\\mathbf{G}}\n\\def\\cM{\\cal M}\n\\def\\D{\\Delta}\n\\def \\shat{{\\hat{\\sigma}}^2}\n\\def\\uv{\\mathbf{u}}\n\\def\\l {\\lambda}\n\\def\\d{\\delta}\n\\def\\Sigmab{\\boldsymbol{\\Sigma}}\n\\def\\Phib{\\boldsymbol{\\Phi}}\n\\def\\Lambdab{\\boldsymbol{\\Lambda}}\n\\def\\lambdab{\\boldsymbol{\\lambda}}\n\\def\\Mg{{\\cal M}_\\gamma}\n\\def\\S{{\\cal{S}}}\n\\def\\qg{p_{\\boldsymbol{\\gamma}}}\n\\def\\pg{p_{\\boldsymbol{\\gamma}}}\n\\def\\T{\\boldsymbol{\\Theta}}\n\\def\\Tb{\\boldsymbol{\\Theta}}\n\\def\\t{\\mathbf{t}}\n\n\n\n- Likelihood Function\n- Projections\n- Maximum Likelihood Estimates\n\n\n. . .\n\nReadings: Christensen Chapter 1-2, Appendix A, and Appendix B\n\n## Normal Model\n\nTake an random vector $\\Y \\in \\bbR^n$ which is observable and decompose  \n$$ \\Y = \\mub + \\eps$$ \n\n- $\\mub \\in \\bbR^n$ (unknown, fixed)  \n- $\\eps \\in \\bbR^n$ unobservable error vector (random)\n\n. . .\n\nUsual assumptions? \n\n- $E[\\epsilon_i] = 0 \\ \\forall i \\Leftrightarrow \\E[\\eps] = \\zero$  $\\quad \\Rightarrow \\E[\\Y] = \\mub$\n  (mean vector) \n- $\\epsilon_i$ independent with $\\Var(\\epsilon_i) = \\sigma^2$ and\n  $\\Cov(\\epsilon_i, \\epsilon_j) = 0$\n- Matrix version \n$\\Cov[\\eps] \\equiv \\left[ (\\E\\left[(\\epsilon_i -\\E[\\epsilon_i])(\\epsilon_j - \\E[\\epsilon_j])\\right]\\right]_{ij} = \\sigma^2 \\I_n  \n\\quad \\Rightarrow \\Cov[\\Y] = \\sigma^2 \\I_n$  (errors are uncorrelated) \n- $\\eps_i \\iid \\N(0, \\sigma^2)$  implies that $Y_i \\ind \\N(\\mu_i, \\sigma^2)$\n\n## Likelihood Function\n\nThe likelihood function for $\\mub, \\sigma^2$ is proportional to the\nsampling distribution of the data \n\n\\begin{eqnarray*}\n \\cL(\\mub, \\sigma^2) & \\propto & \\prod_{i = 1}^n \\frac{1}{\\sqrt{(2 \\pi\n                                 \\sigma^2)}} \\exp{- \\frac{1}{2}\n                                 \\left\\{ \\frac{( Y_i\n                                 - \\mu_i)^2}{\\sigma^2} \\right\\}}\n                                 \\\\\n & \\propto & ({2 \\pi} \\sigma^2)^{-n/2}\n \\exp{\\left\\{ - \\frac 1 2  \\frac{ \\sum_i(Y_i - \\mu_i)^2 )}{\\sigma^2}\n\\right\\}}   \\\\\n   & \\propto & (\\sigma^2)^{-n/2}\n \\exp{\\left\\{ - \\frac 1 2 \\frac{\\| \\Y - \\mub \\|^2}{\\sigma^2}\n\\right\\}} \\\\ \n  & \\propto &  (2 \\pi)^{-n/2}\n| \\I_n\\sigma^2|^{-1/2}\n \\exp{\\left\\{ - \\frac 1 2 \\frac{\\| \\Y - \\mub \\|^2}{\\sigma^2}\n\\right\\}}  \n\\end{eqnarray*}\n\n. . .\n\nLast line is the density of $\\Y \\sim \\N_n\\left(\\mub, \\sigma^2 \\I_n\\right)$\n\n## MLEs\n\n\\frametitle{MLEs}\n\nFind values of $\\muhat$ and $\\shat$ that maximize the likelihood\n$\\cL(\\mub, \\sigma^2)$ for $\\mub \\in \\bbR^n$ and $\\sigma^2 \\in \\bbR^+$\n\\begin{eqnarray*}\n \\cL(\\mub, \\sigma^2)\n    & \\propto & (\\sigma^2)^{-n/2}\n \\exp{\\left\\{ - \\frac 1 2 \\frac{\\| \\Y - \\mub \\|^2}{\\sigma^2}\n\\right\\}} \\\\ \n \\log(\\cL(\\mub, \\sigma^2) )\n   & \\propto & -\\frac{n}{2} \\log(\\sigma^2)  - \\frac 1 2 \\frac{\\| \\Y - \\mub \\|^2}{\\sigma^2}\n \\\\ \n\\end{eqnarray*}\nor equivalently the log likelihood\n\n\n\n- Clearly, $\\muhat = \\Y$ but $\\shat = 0$  is outside the parameter space\n\n- If $\\mub = \\X \\b$,  can show that $\\bhat = (\\X^T\\X)^{-1}\\X^T\\Y$ is the MLE/OLS estimator of $\\b$ and $\\muhat = \\X\\bhat$ if $\\X$ is full column rank.\n\n\n- show via projections\n\n## Projections\n\ntake any point $\\y \\in \\bbR^n$ and \"project\" it onto $C(\\X) = \\M$\n\n- any point already in $\\M$ stays the same\n\n- so if $\\P_\\X$ is a projection onto the column space of $\\X$ then for $\\m \\in C(\\X)$ \n$\\P_\\X \\m = \\m$\n\n- $\\P_\\X$ is a linear transformation from $\\bbR^n \\to \\bbR^n$\n- maps vectors in $\\bbR^n$ into $C(\\X)$\n- if $\\z \\in \\bbR^n$ then $\\P_\\X \\z = \\X \\a \\in C(\\X)$ for some $\\a \\in \\bbR^p$\n\n. . .\n\n::: {.callout-note }\n\n## Example\nFor $\\X \\in \\bbR^{n \\times p}$, rank $p$, $\\P_\\X = \\X(\\X^T\\X)^{-1}\\X$ is a projection onto the $p$ dimensional subspace $\\M = C(\\X)$\n\n:::\n\n\n\n\n## Idempotent Matrix\n\nWhat if we project a projection?\n\n- $\\P_\\X \\z = \\X \\a \\in C(\\X)$\n- $\\P_\\X \\X \\a  = \\X \\a$ \n- since $\\P_\\X^2 \\z =  \\P_\\X \\z$ for all $\\z \\in \\bbR^n$ we have $\\P_\\X^2 = \\P_\\X$ \n\n. . .\n\n::: {.Definition #Projection .unnumbered}\n\n### Projection\n\nFor a matrix  $\\P$ in $\\bbR^{n \\times n}$ is a projection matrix if $\\P^2 = \\P$. \nThat is all projections $\\P$ are idempotent matrix.\n:::\n\n. . .\n\n::: {.callout-note}\n\n## Exercise\nFor $\\X \\in \\bbR^{n \\times p}$, rank $p$, if $\\P_\\X = \\X(\\X^T\\X)^{-1}\\X$ use the definition to show that it is a projection onto the $p$ dimensional subspace $\\M = C(\\X)$\n\n:::\n\n## Null Space\n\n::: {.Definition #OrthComp .unnumbered}\n## Orthogonal Complement\nThe set of all vectors that are orthogonal to a given subspace $\\M$ is called the _orthogonal complement_ of the subspace denoted as $\\M^\\perp$. Under the usual inner product,\n$\\M^\\perp \\equiv \\{\\n \\in \\bbR^n \\ni \\m^T\\n = 0 {\\text{ for }} \\m \\in \\M\\}$\n:::\n\n. . .\n\n::: {.Definition #Nullspace .unnumbered}\n## Null Space\nFor a matrix $\\A$, the _null space_ of $\\A$ is defined as $N(\\A) = \\{\\n \\ni \\A \\n = \\zero\\}$ \n:::\n. . .\n\n::: {.callout-note}\n## Exercise\n\nShow that  $C(\\X)^\\perp$ (the _orthogonal complement_ of $C(\\X)$) is the _null space_ of $\\X^T$, $\\, N(\\X^T)$.\n:::\n\n## Orthogonal Projection\n\n::: {.Definition #orthproj .unnumbered}\n## Orthogonal Projections\nFor a vector space $\\VS$ with an inner product $\\langle \\x, \\y \\rangle$ for $\\x, \\y \\in \\VS$, $\\x$ and $\\y$ are orthogonal if $\\langle \\x, \\y \\rangle = 0$. A projection $\\P$ is an _orthogonal projection_ onto a subspace $\\M$ of $\\VS$ if for any $\\m \\in \\VS$, $\\P \\m = \\m$ and for any $\\n \\in \\M^\\perp$, $\\P \\n = \\zero$.\n\nThe null space of $\\P$ is the orthogonal complement of $\\M$\n:::\n. . .\n\nFor $\\bbR^N$ with the inner product, $\\langle \\x, \\y \\rangle = \\x^T\\y$, $\\P$ is an orthogonal projection onto $\\M$ if $\\P$ is a projection  ($\\P^2 = \\P$) and it is symmetric ($\\P = \\P^T$)\n \n. . .\n\n::: {.callout-note}\n## Exercise\n\nShow that $\\P_\\X$ is an orthogonal projection on $C(\\X)$.\n:::\n\n\n## Decompsition\n\n- For any $\\y \\in \\bbR^n$, we can write it uniquely as a vector $\\m + \\n$ as $\\m \\in \\M$ and $\\n \\M^\\perp$ \n\n- write $\\y = \\P \\y + (\\y - \\P \\y ) = \\P \\y + (\\I - \\P)\\y$\n\n- claim that if $\\P$ is an orthogonal projection, $(\\I - \\P)$ is an orthogonal projection onto $\\M^\\perp$\n\n- if $\\n \\in \\M^\\perp$, then $(\\I - \\P)\\n = \\n - \\P \\n = \\n$\n\n## Back to MLEs\n\n- $\\Y \\sim \\N(\\mub, \\sigma^2 \\I_n)$ with\n    $\\mub = \\X \\b$  and $\\X$ full column rank \n\n- Claim: Maximum Likelihood Estimator (MLE) of $\\mub$ is\n    $\\P_\\X \\Y$  \n\n- Log Likelihood: \n$$ \\log \\cL(\\mub, \\sigma^2) =\n-\\frac{n}{2} \\log(\\sigma^2)\n  - \\frac 1 2 \\frac{\\| \\Y - \\mub \\|^2}{\\sigma^2}\n$$\n- Decompose $\\Y = \\P_\\X \\Y + (\\I - \\P_\\X) \\Y$  \n- Use $\\P_\\X \\mub = \\mub$  \n- Simplify $\\| \\Y - \\mub \\|^2$\n\n## Expand\n\\begin{eqnarray*}\n    \\| \\Y - \\mub \\|^2 & = & \\|  { (\\I- \\P_\\X) \\Y + \\P_x \\Y} -\n    \\mub \\|^2  \\\\\n  & = & \\| (\\I - \\P_\\X) \\Y + \\P_x \\Y - {\\P_\\X}\\mub \\|^2  \\\\\n  & = & {\\|(\\I -\\P_\\x)}\\Y +  {\\P_\\X}(\\Y  - \\mub)\n  \\|^2 \\\\\n & = & {\\|(\\I -\\P_\\x)\\Y \\|^2} +  {\\|\n   {\\P_\\X}(\\Y  - \\mub) \\|^2}  + {\\small{2 (\\Y -\n\\mub)^T \\P_\\X^T (\\I - \\P_\\X) \\Y }}\\\\ \n & = & \\|(\\I -\\P_\\x)\\Y \\|^2 +  \\| {\\P_\\X}(\\Y  - \\mub) \\|^2 + {0} \n \\\\\n & = & \\|(\\I -\\P_\\x)\\Y \\|^2 +  \\| {\\P_\\X}\\Y  - \\mub \\|^2\n  \\end{eqnarray*}  \n  \n  . . .\n  \nCrossproduct term is zero:\n\\begin{eqnarray*}\n  \\P_\\X^T (\\I - \\P_\\X) & = &  \\P_\\X (\\I - \\P_\\X)  \\\\\n  & = &  \\P_\\X - \\P_\\X\\P_\\X  \\\\\n & = &  \\P_\\X - \\P_\\X   \\\\\n& = & \\zero\n\\end{eqnarray*}\n\n## Log Likelihood\nSubstitute decomposition into log likelihood\n\\begin{eqnarray*}\n \\log \\cL(\\mub, \\sigma^2)  & = &\n-\\frac{n}{2} \\log(\\sigma^2) - \\frac 1 2 \\frac{\\| \\Y - \\mub \\|^2}{\\sigma^2}  \\\\\n  & = & -\\frac{n}{2} \\log(\\sigma^2)  - \\frac 1 2 \\left( \\frac{\\|(\\I - \\P_\\X)\n  \\Y \\|^2}{\\sigma^2} + \\frac{\\| \\P_\\X \\Y - \\mub\\|^2 } {\\sigma^2} \\right)   \\\\\n & = &  \\underbrace { -\\frac{n}{2} \\log(\\sigma^2)  - \\frac 1 2  \\frac{\\|(\\I - \\P_\\X)\n  \\Y \\|^2}{\\sigma^2} }  +  \\underbrace{- \\frac 1 2  \\frac{\\| \\P_\\X \\Y -\n  \\mub\\|^2 } {\\sigma^2}}   \\\\\n & = &  \\text{ constant with respect to } \\mub  \\qquad  \\leq 0\n\\end{eqnarray*}   \n\n- Maximize with respect to $\\mub$ for each $\\sigma^2$ \n\n- RHS is largest when $\\mub = \\P_\\X \\Y$  for any choice of $\\sigma^2$\n$$\\therefore \\quad \\muhat = \\P_\\X \\Y$$\nis the MLE of $\\mub$  (fitted values $\\Yhat = \\P_\\X \\Y$)\n\n## MLE of $\\b$\n$$\\cL(\\mub, \\sigma^2)   =  -\\frac{n}{2} \\log(\\sigma^2)  - \\frac 1 2 \\left( \\frac{\\|(\\I - \\P_\\X)\n  \\Y \\|^2}{\\sigma^2} + \\frac{\\| \\P_\\X \\Y - \\mub\\|^2 } {\\sigma^2} \\right)$$\n\n. . .\n\nRewrite as likeloood function for $\\b, \\sigma^2$:\n$$\\cL(\\b, \\sigma^2 )  =  -\\frac{n}{2} \\log(\\sigma^2)  - \\frac 1 2 \\left( \\frac{\\|(\\I - \\P_\\X)\n  \\Y \\|^2}{\\sigma^2} + \\frac{\\| \\P_\\X \\Y - \\X\\b\\|^2 } {\\sigma^2}\n\\right)$$\n\n\n. . .\n\n- Similar argument to show that RHS is maximized by minimizing $$\\| \\P_\\X\n\\Y - \\X\\b\\|^2$$ \\pause\n- Therefore $\\bhat$ is  a MLE of $\\b$ if and only if satisfies\n$$\\P_\\X \\Y = \\X \\bhat$$ \n- If $\\X^T\\X$ is full rank, the MLE of $\\b$ is $(\\X^T\\X)^{-1}\\X^T\\Y = \\bhat$\n\n## MLE of $\\sigma^2$\n\n- Plug-in MLE of $\\muhat$ for $\\mub$ \n$$ \\log \\cL(\\muhat, \\sigma^2)  =   -\\frac{n}{2} \\log \\sigma^2 - \\frac 1 2\n\\frac{\\| (\\I - \\P_\\X) \\Y \\|^2  }{\\sigma^2}$$\n- Differentiate  with respect to $\\sigma^2$ \n$$\\frac{\\partial \\, \\log \\cL(\\muhat, \\sigma^2)}{\\partial \\, \\sigma^2} =  -\\frac{n}{2} \\frac{1}{\\sigma^2}  +  \\frac 1 2\n\\| (\\I - \\P_\\X) \\Y \\|^2 \\left(\\frac{1}{\\sigma^2}\\right)^2 $$\n- Set derivative to zero and solve for MLE\n\\begin{eqnarray*}\n0 & = &  -\\frac{n}{2} \\frac{1}{\\shat}  +  \\frac 1 2\n\\| (\\I - \\P_\\X) \\Y \\|^2 \\left(\\frac{1}{\\shat}\\right)^2  \\\\\n\\frac{n}{2} \\shat & = & \\frac 1 2\n\\| (\\I - \\P_\\X) \\Y \\|^2 \\\\\n\\shat & = & \\frac{\\| (\\I - \\P_\\X) \\Y \\|^2}{n}\n\\end{eqnarray*}\n\n## MLE Estimate of $\\sigma^2$\nMaximum Likelihood Estimate of $\\sigma^2$\n\\begin{eqnarray*}\n    \\shat & = & \\frac{\\| (\\I - \\P_\\X) \\Y \\|^2}{n} \\\\\n      & = & \\frac{\\Y^T(\\I - \\P_\\X)^T(\\I-\\P_\\X) \\Y }{n} \\\\\n & = & \\frac{ \\Y^T(\\I - \\P_\\X) \\Y}{n} \\\\\n & = & \\frac{\\e^T\\e} {n} \n  \\end{eqnarray*}\nwhere $\\e = (\\I - \\P_\\X)\\Y$  {residuals} from the regression of $\\Y$\non $\\X$\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}