{
  "hash": "9333aabb0444790829ac3a21a0400b67",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Introduction to STA721\"\nauthor: \"Merlise Clyde (clyde@duke.edu)\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    smaller: true\n    logo: ../../img/icon.png\n    footer: <https://sta721-F24.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"   \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\nnumber-sections: false\nfilters:\n  - custom-numbered-blocks  \ncustom-numbered-blocks:\n  groups:\n    thmlike:\n      colors: [948bde, 584eab]\n      boxstyle: foldbox.simple\n      collapse: false\n      listin: [mathstuff]\n    todos: default  \n  classes:\n    Theorem:\n      group: thmlike\n    Corollary:\n      group: thmlike\n    Conjecture:\n      group: thmlike\n      collapse: true  \n    Definition:\n      group: thmlike\n      colors: [d999d3, a01793]\n    Feature: \n       numbered: false\n    TODO:\n      label: \"To do\"\n      colors: [e7b1b4, 8c3236]\n      group: todos\n      listin: [stilltodo]\n    DONE:\n      label: \"Done\"\n      colors: [cce7b1, 86b754]  \n      group: todos  \n---\n\n\n\n\n\n## Introduction to STA721\n\n\\usepackage{xcolor}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator{\\sgn}{sgn}\n\\newcommand{\\e}{\\mathbf{e}}\n\\newcommand{\\Mb}{\\mathbf{M}}\n\\renewcommand{\\P}{\\mathbf{P}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\R}{\\textsf{R}}\n\\newcommand{\\mat}[1] {\\mathbf{#1}}\n\\newcommand{\\E}{\\textsf{E}}\n\\newcommand{\\SE}{\\textsf{SE}}\n\\newcommand{\\SSE}{\\textsf{SSE}}\n\\newcommand{\\RSS}{\\textsf{RSS}}\n\\newcommand{\\FSS}{\\textsf{FSS}}\n\\renewcommand{\\SS}{\\textsf{SS}}\n\\newcommand{\\MSE}{\\textsf{MSE}}\n\\newcommand{\\SSR}{\\textsf{SSR}}\n\\newcommand{\\Be}{\\textsf{Beta}}\n\\newcommand{\\St}{\\textsf{St}}\n\\newcommand{\\Ca}{\\textsf{C}}\n\\newcommand{\\Lv}{\\textsf{LÃ©vy}}\n\\newcommand{\\Exp}{\\textsf{Exp}}\n\\newcommand{\\GDP}{\\textsf{GDP}}\n\\newcommand{\\NcSt}{\\textsf{NcSt}}\n\\newcommand{\\Bin}{\\textsf{Bin}}\n\\newcommand{\\NB}{\\textsf{NegBin}}\n\\newcommand{\\Mult}{\\textsf{MultNom}}\n\\renewcommand{\\NG}{\\textsf{NG}}\n\\newcommand{\\N}{\\textsf{N}}\n\\newcommand{\\Ber}{\\textsf{Ber}}\n\\newcommand{\\Poi}{\\textsf{Poi}}\n\\newcommand{\\Gam}{\\textsf{Gamma}}\n\\newcommand{\\BB}{\\textsf{BB}}\n\\newcommand{\\BF}{\\textsf{BF}}\n\\newcommand{\\Gm}{\\textsf{G}}\n\\newcommand{\\Un}{\\textsf{Unif}}\n\\newcommand{\\Ex}{\\textsf{Exp}}\n\\newcommand{\\DE}{\\textsf{DE}}\n\\newcommand{\\tr}{\\textsf{tr}}\n\\newcommand{\\cF}{{\\cal{F}}}\n\\newcommand{\\cL}{{\\cal{L}}}\n\\newcommand{\\cI}{{\\cal{I}}}\n\\newcommand{\\cB}{{\\cal{B}}}\n\\newcommand{\\cP}{{\\cal{P}}}\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\pperp}{\\mathrel{{\\rlap{$\\,\\perp$}\\perp\\,\\,}}}\n\\newcommand{\\OFP}{(\\Omega,\\cF, \\P)}\n\\newcommand{\\eps}{\\boldsymbol{\\epsilon}}\n\\newcommand{\\Psib}{\\boldsymbol{\\Psi}}\n\\newcommand{\\1}{\\mathbf{1}_n}\n\\newcommand{\\gap}{\\vspace{8mm}}\n\\newcommand{\\ind}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}}\n\\newcommand{\\iid}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}}\n\\newcommand{\\simiid}{\\ensuremath{\\mathrel{\\mathop{\\sim}\\limits^{\\rm\niid}}}}\n\\newcommand{\\eqindis}{\\mathrel{\\mathop{=}\\limits^{\\rm D}}}\n\\newcommand{\\SSZ}{S_{zz}}\n\\newcommand{\\SZW}{S_{zw}}\n\\newcommand{\\Var}{\\textsf{Var}}\n\\newcommand{\\corr}{\\textsf{corr}}\n\\newcommand{\\cov}{\\textsf{cov}}\n\\newcommand{\\diag}{\\textsf{diag}}\n\\newcommand{\\var}{\\textsf{var}}\n\\newcommand{\\Cov}{\\textsf{Cov}}\n\\newcommand{\\Sam}{{\\cal S}}\n\\def\\H{\\mathbf{H}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\tY}{\\tilde{\\mathbf{Y}}}\n\\newcommand{\\Yhat}{\\hat{\\mathbf{Y}}}\n\\newcommand{\\Yobs}{\\mathbf{Y}_{{\\cal S}}}\n\\newcommand{\\barYobs}{\\bar{Y}_{{\\cal S}}}\n\\newcommand{\\barYmiss}{\\bar{Y}_{{\\cal S}^c}}\n\\def\\bv{\\mathbf{b}}\n\\def\\X{\\mathbf{X}}\n\\def\\tX{\\tilde{\\mathbf{X}}}\n\\def\\x{\\mathbf{x}}\n\\def\\xbar{\\bar{\\mathbf{x}}}\n\\def\\Xbar{\\bar{\\mathbf{X}}}\n\\def\\Xg{\\mathbf{X}_{\\boldsymbol{\\gamma}}}\n\\def\\Ybar{\\bar{\\Y}}\n\\def\\ybar{\\bar{y}}\n\\def\\y{\\mathbf{y}}\n\\def\\Yf{\\mathbf{Y_f}}\n\\def\\W{\\mathbf{W}}\n\\def\\L{\\mathbf{L}}\n\\def\\w{\\mathbf{w}}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\z{\\mathbf{z}}\n\\def\\v{\\mathbf{v}}\n\\def\\u{\\mathbf{u}}\n\n\\def\\zero{\\mathbf{0}}\n\\def\\one{\\mathbf{1}}\n\\newcommand{\\taub}{\\boldsymbol{\\tau}}\n\\newcommand{\\betav}{\\boldsymbol{\\beta}}\n\\newcommand{\\alphav}{\\boldsymbol{\\alpha}}\n\\newcommand{\\bfomega}{\\boldsymbol{\\omega}}\n\\newcommand{\\bfchi}{\\boldsymbol{\\chi}}\n\\newcommand{\\bfLambda}{\\boldsymbol{\\Lambda}}\n\\newcommand{\\Lmea}{{\\cal L}} \n\\newcommand{\\scale}{\\bflambda} \n\\newcommand{\\Scale}{\\bfLambda} \n\\newcommand{\\bfscale}{\\bflambda} \n\\newcommand{\\mean}{\\bfchi}\n\\newcommand{\\loc}{\\bfchi}\n\\newcommand{\\bfmean}{\\bfchi}\n\\newcommand{\\bfx}{\\x}\n\\renewcommand{\\k}{g}\n\\newcommand{\\Gen}{{\\cal G}}\n\\newcommand{\\Levy}{L{\\'e}vy}\n\\def\\bfChi    {\\boldsymbol{\\Chi}}\n\\def\\bfOmega  {\\boldsymbol{\\Omega}}\n\\newcommand{\\Po}{\\mbox{\\sf P}}\n\\newcommand{\\A}{\\mathbf{A}}\n\\def\\a{\\mathbf{a}}\n\\def\\K{\\mathbf{K}}\n\\newcommand{\\B}{\\mathbf{B}}\n\\def\\b{\\boldsymbol{\\beta}}\n\\def\\bhat{\\hat{\\boldsymbol{\\beta}}}\n\\def\\btilde{\\tilde{\\boldsymbol{\\beta}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\bg{\\boldsymbol{\\beta_\\gamma}}\n\\def\\bgnot{\\boldsymbol{\\beta_{(-\\gamma)}}}\n\\def\\mub{\\boldsymbol{\\mu}}\n\\def\\tmub{\\tilde{\\boldsymbol{\\mu}}}\n\\def\\muhat{\\hat{\\boldsymbol{\\mu}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\tk{\\boldsymbol{\\theta}_k}\n\\def\\tj{\\boldsymbol{\\theta}_j}\n\\def\\Mk{\\boldsymbol{{\\cal M}}_k}\n\\def\\M{\\boldsymbol{{\\cal M}}}\n\\def\\Mj{\\boldsymbol{{\\cal M}}_j}\n\\def\\Mi{\\boldsymbol{{\\cal M}}_i}\n\\def\\Mg{{\\boldsymbol{{\\cal M}_\\gamma}}}\n\\def\\Mnull{\\boldsymbol{{\\cal M}}_{N}}\n\\def\\gMPM{\\boldsymbol{\\gamma}_{\\text{MPM}}}\n\\def\\gHPM{\\boldsymbol{\\gamma}_{\\text{HPM}}}\n\\def\\Mfull{\\boldsymbol{{\\cal M}}_{F}}\n\\def\\tg{\\boldsymbol{\\theta}_{\\boldsymbol{\\gamma}}}\n\\def\\g{\\boldsymbol{\\gamma}}\n\\def\\eg{\\boldsymbol{\\eta}_{\\boldsymbol{\\gamma}}}\n\\def\\G{\\mathbf{G}}\n\\def\\cM{\\cal M}\n\\def\\D{\\Delta}\n\\def \\shat{{\\hat{\\sigma}}^2}\n\\def\\uv{\\mathbf{u}}\n\\def\\l {\\lambda}\n\\def\\d{\\delta}\n\\def\\Sigmab{\\boldsymbol{\\Sigma}}\n\\def\\Phib{\\boldsymbol{\\Phi}}\n\\def\\Lambdab{\\boldsymbol{\\Lambda}}\n\\def\\lambdab{\\boldsymbol{\\lambda}}\n\\def\\Mg{{\\cal M}_\\gamma}\n\\def\\S{{\\cal{S}}}\n\\def\\qg{p_{\\boldsymbol{\\gamma}}}\n\\def\\pg{p_{\\boldsymbol{\\gamma}}}\n\\def\\T{\\boldsymbol{\\Theta}}\n\\def\\Tb{\\boldsymbol{\\Theta}}\n\\def\\t{\\mathbf{t}}\n\n\n\n- Course: Theory and Application of linear models from both a\nfrequentist (classical) and Bayesian perspective \n- Prerequisites:   linear algebra and a mathematical statistics\n  course covering likelihoods and distribution theory (normal, t, F,\n  chi-square, gamma distributions) \n- Introduce  R programming as needed in the lab\n- Introduce  Bayesian methods, but assume that you are\n  co-registered in 702 or have taken it previously \n- more info on  Course website [https://sta721-F24.github.io/website/](https://sta721-F24.github.io/website/)\n  - schedule and slides, HW, etc\n  - critical dates (Midterms and Finals)\n  - office hours\n- Canvas for grades, email, announcements \n\n. . .\n\nPlease let me know if there are broken links for slides, etc!\n\n## Notation \n\n- scalors are $a$ (italics or math italics) \n- vectors are in bold lower case, $\\a$, with the exception of random variables\n- all vectors are column vectors\n $$\\a = \\left[\\begin{array}{c}\n      a_1 \\\\\n      a_2 \\\\\n      \\vdots \\\\\n      a_n\n       \\end{array} \\right]\n  $$  \n- $\\one_n$ is a $n \\times 1$ vector of all ones       \n- inner product $\\langle    \\a, \\a \\rangle = \\a^T\\a = \\|\\a \\|^2 = \\sum_{i=1}^n a_i^2$;   $\\langle    \\a, \\bv \\rangle = \\a^T\\bv$\n- length or norm of $\\a$ is $\\|\\a\\|$\n\n## Matrices\n\n- Matrices are represented in bold $\\A = (a_{ij})$\n$$\\A = \\left[\\begin{array}{cccc}\n      a_{11} & a_{12} & \\cdots & a_{1m}  \\\\\n      a_{21} & a_{22} & \\cdots & a_{2m} \\\\\n      \\vdots & \\vdots & \\vdots & \\vdots\\\\\n      a_{n1} & a_{n2} & \\cdots & a_{nm}\n       \\end{array} \\right]\n$$  \n- identity matrix $\\I_n$ square matrix with diagonal elements 1 and off diagonal 0\n- trace: if $\\A$ is $n \\times m$ $\\tr(\\A) = \\sum_i^{\\max n,m } a_{ii}$ \n- determinant:  for $\\A$ is $n \\times n$ then the determinant is $\\det(A)$\n- inverse:  if $\\A$ is nonsingular $\\A > 0$, then its inverse is $\\A^{-1}$\n\n## Statistical Models\n\nOhm's Law: $Y$ is voltage across a resistor of $r$ ohms and $X$ is the amperes of the current through the resistor (in theory)\n$$Y = rX$$ \n\n- Simple linear regression for observational data \n$$Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\text{  for  } i = 1,\n\\ldots, n$$  \n\n- Rewrite in vectors:\n\\begin{eqnarray*}\n  \\left[\n\\begin{array}{c}  y_1 \\\\ \\vdots \\\\  y_n \\end{array}\n  \\right]   =  &\n \\left[ \\begin{array}{c}  1 \\\\ \\vdots \\\\ 1 \\end{array}  \\right]   \\beta_0 +\n \\left[ \\begin{array}{c}  x_1 \\\\ \\vdots \\\\  x_n \\end{array}\n \\right] \\beta_1 +\n\\left[ \\begin{array}{c}  \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n  \\end{array}\n\\right]\n  =  &\n \\left[ \\begin{array}{cc}  1 &  x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_n\\end{array}  \\right]\n \\left[ \\begin{array}{c}  \\beta_0  \\\\  \\beta_1 \\end{array}\n \\right] +\n\\left[ \\begin{array}{c}  \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n  \\end{array}\n\\right] \\\\\n\\\\\n\\Y = & \\X \\b + \\eps\n\\end{eqnarray*}\n\n## Nonlinear Models\nGravitational Law:  $F = \\alpha/d^\\beta$ where $d$ is distance between 2 objects\nand $F$ is the force of gravity between them\n\n- log transformations\n$$\\log(F) = \\log(\\alpha) - \\beta \\log(d)$$ \n\n- compare to noisy experimental data $Y_i =\\log(F_i)$ observed at $x_i = \\log(d_i)$\n\n- write $\\X = [\\one_n \\, \\x]$ \n- $\\b = (\\log(\\alpha), -\\beta)^T$ \n- model with additive error on log scale $\\Y = \\X\\b + \\e$\n- test if $\\beta = 2$\n- error assumptions?\n\n## Intrinsically Nonlinear Models\nRegression function may be an intrinsically nonlinear function of $t_i$ (time) and parameters $\\tb$\n  $$Y_i = f(t_i, \\tb) + \\epsilon_i$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01-introduction_files/figure-revealjs/concentration-1.png){fig-align='center' width=75% height=65%}\n:::\n:::\n\n\n\n## Quadratic Linear Regression {.smaller}\nTaylor's Theorem:\n$$f(t_i, \\tb) = f(t_0, \\tb) + (t_i - t_0) f'(t_0, \\tb) + (t_i - t_0)^2\n\\frac{f^{''}(t_0, \\tb)}{2}  + R(t_i, \\tb)$$\n\n. . .\n\n\n$$Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i \\text{  for  } i = 1, \\ldots, n$$\n\n. . .\n\nRewrite in vectors:\n\\begin{eqnarray*}\n\\left[\n\\begin{array}{c}  y_1 \\\\ \\vdots \\\\  y_n \\end{array}\n  \\right]   =  &\n \\left[ \\begin{array}{ccc}  1 &  x_1 & x_1^2 \\\\ \\vdots & \\vdots \\\\ 1 &\n     x_n &  x_n^2\\end{array}  \\right]\n \\left[ \\begin{array}{c}  \\beta_0  \\\\  \\beta_1 \\\\ \\beta_2 \\end{array}\n \\right] +\n\\left[ \\begin{array}{c}  \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n  \\end{array}\n\\right] \\\\\n & \\\\ \n\\Y = & \\X \\b + \\eps \n\\end{eqnarray*}\n\n. . .\n\nQuadratic in $x$, but linear in $\\beta$'s - how do we know this model is adequate?\n\n## Linear Regression Models\nResponse $Y_i$ and $p$ predictors $x_{i1}, x_{i2}, \\dots x_ip$\n$$Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\ldots \\beta_{p}\n  x_{ip} + \\epsilon_i$$\n\n\n- Design matrix  $$\\X =\n\\left[\\begin{array}{cccc}\n  1 & x_{11} & \\ldots & x_{1p} \\\\\n  1 & x_{21}  & \\ldots & x_{2p} \\\\\n  \\vdots & \\vdots  & \\vdots & \\vdots \\\\\n  1 & x_{n1} & \\ldots & x_{np} \\\\\n\\end{array} \\right] = \\left[ \\begin{array}{cc}\n1 & \\x_1^T  \\\\\n\\vdots & \\vdots \\\\\n1 & \\x_n^T \n\\end{array} \\right] = \n\\left[\\begin{array}{cccc}\n\\one_n & \\X_1 & \\X_2 \\cdots \\X_p\n\\end{array} \\right]\n$$\n\n- matrix version\n$$\\Y = \\X \\b + \\epsilon$$\n\\pause\nwhat should go into $\\X$ and do we need all columns of $\\X$ for\ninference about $\\Y$?\n\n## Linear  Model \n\n- $\\Y  = \\X \\, \\b + \\eps$\n- $\\Y$ ($n \\times 1$) vector of random response   (observe $\\y$); $\\Y, \\y \\in \\bbR^n$\n- $\\X$ ($n \\times p$)  design matrix  (observe)\n- $\\b$ ($p \\times 1$) vector of coefficients  (unknown)\n- $\\eps$ ($n \\times 1$) vector of \"errors\" (unobservable)\n\n. . .\n\nGoals: \n\n- What goes into $\\X$?   (model building, model selection - post-selection inference?) \n- What if multiple models are \"good\"?  (model averaging or ensembles) \\pause\n- What about the future?  (Prediction) \n- Uncertainty Quantification - assumptions about $\\eps$\n\n. . .\n\n_All models are wrong, but some may be useful  (George Box)_\n\n## Ordinary Least Squares\nGoal: Find the best fitting \"line\" or \"hyper-plane\" that\n  minimizes\n$$\\sum_i  (Y_i - \\x_i^T \\b)^2 = (\\Y - \\X\\b)^T(\\Y - \\X \\b) = \\| \\Y -\n\\X\\b \\|^2 $$ \n\n\n- Optimization problem - seek $\\b \\ni \\X\\b$ is close to $\\Y$ in squared error\n- May over-fit $\\Rightarrow$ add other criteria that provide a penalty\n  **Penalized Least Squares** \n- Robustness to extreme points $\\Rightarrow$ replace quadratic\n  loss with other functions  \n- no notion of uncertainty of estimates  \n- no structure of problem  (repeated measures on individual,\n  randomization restrictions, etc) \n\n. . .\n\nNeed  Distribution Assumptions of $\\Y$ (or $\\eps$) for testing and\nuncertainty measures $\\Rightarrow$ Likelihood  and Bayesian inference\n\n## Random Vectors\n\n- Let $Y_1, \\ldots Y_n$ be random variables in $\\bbR$ \nThen $$\\Y \\equiv\n\\left[ \\begin{array}{c}\n  Y_1 \\\\\n\\vdots \\\\\nY_n\n\\end{array} \\right]$$\nis a random vector in $\\bbR^n$\n\n\n- Expectations of random vectors are defined element-wise:\n$$\\E[\\Y] \\equiv\n\\E \\left[ \\begin{array}{c}\n  Y_1 \\\\\n\\vdots \\\\\nY_n\n\\end{array} \\right] \\equiv\n\\left[ \\begin{array}{c}\n  \\E[Y_1] \\\\\n\\vdots \\\\\n\\E[Y_n]\n\\end{array} \\right] =\n\\left[ \\begin{array}{c}\n  \\mu_1 \\\\\n\\vdots \\\\\n\\mu_n\n\\end{array} \\right]\n\\equiv \\mub \\in \\bbR^n\n$$\nwhere mean or expected value $\\E[Y_i] = \\mu_i$   \n\n## Model Space\n\nWe will work with inner product spaces: a vector spaces, say $\\bbR^n$ equipped with an inner product $\\langle \\x ,\\y \\rangle \\equiv \\x^T\\y, \\quad \\x, \\y \\in \\bbR^n$\n\n. . . \n \n::: {.Definition #Subspace .unnumbered}\n### Subspace\nA set $\\M$ is a subspace of $\\bbR^n$ if is a subset of $\\bbR^n$ and also a vector space.\n\nThat is, if $\\x_1 \\in \\M$ and $\\x_2 \\in \\M$, then $b_1\\x_1 + b_2 \\x_2 \\in \\M$ for all $b_1, b_2 \\in \\bbR$\n:::\n\n. . .\n\n::: {.Definition #ColumnSpace .unnumbered}\n### Column Space\nThe column space of $\\X$ is $C(\\X) = \\X\\b$ for  $\\b \\in \\bbR^p$\n:::\n\n. . .\n\nIf $\\X$ is full column rank, then the columns of $\\X$ form a basis for $C(\\X)$ and $C(\\X)$ is a p-dimensional subspace of $\\bbR^n$\n\n. . .\n\nIf we have just a single model matrix $\\X$, then the subspace $\\M$ is the _model space_.\n\n##  Philosophy\n \n- for many problems frequentist and Bayesian methods will give\n  similar answers (more a matter of taste in interpretation) \n  - For small problems, Bayesian methods allow us to incorporate\n    prior information which provides better calibrated answers  \n  - for problems with complex designs and/or missing data Bayesian\n    methods are often easier to implement (do not need to rely\n    on asymptotics)  \\pause\n- For problems involving hypothesis testing or model selection\n  frequentist and Bayesian methods can be strikingly different.  \n- Frequentist methods often faster (particularly with \"big\n  data\") so great for exploratory analysis and for building a\n  \"data-sense\"  \n- Bayesian methods sit on top of Frequentist Likelihood  \n- Goemetric perspective important in both!\n\n. . .\n\nImportant to understand advantages and problems of each perspective!\n\n\n\n",
    "supporting": [
      "01-introduction_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}