[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "STA721: Linear Models (Fall 2024)",
    "section": "Course Description",
    "text": "Course Description\nThis course introduces students to linear models and extensions for model building from the frequentist (OLS/MLE and penalized likelihoods) and Bayesian paradigms, with an emphasis on a geometric perspective. Course topics include optimal estimation and prediction, distributional assumptions and model checking, hypothesis testing and model selection including Bayes factors and intrinsic Bayes factors, and Bayesian Model Averaging. Students should have a strong background in matrix algebra and distribution theory. Co-requisite: STA 602L, 702L or equivalent."
  },
  {
    "objectID": "index.html#course-info",
    "href": "index.html#course-info",
    "title": "STA721: Linear Models (Fall 2024)",
    "section": "Course Info",
    "text": "Course Info\n\nTextbooks\n\n\n\nTextbook\nOrdering Information\n\n\n\n\n\nPlane Answers to Complex Questions Ronald Christensen (2011) 4th Edition Springer-Verlag, NY. The textbook is freely available as an eBook thru the Duke Library. You’re welcomed to read on screen or print it out. If you prefer a paperback version you can buy it at the cost of printing from Springer or purchase a hardback version at your favorite vendor.\n\n\n\nLinear Regression Analysis, George A.F Seber and Alan J. Lee (2003) 2nd Edition, Wiley eBook in Duke Library. Duke Library is aware the link to the ebook is broken See the\n\n\n\nCanvas site for a pdf version until the links are fixed.\n\n\nLecture\n   Tuesday and Thursday\n   1:25pm - 2:40pm\n   Old Chemistry 123 \n\n\nLabs\n   Fridays\n  10:05am - 11:20pm\n   LINK 088 (Clasroom 4) \n\n\nFinal Exam\n   December 15\n   9:00am - 12:00pm\n   Old Chemistry 123 \n\n\nInstructional Team and Office Hours\n\n\n\nRole\nName\nEmail\nOffice Hours\nLocation\n\n\n\n\nInstructor\nDr Merlise Clyde\n\nTues 2:45 - 3:45  or by appointment \n223E Old Chem\n\n\nTA\nBongjung Sung\n\nMon 9:30-11:30am\n203B Old Chem"
  },
  {
    "objectID": "index.html#course-topics",
    "href": "index.html#course-topics",
    "title": "STA721: Linear Models (Fall 2024)",
    "section": "Course Topics",
    "text": "Course Topics\nCourse topics will be drawn (but subject to change) from\n\nMotivation for Studying Linear Models as Foundation\nRandom Vectors and Matrices\nMultivariate Normal Distribution Theory\nConditional Normal Distribution Theory\nLinear Models via Coordinate free representations (examples)\nMaximum Likelihood Estimation & Projections\nInterval Estimation: Distribution of Quadratic Forms\nGauss-Markov Theorem & Optimality of OLS/GLS\nFormulation of Bayesian Inference\nSubjective and Default Priors\nRelated Shrinkage Methods and Penalized Likelihoods (Ridge regression, lasso)\nModel Selection (comparison of classical and Bayesian approaches)\nBayes Factors\nBayesian Model Averaging\nModel Checking: Residual Analysis & Diagnostics\nRobust Methods for Outliers\nGeneralized Linear Model\nHierarchical Models\n\nPlease check the website for updates, slides and current readings."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": " Resources",
    "section": "",
    "text": "Primary Textbooks\nThese textbooks are great resources for some of the topics we will cover.\n\nPlane Answers to Complex Questions, Ronald Christensen. eBook in Duke Library\nLinear Regression Analysis, George A.F Seber and Alan J. Lee eBook in Duke Library. Duke Library is aware the link is broken\nThe Linear Model and Hypothesis, George A. F. Seber eBook in Duke Library\n\n\n\nSupplementary Textbooks on Linear/Matrix Algebra\n\nGilbert Strang’s Online Course at MIT\nVideo Lectures\n\nIntroduction to Linear Algebra. Strang, Gilbert. 4th ed. Wellesley, MA: Wellesley-Cambridge Press, 2009. ISBN: 9780980232714. Buy @ Amazon\n\nMatrix Algebra from a Statistician’s Perspective. Harville, David A. eBook in Duke Library\n\n\n\nR and R Markdown Resources\nQuarto/R Markdown/LaTeX can be used to create high quality reports and presentations with embedded chunks of R code and LaTeX equations! You are required to use Quarto in RStudio to type up your homework assignments that involve Data Analysis/Simulation for this course, but you are welcome to use any word processor of your choice for those. To learn more about Quarto/R Markdown and for other resources for programming in R, see the links below.\n\nUsing R in Quarto for Documents\nR for Data Science (by Hadley Wickham & Garrett Grolemund)\nIntroduction to R Markdown (Article by Garrett Grolemund)\nIntroduction to R Markdown (Slides by Andrew Cho)\nR Markdown Cheat Sheet\nData Visualization with ggplot2 Cheat Sheet\nOther Useful Cheat Sheets\nA very (very!) basic R Markdown template\n\n\n\nLaTeX\nYou may also use LaTeX to type up your assignments. You may find it easier to create your TeX and LaTeX documents using online editors such as Overleaf (simply create a free account and you are good to go!). However, that need not be the case. If you prefer to create them locally/offline on your personal computers, you will need to download a TeX distribution (the most popular choices are MiKTeX for Windows and MacTeX for macOS) plus an editor (I personally prefer TeXstudio but feel free to download any editor of your choice). Follow the links below for some options, and to also learn how to use LaTeX.\n\nLearn LaTeX in 30 minutes\nChoosing a LaTeX Compiler.\n\n\n\nInteresting Articles\nI will add articles I find interesting below. These are articles I find useful as supplementary readings for topics covered in class, or as good sources that cover concepts I think you should know, but which we may not have time to cover. I strongly suggest you find time to (at the very least) take a “quick peek” at each article.\n\nEfron, B., 1986. Why isn’t everyone a Bayesian?. The American Statistician, 40(1), pp. 1-5.\nGelman, A., 2008. Objections to Bayesian statistics. Bayesian Analysis, 3(3), pp. 445-449.\nDiaconis, P., 1977. Finite forms of de Finetti’s theorem on exchangeability. Synthese, 36(2), pp. 271-281.\nGelman, A., Meng, X. L. and Stern, H., 1996. Posterior predictive assessment of model fitness via realized discrepancies. Statistica sinica, pp. 733-760.\nDunson, D. B., 2018. Statistics in the big data era: Failures of the machine. Statistics & Probability Letters, 136, pp. 4-9."
  },
  {
    "objectID": "reading/01-introduction.html",
    "href": "reading/01-introduction.html",
    "title": "Lecture 1 Readings",
    "section": "",
    "text": "links to eBooks are on the Home page of the website or Resources page."
  },
  {
    "objectID": "reading/01-introduction.html#introduction-to-linear-models",
    "href": "reading/01-introduction.html#introduction-to-linear-models",
    "title": "Lecture 1 Readings",
    "section": "Introduction to Linear Models",
    "text": "Introduction to Linear Models\n\nreview the course website and syllabus for policies"
  },
  {
    "objectID": "reading/01-introduction.html#vector-spaces",
    "href": "reading/01-introduction.html#vector-spaces",
    "title": "Lecture 1 Readings",
    "section": "Vector Spaces",
    "text": "Vector Spaces\nChristensen: Read\n\nChapter 1: pages 1-3\nAppendix A in Christensen pages 411-413\nAppendix B section B.1\n\nSee also Seber & Lee Chapter 1."
  },
  {
    "objectID": "reading/06-GLS.html",
    "href": "reading/06-GLS.html",
    "title": "Generalize Least Squares, MLEs, MVUEs and BUEs",
    "section": "",
    "text": "We will develop GLS/MLE estimators when the covariance of the errors is is not a scale multiple of the identity matrix, and establish conditions for when the OLS and GLS are equivalent. Inthe case that the errors have a multivatiate normal distribution, the MLEs are equivalent to GLS and are not only the BLUE but are also Minimum Variance Unbiased Estimators or “BUE” out of all unbiased estimators linear or non-linear.\nReadings:\n\nChristensen Chapter 2 and 10 Appendix B\nSeber & Lee Chapter 3"
  },
  {
    "objectID": "reading/03-non-full-rank.html",
    "href": "reading/03-non-full-rank.html",
    "title": "Rank Deficient Models",
    "section": "",
    "text": "We will explore MLEs/OLS using rank deficient models using generalized inverses. We will characterize orthogonal projections using Spectral Decompositions and review how that is related to the Singular Value Decomposition of the model matrix.\nReadings:\n\nChristensen Chapter 1-2, Appendix B\nSeber & Lee Chapter 3, Appendix A & Appendix B"
  },
  {
    "objectID": "HW/hw-01.html",
    "href": "HW/hw-01.html",
    "title": "Homework 1",
    "section": "",
    "text": "See Gradescope for any updates on due dates."
  },
  {
    "objectID": "HW/hw-01.html#due-1100pm-thurs-sept-5",
    "href": "HW/hw-01.html#due-1100pm-thurs-sept-5",
    "title": "Homework 1",
    "section": "",
    "text": "See Gradescope for any updates on due dates."
  },
  {
    "objectID": "HW/hw-01.html#rstudio",
    "href": "HW/hw-01.html#rstudio",
    "title": "Homework 1",
    "section": "RStudio",
    "text": "RStudio\nYou all should have R and RStudio installed on your computers by now or access to the Department RStudio Server https://rstudio.stat.duke.edu. If you need to setup your local version, first install the latest version of R here: https://cran.rstudio.com - remember to select the right installer for your operating system). Next, install the latest version of RStudio here: https://www.rstudio.com/products/rstudio/download/. Scroll down to the “Installers for Supported Platforms” section and find the right installer for your operating system."
  },
  {
    "objectID": "HW/hw-01.html#r-quarto",
    "href": "HW/hw-01.html#r-quarto",
    "title": "Homework 1",
    "section": "R & Quarto",
    "text": "R & Quarto\nYou are required to use the .qmd format to type up this report report. To get started see technical writing with Quarto. Feel free to include images of technical derivations if you are short of time to typeset in LaTeX. The macros.tex file should help with shortcuts."
  },
  {
    "objectID": "HW/hw-01.html#getting-started-with-github-classroom",
    "href": "HW/hw-01.html#getting-started-with-github-classroom",
    "title": "Homework 1",
    "section": "Getting started with Github Classroom",
    "text": "Getting started with Github Classroom\nGo to Github classroom to accept the invitation to create a repository for this assignment at HW1\nThis will create a private repo in the STA721-F24 organization on Github with your github user name. Note: You may receive an email invitation to join STA702-F24 on your behalf.\nHit refresh, to see the link for the repo, and then click on it to go to the STA721-F24 organization in Github.\nFollow the instructions in the README in your repo and in the hw1.qmd file to complete the assignment and push to GitHub. If you encounter issues with Gradescope submission, it is critical that you have pushed your final assignment to GitHub by the due date to validate timestamps."
  },
  {
    "objectID": "HW/hw-01.html#gradescope-submission",
    "href": "HW/hw-01.html#gradescope-submission",
    "title": "Homework 1",
    "section": "Gradescope Submission",
    "text": "Gradescope Submission\nYou MUST submit both your final .qmd .pdf files to the repo on Github and upload the pdf to Gradescope here: https://www.gradescope.com/courses/843802/assignments\nMake sure to Render to pdf; ask the TA about knitting to pdf if you cannot figure it out. Be sure to submit under the right assignment entry by the due date!"
  },
  {
    "objectID": "HW/hw-01.html#grading",
    "href": "HW/hw-01.html#grading",
    "title": "Homework 1",
    "section": "Grading",
    "text": "Grading\nTotal: 20 points."
  },
  {
    "objectID": "labs/lab-01.html",
    "href": "labs/lab-01.html",
    "title": "lab-01",
    "section": "",
    "text": "Please see the instructions for HW-01 and be preapred to ask questions in lab (using R, theory, etc)"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#outline",
    "href": "resources/slides/04-BLUE.html#outline",
    "title": "Best Linear Unbiased Estimators",
    "section": "Outline",
    "text": "Outline\n\nCharacterizing Linear Unbiased Estimators\nGauss-Markov Theorem\nBest Linear Unbiased Estimators\n\n\nReadings: - Christensen Chapter 1-2 and Appendix B - Seber & Lee Chapter 3"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#full-rank-case",
    "href": "resources/slides/04-BLUE.html#full-rank-case",
    "title": "Best Linear Unbiased Estimators",
    "section": "Full Rank Case",
    "text": "Full Rank Case\n\nModel: \\(\\mathbf{Y}= \\boldsymbol{\\mu}+ \\boldsymbol{\\epsilon}\\)\nMinimal Assumptions:\n\nMean \\(\\boldsymbol{\\mu}\\in C(\\mathbf{X})\\) for \\(\\mathbf{X}\\in \\mathbb{R}^{n \\times p}\\)\nErrors \\(\\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\)\n\n\n\n\nDefinition: Linear Unbiased Estimators (LUEs)\nAn estimator \\(\\tilde{\\boldsymbol{\\beta}}\\) is a Linear Unbiased Estimator (LUE) of \\(\\boldsymbol{\\beta}\\) if\n\nlinearity: \\(\\tilde{\\boldsymbol{\\beta}}= \\mathbf{A}\\mathbf{Y}\\) for \\(\\mathbf{A}\\in \\mathbb{R}^{p \\times n}\\)\nunbiasedness: \\(\\textsf{E}[\\tilde{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\\) for all \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\)\n\n\n\n\n\nThe class of linear unbiased estimators is the same for every model with parameter space \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\) and \\(P \\in \\cal{P}\\), for any collection \\(\\cal{P}\\) of mean-zero distributions over \\(\\mathbb{R}^n\\)."
  },
  {
    "objectID": "resources/slides/04-BLUE.html#linear-unbiased-estimators-lues-1",
    "href": "resources/slides/04-BLUE.html#linear-unbiased-estimators-lues-1",
    "title": "Best Linear Unbiased Estimators",
    "section": "Linear Unbiased Estimators (LUEs)",
    "text": "Linear Unbiased Estimators (LUEs)\n\nLet \\(\\textsf{N}\\) be an ONB for \\(\\boldsymbol{{\\cal N}}= \\boldsymbol{{\\cal M}}^\\perp = N(\\mathbf{X}^T)\\):\n\n\\(\\textsf{N}^T\\mathbf{m}=  \\textsf{N}^T\\mathbf{X}\\mathbf{b}= \\mathbf{0}\\quad \\forall \\mathbf{m}=\\mathbf{X}\\mathbf{b}\\in \\boldsymbol{{\\cal M}}\\)\n\\(\\textsf{N}^T\\textsf{N}= \\mathbf{I}_{n-p}\\)\n\n\n\nConsider another linear estimator \\(\\tilde{\\boldsymbol{\\beta}}= \\mathbf{A}\\mathbf{Y}\\)\n\nDifference between \\(\\tilde{\\boldsymbol{\\beta}}\\) and \\(\\hat{\\boldsymbol{\\beta}}\\) (OLS/MLE): \\[\\begin{align*}\n  \\mathbf{\\delta}= \\tilde{\\boldsymbol{\\beta}}- \\hat{\\boldsymbol{\\beta}}& = \\left(\\mathbf{A}- (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\right)\\mathbf{Y}\\\\\n                  & \\equiv \\mathbf{H}^T \\mathbf{Y}\n\\end{align*}\\]\nSince both \\(\\tilde{\\boldsymbol{\\beta}}\\) and \\(\\hat{\\boldsymbol{\\beta}}\\) are unbiased, \\(\\textsf{E}[\\mathbf{\\delta}] = \\mathbf{0}_p \\quad \\forall \\boldsymbol{\\beta}\\in \\mathbb{R}^p\\) \\[\\mathbf{0}_p = \\textsf{E}[\\mathbf{H}^T \\mathbf{Y}] = \\mathbf{H}^T \\mathbf{X}\\boldsymbol{\\beta}\\quad \\forall \\boldsymbol{\\beta}\\in \\mathbb{R}^p\\]\n\\(\\mathbf{X}^T \\mathbf{H}= \\mathbf{0}\\) so each column of \\(\\mathbf{H}\\) is in \\(\\boldsymbol{{\\cal M}}^\\perp \\equiv \\boldsymbol{{\\cal N}}\\)"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#lues-continued",
    "href": "resources/slides/04-BLUE.html#lues-continued",
    "title": "Best Linear Unbiased Estimators",
    "section": "LUEs continued",
    "text": "LUEs continued\nSince each column of \\(\\mathbf{H}\\) is in \\(\\boldsymbol{{\\cal N}}\\) there exists a \\(\\mathbf{G}\\in \\mathbb{R}^{p \\times (n-p)} \\ni \\mathbf{H}= \\textsf{N}\\mathbf{G}^T\\)\n\nRewriting \\(\\mathbf{\\delta}= \\tilde{\\boldsymbol{\\beta}}- \\hat{\\boldsymbol{\\beta}}\\): \\[\\begin{align*}\n\\tilde{\\boldsymbol{\\beta}}& = \\hat{\\boldsymbol{\\beta}}+ \\mathbf{\\delta}\\\\\n         & = \\hat{\\boldsymbol{\\beta}}+ \\mathbf{H}^T\\mathbf{Y}\\\\\n         & = \\hat{\\boldsymbol{\\beta}}+ \\mathbf{G}\\textsf{N}^T\\mathbf{Y}\n\\end{align*}\\]\n\ntherefore \\(\\tilde{\\boldsymbol{\\beta}}\\) is linear and unbiased: \\[\\begin{align*}\n\\textsf{E}[\\tilde{\\boldsymbol{\\beta}}] & = \\textsf{E}[\\hat{\\boldsymbol{\\beta}}+ \\mathbf{G}\\textsf{N}^T\\mathbf{Y}] \\\\\n           & =  \\textsf{E}[\\hat{\\boldsymbol{\\beta}}] + \\textsf{E}[\\mathbf{G}\\textsf{N}^T\\mathbf{Y}] \\\\\n           & =  \\boldsymbol{\\beta}+ \\mathbf{G}\\textsf{N}^T\\mathbf{X}\\boldsymbol{\\beta}\\\\\n           & = \\boldsymbol{\\beta}\n\\end{align*}\\]"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#characterization-of-lues",
    "href": "resources/slides/04-BLUE.html#characterization-of-lues",
    "title": "Best Linear Unbiased Estimators",
    "section": "Characterization of LUEs",
    "text": "Characterization of LUEs\nSummary of previous results:\n\nTheoremAn estimator \\(\\tilde{\\boldsymbol{\\beta}}\\) is a linear unbiased estimator of \\(\\boldsymbol{\\beta}\\) in a linear statistical model if and only if \\[\\tilde{\\boldsymbol{\\beta}}= \\hat{\\boldsymbol{\\beta}}+ \\mathbf{H}^T\\mathbf{Y}\\] for some \\(\\mathbf{H}\\in \\mathbb{R}^{n \\times p}\\) such that \\(\\mathbf{X}^T \\mathbf{H}= \\mathbf{0}\\) or equivalently for some \\(\\mathbf{G}\\in \\mathbb{R}^{p \\times (n-p)}\\) \\[\\tilde{\\boldsymbol{\\beta}}= \\hat{\\boldsymbol{\\beta}}+ \\mathbf{G}\\textsf{N}^T\\mathbf{Y}\\]"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#numerical",
    "href": "resources/slides/04-BLUE.html#numerical",
    "title": "Best Linear Unbiased Estimators",
    "section": "Numerical",
    "text": "Numerical\n\n# X is model matrix; Y is response\n  p = ncol(X)\n  n = nrow(X)\n  G = matrix(rnorm(p*(n-p)), nrow=p, ncol=n-p)\n  H = MASS::Null(X) %*% t(G)\n  btilde = bhat + t(H) %*% Y\n\ninfinite number of LUEs!"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#lues-via-generalized-inverses",
    "href": "resources/slides/04-BLUE.html#lues-via-generalized-inverses",
    "title": "Best Linear Unbiased Estimators",
    "section": "LUEs via Generalized Inverses",
    "text": "LUEs via Generalized Inverses\nLet \\(\\tilde{\\boldsymbol{\\beta}}= \\mathbf{A}\\mathbf{Y}\\) be a LUE in the statistical linear model \\(\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\) with \\(\\mathbf{X}\\) full column rank \\(p\\) \\[\\begin{align*}\n\\textsf{E}[\\tilde{\\boldsymbol{\\beta}}] & = \\textsf{E}[\\mathbf{A}\\mathbf{Y}] \\\\\n            & = \\mathbf{A}\\textsf{E}[\\mathbf{Y}] \\\\\n            & = \\mathbf{A}\\mathbf{X}\\boldsymbol{\\beta}\\quad \\forall \\boldsymbol{\\beta}\\in \\mathbb{R}^p\n\\end{align*}\\]\n\nMust have \\(\\mathbf{A}\\mathbf{X}= \\mathbf{I}_p\\) (\\(\\mathbf{A}\\) is a generalized inverse of \\(\\mathbf{X}\\))\n\\(\\mathbf{X}\\mathbf{X}^- \\mathbf{X}= \\mathbf{X}\\)\none generalized inverse is \\(\\mathbf{X}_{MP}^- = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\)\n\\(\\mathbf{X}_{MP}^- = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T = \\mathbf{V}\\boldsymbol{\\Delta}^{-1} \\mathbf{U}^T\\) (using SVD of \\(\\mathbf{X}= \\mathbf{U}\\boldsymbol{\\Delta}\\mathbf{V}^T\\))\n\\(\\mathbf{A}\\) is a generalized inverse of \\(\\mathbf{X}\\) iff \\(\\mathbf{A}= \\mathbf{X}_{MP}^- + \\mathbf{H}^T\\) for \\(\\mathbf{H}\\in \\mathbb{R}^{n \\times p} \\ni \\mathbf{H}^T \\mathbf{U}= \\mathbf{0}\\)\n\\(\\mathbf{A}\\mathbf{Y}= (\\mathbf{X}_{MP}^- + \\mathbf{H}^T)\\mathbf{Y}= \\hat{\\boldsymbol{\\beta}}+  \\mathbf{H}^T \\mathbf{Y}\\)"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#best-linear-unbiased-estimators",
    "href": "resources/slides/04-BLUE.html#best-linear-unbiased-estimators",
    "title": "Best Linear Unbiased Estimators",
    "section": "Best Linear Unbiased Estimators",
    "text": "Best Linear Unbiased Estimators\n\nthe distribution of values of any unbiased estimator is centered around \\(\\boldsymbol{\\beta}\\)\nout of the infinite number of LUEs is there one that is more concentrated around \\(\\boldsymbol{\\beta}\\)?\nis there an unbiased estimator that has a lower variance than all other unbiased estimators?\nRecall variance-covariance matrix of a random vector \\(\\mathbf{Z}\\) with mean \\(\\boldsymbol{\\theta}\\) \\[\\begin{align*}\n\\textsf{Cov}[\\mathbf{Z}]      & \\equiv \\textsf{E}[(\\mathbf{Z}- \\boldsymbol{\\theta})(\\mathbf{Z}- \\boldsymbol{\\theta})^T] \\\\\n\\textsf{Cov}[\\mathbf{Z}]_{ij} &  =     \\textsf{E}[(z_i - \\theta_i)(z_j - \\theta_j)]\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\nLemma\n\n\nLet \\(\\mathbf{A}\\in \\mathbb{R}^{q \\times p}\\) and \\(\\mathbf{b}\\in \\mathbb{R}^q\\) with \\(\\mathbf{Z}\\) a random vector in \\(\\mathbb{R}^p\\) then \\[\\textsf{Cov}[\\mathbf{A}\\mathbf{Z}+ \\mathbf{b}] = \\mathbf{A}\\textsf{Cov}[\\mathbf{Z}] \\mathbf{A}^T \\ge 0\\]"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#variance-of-linear-unbiased-estimators",
    "href": "resources/slides/04-BLUE.html#variance-of-linear-unbiased-estimators",
    "title": "Best Linear Unbiased Estimators",
    "section": "Variance of Linear Unbiased Estimators",
    "text": "Variance of Linear Unbiased Estimators\nLet’s look at the variance of any LUE under assumption \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\sigma^2 \\mathbf{I}_n\\)\n\nfor \\(\\hat{\\boldsymbol{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{Y}= \\boldsymbol{\\beta}+ (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\boldsymbol{\\epsilon}\\) \\[\\begin{align*}\n\\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}] & = \\textsf{Cov}[\\boldsymbol{\\beta}+ (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\boldsymbol{\\epsilon}] \\\\\n          & =  (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\textsf{Cov}[\\boldsymbol{\\epsilon}] \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n          & = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n          & = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1}\n\\end{align*}\\]\nCovariance is increasing in \\(\\sigma^2\\) and generally decreasing in \\(n\\)\nRewrite \\(\\mathbf{X}^T\\mathbf{X}\\) as \\(\\mathbf{X}^T\\mathbf{X}= \\sum_{i=1}^n \\mathbf{x}_i \\mathbf{x}_i^T\\) (a sum of \\(n\\) outer-products)"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#variance-of-arbitrary-lue",
    "href": "resources/slides/04-BLUE.html#variance-of-arbitrary-lue",
    "title": "Best Linear Unbiased Estimators",
    "section": "Variance of Arbitrary LUE",
    "text": "Variance of Arbitrary LUE\n\nfor \\(\\tilde{\\boldsymbol{\\beta}}= \\left((\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T + \\mathbf{H}^T \\right)\\mathbf{Y}= \\boldsymbol{\\beta}+ \\left((\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T + \\mathbf{H}^T \\right)\\boldsymbol{\\epsilon}\\)\nrecall \\(\\mathbf{X}_{MP}^- \\equiv  (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\) \\[\\begin{align*}\n\\textsf{Cov}[\\tilde{\\boldsymbol{\\beta}}] & = \\textsf{Cov}[\\left(\\mathbf{X}_{MP}^- + \\mathbf{H}^T \\right)\\boldsymbol{\\epsilon}]  \\\\\n            & = \\sigma^2 \\left(\\mathbf{X}_{MP}^- + \\mathbf{H}^T \\right)\\left(\\mathbf{X}_{MP}^- + \\mathbf{H}^T \\right)^T \\\\\n            & = \\sigma^2\\left( \\mathbf{X}_{MP}^-(\\mathbf{X}_{MP}^-)^T + \\mathbf{X}_{MP}^-\\mathbf{H}+\n                \\mathbf{H}^T (\\mathbf{X}_{MP}^-)^T + \\mathbf{H}^T \\mathbf{H}\\right) \\\\\n            & =   \\sigma^2\\left( (\\mathbf{X}^T\\mathbf{X})^{-1} +  \\mathbf{H}^T \\mathbf{H}\\right)\n\\end{align*}\\]\nCross-product term \\(\\mathbf{H}^T(\\mathbf{X}_{MP}^-)^T = \\mathbf{H}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} =  \\mathbf{0}\\)\nTherefor the \\(\\textsf{Cov}[\\tilde{\\boldsymbol{\\beta}}] = \\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}] + \\mathbf{H}^T\\mathbf{H}\\)\nthe sum of a positive definite matrix plus a positive semi-definite matrix"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#gauss-markov-theorem",
    "href": "resources/slides/04-BLUE.html#gauss-markov-theorem",
    "title": "Best Linear Unbiased Estimators",
    "section": "Gauss-Markov Theorem",
    "text": "Gauss-Markov Theorem\nIs \\(\\textsf{Cov}[\\tilde{\\boldsymbol{\\beta}}] \\ge \\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}]\\) in some sense?\n\n\nDefinition: Loewner OrderingFor two positive semi-definite matrices \\(\\boldsymbol{\\Sigma}_1\\) and \\(\\boldsymbol{\\Sigma}_2\\), we say that \\(\\boldsymbol{\\Sigma}_1 &gt; \\boldsymbol{\\Sigma}_2\\) if \\(\\boldsymbol{\\Sigma}_1 - \\boldsymbol{\\Sigma}_2\\) is positive definite, \\(\\mathbf{x}^T(\\boldsymbol{\\Sigma}_1 - \\boldsymbol{\\Sigma}_2)\\mathbf{x}) &gt; 0\\), and \\(\\boldsymbol{\\Sigma}_1 \\ge \\boldsymbol{\\Sigma}_2\\) if \\(\\boldsymbol{\\Sigma}_1 - \\boldsymbol{\\Sigma}_2\\) is positive semi-definite, \\(\\mathbf{x}^T(\\boldsymbol{\\Sigma}_1 - \\boldsymbol{\\Sigma}_2)\\mathbf{x}) \\ge 0\\)\n\n\n\nSince \\(\\textsf{Cov}[\\tilde{\\boldsymbol{\\beta}}] - \\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}] = \\mathbf{H}^T\\mathbf{H}\\), we have that \\(\\textsf{Cov}[\\tilde{\\boldsymbol{\\beta}}] \\ge \\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}]\\)\n\n\n\n\nTheorem: Gauss-MarkovLet \\(\\tilde{\\boldsymbol{\\beta}}\\) be a linear unbiased estimator of \\(\\boldsymbol{\\beta}\\) in a linear model where \\(\\textsf{E}[\\mathbf{Y}] = \\mathbf{X}\\boldsymbol{\\beta}, \\boldsymbol{\\beta}\\in \\mathbb{R}^p\\), \\(\\mathbf{X}\\) rank \\(p\\), and \\(\\textsf{Cov}[\\mathbf{Y}] = \\sigma^2\\mathbf{I}_n, \\sigma^2 &gt; 0\\). Then \\(\\textsf{Cov}[\\tilde{\\boldsymbol{\\beta}}] \\ge \\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}]\\) where \\(\\hat{\\boldsymbol{\\beta}}\\) is the OLS estimator and is the Best Linear Unbiased Estimator (BLUE) of \\(\\boldsymbol{\\beta}\\)."
  },
  {
    "objectID": "resources/slides/04-BLUE.html#slide13-id",
    "href": "resources/slides/04-BLUE.html#slide13-id",
    "title": "Best Linear Unbiased Estimators",
    "section": "",
    "text": "Theorem: Gauss-Markov Theorem (Classic)For \\(\\mathbf{Y}= \\boldsymbol{\\mu}+ \\boldsymbol{\\epsilon}\\), with \\(\\boldsymbol{\\mu}\\in \\boldsymbol{{\\cal M}}\\), \\(\\textsf{E}[\\boldsymbol{\\epsilon}]= \\mathbf{0}_n\\) and \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] =\\sigma^2 \\mathbf{I}_n\\) and \\(\\mathbf{P}\\) the orthogonal projection onto \\(\\boldsymbol{{\\cal M}}\\), \\(\\mathbf{P}\\mathbf{Y}= \\hat{\\boldsymbol{\\mu}}\\) is the BLUE of \\(\\boldsymbol{\\mu}\\) out of the class of LUEs \\(\\mathbf{A}\\mathbf{Y}\\) where \\(\\textsf{E}[\\mathbf{A}\\mathbf{Y}] = \\boldsymbol{\\mu}\\), \\(\\mathbf{A}\\in \\mathbb{R}^{n \\times n}\\) equality iff \\(\\mathbf{A}= \\mathbf{P}\\)\n\n\n\nProof\n\nwrite \\(\\mathbf{A}= \\mathbf{P}+ \\mathbf{H}^T\\) so \\(\\mathbf{H}^T = \\mathbf{A}- \\mathbf{P}\\)\nsince \\(\\mathbf{A}\\boldsymbol{\\mu}= \\boldsymbol{\\mu}\\), \\(\\mathbf{H}^T\\mu = \\mathbf{0}_n\\) for \\(\\mu \\in \\boldsymbol{{\\cal M}}\\) and \\(\\mathbf{H}^T \\mathbf{P}= \\mathbf{P}\\mathbf{H}= \\mathbf{0}\\) (columns of \\(\\mathbf{H}\\in \\boldsymbol{{\\cal M}}^\\perp\\)) \\[\\begin{align*}\n\\textsf{E}[\\|\\mathbf{A}\\mathbf{Y}- \\boldsymbol{\\mu}\\|^2]  & =  \\textsf{E}[\\|\\mathbf{P}(\\mathbf{Y}- \\boldsymbol{\\mu}) + \\mathbf{H}^T(\\mathbf{Y}- \\boldsymbol{\\mu})\\|^2]  \\\\\n& = \\textsf{E}[\\|\\mathbf{P}(\\mathbf{Y}- \\boldsymbol{\\mu})\\|^2] + \\underbrace{\\textsf{E}[\\|\\mathbf{H}^T(\\mathbf{Y}- \\boldsymbol{\\mu})\\|^2]} + \\underbrace{{\\text{cross-product}}} \\\\\n& \\hspace{4.35in} \\ge 0 \\quad  + \\hspace{1.25in} 0\\\\\n& \\ge \\textsf{E}[\\|\\mathbf{P}(\\mathbf{Y}- \\boldsymbol{\\mu})\\|^2]\n\\end{align*}\\]\nCross-product is \\(2\\textsf{E}[(\\mathbf{H}^T(\\mathbf{Y}- \\mu))^T\\mathbf{P}(\\mathbf{Y}- \\boldsymbol{\\mu})] = 0\\) (see last slide)"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#estimation-of-linear-functionals-of-boldsymbolmu",
    "href": "resources/slides/04-BLUE.html#estimation-of-linear-functionals-of-boldsymbolmu",
    "title": "Best Linear Unbiased Estimators",
    "section": "Estimation of Linear Functionals of \\(\\boldsymbol{\\mu}\\)",
    "text": "Estimation of Linear Functionals of \\(\\boldsymbol{\\mu}\\)\nIf \\(\\mathbf{P}\\mathbf{Y}= \\hat{\\boldsymbol{\\mu}}\\) is the BLUE of \\(\\boldsymbol{\\mu}\\), is \\(\\mathbf{B}\\mathbf{P}\\mathbf{Y}= \\mathbf{B}\\hat{\\boldsymbol{\\mu}}\\) the BLUE of \\(\\mathbf{B}\\boldsymbol{\\mu}\\)?\n\nYes! Similar proof as above to show that out of the class of LUEs \\(\\mathbf{A}\\mathbf{Y}\\) of \\(\\mathbf{B}\\boldsymbol{\\mu}\\) where \\(\\mathbf{A}\\in \\mathbb{R}^{d \\times n}\\) that \\[\\textsf{E}[\\|\\mathbf{A}\\mathbf{Y}- \\mathbf{B}\\boldsymbol{\\mu}\\|^2] \\ge \\textsf{E}[\\|\\mathbf{B}\\mathbf{P}\\mathbf{Y}- \\mathbf{B}\\boldsymbol{\\mu}\\|^2]\\] with equality iff \\(\\mathbf{A}= \\mathbf{B}\\mathbf{P}\\).\n\n\nWhat about linear functionals of \\(\\boldsymbol{\\beta}\\), \\(\\boldsymbol{\\Lambda}^T \\boldsymbol{\\beta}\\), for \\(\\mathbf{X}\\) rank \\(r \\le p\\)?\n\n\\(\\hat{\\boldsymbol{\\beta}}\\) is not unique if \\(r &lt; p\\) even though \\(\\hat{\\boldsymbol{\\mu}}\\) is unique (\\(\\hat{\\boldsymbol{\\beta}}\\) is not BLUE)\nSince \\(\\mathbf{B}\\boldsymbol{\\mu}= \\mathbf{B}\\mathbf{X}\\boldsymbol{\\beta}\\) is always identifiable, the only linear functions of \\(\\boldsymbol{\\beta}\\) that are identifiable and can be estimated uniquely are functions of \\(\\mathbf{X}\\boldsymbol{\\beta}\\), i.e. estimates in the form \\(\\boldsymbol{\\Lambda}^T \\boldsymbol{\\beta}= \\mathbf{B}\\mathbf{X}\\boldsymbol{\\beta}\\) or \\(\\boldsymbol{\\Lambda}= \\mathbf{X}^T \\mathbf{B}^T\\).\ncolumns of \\(\\boldsymbol{\\Lambda}\\) must be in the \\(C(\\mathbf{X}^T)\\)\ndetailed discussion and proof in Christensen Ch. 2 for scalar functionals \\(\\lambda^T\\beta\\)."
  },
  {
    "objectID": "resources/slides/04-BLUE.html#blue-of-boldsymbollambdat-boldsymbolbeta",
    "href": "resources/slides/04-BLUE.html#blue-of-boldsymbollambdat-boldsymbolbeta",
    "title": "Best Linear Unbiased Estimators",
    "section": "BLUE of \\(\\boldsymbol{\\Lambda}^T \\boldsymbol{\\beta}\\)",
    "text": "BLUE of \\(\\boldsymbol{\\Lambda}^T \\boldsymbol{\\beta}\\)\nIf \\(\\boldsymbol{\\Lambda}^T= \\mathbf{B}\\mathbf{X}\\) for some matrix \\(\\mathbf{B}\\) then\n\n\\(\\textsf{E}[\\mathbf{B}\\mathbf{P}\\mathbf{Y}] = \\textsf{E}[\\boldsymbol{\\Lambda}^T \\hat{\\boldsymbol{\\beta}}] = \\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\)\nThe unique OLS estimate of \\(\\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\) is \\(\\boldsymbol{\\Lambda}^T\\hat{\\boldsymbol{\\beta}}\\)\n\\(\\mathbf{B}\\mathbf{P}\\mathbf{Y}= \\boldsymbol{\\Lambda}^T\\hat{\\boldsymbol{\\beta}}\\) is the BLUE of \\(\\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\) \\[\\begin{align*}\n& \\textsf{E}[\\|\\mathbf{B}\\mathbf{P}\\mathbf{Y}- \\mathbf{B}\\boldsymbol{\\mu}\\|^2]  \\le \\textsf{E}[\\|\\mathbf{A}\\mathbf{Y}- \\mathbf{B}\\boldsymbol{\\mu}\\|^2] \\\\\n\\Leftrightarrow & \\\\\n& \\textsf{E}[\\|\\boldsymbol{\\Lambda}^T\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta})\\|^2]  \\le \\textsf{E}[\\|\\mathbf{L}^T\\tilde{\\boldsymbol{\\beta}}- \\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\|^2]\n\\end{align*}\\] for LUE \\(\\mathbf{A}\\mathbf{Y}= \\mathbf{L}^T\\tilde{\\boldsymbol{\\beta}}\\) of \\(\\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\)\nProof proceeds as the classic case."
  },
  {
    "objectID": "resources/slides/04-BLUE.html#proof-of-cross-product",
    "href": "resources/slides/04-BLUE.html#proof-of-cross-product",
    "title": "Best Linear Unbiased Estimators",
    "section": "Proof of Cross-Product",
    "text": "Proof of Cross-Product\nLet \\(\\mathbf{D}= \\mathbf{H}\\mathbf{P}\\) and write \\[\\begin{align*}\n\\textsf{E}[(\\mathbf{H}^T(\\mathbf{Y}- \\mu))^T\\mathbf{P}(\\mathbf{Y}- \\boldsymbol{\\mu})] & = \\textsf{E}[(\\mathbf{Y}- \\mu))^T\\mathbf{H}\\mathbf{P}(\\mathbf{Y}- \\boldsymbol{\\mu})] \\\\\n& = \\textsf{E}[(\\mathbf{Y}- \\mu))^T\\mathbf{D}(\\mathbf{Y}- \\boldsymbol{\\mu})]\n\\end{align*}\\]\n\n\\[\\begin{align*}\n\\textsf{E}[(\\mathbf{Y}- \\mu))^T\\mathbf{D}(\\mathbf{Y}- \\boldsymbol{\\mu})] = & \\textsf{E}[\\textsf{tr}(\\mathbf{Y}- \\mu))^T\\mathbf{D}(\\mathbf{Y}- \\boldsymbol{\\mu}))]  \\\\\n= & \\textsf{E}[\\textsf{tr}(\\mathbf{D}(\\mathbf{Y}- \\boldsymbol{\\mu})(\\mathbf{Y}- \\mu)^T)] \\\\\n= & \\textsf{tr}(\\textsf{E}[\\mathbf{D}(\\mathbf{Y}- \\boldsymbol{\\mu})(\\mathbf{Y}- \\mu)^T]) \\\\\n= & \\textsf{tr}(\\mathbf{D}\\textsf{E}[(\\mathbf{Y}- \\boldsymbol{\\mu})(\\mathbf{Y}- \\mu)^T]) \\\\\n  = & \\sigma^2 \\textsf{tr}(\\mathbf{D}\\mathbf{I}_n)\\\\\n\\end{align*}\\]\n\n\nSince \\(\\textsf{tr}(\\mathbf{D}) = \\textsf{tr}(\\mathbf{H}\\mathbf{P}) = \\textsf{tr}(\\mathbf{P}\\mathbf{H})\\) we can conclude that the cross-product term is zero.\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/example.html",
    "href": "resources/slides/example.html",
    "title": "Custom blocks and crossreferencing",
    "section": "",
    "text": "Custom blocks and crossreferencing\nWith this filter, you can define custom div classes (environments) that come with numbering, such as theorems, examples, exercises. The filter supports output formats pdf and html.\n\n\nNumbering\nNumbering is (currently) within section for single documents, or within chapter for books. Grouped classes share the same counter, and the same default style.\nNumbered custom blocks can be cross-referenced with \\ref.\nDefault numbering can be switched off for the whole class by setting the numbered to false, or for an individual block by adding the class unnumbered.\nCrossreferences my need a re-run to update.\n\n\nBoxes can be nested\nHowever, inner boxes are not numbered – it would be hard to put them in a sequence with outer boxes anyway.\n\n\n\nThe default style for custom divs is foldbox: a collapsible similar to quarto’s callouts, with a collapse button at the bottom that makes it easier collapse long boxes, and box open to the right. It comes with the variant foldbox.simple, with closed box and no additional close button. (needs a fix for the moment)\n\n\nCustom styles\n\ncreate an API for user defined block styles\nprovide an example\nand documentation\n\n\n\nCustom list of blocks\nGenerate .qmd files that contains a list of selected block classes, intermitted with headers from the document for easy customization and commenting. This way, one can make a list of all definitions, examples, or {theorems, propositions and lemmas} etc., edit it later and attach to the main document. If you edit, make sure to rename the autogenerated list first, otherwise it will get overwritten in the next run and all work is lost …\nCurrently, you need to give a key listin for any class or group of classes that should appear in a list of things. The value of this key is an array of names, also if only one list is to be generated. These names are turned into files list-of-name.qmd. I am considering replacing the yaml interface by a sub-key to custom-numbered-classes. This would allow to define arbitrary classes that can be attached to any custom div block, such as .important.\n\n\n\nPseudomath examples\n\n\nF\\(\\alpha\\)ncybox\nA box is called f\\(\\alpha\\)ncybox if it looks quite fancy.\nIn this context, by fancy we mean that the title of the box appears as a clickable button when rendered as html, where clickable implies that it throws a small shadow that becomes bigger when hovering over it.\n\n\n\nBy Definition \\(\\ref{fancy}\\), foldboxes are fancyboxes.\n\n\nStudents are lured into clicking on the title and unfolding the fancybox.\n\n\nThis extension has been written by a teacher who hopes that students read the course notes…\n\nTheorem \\(\\ref{TeacherHopes}\\) is suggested by Conjecture \\(\\ref{TeachersHope}\\), but it cannot be proven theoretically. It does give rise to more conjectures, though.\n\nThe teacher mentioned in Theorem \\(\\ref{TeacherHopes}\\) is a statistician who got addicted to quarto due to James J Balamuta’s web-r extension, and desparately wanted to have a common counter for theorem and alike. She got also convinced that everything is possible in quarto by the many nice extensions from Shafayet Khan Shafee."
  },
  {
    "objectID": "resources/slides/template.html#outline",
    "href": "resources/slides/template.html#outline",
    "title": "template",
    "section": "Outline",
    "text": "Outline\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#outline",
    "href": "resources/slides/03-non-full-rank.html#outline",
    "title": "Rank Deficient Models",
    "section": "Outline",
    "text": "Outline\n\nRank Deficient Models\nGeneralized Inverses, Projections and MLEs/OLS\nClass of Unbiased Estimators\n\n\nReadings: - Christensen Chapter 2 and Appendix B - Seber & Lee Chapter 3"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#geometric-view",
    "href": "resources/slides/03-non-full-rank.html#geometric-view",
    "title": "Rank Deficient Models",
    "section": "Geometric View",
    "text": "Geometric View"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#non-full-rank-case",
    "href": "resources/slides/03-non-full-rank.html#non-full-rank-case",
    "title": "Rank Deficient Models",
    "section": "Non-Full Rank Case",
    "text": "Non-Full Rank Case\n\nModel: \\(\\mathbf{Y}= \\boldsymbol{\\mu}+ \\boldsymbol{\\epsilon}\\)\nAssumption: \\(\\boldsymbol{\\mu}\\in C(\\mathbf{X})\\) for \\(\\mathbf{X}\\in \\mathbb{R}^{n \\times p}\\)\nWhat if the rank of \\(\\mathbf{X}\\), \\(r(\\mathbf{X}) \\equiv r \\ne p\\)?\nStill have result that the OLS/MLE solution satisfies \\[\\mathbf{P}_\\mathbf{X}\\mathbf{Y}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\]\nHow can we characterize \\(\\mathbf{P}\\) and \\(\\hat{\\boldsymbol{\\beta}}\\) in this case? 2 cases\n\n\n\n\\(p \\le n\\), \\(r(\\mathbf{X}) \\ne p\\) \\(\\Rightarrow r(\\mathbf{X}) &lt; p\\)\n\\(p \\gt n\\), \\(r(\\mathbf{X}) \\ne p\\)\n\n\n\nFocus on the first case for OLS/MLE for now…"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#model-space",
    "href": "resources/slides/03-non-full-rank.html#model-space",
    "title": "Rank Deficient Models",
    "section": "Model Space",
    "text": "Model Space\n\n\\(\\boldsymbol{{\\cal M}}= C(\\mathbf{X})\\) is an \\(r\\)-dimensional subspace of \\(\\mathbb{R}^n\\)\n\\(\\boldsymbol{{\\cal M}}\\) has an \\((n - r)\\)-dimensional orthogonal complement \\(\\boldsymbol{{\\cal N}}\\)\neach \\(\\mathbf{y}\\in \\mathbb{R}^n\\) has a unique representation as \\[ \\mathbf{y}= \\hat{\\mathbf{y}}+ \\mathbf{e}\\] for \\(\\hat{\\mathbf{y}}\\in \\boldsymbol{{\\cal M}}\\) and \\(\\mathbf{e}\\in \\boldsymbol{{\\cal N}}\\)\n\\(\\hat{\\mathbf{y}}\\) is the orthogonal projection of \\(\\mathbf{y}\\) onto \\(\\boldsymbol{{\\cal M}}\\) and is the OLS/MLE estimate of \\(\\boldsymbol{\\mu}\\) that satisfies \\[\\mathbf{P}_\\mathbf{X}\\mathbf{y}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}= \\hat{\\mathbf{y}}\\]\n\\(\\mathbf{X}^T\\mathbf{X}\\) is not invertible so need another way to represent \\(\\mathbf{P}_\\mathbf{X}\\) and \\(\\hat{\\boldsymbol{\\beta}}\\)"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#spectral-decomposition-sd",
    "href": "resources/slides/03-non-full-rank.html#spectral-decomposition-sd",
    "title": "Rank Deficient Models",
    "section": "Spectral Decomposition (SD)",
    "text": "Spectral Decomposition (SD)\nEvery symmetric \\(n \\times n\\) matrix, \\({\\mathbf{S}}\\), has an eigen decomposition \\({\\mathbf{S}}= \\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{U}^T\\)\n\n\\(\\boldsymbol{\\Lambda}\\) is a diagonal matrix with eigenvalues \\((\\lambda_1, \\ldots, \\lambda_n)\\) of \\({\\mathbf{S}}\\)\n\\(\\mathbf{U}\\) is a \\(n \\times n\\) orthogonal matrix \\(\\mathbf{U}^T\\mathbf{U}= \\mathbf{U}\\mathbf{U}^T = \\mathbf{I}_n\\) ( \\(\\mathbf{U}^{-1} = \\mathbf{U}^T\\))\nthe columns of \\(\\mathbf{U}\\) from an Orthonormal Basis (ONB) for \\(\\mathbb{R}^n\\)\nthe columns of \\(\\mathbf{U}\\) associated with non-zero eigenvalues form an ONB for \\(C({\\mathbf{S}})\\)\nthe number of non-zero eigenvalues is the rank of \\({\\mathbf{S}}\\)\nthe columns of \\(\\mathbf{U}\\) associated with zero eigenvalues form an ONB for \\(C({\\mathbf{S}})^\\perp\\)\n\\({\\mathbf{S}}^d = \\mathbf{U}\\boldsymbol{\\Lambda}^d \\mathbf{U}^T\\) (matrix powers)"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#positive-definite-and-non-negative-definite-matrices",
    "href": "resources/slides/03-non-full-rank.html#positive-definite-and-non-negative-definite-matrices",
    "title": "Rank Deficient Models",
    "section": "Positive Definite and Non-Negative Definite Matrices",
    "text": "Positive Definite and Non-Negative Definite Matrices\n\nDefinition: B.21 Positive Definite and Non-Negative DefiniteA symmetric matrix \\({\\mathbf{S}}\\) is positive definite (\\({\\mathbf{S}}\\gt 0\\)) if and only if \\(\\mathbf{x}^T{\\mathbf{S}}\\mathbf{x}&gt; 0\\) for \\(\\mathbf{x}\\in \\mathbb{R}^n\\), \\(\\mathbf{x}\\ne \\mathbf{0}_n\\), and positive semi-definite or non-negative definite (\\({\\mathbf{S}}\\ge 0\\)) if and only if \\(\\mathbf{x}^T{\\mathbf{S}}\\mathbf{x}\\ge 0\\) for \\(\\mathbf{x}\\in \\mathbb{R}^n\\), \\(\\mathbf{x}\\ne \\mathbf{0}_n\\)\n\n\n\n\n\n\n\n\n\nExercise\n\n\nShow that a symmetric matrix \\({\\mathbf{S}}\\) is positive definite if and only if its eigenvalues are all strictly greater than zero, and positive semi-definite if all the eigenvalues are non-negative."
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#projections",
    "href": "resources/slides/03-non-full-rank.html#projections",
    "title": "Rank Deficient Models",
    "section": "Projections",
    "text": "Projections\nLet \\(\\mathbf{P}\\) be an orthogonal projection matrix onto \\(\\boldsymbol{{\\cal M}}\\), then\n\nthe eigenvalues of \\(\\mathbf{P}\\), \\(\\lambda_i\\), are either zero or one\nthe trace of \\(\\mathbf{P}\\) is the rank of \\(\\mathbf{P}\\)\nthe dimension of the subspace that \\(\\mathbf{P}\\) projects onto is the rank of \\(\\mathbf{P}\\)\nthe columns of \\(\\mathbf{U}_r = [u_1, u_2, \\ldots u_r]\\) form an ONB for the \\(C(\\mathbf{P})\\)\nthe projection \\(\\mathbf{P}\\) has the representation \\(\\mathbf{P}= \\mathbf{U}_r \\mathbf{U}_r^T = \\sum_{i = 1}^r u_i u_i^T\\) (the sum of \\(r\\) rank \\(1\\) projections)\nthe projection \\(\\mathbf{I}_n - \\mathbf{P}= \\mathbf{I}- \\mathbf{U}_r \\mathbf{U}_r^T = \\mathbf{U}_\\perp \\mathbf{U}_\\perp^T\\) where \\(\\mathbf{U}_\\perp = [u_{r+1}, \\ldots u_n]\\) is an orthogonal projection onto \\(\\boldsymbol{{\\cal N}}\\)\n\n\nMLE/OLS:\n\n\\(\\mathbf{P}_X \\mathbf{y}= \\mathbf{U}_r \\mathbf{U}_r^T \\mathbf{y}= \\mathbf{U}_r \\tilde{\\boldsymbol{\\beta}}\\)\nClaim \\(\\tilde{\\boldsymbol{\\beta}}\\) is a MLE/OLS estimate of \\(\\boldsymbol{\\beta}\\) where \\(\\tilde{\\mathbf{X}}= \\mathbf{U}_r\\)."
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#singular-value-decomposition-connections-to-spectral-decompositions",
    "href": "resources/slides/03-non-full-rank.html#singular-value-decomposition-connections-to-spectral-decompositions",
    "title": "Rank Deficient Models",
    "section": "Singular Value Decomposition & Connections to Spectral Decompositions",
    "text": "Singular Value Decomposition & Connections to Spectral Decompositions\nA matrix \\(\\mathbf{X}\\in \\mathbb{R}^{n \\times p}\\), \\(p \\le n\\) has a singular value decomposition \\[\\mathbf{X}= \\mathbf{U}_p \\mathbf{D}\\mathbf{V}^T\\]\n\n\\(\\mathbf{U}_p\\) is a \\(n \\times p\\) matrix with the first \\(p\\) eigenvectors in \\(\\mathbf{U}\\) associated with the \\(p\\) largest eigenvectors of \\(\\mathbf{X}\\mathbf{X}^T = \\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{U}^T\\) with \\(\\mathbf{U}_p^T\\mathbf{U}_p = I_p\\) but \\(\\mathbf{U}_p \\mathbf{U}_p^T \\ne \\mathbf{I}_n\\) (or \\(\\mathbf{P}_p\\))\n\\(\\mathbf{V}\\) is a \\(p \\times p\\) orthogonal matrix associated with the \\(p\\) eigenvectors of \\(\\mathbf{X}^T\\mathbf{X}= \\mathbf{V}\\boldsymbol{\\Lambda}_p \\mathbf{V}^T\\) where \\(\\boldsymbol{\\Lambda}_p\\) is the diagonal matrix of eigenvalues associated with the \\(p\\) largest eigenvalues of \\(\\boldsymbol{\\Lambda}\\)\n\\(\\mathbf{D}\\) = \\(\\boldsymbol{\\Lambda}_p^{1/2}\\) are the singular values\nif \\(\\mathbf{X}\\) has rank \\(r &lt; p\\), then \\(C(\\mathbf{X}) = C(\\mathbf{U}_p) = C(\\mathbf{U}_r)\\), where \\(\\mathbf{U}_r\\) are the eigenvectors of \\(\\mathbf{U}\\) or \\(\\mathbf{U}_p\\) associated with the non-zero eigenvalues.\n\\(\\mathbf{U}_r\\) is an ONB for \\(C(X)\\)"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#mleols-for-non-full-rank-case",
    "href": "resources/slides/03-non-full-rank.html#mleols-for-non-full-rank-case",
    "title": "Rank Deficient Models",
    "section": "MLE/OLS for non-full rank case",
    "text": "MLE/OLS for non-full rank case\n\nif \\(\\mathbf{X}^T\\mathbf{X}\\) is invertible, \\(\\mathbf{P}_X = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\) and \\(\\hat{\\boldsymbol{\\beta}}\\) is the unique estimator that satisfies \\(\\mathbf{P}_\\mathbf{X}\\mathbf{y}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\) or \\(\\hat{\\boldsymbol{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{y}\\)\nif \\(\\mathbf{X}^T\\mathbf{X}\\) is not invertible, replace \\(\\mathbf{X}\\) by \\(\\tilde{\\mathbf{X}}\\) that is rank \\(r\\)\nor represent \\(\\mathbf{P}_\\mathbf{X}= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-} \\mathbf{X}^T\\) where \\((\\mathbf{X}^T\\mathbf{X})^{-}\\) is a generalized inverse of \\(\\mathbf{X}^T\\mathbf{X}\\) and \\(\\hat{\\boldsymbol{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-}\\mathbf{X}^T \\mathbf{y}\\)"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#generalized-inverses",
    "href": "resources/slides/03-non-full-rank.html#generalized-inverses",
    "title": "Rank Deficient Models",
    "section": "Generalized Inverses",
    "text": "Generalized Inverses\n\nDefinition: Generalized-Inverse (B.36)A generalized inverse of any matrix \\(\\mathbf{A}\\): \\(\\mathbf{A}^{-}\\) satisfies \\(\\mathbf{A}\\mathbf{A}^- \\mathbf{A}= \\mathbf{A}\\)\n\n\n\nA generalized inverse of \\(\\mathbf{A}\\) symmetric always exists!\n\n\n\nTheorem: Christensen B.39If \\(\\mathbf{G}_1\\) and \\(\\mathbf{G}_2\\) are generalized inverses of \\(\\mathbf{A}\\) then \\(\\mathbf{G}_1 \\mathbf{A}\\mathbf{G}_2\\) is also a generalized inverse of \\(\\mathbf{A}\\)\n\n\n\nif \\(\\mathbf{A}\\) is symmetric, then \\(\\mathbf{A}^-\\) need not be!"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#orthogonal-projections-in-general",
    "href": "resources/slides/03-non-full-rank.html#orthogonal-projections-in-general",
    "title": "Rank Deficient Models",
    "section": "Orthogonal Projections in General",
    "text": "Orthogonal Projections in General\n\n\n\n\n\n\n\nLemma B.43\n\n\nIf \\(\\mathbf{G}\\) and \\(\\mathbf{H}\\) are generalized inverses of \\(\\mathbf{X}^T\\mathbf{X}\\) then \\[\\begin{align*}\n\\mathbf{X}\\mathbf{G}\\mathbf{X}^T \\mathbf{X}& = \\mathbf{X}\\mathbf{H}\\mathbf{X}^T \\mathbf{X}= \\mathbf{X}\\\\\n\\mathbf{X}\\mathbf{G}\\mathbf{X}^T & = \\mathbf{X}\\mathbf{H}\\mathbf{X}^T\n\\end{align*}\\]\n\n\n\n\n\n\nTheorem: B.44\\(\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^-\\mathbf{X}^T\\) is an orthogonal projection onto \\(C(\\mathbf{X})\\).\n\n\n\n\n\nWe need to show that (i) \\(\\mathbf{P}\\mathbf{m}= \\mathbf{m}\\) for \\(\\mathbf{m}\\in C(\\mathbf{X})\\) and (ii) \\(\\mathbf{P}\\mathbf{n}= 0\\) for \\(\\mathbf{n}\\in C(\\mathbf{X})^\\perp\\).\n\nFor \\(\\mathbf{m}\\in C(\\mathbf{X})\\), write \\(\\mathbf{m}= \\mathbf{X}\\mathbf{b}\\). Then \\(\\mathbf{P}\\mathbf{m}= \\mathbf{P}\\mathbf{X}\\mathbf{b}= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^-\\mathbf{X}^T \\mathbf{X}\\mathbf{b}\\) and by Lemma B43, we have that \\(\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^-\\mathbf{X}^T \\mathbf{X}\\mathbf{b}= \\mathbf{X}\\mathbf{b}= \\mathbf{m}\\)\nFor \\(\\mathbf{n}\\perp C(\\mathbf{X})\\), \\(\\mathbf{P}\\mathbf{n}= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^-\\mathbf{X}^T \\mathbf{n}= \\mathbf{0}_n\\) as \\(C(\\mathbf{X})^\\perp = N(\\mathbf{X}^T)\\)."
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#mles-ols",
    "href": "resources/slides/03-non-full-rank.html#mles-ols",
    "title": "Rank Deficient Models",
    "section": "MLEs & OLS",
    "text": "MLEs & OLS\nMLE/OLS satisfies\n\n\\(\\mathbf{P}\\mathbf{y}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\)\n\\(\\mathbf{P}\\mathbf{y}= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^-\\mathbf{X}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\) (does not depend on choice of generalized inverse)\n\\(\\hat{\\boldsymbol{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^-\\mathbf{X}^T \\mathbf{y}\\)\n\\(\\hat{\\boldsymbol{\\beta}}\\) is not unique - does depend on choice of generalized inverse unless \\(\\mathbf{X}\\) is full rank"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#moore-penrose-generalized-inverse",
    "href": "resources/slides/03-non-full-rank.html#moore-penrose-generalized-inverse",
    "title": "Rank Deficient Models",
    "section": "Moore-Penrose Generalized Inverse:",
    "text": "Moore-Penrose Generalized Inverse:\n\nDecompose symmetric \\(\\mathbf{A}= \\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{U}^T\\) (i.e \\(\\mathbf{X}^T\\mathbf{X}\\))\n\\(\\mathbf{A}^-_{MP} = \\mathbf{U}\\boldsymbol{\\Lambda}^- \\mathbf{U}^T\\)\n\n\\(\\boldsymbol{\\Lambda}^-\\) is diagonal with \\[ \\lambda_i^- = \\left\\{\n\\begin{array}{l}\n1/\\lambda_i \\text{ if } \\lambda_i \\neq 0 \\\\\n0 \\quad \\, \\text{  if } \\lambda_i = 0\n\\end{array}\n\\right.\\]\n\nSymmetric \\(\\mathbf{A}^-_{MP} = (\\mathbf{A}^-_{MP})^T\\)\n\nReflexive \\(\\mathbf{A}^-_{MP}\\mathbf{A}\\mathbf{A}^-_{MP} = \\mathbf{A}^-_{MP}\\)\n\n\n\nShow that \\(\\mathbf{A}_{MP}^-\\) is a generalized inverse of \\(\\mathbf{A}\\)\nCan you construct another generalized inverse of \\(\\mathbf{X}^T\\mathbf{X}\\) ?\nCan you find the Moore-Penrose generalized inverse of \\(\\mathbf{X}\\in \\mathbb{R}^{n \\times p}\\)?"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#properties-of-ols-full-rank-case",
    "href": "resources/slides/03-non-full-rank.html#properties-of-ols-full-rank-case",
    "title": "Rank Deficient Models",
    "section": "Properties of OLS (full rank case)",
    "text": "Properties of OLS (full rank case)\nHow good is \\(\\hat{\\boldsymbol{\\beta}}\\) as an estimator of \\(\\beta\\)\n\n\\(\\hat{\\boldsymbol{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{Y}=  (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{X}\\boldsymbol{\\beta}+ (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}= \\boldsymbol{\\beta}+  (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}\\)\ndon’t know \\(\\boldsymbol{\\epsilon}\\), but can talk about behavior on average over\n\ndifferent runs of an experiment\ndifferent samples from a population\ndifferent values of \\(\\boldsymbol{\\epsilon}\\)\n\nwith minimal assumption \\(\\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\), \\[\\begin{align*}\n\\textsf{E}[\\hat{\\boldsymbol{\\beta}}] & = \\textsf{E}[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}+ (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}]\\\\\n& = \\boldsymbol{\\beta}+ (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\textsf{E}[\\boldsymbol{\\epsilon}] \\\\\n& = \\boldsymbol{\\beta}\n\\end{align*}\\]\nBias of \\(\\hat{\\boldsymbol{\\beta}}\\), \\(\\text{Bias}[\\hat{\\boldsymbol{\\beta}}] =  \\textsf{E}[\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}] = \\mathbf{0}_p\\)\n\\(\\hat{\\boldsymbol{\\beta}}\\) is an unbiased estimator of \\(\\boldsymbol{\\beta}\\) if \\(\\boldsymbol{\\mu}\\in C(\\mathbf{X})\\)"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#class-of-unbiased-estimators",
    "href": "resources/slides/03-non-full-rank.html#class-of-unbiased-estimators",
    "title": "Rank Deficient Models",
    "section": "Class of Unbiased Estimators",
    "text": "Class of Unbiased Estimators\nClass of linear statistical models: \\[\\begin{align*}\n\\mathbf{Y}& = \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\\\\n\\boldsymbol{\\epsilon}& \\sim P \\\\\nP & \\in \\cal{P}\n\\end{align*}\\]\n\nAn estimator \\(\\tilde{\\boldsymbol{\\beta}}\\) is unbiased for \\(\\boldsymbol{\\beta}\\) if \\(\\textsf{E}_P[\\tilde{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\\quad \\forall \\boldsymbol{\\beta}\\in \\mathbb{R}^p\\) and \\(P \\in \\cal{P}\\)\n\n\nExamples:\n\n\n\\(\\cal{P}_1= \\{P = \\textsf{N}(\\mathbf{0}_n ,\\mathbf{I}_n)\\}\\)\n\n\n\\(\\cal{P}_2 = \\{P = \\textsf{N}(\\mathbf{0}_n ,\\sigma^2 \\mathbf{I}_n), \\sigma^2 &gt;0\\}\\)\n\n\n\\(\\cal{P}_3 = \\{P = \\textsf{N}(\\mathbf{0}_n ,\\boldsymbol{\\Sigma}), \\boldsymbol{\\Sigma}\\in \\cal{{\\cal{S}}}^+ \\}\\) (\\(\\cal{{\\cal{S}}}^+\\) is the set of all \\(n \\times n\\) symmetric positive definite matrices.)\n\n\n\\(\\cal{P}_4\\) is the set of distributions with \\(\\textsf{E}_P[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\) and \\(\\textsf{E}_P[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] \\gt 0\\)\n\n\n\\(\\cal{P}_5\\) is the set of distributions with \\(\\textsf{E}_P[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\) and \\(\\textsf{E}_P[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] \\ge 0\\)"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#linear-unbiased-estimation",
    "href": "resources/slides/03-non-full-rank.html#linear-unbiased-estimation",
    "title": "Rank Deficient Models",
    "section": "Linear Unbiased Estimation",
    "text": "Linear Unbiased Estimation\n\n\n\n\n\n\n\nExercise\n\n\n\nExplain why an estimator that is unbiased for the model with parameter space \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\) and \\(P \\in  \\cal{P}_{k+1}\\) is unbiased for the model with parameter space \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\) and \\(P \\in  \\cal{P}_{k}\\) .\nFind an estimator that is unbiased for \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\) and \\(P \\in  \\cal{P}_{1}\\) that but is biased for \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\) and \\(P \\in  \\cal{P}_{2}\\).\n\n\n\n\n\n\nRestrict attention to linear unbiased estimators\n\n\n\nDefinition: Linear Unbiased Estimators (LUEs)\nAn estimator \\(\\tilde{\\boldsymbol{\\beta}}\\) is a Linear Unbiased Estimator (LUE) of \\(\\boldsymbol{\\beta}\\) if\n\nlinearity: \\(\\tilde{\\boldsymbol{\\beta}}= \\mathbf{A}\\mathbf{Y}\\) for \\(\\mathbf{A}\\in \\mathbb{R}^{p \\times n}\\)\nunbiasedness: \\(\\textsf{E}[\\tilde{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\\) for all \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\)\n\n\n\n\n\n\nAre there other LUEs besides the OLS/MLE estimator?\nWhich is “best”? (and in what sense?)\n\n\n\n\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/list-of-mathstuff.html",
    "href": "resources/slides/list-of-mathstuff.html",
    "title": "STA721-F24: Linear Models",
    "section": "",
    "text": "Definition: Projection\nDefinition: Orthogonal Complement\nDefinition: Null Space\nDefinition: Orthogonal Projections"
  },
  {
    "objectID": "resources/slides/02-mles.html#outline",
    "href": "resources/slides/02-mles.html#outline",
    "title": "Maximum Likelihood Estimation",
    "section": "Outline",
    "text": "Outline\n\nLikelihood Function\nProjections\nMaximum Likelihood Estimates\n\n\nReadings: Christensen Chapter 1-2, Appendix A, and Appendix B"
  },
  {
    "objectID": "resources/slides/02-mles.html#normal-model",
    "href": "resources/slides/02-mles.html#normal-model",
    "title": "Maximum Likelihood Estimation",
    "section": "Normal Model",
    "text": "Normal Model\nTake an random vector \\(\\mathbf{Y}\\in \\mathbb{R}^n\\) which is observable and decompose\n\\[ \\mathbf{Y}= \\boldsymbol{\\mu}+ \\boldsymbol{\\epsilon}\\]\n\n\\(\\boldsymbol{\\mu}\\in \\mathbb{R}^n\\) (unknown, fixed)\n\n\\(\\boldsymbol{\\epsilon}\\in \\mathbb{R}^n\\) unobservable error vector (random)\n\n\nUsual assumptions?\n\n\\(E[\\epsilon_i] = 0 \\ \\forall i \\Leftrightarrow \\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}\\) \\(\\quad \\Rightarrow \\textsf{E}[\\mathbf{Y}] = \\boldsymbol{\\mu}\\) (mean vector)\n\\(\\epsilon_i\\) independent with \\(\\textsf{Var}(\\epsilon_i) = \\sigma^2\\) and \\(\\textsf{Cov}(\\epsilon_i, \\epsilon_j) = 0\\)\nMatrix version \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] \\equiv \\left[ (\\textsf{E}\\left[(\\epsilon_i -\\textsf{E}[\\epsilon_i])(\\epsilon_j - \\textsf{E}[\\epsilon_j])\\right]\\right]_{ij} = \\sigma^2 \\mathbf{I}_n\n\\quad \\Rightarrow \\textsf{Cov}[\\mathbf{Y}] = \\sigma^2 \\mathbf{I}_n\\) (errors are uncorrelated)\n\\(\\boldsymbol{\\epsilon}_i \\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}\\textsf{N}(0, \\sigma^2)\\) implies that \\(Y_i \\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}\\textsf{N}(\\mu_i, \\sigma^2)\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#likelihood-function",
    "href": "resources/slides/02-mles.html#likelihood-function",
    "title": "Maximum Likelihood Estimation",
    "section": "Likelihood Function",
    "text": "Likelihood Function\nThe likelihood function for \\(\\boldsymbol{\\mu}, \\sigma^2\\) is proportional to the sampling distribution of the data\n\\[\\begin{eqnarray*}\n{\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2) & \\propto & \\prod_{i = 1}^n \\frac{1}{\\sqrt{(2 \\pi\n                                 \\sigma^2)}} \\exp{- \\frac{1}{2}\n                                 \\left\\{ \\frac{( Y_i\n                                 - \\mu_i)^2}{\\sigma^2} \\right\\}}\n                                 \\\\\n& \\propto & ({2 \\pi} \\sigma^2)^{-n/2}\n\\exp{\\left\\{ - \\frac 1 2  \\frac{ \\sum_i(Y_i - \\mu_i)^2 )}{\\sigma^2}\n\\right\\}}   \\\\\n   & \\propto & (\\sigma^2)^{-n/2}\n\\exp{\\left\\{ - \\frac 1 2 \\frac{\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2}{\\sigma^2}\n\\right\\}} \\\\\n  & \\propto &  (2 \\pi)^{-n/2}\n| \\mathbf{I}_n\\sigma^2|^{-1/2}\n\\exp{\\left\\{ - \\frac 1 2 \\frac{\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2}{\\sigma^2}\n\\right\\}}  \n\\end{eqnarray*}\\]\n\nLast line is the density of \\(\\mathbf{Y}\\sim \\textsf{N}_n\\left(\\boldsymbol{\\mu}, \\sigma^2 \\mathbf{I}_n\\right)\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#mles",
    "href": "resources/slides/02-mles.html#mles",
    "title": "Maximum Likelihood Estimation",
    "section": "MLEs",
    "text": "MLEs\nFind values of \\(\\hat{\\boldsymbol{\\mu}}\\) and \\({\\hat{\\sigma}}^2\\) that maximize the likelihood \\({\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2)\\) for \\(\\boldsymbol{\\mu}\\in \\mathbb{R}^n\\) and \\(\\sigma^2 \\in \\mathbb{R}^+\\) \\[\\begin{eqnarray*}\n{\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2)\n    & \\propto & (\\sigma^2)^{-n/2}\n\\exp{\\left\\{ - \\frac 1 2 \\frac{\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2}{\\sigma^2}\n\\right\\}} \\\\\n\\log({\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2) )\n   & \\propto & -\\frac{n}{2} \\log(\\sigma^2)  - \\frac 1 2 \\frac{\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2}{\\sigma^2}\n\\\\\n\\end{eqnarray*}\\] or equivalently the log likelihood\n\nClearly, \\(\\hat{\\boldsymbol{\\mu}}= \\mathbf{Y}\\) but \\({\\hat{\\sigma}}^2= 0\\) is outside the parameter space\nIf \\(\\boldsymbol{\\mu}= \\mathbf{X}\\boldsymbol{\\beta}\\), can show that \\(\\hat{\\boldsymbol{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) is the MLE/OLS estimator of \\(\\boldsymbol{\\beta}\\) and \\(\\hat{\\boldsymbol{\\mu}}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\) if \\(\\mathbf{X}\\) is full column rank.\nshow via projections"
  },
  {
    "objectID": "resources/slides/02-mles.html#projections",
    "href": "resources/slides/02-mles.html#projections",
    "title": "Maximum Likelihood Estimation",
    "section": "Projections",
    "text": "Projections\ntake any point \\(\\mathbf{y}\\in \\mathbb{R}^n\\) and “project” it onto \\(C(\\mathbf{X}) = \\boldsymbol{{\\cal M}}\\)\n\nany point already in \\(\\boldsymbol{{\\cal M}}\\) stays the same\nso if \\(\\mathbf{P}_\\mathbf{X}\\) is a projection onto the column space of \\(\\mathbf{X}\\) then for \\(\\mathbf{m}\\in C(\\mathbf{X})\\) \\(\\mathbf{P}_\\mathbf{X}\\mathbf{m}= \\mathbf{m}\\)\n\\(\\mathbf{P}_\\mathbf{X}\\) is a linear transformation from \\(\\mathbb{R}^n \\to \\mathbb{R}^n\\)\nmaps vectors in \\(\\mathbb{R}^n\\) into \\(C(\\mathbf{X})\\)\nif \\(\\mathbf{z}\\in \\mathbb{R}^n\\) then \\(\\mathbf{P}_\\mathbf{X}\\mathbf{z}= \\mathbf{X}\\mathbf{a}\\in C(\\mathbf{X})\\) for some \\(\\mathbf{a}\\in \\mathbb{R}^p\\)\n\n\n\n\n\n\n\n\nExample\n\n\nFor \\(\\mathbf{X}\\in \\mathbb{R}^{n \\times p}\\), rank \\(p\\), \\(\\mathbf{P}_\\mathbf{X}= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}\\) is a projection onto the \\(p\\) dimensional subspace \\(\\boldsymbol{{\\cal M}}= C(\\mathbf{X})\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#idempotent-matrix",
    "href": "resources/slides/02-mles.html#idempotent-matrix",
    "title": "Maximum Likelihood Estimation",
    "section": "Idempotent Matrix",
    "text": "Idempotent Matrix\nWhat if we project a projection?\n\n\\(\\mathbf{P}_\\mathbf{X}\\mathbf{z}= \\mathbf{X}\\mathbf{a}\\in C(\\mathbf{X})\\)\n\\(\\mathbf{P}_\\mathbf{X}\\mathbf{X}\\mathbf{a}= \\mathbf{X}\\mathbf{a}\\)\nsince \\(\\mathbf{P}_\\mathbf{X}^2 \\mathbf{z}=  \\mathbf{P}_\\mathbf{X}\\mathbf{z}\\) for all \\(\\mathbf{z}\\in \\mathbb{R}^n\\) we have \\(\\mathbf{P}_\\mathbf{X}^2 = \\mathbf{P}_\\mathbf{X}\\)\n\n\n\nDefinition: ProjectionFor a matrix \\(\\mathbf{P}\\) in \\(\\mathbb{R}^{n \\times n}\\) is a projection matrix if \\(\\mathbf{P}^2 = \\mathbf{P}\\). That is all projections \\(\\mathbf{P}\\) are idempotent matrix.\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\nFor \\(\\mathbf{X}\\in \\mathbb{R}^{n \\times p}\\), rank \\(p\\), if \\(\\mathbf{P}_\\mathbf{X}= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}\\) use the definition to show that it is a projection onto the \\(p\\) dimensional subspace \\(\\boldsymbol{{\\cal M}}= C(\\mathbf{X})\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#null-space",
    "href": "resources/slides/02-mles.html#null-space",
    "title": "Maximum Likelihood Estimation",
    "section": "Null Space",
    "text": "Null Space\n\nDefinition: Orthogonal ComplementThe set of all vectors that are orthogonal to a given subspace \\(\\boldsymbol{{\\cal M}}\\) is called the orthogonal complement of the subspace denoted as \\(\\boldsymbol{{\\cal M}}^\\perp\\). Under the usual inner product, \\(\\boldsymbol{{\\cal M}}^\\perp \\equiv \\{\\mathbf{n}\\in \\mathbb{R}^n \\ni \\mathbf{m}^T\\mathbf{n}= 0 {\\text{ for }} \\mathbf{m}\\in \\boldsymbol{{\\cal M}}\\}\\)\n\n\n\n\nDefinition: Null SpaceFor a matrix \\(\\mathbf{A}\\), the null space of \\(\\mathbf{A}\\) is defined as \\(N(\\mathbf{A}) = \\{\\mathbf{n}\\ni \\mathbf{A}\\mathbf{n}= \\mathbf{0}\\}\\)\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\nShow that \\(C(\\mathbf{X})^\\perp\\) (the orthogonal complement of \\(C(\\mathbf{X})\\)) is the null space of \\(\\mathbf{X}^T\\), \\(\\, N(\\mathbf{X}^T)\\)."
  },
  {
    "objectID": "resources/slides/02-mles.html#orthogonal-projection",
    "href": "resources/slides/02-mles.html#orthogonal-projection",
    "title": "Maximum Likelihood Estimation",
    "section": "Orthogonal Projection",
    "text": "Orthogonal Projection\n\nDefinition: Orthogonal ProjectionsFor a vector space \\({\\cal V}\\) with an inner product \\(\\langle \\mathbf{x}, \\mathbf{y}\\rangle\\) for \\(\\mathbf{x}, \\mathbf{y}\\in {\\cal V}\\), \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are orthogonal if \\(\\langle \\mathbf{x}, \\mathbf{y}\\rangle = 0\\). A projection \\(\\mathbf{P}\\) is an orthogonal projection onto a subspace \\(\\boldsymbol{{\\cal M}}\\) of \\({\\cal V}\\) if for any \\(\\mathbf{m}\\in {\\cal V}\\), \\(\\mathbf{P}\\mathbf{m}= \\mathbf{m}\\) and for any \\(\\mathbf{n}\\in \\boldsymbol{{\\cal M}}^\\perp\\), \\(\\mathbf{P}\\mathbf{n}= \\mathbf{0}\\).\nThe null space of \\(\\mathbf{P}\\) is the orthogonal complement of \\(\\boldsymbol{{\\cal M}}\\)\n\n\n\nFor \\(\\mathbb{R}^N\\) with the inner product, \\(\\langle \\mathbf{x}, \\mathbf{y}\\rangle = \\mathbf{x}^T\\mathbf{y}\\), \\(\\mathbf{P}\\) is an orthogonal projection onto \\(\\boldsymbol{{\\cal M}}\\) if \\(\\mathbf{P}\\) is a projection (\\(\\mathbf{P}^2 = \\mathbf{P}\\)) and it is symmetric (\\(\\mathbf{P}= \\mathbf{P}^T\\))\n\n\n\n\n\n\n\n\nExercise\n\n\nShow that \\(\\mathbf{P}_\\mathbf{X}\\) is an orthogonal projection on \\(C(\\mathbf{X})\\)."
  },
  {
    "objectID": "resources/slides/02-mles.html#decompsition",
    "href": "resources/slides/02-mles.html#decompsition",
    "title": "Maximum Likelihood Estimation",
    "section": "Decompsition",
    "text": "Decompsition\n\nFor any \\(\\mathbf{y}\\in \\mathbb{R}^n\\), we can write it uniquely as a vector \\[ \\mathbf{y}= \\mathbf{m}+ \\mathbf{n}, \\quad \\mathbf{m}\\in \\boldsymbol{{\\cal M}}\\quad \\mathbf{n}\\in \\boldsymbol{{\\cal M}}^\\perp\\]\nwrite \\(\\mathbf{y}= \\mathbf{P}\\mathbf{y}+ (\\mathbf{y}- \\mathbf{P}\\mathbf{y}) = \\mathbf{P}\\mathbf{y}+ (\\mathbf{I}- \\mathbf{P})\\mathbf{y}\\)\nclaim that if \\(\\mathbf{P}\\) is an orthogonal projection, \\((\\mathbf{I}- \\mathbf{P})\\) is an orthogonal projection onto \\(\\boldsymbol{{\\cal M}}^\\perp\\)\nif \\(\\mathbf{n}\\in \\boldsymbol{{\\cal M}}^\\perp\\), then \\((\\mathbf{I}- \\mathbf{P})\\mathbf{n}= \\mathbf{n}- \\mathbf{P}\\mathbf{n}= \\mathbf{n}\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#back-to-mles",
    "href": "resources/slides/02-mles.html#back-to-mles",
    "title": "Maximum Likelihood Estimation",
    "section": "Back to MLEs",
    "text": "Back to MLEs\n\n\\(\\mathbf{Y}\\sim \\textsf{N}(\\boldsymbol{\\mu}, \\sigma^2 \\mathbf{I}_n)\\) with \\(\\boldsymbol{\\mu}= \\mathbf{X}\\boldsymbol{\\beta}\\) and \\(\\mathbf{X}\\) full column rank\nClaim: Maximum Likelihood Estimator (MLE) of \\(\\boldsymbol{\\mu}\\) is \\(\\mathbf{P}_\\mathbf{X}\\mathbf{Y}\\)\nLog Likelihood: \\[ \\log {\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2) =\n-\\frac{n}{2} \\log(\\sigma^2)\n- \\frac 1 2 \\frac{\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2}{\\sigma^2}\n\\]\nDecompose \\(\\mathbf{Y}= \\mathbf{P}_\\mathbf{X}\\mathbf{Y}+ (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\)\n\nUse \\(\\mathbf{P}_\\mathbf{X}\\boldsymbol{\\mu}= \\boldsymbol{\\mu}\\)\n\nSimplify \\(\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#expand",
    "href": "resources/slides/02-mles.html#expand",
    "title": "Maximum Likelihood Estimation",
    "section": "Expand",
    "text": "Expand\n\\[\\begin{eqnarray*}\n    \\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2 & = & \\|  { (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}+ \\mathbf{P}_x \\mathbf{Y}} -\n    \\boldsymbol{\\mu}\\|^2  \\\\\n  & = & \\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}+ \\mathbf{P}_x \\mathbf{Y}- {\\mathbf{P}_\\mathbf{X}}\\boldsymbol{\\mu}\\|^2  \\\\\n  & = & {\\|(\\mathbf{I}-\\mathbf{P}_\\mathbf{x})}\\mathbf{Y}+  {\\mathbf{P}_\\mathbf{X}}(\\mathbf{Y}- \\boldsymbol{\\mu})\n  \\|^2 \\\\\n& = & {\\|(\\mathbf{I}-\\mathbf{P}_\\mathbf{x})\\mathbf{Y}\\|^2} +  {\\|\n   {\\mathbf{P}_\\mathbf{X}}(\\mathbf{Y}- \\boldsymbol{\\mu}) \\|^2}  + {\\small{2 (\\mathbf{Y}-\n\\boldsymbol{\\mu})^T \\mathbf{P}_\\mathbf{X}^T (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}}}\\\\\n& = & \\|(\\mathbf{I}-\\mathbf{P}_\\mathbf{x})\\mathbf{Y}\\|^2 +  \\| {\\mathbf{P}_\\mathbf{X}}(\\mathbf{Y}- \\boldsymbol{\\mu}) \\|^2 + {0}\n\\\\\n& = & \\|(\\mathbf{I}-\\mathbf{P}_\\mathbf{x})\\mathbf{Y}\\|^2 +  \\| {\\mathbf{P}_\\mathbf{X}}\\mathbf{Y}- \\boldsymbol{\\mu}\\|^2\n  \\end{eqnarray*}\\]\n\nCrossproduct term is zero: \\[\\begin{eqnarray*}\n  \\mathbf{P}_\\mathbf{X}^T (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) & = &  \\mathbf{P}_\\mathbf{X}(\\mathbf{I}- \\mathbf{P}_\\mathbf{X})  \\\\\n  & = &  \\mathbf{P}_\\mathbf{X}- \\mathbf{P}_\\mathbf{X}\\mathbf{P}_\\mathbf{X}\\\\\n& = &  \\mathbf{P}_\\mathbf{X}- \\mathbf{P}_\\mathbf{X}\\\\\n& = & \\mathbf{0}\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "resources/slides/02-mles.html#log-likelihood",
    "href": "resources/slides/02-mles.html#log-likelihood",
    "title": "Maximum Likelihood Estimation",
    "section": "Log Likelihood",
    "text": "Log Likelihood\nSubstitute decomposition into log likelihood \\[\\begin{eqnarray*}\n\\log {\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2)  & = &\n-\\frac{n}{2} \\log(\\sigma^2) - \\frac 1 2 \\frac{\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2}{\\sigma^2}  \\\\\n  & = & -\\frac{n}{2} \\log(\\sigma^2)  - \\frac 1 2 \\left( \\frac{\\|(\\mathbf{I}- \\mathbf{P}_\\mathbf{X})\n  \\mathbf{Y}\\|^2}{\\sigma^2} + \\frac{\\| \\mathbf{P}_\\mathbf{X}\\mathbf{Y}- \\boldsymbol{\\mu}\\|^2 } {\\sigma^2} \\right)   \\\\\n& = &  \\underbrace { -\\frac{n}{2} \\log(\\sigma^2)  - \\frac 1 2  \\frac{\\|(\\mathbf{I}- \\mathbf{P}_\\mathbf{X})\n  \\mathbf{Y}\\|^2}{\\sigma^2} }  +  \\underbrace{- \\frac 1 2  \\frac{\\| \\mathbf{P}_\\mathbf{X}\\mathbf{Y}-\n  \\boldsymbol{\\mu}\\|^2 } {\\sigma^2}}   \\\\\n& = &  \\text{ constant with respect to } \\boldsymbol{\\mu}\\qquad  \\leq 0\n\\end{eqnarray*}\\]\n\nMaximize with respect to \\(\\boldsymbol{\\mu}\\) for each \\(\\sigma^2\\)\nRHS is largest when \\(\\boldsymbol{\\mu}= \\mathbf{P}_\\mathbf{X}\\mathbf{Y}\\) for any choice of \\(\\sigma^2\\) \\[\\therefore \\quad \\hat{\\boldsymbol{\\mu}}= \\mathbf{P}_\\mathbf{X}\\mathbf{Y}\\] is the MLE of \\(\\boldsymbol{\\mu}\\) (fitted values \\(\\hat{\\mathbf{Y}}= \\mathbf{P}_\\mathbf{X}\\mathbf{Y}\\))"
  },
  {
    "objectID": "resources/slides/02-mles.html#mle-of-boldsymbolbeta",
    "href": "resources/slides/02-mles.html#mle-of-boldsymbolbeta",
    "title": "Maximum Likelihood Estimation",
    "section": "MLE of \\(\\boldsymbol{\\beta}\\)",
    "text": "MLE of \\(\\boldsymbol{\\beta}\\)\n\\[{\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2)   =  -\\frac{n}{2} \\log(\\sigma^2)  - \\frac 1 2 \\left( \\frac{\\|(\\mathbf{I}- \\mathbf{P}_\\mathbf{X})\n  \\mathbf{Y}\\|^2}{\\sigma^2} + \\frac{\\| \\mathbf{P}_\\mathbf{X}\\mathbf{Y}- \\boldsymbol{\\mu}\\|^2 } {\\sigma^2} \\right)\\]\n\nRewrite as likeloood function for \\(\\boldsymbol{\\beta}, \\sigma^2\\): \\[{\\cal{L}}(\\boldsymbol{\\beta}, \\sigma^2 )  =  -\\frac{n}{2} \\log(\\sigma^2)  - \\frac 1 2 \\left( \\frac{\\|(\\mathbf{I}- \\mathbf{P}_\\mathbf{X})\n  \\mathbf{Y}\\|^2}{\\sigma^2} + \\frac{\\| \\mathbf{P}_\\mathbf{X}\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}\\|^2 } {\\sigma^2}\n\\right)\\]\n\n\n\nSimilar argument to show that RHS is maximized by minimizing \\[\\| \\mathbf{P}_\\mathbf{X}\n\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}\\|^2\\] \nTherefore \\(\\hat{\\boldsymbol{\\beta}}\\) is a MLE of \\(\\boldsymbol{\\beta}\\) if and only if satisfies \\[\\mathbf{P}_\\mathbf{X}\\mathbf{Y}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\]\nIf \\(\\mathbf{X}^T\\mathbf{X}\\) is full rank, the MLE of \\(\\boldsymbol{\\beta}\\) is \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}= \\hat{\\boldsymbol{\\beta}}\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#mle-of-sigma2",
    "href": "resources/slides/02-mles.html#mle-of-sigma2",
    "title": "Maximum Likelihood Estimation",
    "section": "MLE of \\(\\sigma^2\\)",
    "text": "MLE of \\(\\sigma^2\\)\n\nPlug-in MLE of \\(\\hat{\\boldsymbol{\\mu}}\\) for \\(\\boldsymbol{\\mu}\\) \\[ \\log {\\cal{L}}(\\hat{\\boldsymbol{\\mu}}, \\sigma^2)  =   -\\frac{n}{2} \\log \\sigma^2 - \\frac 1 2\n\\frac{\\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\|^2  }{\\sigma^2}\\]\nDifferentiate with respect to \\(\\sigma^2\\) \\[\\frac{\\partial \\, \\log {\\cal{L}}(\\hat{\\boldsymbol{\\mu}}, \\sigma^2)}{\\partial \\, \\sigma^2} =  -\\frac{n}{2} \\frac{1}{\\sigma^2}  +  \\frac 1 2\n\\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\|^2 \\left(\\frac{1}{\\sigma^2}\\right)^2 \\]\nSet derivative to zero and solve for MLE \\[\\begin{eqnarray*}\n0 & = &  -\\frac{n}{2} \\frac{1}{{\\hat{\\sigma}}^2}  +  \\frac 1 2\n\\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\|^2 \\left(\\frac{1}{{\\hat{\\sigma}}^2}\\right)^2  \\\\\n\\frac{n}{2} {\\hat{\\sigma}}^2& = & \\frac 1 2\n\\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\|^2 \\\\\n{\\hat{\\sigma}}^2& = & \\frac{\\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\|^2}{n}\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "resources/slides/02-mles.html#mle-estimate-of-sigma2",
    "href": "resources/slides/02-mles.html#mle-estimate-of-sigma2",
    "title": "Maximum Likelihood Estimation",
    "section": "MLE Estimate of \\(\\sigma^2\\)",
    "text": "MLE Estimate of \\(\\sigma^2\\)\nMaximum Likelihood Estimate of \\(\\sigma^2\\) \\[\\begin{eqnarray*}\n    {\\hat{\\sigma}}^2& = & \\frac{\\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\|^2}{n} \\\\\n      & = & \\frac{\\mathbf{Y}^T(\\mathbf{I}- \\mathbf{P}_\\mathbf{X})^T(\\mathbf{I}-\\mathbf{P}_\\mathbf{X}) \\mathbf{Y}}{n} \\\\\n& = & \\frac{ \\mathbf{Y}^T(\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}}{n} \\\\\n& = & \\frac{\\mathbf{e}^T\\mathbf{e}} {n}\n  \\end{eqnarray*}\\] where \\(\\mathbf{e}= (\\mathbf{I}- \\mathbf{P}_\\mathbf{X})\\mathbf{Y}\\) are the residuals from the regression of \\(\\mathbf{Y}\\) on \\(\\mathbf{X}\\)\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/06-GLS.html#outline",
    "href": "resources/slides/06-GLS.html#outline",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Outline",
    "text": "Outline\n\nGeneral Least Squares and MLEs\nGauss-Markov Theorem & BLUEs\nMVUE\n\n\nReadings:\n\nChristensen Chapter 2 and 10 (Appendix B as needed)\nSeber & Lee Chapter 3"
  },
  {
    "objectID": "resources/slides/06-GLS.html#other-error-distributions",
    "href": "resources/slides/06-GLS.html#other-error-distributions",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Other Error Distributions",
    "text": "Other Error Distributions\nModel:\n\\[\\begin{align} \\mathbf{Y}& = \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\quad\n              \\textsf{E}[\\boldsymbol{\\epsilon}]  = \\mathbf{0}_n \\\\\n              \\textsf{Cov}[\\boldsymbol{\\epsilon}] & = \\sigma^2 \\mathbf{V}\n\\end{align}\\] where \\(\\sigma^2\\) is a scalar and \\(\\mathbf{V}\\) is a \\(n \\times n\\) symmetric matrix\n\nExamples:\n\nHeteroscedasticity: \\(\\mathbf{V}\\) is a diagonal matrix with \\([\\mathbf{V}]_{ii} =  v_i\\)\n\n\\(v_{i} = 1/n_i\\) if \\(y_i\\) is the mean of \\(n_i\\) observations\nsurvey weights or propogation of measurement errors in physics models\n\nCorrelated data:\n\ntime series; first order auto-regressive model with equally spaced data \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\sigma^2 \\mathbf{V}\\), where \\(v_{ij} = \\rho^{|i−j|}\\).\n\nHierarchical models with random effects"
  },
  {
    "objectID": "resources/slides/06-GLS.html#ols-under-a-general-covariance",
    "href": "resources/slides/06-GLS.html#ols-under-a-general-covariance",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "OLS under a General Covariance",
    "text": "OLS under a General Covariance\n\nIs it still unbiased? What’s its variance? Is it still the BLUE?\nUnbiasedness of \\(\\hat{\\boldsymbol{\\beta}}\\) \\[\\begin{align}\n\\textsf{E}[\\hat{\\boldsymbol{\\beta}}] & = \\textsf{E}[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{Y}] \\\\\n        & = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\textsf{E}[\\mathbf{Y}] =  (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\textsf{E}[\\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}] \\\\\n        & = \\boldsymbol{\\beta}+ \\mathbf{0}_p = \\boldsymbol{\\beta}\n\\end{align}\\]\nCovariance of \\(\\hat{\\boldsymbol{\\beta}}\\) \\[\\begin{align}\n\\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}] & = \\textsf{Cov}[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{Y}] \\\\\n       & = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\textsf{Cov}[\\mathbf{Y}]  \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n       & = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T  \\mathbf{V}\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\n\\end{align}\\]\nNot necessarily \\(\\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1}\\) unless \\(\\mathbf{V}\\) has a special form"
  },
  {
    "objectID": "resources/slides/06-GLS.html#gls-via-whitening",
    "href": "resources/slides/06-GLS.html#gls-via-whitening",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "GLS via Whitening",
    "text": "GLS via Whitening\nTransform the data and reduce problem to one we have solved!\n\nFor \\(\\mathbf{V}&gt; 0\\) use the Spectral Decomposition \\[\\mathbf{V}= \\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{U}^T = \\mathbf{U}\\boldsymbol{\\Lambda}^{1/2} \\boldsymbol{\\Lambda}^{1/2} \\mathbf{U}^T\\]\ndefine the symmetric square root of \\(\\mathbf{V}\\) as \\[\\mathbf{V}^{1/2} \\equiv \\mathbf{U}\\boldsymbol{\\Lambda}^{1/2} \\mathbf{U}^T\\]\ntransform model: \\[\\begin{align*}\n\\mathbf{V}^{-1/2} \\mathbf{Y}& = \\mathbf{V}^{-1/2} \\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{V}^{-1/2}\\boldsymbol{\\epsilon}\\\\\n\\tilde{\\mathbf{Y}} & = \\tilde{\\mathbf{X}} \\boldsymbol{\\beta}+ \\tilde{\\boldsymbol{\\epsilon}}\n\\end{align*}\\]\nSince \\(\\textsf{Cov}[\\tilde{\\boldsymbol{\\epsilon}}] = \\sigma^2\\mathbf{V}^{-1/2} \\mathbf{V}\\mathbf{V}^{-1/2} = \\sigma^2 \\mathbf{I}_n\\), we know that \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\equiv (\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}})^{-1} \\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{Y}}\\) is the BLUE for \\(\\boldsymbol{\\beta}\\) based on \\(\\tilde{\\mathbf{Y}}\\) (\\(\\mathbf{X}\\) full rank)"
  },
  {
    "objectID": "resources/slides/06-GLS.html#gls",
    "href": "resources/slides/06-GLS.html#gls",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "GLS",
    "text": "GLS\n\nIf \\(\\mathbf{V}\\) is known, then \\(\\tilde{\\mathbf{Y}}\\) and \\(\\mathbf{Y}\\) are known linear transformations of each other\nany estimator of \\(\\boldsymbol{\\beta}\\) that is linear in \\(\\mathbf{Y}\\) is linear in \\(\\tilde{\\mathbf{Y}}\\) and vice versa from previous results\n\\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\) is the BLUE of \\(\\boldsymbol{\\beta}\\) based on either \\(\\tilde{\\mathbf{Y}}\\) or \\(\\mathbf{Y}\\)!\nSubstituting back, we have \\[\\begin{align}\n\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}& =  (\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}})^{-1} \\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{Y}}\\\\\n       & = (\\mathbf{X}^T \\mathbf{V}^{-1/2}\\mathbf{V}^{-1/2} \\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{V}^{-1/2}\\mathbf{V}^{-1/2}\\mathbf{Y}\\\\\n       & = (\\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{Y}\n\\end{align}\\] which is the Generalized Least Squares Estimator of \\(\\boldsymbol{\\beta}\\)\n\n\n\nExercise: Weighted RegressionConsider the model \\(\\mathbf{Y}= \\beta \\mathbf{x}+ \\boldsymbol{\\epsilon}\\) where \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}]\\) is a known diagonal matrix \\(\\mathbf{V}\\). Write out the GLS estimator in terms of sums and interpret."
  },
  {
    "objectID": "resources/slides/06-GLS.html#gls-of-boldsymbolmu-full-rank-casedagger",
    "href": "resources/slides/06-GLS.html#gls-of-boldsymbolmu-full-rank-casedagger",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "GLS of \\(\\boldsymbol{\\mu}\\) (Full Rank Case)\\(^{\\dagger}\\)",
    "text": "GLS of \\(\\boldsymbol{\\mu}\\) (Full Rank Case)\\(^{\\dagger}\\)\n\nthe OLS/MLE of \\(\\boldsymbol{\\mu}\\in C(\\mathbf{X})\\) with transformed variables is \\[\\begin{align*}\n\\mathbf{P}_{\\tilde{\\mathbf{X}}} \\tilde{\\mathbf{Y}}& = \\tilde{\\mathbf{X}}\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\\\\n\\tilde{\\mathbf{X}}\\left(\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}\\right)^{-1}\\tilde{\\mathbf{X}}^T \\tilde{\\mathbf{Y}}& = \\tilde{\\mathbf{X}}\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\\\\n\\mathbf{V}^{-1/2} \\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-1}\\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{Y}& = \\mathbf{V}^{-1/2} \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\end{align*}\\]\nsince \\(\\mathbf{V}\\) is positive definite, multiple thru by \\(\\mathbf{V}^{1/2}\\), to show that \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\) is a GLS/MLE estimator of \\(\\boldsymbol{\\beta}\\) iff \\[\\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-1}\\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{Y}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\]\nIs \\(\\mathbf{P}_\\mathbf{V}\\equiv \\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-1}\\mathbf{X}^T \\mathbf{V}^{-1}\\) a projection onto \\(C(\\mathbf{X})\\)? Is it an orthogonal projection onto \\(C(\\mathbf{X})\\)?\n\n\n\\(\\dagger\\) if \\(\\mathbf{X}\\) is not full rank replace \\(\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-1}\\) with \\(\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-}\\)"
  },
  {
    "objectID": "resources/slides/06-GLS.html#projections",
    "href": "resources/slides/06-GLS.html#projections",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Projections",
    "text": "Projections\nWe want to show that \\(\\mathbf{P}_\\mathbf{V}\\equiv \\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-1}\\mathbf{X}^T \\mathbf{V}^{-1}\\) is a projection onto \\(C(\\mathbf{X})\\)\n\nfrom the definition of \\(\\mathbf{P}_\\mathbf{V}\\) it follows that \\(\\mathbf{m}\\in C(\\mathbf{P}_\\mathbf{v})\\) implies that \\(\\mathbf{m}= \\mathbf{P}_\\mathbf{V}\\mathbf{m}= \\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{m}\\) so \\(C(\\mathbf{P}_\\mathbf{V}) \\subset C(\\mathbf{X})\\)\nsince \\(\\mathbf{P}_\\tilde{\\mathbf{X}}\\) is a projection onto \\(C(\\tilde{\\mathbf{X}})\\) we have \\[\\begin{align*}\n\\mathbf{P}_{\\tilde{\\mathbf{X}}} \\tilde{\\mathbf{X}}& = \\tilde{\\mathbf{X}}\\\\\n\\tilde{\\mathbf{X}}\\left(\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}\\right)^{-1}\\tilde{\\mathbf{X}}^T \\tilde{\\mathbf{X}}& = \\tilde{\\mathbf{X}}\\\\\n\\mathbf{V}^{-1/2} \\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-1}\\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{X}& = \\mathbf{V}^{-1/2} \\mathbf{X}\\\\\n\\mathbf{V}^{-1/2} \\mathbf{P}_\\mathbf{V}\\mathbf{X}& = \\mathbf{V}^{-1/2} \\mathbf{X}\n\\end{align*}\\]\nWe can multiply both sides by \\(\\mathbf{V}^{1/2} &gt; 0\\), so that \\(\\mathbf{P}_\\mathbf{V}\\mathbf{X}= \\mathbf{X}\\)\nfor \\(\\mathbf{m}\\in C(\\mathbf{X})\\), \\(\\mathbf{P}_\\mathbf{V}\\mathbf{m}= \\mathbf{m}\\) and \\(C(\\mathbf{X}) \\subset C(\\mathbf{P}_\\mathbf{V})\\)\n\\(\\quad \\quad \\therefore C(\\mathbf{P}_\\mathbf{V}) = C(\\mathbf{X})\\) so that \\(\\mathbf{P}_\\mathbf{V}\\) is a projection onto \\(C(\\mathbf{X})\\)"
  },
  {
    "objectID": "resources/slides/06-GLS.html#oblique-projections",
    "href": "resources/slides/06-GLS.html#oblique-projections",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Oblique Projections",
    "text": "Oblique Projections\n\nProposition: ProjectionThe \\(n \\times n\\) matrix \\(\\mathbf{P}_\\mathbf{V}\\equiv \\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-1}\\mathbf{X}^T \\mathbf{V}^{-1}\\) is a projection onto the \\(C(\\mathbf{X})\\)\n\n\n\nShow that \\(\\mathbf{P}_\\mathbf{V}^2 = \\mathbf{P}_\\mathbf{V}\\) (idempotent)\nevery vector \\(\\mathbf{y}\\in \\mathbb{R}^n\\) may be written as \\(\\mathbf{y}= \\mathbf{m}+ \\mathbf{n}\\) where \\(\\mathbf{P}_\\mathbf{v}\\mathbf{y}= \\mathbf{m}\\) and \\((\\mathbf{I}_n - \\mathbf{P}_\\mathbf{v})\\mathbf{y}= \\mathbf{n}\\) where \\(\\mathbf{m}\\in C(\\mathbf{P}_\\mathbf{V})\\) and \\(\\mathbf{u}\\in N(\\mathbf{P}_\\mathbf{V})\\)\nIs \\(\\mathbf{P}_\\mathbf{V}\\) an orthogonal projection onto \\(C(\\mathbf{X})\\) for the inner product space \\((\\mathbb{R}^n, \\langle \\mathbf{v}, \\mathbf{u}\\rangle = \\mathbf{v}^T\\mathbf{u})\\)?\n\n\n\nDefinition: Oblique ProjectionFor the inner product space \\((\\mathbb{R}^n, \\langle \\mathbf{v}, \\mathbf{u}\\rangle = \\mathbf{v}^T\\mathbf{u})\\), a projection \\(\\mathbf{P}\\) that is not an orthogonal projection is called an oblique projection"
  },
  {
    "objectID": "resources/slides/06-GLS.html#loss-function",
    "href": "resources/slides/06-GLS.html#loss-function",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Loss Function",
    "text": "Loss Function\nThe GLS estimator minimizes the following generalized squared error loss: \\[\\begin{align}\n\\| \\tilde{\\mathbf{Y}}- \\tilde{\\mathbf{X}}\\boldsymbol{\\beta}\\|^2 & = (\\tilde{\\mathbf{Y}}- \\tilde{\\mathbf{X}}\\boldsymbol{\\beta})^T(\\tilde{\\mathbf{Y}}- \\tilde{\\mathbf{X}}\\boldsymbol{\\beta}) \\\\\n                    & = (\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta})^T \\mathbf{V}^{-1/2}\\mathbf{V}^{-1/2}(\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}) \\\\\n                    & = (\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta})^T \\mathbf{V}^{-1}(\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta})  \\\\\n                    & = \\| \\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}\\|^2_{\\mathbf{V}^{-1}}\n\\end{align}\\] where we can change the inner product to be \\[\\langle \\mathbf{u}, \\mathbf{v}\\rangle_{\\mathbf{V}^{-1}} \\equiv \\mathbf{u}^T\\mathbf{V}^{-1} \\mathbf{v}\\]"
  },
  {
    "objectID": "resources/slides/06-GLS.html#orthogonality-in-an-inner-product-space",
    "href": "resources/slides/06-GLS.html#orthogonality-in-an-inner-product-space",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Orthogonality in an Inner Product Space",
    "text": "Orthogonality in an Inner Product Space\n\nDefinition: Orthogonal ProjectonFor an inner product space, (\\(\\mathbb{R}^n, \\langle , \\rangle\\)). The projection \\(\\mathbf{P}\\) is an orthogonal projection if for every vector \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) in \\(\\mathbb{R}^n\\), \\[\n  \\langle \\mathbf{P}\\mathbf{x}, (\\mathbf{I}_n -\\mathbf{P})\\mathbf{y}\\rangle = \\langle (\\mathbf{I}_n - \\mathbf{P}) \\mathbf{x},\\mathbf{P}\\mathbf{y}\\rangle = 0\n\\] Equivalently: \\[\n  \\langle \\mathbf{x},\\mathbf{P}\\mathbf{y}\\rangle = \\langle \\mathbf{P}\\mathbf{x}, \\mathbf{P}\\mathbf{y}\\rangle =\\langle \\mathbf{P}\\mathbf{x},\\mathbf{y}\\rangle\n\\]\n\n\n\nExerciseShow that \\(\\mathbf{P}_\\mathbf{V}\\) is an orthogonal projection under the inner product \\(\\langle \\mathbf{x}, \\mathbf{y}\\rangle_{\\mathbf{V}^{-1}} \\equiv \\mathbf{x}^T\\mathbf{V}^{-1} \\mathbf{y}\\)"
  },
  {
    "objectID": "resources/slides/06-GLS.html#variance-of-gls",
    "href": "resources/slides/06-GLS.html#variance-of-gls",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Variance of GLS",
    "text": "Variance of GLS\n\nVariance of the GLS estimator \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}=  (\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})^{−1}\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{Y}\\) is much simpler \\[\\begin{align}\n\\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}] & = (\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})^{−1}\\mathbf{X}^T\\mathbf{V}^{−1}\\textsf{Cov}[\\mathbf{Y}]\\mathbf{V}^{−1}\\mathbf{X}(\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})^{−1} \\\\\n& = (\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})^{−1}\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{V}\\mathbf{V}^{−1}\\mathbf{X}(\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})^{−1} \\\\\n& = \\sigma^2(\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})^{−1}(\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})(\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})^{−1} \\\\\n& = \\sigma^2(\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})^{−1}\n\\end{align}\\]\n\n\n\nTheorem: Gauss-Markov-AitkinLet \\(\\tilde{\\boldsymbol{\\beta}}\\) be a linear unbiased estimator of \\(\\boldsymbol{\\beta}\\) and \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\) be the GLS estimator of \\(\\boldsymbol{\\beta}\\) in the linear model \\(\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\) with \\(\\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\) and \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\sigma^2 \\mathbf{V}\\) with \\(\\mathbf{X}\\) and \\(\\mathbf{V}&gt;0\\) known. Then \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\) is the BLUE where \\[\\textsf{Cov}[\\tilde{\\boldsymbol{\\beta}}] \\ge  \\sigma^2 (\\mathbf{X}^T\\mathbf{V}^{-1} \\mathbf{X})^{-1} = \\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}] \\]"
  },
  {
    "objectID": "resources/slides/06-GLS.html#when-will-ols-and-gls-be-equal",
    "href": "resources/slides/06-GLS.html#when-will-ols-and-gls-be-equal",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "When will OLS and GLS be Equal?",
    "text": "When will OLS and GLS be Equal?\n\nFor what covariance matrices \\(\\mathbf{V}\\) will the OLS and GLS estimators be the same?\nFiguring this out can help us understand why the GLS estimator has a lower variance in general.\n\n\n\nTheoremThe estimators \\(\\hat{\\boldsymbol{\\beta}}\\) (OLS) and \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\) (GLS) are the same for all \\(\\mathbf{Y}\\in \\mathbb{R}^n\\) iff \\[\\mathbf{V}= \\mathbf{X}\\boldsymbol{\\Psi}\\mathbf{X}^T + \\mathbf{H}\\boldsymbol{\\Phi}\\mathbf{H}^T\\] for some positive definite matrices \\(\\boldsymbol{\\Psi}\\) and \\(\\boldsymbol{\\Phi}\\) and a matrix \\(\\mathbf{H}\\) such that \\(\\mathbf{H}^T \\mathbf{X}= \\mathbf{0}\\)."
  },
  {
    "objectID": "resources/slides/06-GLS.html#outline-of-proof",
    "href": "resources/slides/06-GLS.html#outline-of-proof",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Outline of Proof",
    "text": "Outline of Proof\nWe need to show that \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\) are the same for all \\(\\mathbf{Y}\\). Since both \\(\\mathbf{P}\\) and \\(\\mathbf{P}_\\mathbf{V}\\) are projections onto \\(C(\\mathbf{X})\\), \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\) will be the same iff \\(\\mathbf{P}_\\mathbf{V}\\) is an orthogonal projection onto \\(C(\\mathbf{X})\\) so that \\(\\mathbf{P}_\\mathbf{V}\\mathbf{n}= 0\\) for \\(\\mathbf{n}\\in C(\\mathbf{X})^\\perp\\) (they have the same null spaces)\n\nShow that \\(C(\\mathbf{X}) = C(\\mathbf{V}\\mathbf{X})\\) iff \\(\\mathbf{V}\\) can be written as \\[\\mathbf{V}= \\mathbf{X}\\boldsymbol{\\Psi}\\mathbf{X}^T + \\mathbf{H}\\boldsymbol{\\Phi}\\mathbf{H}^T\\] (Show \\(C(\\mathbf{V}\\mathbf{X}) \\subset C( \\mathbf{X})\\) iff \\(\\mathbf{V}\\) has the above form and since the two subspaces have the same rank \\(C(\\mathbf{X}) = C(\\mathbf{V}\\mathbf{X})\\)\nShow that \\(C(\\mathbf{X}) = C(\\mathbf{V}^{-1} \\mathbf{X})\\) iff \\(C(\\mathbf{X}) = C(\\mathbf{V}\\mathbf{X})\\)\nShow that \\(C(\\mathbf{X})^\\perp = C(\\mathbf{V}^{-1} \\mathbf{X})^\\perp\\) iff \\(C(\\mathbf{X}) = C(\\mathbf{V}^{-1} \\mathbf{X})\\)\nShow that \\(\\mathbf{n}\\in C(\\mathbf{X})^\\perp\\) iff \\(\\mathbf{n}\\in C(\\mathbf{V}^{-1}\\mathbf{X})^\\perp\\) so \\(\\mathbf{P}_\\mathbf{V}\\mathbf{n}= 0\\)\n\n\nSee Proposition 2.7.5 and Proof in Christensen"
  },
  {
    "objectID": "resources/slides/06-GLS.html#some-intuition",
    "href": "resources/slides/06-GLS.html#some-intuition",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Some Intuition",
    "text": "Some Intuition\nFor the linear model \\(\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\) with \\(\\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\) and \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\sigma^2 \\mathbf{V}\\), we can always write\n\\[\\begin{align} \\boldsymbol{\\epsilon}& = \\mathbf{P}\\boldsymbol{\\epsilon}+ (\\mathbf{I}- \\mathbf{P})\\boldsymbol{\\epsilon}\\\\\n                   & = \\boldsymbol{\\epsilon}_\\mathbf{X}+ \\boldsymbol{\\epsilon}_N   \n\\end{align}\\]\n\nwe can recover \\(\\boldsymbol{\\epsilon}_N\\) from the data \\(\\mathbf{Y}\\) but not \\(\\boldsymbol{\\epsilon}_\\mathbf{X}\\): \\[\\begin{align} \\mathbf{P}\\mathbf{Y}& = \\mathbf{P}( \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}_\\mathbf{X}+ \\boldsymbol{\\epsilon}_n )\\\\\n                   & =  \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}_\\mathbf{X}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\\\\n    (\\mathbf{I}_n - \\mathbf{P}) \\mathbf{Y}& =  \\boldsymbol{\\epsilon}_N = \\hat{\\boldsymbol{\\epsilon}} = \\mathbf{e}\n\\end{align}\\]\nCan \\(\\boldsymbol{\\epsilon}_\\textsf{N}\\) help us estimate \\(\\mathbf{X}\\boldsymbol{\\beta}\\)? What if \\(\\boldsymbol{\\epsilon}_N\\) could tell us something about \\(\\boldsymbol{\\epsilon}_X\\)?\nYes if they were highly correlated! But if they were independent or uncorrelated then knowing \\(\\boldsymbol{\\epsilon}_\\textsf{N}\\) doesn’t help us!"
  },
  {
    "objectID": "resources/slides/06-GLS.html#intuition-continued",
    "href": "resources/slides/06-GLS.html#intuition-continued",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Intuition Continued",
    "text": "Intuition Continued\n\nFor what matrices are \\(\\boldsymbol{\\epsilon}_\\mathbf{X}\\) and \\(\\boldsymbol{\\epsilon}_N\\) uncorrelated?\nUnder \\(\\mathbf{V}= \\mathbf{I}_n\\): \\[\\begin{align}\n\\textsf{E}[\\boldsymbol{\\epsilon}_X \\boldsymbol{\\epsilon}_N] & = \\mathbf{P}\\textsf{E}[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T](\\mathbf{I}-\\mathbf{P}) \\\\\n                & = \\sigma^2 \\mathbf{P}(\\mathbf{I}- \\mathbf{P}) = \\mathbf{0}\n\\end{align}\\] so they are uncorrelated\nFor the \\(\\mathbf{V}\\) in the theorem, introduce\n\n\\(\\mathbf{Z}_\\mathbf{X}\\) where \\(\\textsf{E}[\\mathbf{Z}_\\mathbf{X}]= \\mathbf{0}_n\\) and \\(\\textsf{Cov}[\\mathbf{Z}_\\mathbf{X}] = \\boldsymbol{\\Psi}\\)\n\\(\\mathbf{Z}_\\textsf{N}\\) where \\(\\textsf{E}[\\mathbf{Z}_\\textsf{N}]= \\mathbf{0}_n\\) and \\(\\textsf{Cov}[\\mathbf{Z}_\\textsf{N}] = \\boldsymbol{\\Phi}\\)\n\\(\\mathbf{Z}_\\mathbf{X}\\) and \\(\\mathbf{Z}_\\textsf{N}\\) are uncorrelated, \\(\\textsf{E}[\\mathbf{Z}_\\mathbf{X}\\mathbf{Z}_\\textsf{N}] = \\mathbf{0}\\)\n\\(\\boldsymbol{\\epsilon}= \\mathbf{X}\\mathbf{Z}_\\mathbf{X}+ \\mathbf{H}\\mathbf{Z}_\\textsf{N}\\) so that \\(\\boldsymbol{\\epsilon}\\) has the desired mean and covariance \\(\\mathbf{V}\\) in the theorem"
  },
  {
    "objectID": "resources/slides/06-GLS.html#intuition-continued-1",
    "href": "resources/slides/06-GLS.html#intuition-continued-1",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Intuition Continued",
    "text": "Intuition Continued\nAs a consequence we have\n\n\\(\\boldsymbol{\\epsilon}_\\mathbf{X}= \\mathbf{P}\\boldsymbol{\\epsilon}= \\mathbf{X}\\mathbf{Z}_\\mathbf{X}\\)\n\\(\\boldsymbol{\\epsilon}_\\textsf{N}= (\\mathbf{I}_n - \\mathbf{P})\\boldsymbol{\\epsilon}= \\mathbf{H}\\mathbf{Z}_\\textsf{N}\\)\n\\(\\boldsymbol{\\epsilon}_\\mathbf{X}\\) and \\(\\boldsymbol{\\epsilon}_\\textsf{N}\\) are uncorrelated \\[\\begin{align}\n\\textsf{E}[\\boldsymbol{\\epsilon}_\\mathbf{X}\\boldsymbol{\\epsilon}_\\textsf{N}] & = \\textsf{E}[\\mathbf{X}\\mathbf{Z}_\\mathbf{X}\\mathbf{Z}_\\textsf{N}^T \\mathbf{H}^T] \\\\\n                  & = \\mathbf{X}\\mathbf{0}\\mathbf{H}^T \\\\\n                  & = \\mathbf{0}\n\\end{align}\\]\nso that \\(\\boldsymbol{\\epsilon}_\\mathbf{X}\\) and \\(\\boldsymbol{\\epsilon}_\\textsf{N}\\) are uncorrelated with \\(\\mathbf{V}= \\mathbf{X}\\boldsymbol{\\Psi}\\mathbf{X}^T + \\mathbf{H}\\boldsymbol{\\Phi}\\mathbf{H}\\) ^T$\nAlternative Statement of Theorem: \\(\\hat{\\boldsymbol{\\beta}}= \\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\) for all \\(\\mathbf{Y}\\) under \\(\\textsf{Cov}[\\mathbf{Y}] = \\sigma^2 \\mathbf{V}\\) iff \\(\\mathbf{P}\\mathbf{Y}\\) and \\((\\mathbf{I}- \\mathbf{P})\\mathbf{Y}\\) are uncorrelated"
  },
  {
    "objectID": "resources/slides/06-GLS.html#equivalence-of-gls-estimators",
    "href": "resources/slides/06-GLS.html#equivalence-of-gls-estimators",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Equivalence of GLS estimators",
    "text": "Equivalence of GLS estimators\nThe following corollary to the theorem establishes when two GLS estimators for different \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}]\\) are equivalent :\n\nCorollarySuppose \\(\\mathbf{V}= \\mathbf{X}\\boldsymbol{\\Psi}\\mathbf{X}^ T + \\boldsymbol{\\Omega}\\). Then \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}= \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{\\Omega}\\)\n\n\n\nCan you construct an equivalent representation based on zero correlation of \\(\\mathbf{P}_\\boldsymbol{\\Omega}\\mathbf{Y}\\) and \\((\\mathbf{I}_n - \\mathbf{P}_\\boldsymbol{\\Omega})\\mathbf{Y}\\) when \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\sigma^2 \\mathbf{V}?\\)\n\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/01-introduction.html#introduction-to-sta721",
    "href": "resources/slides/01-introduction.html#introduction-to-sta721",
    "title": "Introduction to STA721",
    "section": "Introduction to STA721",
    "text": "Introduction to STA721\n\nCourse: Theory and Application of linear models from both a frequentist (classical) and Bayesian perspective\nPrerequisites: linear algebra and a mathematical statistics course covering likelihoods and distribution theory (normal, t, F, chi-square, gamma distributions)\nIntroduce R programming as needed in the lab\nIntroduce Bayesian methods, but assume that you are co-registered in 702 or have taken it previously\nmore info on Course website https://sta721-F24.github.io/website/\n\nschedule and slides, HW, etc\ncritical dates (Midterms and Finals)\noffice hours\n\nCanvas for grades, email, announcements\n\n\nPlease let me know if there are broken links for slides, etc!"
  },
  {
    "objectID": "resources/slides/01-introduction.html#notation",
    "href": "resources/slides/01-introduction.html#notation",
    "title": "Introduction to STA721",
    "section": "Notation",
    "text": "Notation\n\nscalors are \\(a\\) (italics or math italics)\nvectors are in bold lower case, \\(\\mathbf{a}\\), with the exception of random variables\nall vectors are column vectors \\[\\mathbf{a}= \\left[\\begin{array}{c}\n    a_1 \\\\\n    a_2 \\\\\n    \\vdots \\\\\n    a_n\n     \\end{array} \\right]\n\\]\n\n\\(\\mathbf{1}_n\\) is a \\(n \\times 1\\) vector of all ones\n\ninner product \\(\\langle    \\mathbf{a}, \\mathbf{a}\\rangle = \\mathbf{a}^T\\mathbf{a}= \\|\\mathbf{a}\\|^2 = \\sum_{i=1}^n a_i^2\\); \\(\\langle    \\mathbf{a}, \\mathbf{b}\\rangle = \\mathbf{a}^T\\mathbf{b}\\)\nlength or norm of \\(\\mathbf{a}\\) is \\(\\|\\mathbf{a}\\|\\)"
  },
  {
    "objectID": "resources/slides/01-introduction.html#matrices",
    "href": "resources/slides/01-introduction.html#matrices",
    "title": "Introduction to STA721",
    "section": "Matrices",
    "text": "Matrices\n\nMatrices are represented in bold \\(\\mathbf{A}= (a_{ij})\\) \\[\\mathbf{A}= \\left[\\begin{array}{cccc}\n    a_{11} & a_{12} & \\cdots & a_{1m}  \\\\\n    a_{21} & a_{22} & \\cdots & a_{2m} \\\\\n    \\vdots & \\vdots & \\vdots & \\vdots\\\\\n    a_{n1} & a_{n2} & \\cdots & a_{nm}\n     \\end{array} \\right]\n\\]\n\nidentity matrix \\(\\mathbf{I}_n\\) square matrix with diagonal elements 1 and off diagonal 0\ntrace: if \\(\\mathbf{A}\\) is \\(n \\times m\\) \\(\\textsf{tr}(\\mathbf{A}) = \\sum_i^{\\max n,m } a_{ii}\\)\ndeterminant: for \\(\\mathbf{A}\\) is \\(n \\times n\\) then the determinant is \\(\\det(A)\\)\ninverse: if \\(\\mathbf{A}\\) is nonsingular \\(\\mathbf{A}&gt; 0\\), then its inverse is \\(\\mathbf{A}^{-1}\\)"
  },
  {
    "objectID": "resources/slides/01-introduction.html#statistical-models",
    "href": "resources/slides/01-introduction.html#statistical-models",
    "title": "Introduction to STA721",
    "section": "Statistical Models",
    "text": "Statistical Models\nOhm’s Law: \\(Y\\) is voltage across a resistor of \\(r\\) ohms and \\(X\\) is the amperes of the current through the resistor (in theory) \\[Y = rX\\]\n\nSimple linear regression for observational data \\[Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\text{  for  } i = 1,\n\\ldots, n\\]\nRewrite in vectors: \\[\\begin{eqnarray*}\n\\left[\n\\begin{array}{c}  y_1 \\\\ \\vdots \\\\  y_n \\end{array}\n\\right]   =  &\n\\left[ \\begin{array}{c}  1 \\\\ \\vdots \\\\ 1 \\end{array}  \\right]   \\beta_0 +\n\\left[ \\begin{array}{c}  x_1 \\\\ \\vdots \\\\  x_n \\end{array}\n\\right] \\beta_1 +\n\\left[ \\begin{array}{c}  \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n  \\end{array}\n\\right]\n=  &\n\\left[ \\begin{array}{cc}  1 &  x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_n\\end{array}  \\right]\n\\left[ \\begin{array}{c}  \\beta_0  \\\\  \\beta_1 \\end{array}\n\\right] +\n\\left[ \\begin{array}{c}  \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n  \\end{array}\n\\right] \\\\\n\\\\\n\\mathbf{Y}= & \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "resources/slides/01-introduction.html#nonlinear-models",
    "href": "resources/slides/01-introduction.html#nonlinear-models",
    "title": "Introduction to STA721",
    "section": "Nonlinear Models",
    "text": "Nonlinear Models\nGravitational Law: \\(F = \\alpha/d^\\beta\\) where \\(d\\) is distance between 2 objects and \\(F\\) is the force of gravity between them\n\nlog transformations \\[\\log(F) = \\log(\\alpha) - \\beta \\log(d)\\]\ncompare to noisy experimental data \\(Y_i =\\log(F_i)\\) observed at \\(x_i = \\log(d_i)\\)\nwrite \\(\\mathbf{X}= [\\mathbf{1}_n \\, \\mathbf{x}]\\)\n\\(\\boldsymbol{\\beta}= (\\log(\\alpha), -\\beta)^T\\)\nmodel with additive error on log scale \\(\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{e}\\)\ntest if \\(\\beta = 2\\)\nerror assumptions?"
  },
  {
    "objectID": "resources/slides/01-introduction.html#intrinsically-nonlinear-models",
    "href": "resources/slides/01-introduction.html#intrinsically-nonlinear-models",
    "title": "Introduction to STA721",
    "section": "Intrinsically Nonlinear Models",
    "text": "Intrinsically Nonlinear Models\nRegression function may be an intrinsically nonlinear function of \\(t_i\\) (time) and parameters \\(\\boldsymbol{\\theta}\\) \\[Y_i = f(t_i, \\boldsymbol{\\theta}) + \\epsilon_i\\]"
  },
  {
    "objectID": "resources/slides/01-introduction.html#quadratic-linear-regression",
    "href": "resources/slides/01-introduction.html#quadratic-linear-regression",
    "title": "Introduction to STA721",
    "section": "Quadratic Linear Regression",
    "text": "Quadratic Linear Regression\nTaylor’s Theorem: \\[f(t_i, \\boldsymbol{\\theta}) = f(t_0, \\boldsymbol{\\theta}) + (t_i - t_0) f'(t_0, \\boldsymbol{\\theta}) + (t_i - t_0)^2\n\\frac{f^{''}(t_0, \\boldsymbol{\\theta})}{2}  + R(t_i, \\boldsymbol{\\theta})\\]\n\n\\[Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i \\text{  for  } i = 1, \\ldots, n\\]\n\n\nRewrite in vectors: \\[\\begin{eqnarray*}\n\\left[\n\\begin{array}{c}  y_1 \\\\ \\vdots \\\\  y_n \\end{array}\n  \\right]   =  &\n\\left[ \\begin{array}{ccc}  1 &  x_1 & x_1^2 \\\\ \\vdots & \\vdots \\\\ 1 &\n     x_n &  x_n^2\\end{array}  \\right]\n\\left[ \\begin{array}{c}  \\beta_0  \\\\  \\beta_1 \\\\ \\beta_2 \\end{array}\n\\right] +\n\\left[ \\begin{array}{c}  \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n  \\end{array}\n\\right] \\\\\n& \\\\\n\\mathbf{Y}= & \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\n\\end{eqnarray*}\\]\n\n\nQuadratic in \\(x\\), but linear in \\(\\beta\\)’s - how do we know this model is adequate?"
  },
  {
    "objectID": "resources/slides/01-introduction.html#kernel-regression-nonparametric",
    "href": "resources/slides/01-introduction.html#kernel-regression-nonparametric",
    "title": "Introduction to STA721",
    "section": "Kernel Regression (NonParametric)",
    "text": "Kernel Regression (NonParametric)\n\\[y_i =  \\beta_0 + \\sum_{j = 1}^J \\beta_j e^{-\\lambda (x_i - k_j)^d} + \\epsilon_i \\text{  for  } i = 1, \\ldots, n\\] where \\(k_j\\) are kernel locations and \\(\\lambda\\) is a smoothing parameter \\[\\begin{eqnarray*}\n\\left[\n\\begin{array}{c}  y_1 \\\\ \\vdots \\\\  y_n \\end{array}\n  \\right]   =  &\n\\left[ \\begin{array}{cccc}  1 &  e^{-\\lambda (x_1 - k_1)^d} &\n     \\ldots &  e^{-\\lambda (x_1 - k_J)^d}  \\\\\n     \\vdots & \\vdots & & \\vdots \\\\ 1 & e^{-\\lambda (x_n - k_1)^d} &  \\ldots & e^{-\\lambda (x_n - k_J)^d} \\end{array}  \\right]\n\\left[ \\begin{array}{c}  \\beta_0  \\\\  \\beta_1 \\\\\\vdots \\\\ \\beta_J \\end{array}\n\\right] +\n\\left[ \\begin{array}{c}  \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n  \\end{array}\n\\right] \\\\\n& \\\\\n\\mathbf{Y}= & \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\n\\end{eqnarray*}\\]\n\nLinear in \\(\\beta\\) given \\(\\lambda\\) and \\(k_1, \\ldots k_J\\)\nLearn \\(\\lambda\\), \\(k_1, \\ldots k_J\\) and \\(J\\)"
  },
  {
    "objectID": "resources/slides/01-introduction.html#hierarchical-models",
    "href": "resources/slides/01-introduction.html#hierarchical-models",
    "title": "Introduction to STA721",
    "section": "Hierarchical Models",
    "text": "Hierarchical Models\n\n\n\n\n\n\n\n\n\n\n\n\n\neach line represent individual sample trajectories\ncorrelation between an individual’s measurements\nsimilarities within groups\ndifferences among groups?\nallow individual regressions for each individual ?\nadd more structure?"
  },
  {
    "objectID": "resources/slides/01-introduction.html#linear-regression-models",
    "href": "resources/slides/01-introduction.html#linear-regression-models",
    "title": "Introduction to STA721",
    "section": "Linear Regression Models",
    "text": "Linear Regression Models\nResponse \\(Y_i\\) and \\(p\\) predictors \\(x_{i1}, x_{i2}, \\dots x_ip\\) \\[Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\ldots \\beta_{p}\n  x_{ip} + \\epsilon_i\\]\n\nDesign matrix \\[\\mathbf{X}=\n\\left[\\begin{array}{cccc}\n1 & x_{11} & \\ldots & x_{1p} \\\\\n1 & x_{21}  & \\ldots & x_{2p} \\\\\n\\vdots & \\vdots  & \\vdots & \\vdots \\\\\n1 & x_{n1} & \\ldots & x_{np} \\\\\n\\end{array} \\right] = \\left[ \\begin{array}{cc}\n1 & \\mathbf{x}_1^T  \\\\\n\\vdots & \\vdots \\\\\n1 & \\mathbf{x}_n^T\n\\end{array} \\right] =\n\\left[\\begin{array}{cccc}\n\\mathbf{1}_n & \\mathbf{X}_1 & \\mathbf{X}_2 \\cdots \\mathbf{X}_p\n\\end{array} \\right]\n\\]\nmatrix version \\[\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\epsilon\\] what should go into \\(\\mathbf{X}\\) and do we need all columns of \\(\\mathbf{X}\\) for inference about \\(\\mathbf{Y}\\)?"
  },
  {
    "objectID": "resources/slides/01-introduction.html#linear-model",
    "href": "resources/slides/01-introduction.html#linear-model",
    "title": "Introduction to STA721",
    "section": "Linear Model",
    "text": "Linear Model\n\n\\(\\mathbf{Y}= \\mathbf{X}\\, \\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\)\n\\(\\mathbf{Y}\\) (\\(n \\times 1\\)) vector of random response (observe \\(\\mathbf{y}\\)); \\(\\mathbf{Y}, \\mathbf{y}\\in \\mathbb{R}^n\\)\n\\(\\mathbf{X}\\) (\\(n \\times p\\)) design matrix (observe)\n\\(\\boldsymbol{\\beta}\\) (\\(p \\times 1\\)) vector of coefficients (unknown)\n\\(\\boldsymbol{\\epsilon}\\) (\\(n \\times 1\\)) vector of “errors” (unobservable)\n\n\nGoals:\n\nWhat goes into \\(\\mathbf{X}\\)? (model building, model selection - post-selection inference?)\nWhat if multiple models are “good”? (model averaging or ensembles) \nWhat about the future? (Prediction)\nUncertainty Quantification - assumptions about \\(\\boldsymbol{\\epsilon}\\)\n\n\n\nAll models are wrong, but some may be useful (George Box)"
  },
  {
    "objectID": "resources/slides/01-introduction.html#ordinary-least-squares",
    "href": "resources/slides/01-introduction.html#ordinary-least-squares",
    "title": "Introduction to STA721",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\nGoal: Find the best fitting “line” or “hyper-plane” that minimizes \\[\\sum_i  (Y_i - \\mathbf{x}_i^T \\boldsymbol{\\beta})^2 = (\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}) = \\| \\mathbf{Y}-\n\\mathbf{X}\\boldsymbol{\\beta}\\|^2 \\]\n\nOptimization problem - seek \\(\\boldsymbol{\\beta}\\ni \\mathbf{X}\\boldsymbol{\\beta}\\) is close to \\(\\mathbf{Y}\\) in squared error\nMay over-fit \\(\\Rightarrow\\) add other criteria that provide a penalty Penalized Least Squares\nRobustness to extreme points \\(\\Rightarrow\\) replace quadratic loss with other functions\n\nno notion of uncertainty of estimates\n\nno structure of problem (repeated measures on individual, randomization restrictions, etc)\n\n\nNeed Distribution Assumptions of \\(\\mathbf{Y}\\) (or \\(\\boldsymbol{\\epsilon}\\)) for testing and uncertainty measures \\(\\Rightarrow\\) Likelihood and Bayesian inference"
  },
  {
    "objectID": "resources/slides/01-introduction.html#random-vectors",
    "href": "resources/slides/01-introduction.html#random-vectors",
    "title": "Introduction to STA721",
    "section": "Random Vectors",
    "text": "Random Vectors\n\nLet \\(Y_1, \\ldots Y_n\\) be random variables in \\(\\mathbb{R}\\) Then \\[\\mathbf{Y}\\equiv\n\\left[ \\begin{array}{c}\nY_1 \\\\\n\\vdots \\\\\nY_n\n\\end{array} \\right]\\] is a random vector in \\(\\mathbb{R}^n\\)\nExpectations of random vectors are defined element-wise: \\[\\textsf{E}[\\mathbf{Y}] \\equiv\n\\textsf{E}\\left[ \\begin{array}{c}\nY_1 \\\\\n\\vdots \\\\\nY_n\n\\end{array} \\right] \\equiv\n\\left[ \\begin{array}{c}\n\\textsf{E}[Y_1] \\\\\n\\vdots \\\\\n\\textsf{E}[Y_n]\n\\end{array} \\right] =\n\\left[ \\begin{array}{c}\n\\mu_1 \\\\\n\\vdots \\\\\n\\mu_n\n\\end{array} \\right]\n\\equiv \\boldsymbol{\\mu}\\in \\mathbb{R}^n\n\\] where mean or expected value \\(\\textsf{E}[Y_i] = \\mu_i\\)"
  },
  {
    "objectID": "resources/slides/01-introduction.html#model-space",
    "href": "resources/slides/01-introduction.html#model-space",
    "title": "Introduction to STA721",
    "section": "Model Space",
    "text": "Model Space\nWe will work with inner product spaces: a vector spaces, say \\(\\mathbb{R}^n\\) equipped with an inner product \\(\\langle \\mathbf{x},\\mathbf{y}\\rangle \\equiv \\mathbf{x}^T\\mathbf{y}, \\quad \\mathbf{x}, \\mathbf{y}\\in \\mathbb{R}^n\\)\n\n\nDefinition: SubspaceA set \\(\\boldsymbol{{\\cal M}}\\) is a subspace of \\(\\mathbb{R}^n\\) if is a subset of \\(\\mathbb{R}^n\\) and also a vector space.\nThat is, if \\(\\mathbf{x}_1 \\in \\boldsymbol{{\\cal M}}\\) and \\(\\mathbf{x}_2 \\in \\boldsymbol{{\\cal M}}\\), then \\(b_1\\mathbf{x}_1 + b_2 \\mathbf{x}_2 \\in \\boldsymbol{{\\cal M}}\\) for all \\(b_1, b_2 \\in \\mathbb{R}\\)\n\n\n\n\n\nDefinition: Column SpaceThe column space of \\(\\mathbf{X}\\) is \\(C(\\mathbf{X}) = \\mathbf{X}\\boldsymbol{\\beta}\\) for \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\)\n\n\n\n\nIf \\(\\mathbf{X}\\) is full column rank, then the columns of \\(\\mathbf{X}\\) form a basis for \\(C(\\mathbf{X})\\) and \\(C(\\mathbf{X})\\) is a p-dimensional subspace of \\(\\mathbb{R}^n\\)\n\n\nIf we have just a single model matrix \\(\\mathbf{X}\\), then the subspace \\(\\boldsymbol{{\\cal M}}\\) is the model space."
  },
  {
    "objectID": "resources/slides/01-introduction.html#philosophy",
    "href": "resources/slides/01-introduction.html#philosophy",
    "title": "Introduction to STA721",
    "section": "Philosophy",
    "text": "Philosophy\n\nfor many problems frequentist and Bayesian methods will give similar answers (more a matter of taste in interpretation)\n\nFor small problems, Bayesian methods allow us to incorporate prior information which provides better calibrated answers\n\nfor problems with complex designs and/or missing data Bayesian methods are often easier to implement (do not need to rely on asymptotics) \n\nFor problems involving hypothesis testing or model selection frequentist and Bayesian methods can be strikingly different.\n\nFrequentist methods often faster (particularly with “big data”) so great for exploratory analysis and for building a “data-sense”\n\nBayesian methods sit on top of Frequentist Likelihood\n\nGoemetric perspective important in both!\n\n\nImportant to understand advantages and problems of each perspective!\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#outline",
    "href": "resources/slides/05-BLUE-MVUE.html#outline",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Outline",
    "text": "Outline\n\nGauss-Markov Theorem for non-full rank \\(\\mathbf{X}\\) (recap)\nBest Linear Unbiased Estimators for Prediction\nMVUE\nDiscussion of recent papers on Best Unbiased Estimators beyond linearity\n\n\nReadings:\n\nChristensen Chapter 2 (Appendix B as needed)\nSeber & Lee Chapter 3\nFor the curious:\n\nAndersen (1962) Least squares and best unbiased estimates\nHansen (2022) A modern gauss-markov theorem\nWhat Estimators are Unbiased for Linear Models (2023) and references within"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#identifiability",
    "href": "resources/slides/05-BLUE-MVUE.html#identifiability",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Identifiability",
    "text": "Identifiability\n\nDefinition: Identifiable\\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) are identifiable if the distribution of \\(\\mathbf{Y}\\), \\(f_\\mathbf{Y}(\\mathbf{y};\n\\boldsymbol{\\beta}_1, \\sigma^2_1) = f_\\mathbf{Y}(\\mathbf{y};\n\\boldsymbol{\\beta}_2, \\sigma^2_2)\\) implies that \\((\\boldsymbol{\\beta}_1, \\sigma^2_1)^T =  (\\boldsymbol{\\beta}_2, \\sigma^2_2)^T\\)\n\n\n\nFor linear models, equivalent definition is that \\(\\boldsymbol{\\beta}\\) is identifiable if for any \\(\\boldsymbol{\\beta}_1\\) and \\(\\boldsymbol{\\beta}_2\\), \\(\\mu(\\boldsymbol{\\beta}_1)  = \\mu(\\boldsymbol{\\beta}_2)\\) or \\(\\mathbf{X}\\boldsymbol{\\beta}_1 =\\mathbf{X}\\boldsymbol{\\beta}_2\\) implies that \\(\\boldsymbol{\\beta}_1 = \\boldsymbol{\\beta}_2\\).\nIf \\(r(\\mathbf{X}) = p\\) then \\(\\boldsymbol{\\beta}\\) is identifiable\nIf \\(\\mathbf{X}\\) is not full rank, there exists \\(\\boldsymbol{\\beta}_1 \\neq \\boldsymbol{\\beta}_2\\), but \\(\\mathbf{X}\\boldsymbol{\\beta}_1 =\n\\mathbf{X}\\boldsymbol{\\beta}_2\\) and hence \\(\\boldsymbol{\\beta}\\) is not identifiable!\nidentifiable linear functions of \\(\\boldsymbol{\\beta}\\), \\(\\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\) that have an unbiased estimator are historically referred to as estimable in linear models."
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#blue-of-boldsymbollambdat-boldsymbolbeta",
    "href": "resources/slides/05-BLUE-MVUE.html#blue-of-boldsymbollambdat-boldsymbolbeta",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "BLUE of \\(\\boldsymbol{\\Lambda}^T \\boldsymbol{\\beta}\\)",
    "text": "BLUE of \\(\\boldsymbol{\\Lambda}^T \\boldsymbol{\\beta}\\)\nIf \\(\\boldsymbol{\\Lambda}^T= \\mathbf{B}\\mathbf{X}\\) for some matrix \\(\\mathbf{B}\\) (or \\(\\boldsymbol{\\Lambda}= \\mathbf{X}^T\\mathbf{B}\\) then\n\n\\(\\textsf{E}[\\mathbf{B}\\mathbf{P}\\mathbf{Y}] = \\textsf{E}[\\mathbf{B}\\mathbf{X}\\hat{\\boldsymbol{\\beta}}] = \\textsf{E}[\\boldsymbol{\\Lambda}^T \\hat{\\boldsymbol{\\beta}}] = \\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\)\nidentifiable as it is a function of \\(\\boldsymbol{\\mu}\\), linear and unbiased\nThe unique OLS estimate of \\(\\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\) is \\(\\boldsymbol{\\Lambda}^T\\hat{\\boldsymbol{\\beta}}\\)\n\\(\\mathbf{B}\\mathbf{P}\\mathbf{Y}= \\boldsymbol{\\Lambda}^T\\hat{\\boldsymbol{\\beta}}\\) is the BLUE of \\(\\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\) \\[\\begin{align*}\n& \\textsf{E}[\\|\\mathbf{B}\\mathbf{P}\\mathbf{Y}- \\mathbf{B}\\boldsymbol{\\mu}\\|^2]  \\le \\textsf{E}[\\|\\mathbf{A}\\mathbf{Y}- \\mathbf{B}\\boldsymbol{\\mu}\\|^2] \\\\\n\\Leftrightarrow & \\\\\n& \\textsf{E}[\\|\\boldsymbol{\\Lambda}^T\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta})\\|^2]  \\le \\textsf{E}[\\|\\mathbf{L}^T\\tilde{\\boldsymbol{\\beta}}- \\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\|^2]\n\\end{align*}\\] for LUE \\(\\mathbf{A}\\mathbf{Y}= \\mathbf{L}^T\\tilde{\\boldsymbol{\\beta}}\\) of \\(\\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\)"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#non-identifiable-example",
    "href": "resources/slides/05-BLUE-MVUE.html#non-identifiable-example",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Non-Identifiable Example",
    "text": "Non-Identifiable Example\nOne-way ANOVA model \\[\\mu_{ij} = \\mu + \\tau_j \\qquad \\boldsymbol{\\mu}= (\n    \\mu_{11}, \\ldots,\\mu_{n_1 1},\\mu_{12},\\ldots, \\mu_{n_2,2},\\ldots, \\mu_{1J},\n\\ldots,\n\\mu_{n_J J})^T \\]\n\nLet \\(\\boldsymbol{\\beta}_{1} = (\\mu, \\tau_1, \\ldots, \\tau_J)^T\\)\nLet \\(\\boldsymbol{\\beta}_{2} = (\\mu - 42, \\tau_1 + 42, \\ldots, \\tau_J + 42)^T\\)\nThen \\(\\boldsymbol{\\mu}_{1} = \\boldsymbol{\\mu}_{2}\\) even though \\(\\boldsymbol{\\beta}_1 \\neq \\boldsymbol{\\beta}_2\\)\n\\(\\boldsymbol{\\beta}\\) is not identifiable\nyet \\(\\boldsymbol{\\mu}\\) is identifiable, where \\(\\boldsymbol{\\mu}= \\mathbf{X}\\boldsymbol{\\beta}\\) (a linear combination of \\(\\boldsymbol{\\beta}\\))"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#lues-of-individual-beta_j",
    "href": "resources/slides/05-BLUE-MVUE.html#lues-of-individual-beta_j",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "LUEs of Individual \\(\\beta_j\\)",
    "text": "LUEs of Individual \\(\\beta_j\\)\n\nProposition: Christensen 2.1.6For \\(\\boldsymbol{\\mu}= \\mathbf{X}\\boldsymbol{\\beta}= \\sum_j \\mathbf{X}_j \\beta_j\\) \\(\\beta_j\\) is not identifiable if and only if there exists \\(\\alpha_j\\) such that \\(\\mathbf{X}_j = \\sum_{i \\neq j} \\mathbf{X}_i \\alpha_i\\)\n\n\n\nOne-way Anova Model: \\(Y_{ij} = \\mu + \\tau_j + \\epsilon_{ij}\\) \\[\\boldsymbol{\\mu}=  \\left[\n    \\begin{array}{lllll}\n\\mathbf{1}_{n_1} & \\mathbf{1}_{n_1} & \\mathbf{0}_{n_1} &  \\ldots & \\mathbf{0}_{n_1} \\\\\n\\mathbf{1}_{n_2} & \\mathbf{0}_{n_2} & \\mathbf{1}_{n_2} &  \\ldots & \\mathbf{0}_{n_2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbf{1}_{n_J} & \\mathbf{0}_{n_J} & \\mathbf{0}_{n_J} &  \\ldots & \\mathbf{1}_{n_J} \\\\\n    \\end{array} \\right]\n\\left(   \\begin{array}{l}\n      \\mu \\\\\n      \\tau_1 \\\\\n   \\tau_2 \\\\\n\\vdots \\\\\n\\tau_J\n    \\end{array} \\right)\n\\]\n\nAre any parameters \\(\\mu\\) or \\(\\tau_j\\) identifiable?"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#examples-of-boldsymbollambda-of-interest",
    "href": "resources/slides/05-BLUE-MVUE.html#examples-of-boldsymbollambda-of-interest",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Examples of \\(\\boldsymbol{\\lambda}\\) of Interest:",
    "text": "Examples of \\(\\boldsymbol{\\lambda}\\) of Interest:\n\nA \\(j\\)th element of \\(\\boldsymbol{\\beta}\\): \\(\\boldsymbol{\\lambda}= (0, 0, \\ldots,1, 0, \\ldots, 0)^T\\), \\[\\boldsymbol{\\lambda}^T\\boldsymbol{\\beta}= \\beta_j\\]\nDifference between two treatements: \\(\\tau_1 - \\tau_2\\): \\(\\boldsymbol{\\lambda}= (0, 1, -1, \\ldots, 0, \\ldots, 0)^T\\), \\[\\boldsymbol{\\lambda}^T\\boldsymbol{\\beta}= \\tau_1 - \\tau_2\\]\nEstimation at observed \\(\\mathbf{x}_i\\): \\(\\boldsymbol{\\lambda}= \\mathbf{x}_i\\) \\[\\mu_i = \\mathbf{x}_i^T \\boldsymbol{\\beta}\\]\nEstimation or prediction at a new point \\(\\mathbf{x}_*\\): \\(\\boldsymbol{\\lambda}= \\mathbf{x}_*\\), \\[\\mu_* = \\mathbf{x}_*^T \\boldsymbol{\\beta}\\]"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#another-non-full-rank-example",
    "href": "resources/slides/05-BLUE-MVUE.html#another-non-full-rank-example",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Another Non-Full Rank Example",
    "text": "Another Non-Full Rank Example\n\nx1 = -4:4\nx2 = c(-2, 1, -1, 2, 0, 2, -1, 1, -2)\nx3 = 3*x1  -2*x2\nx4 = x2 - x1 + 4\nY = 1+x1+x2+x3+x4 + c(-.5,.5,.5,-.5,0,.5,-.5,-.5,.5)\ndev.set = data.frame(Y, x1, x2, x3, x4)\n\n# Order 1\nlm1234 = lm(Y ~ x1 + x2 + x3 + x4, data=dev.set)\nround(coefficients(lm1234), 4)\n\n(Intercept)          x1          x2          x3          x4 \n          5           3           0          NA          NA \n\n# Order 2\nlm3412 = lm(Y ~ x3 + x4 + x1 + x2, data = dev.set)\nround(coefficients(lm3412), 4)\n\n(Intercept)          x3          x4          x1          x2 \n        -19           3           6          NA          NA"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#in-sample-predictions",
    "href": "resources/slides/05-BLUE-MVUE.html#in-sample-predictions",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "In Sample Predictions",
    "text": "In Sample Predictions\n\ncbind(dev.set, predict(lm1234), predict(lm3412))\n\n     Y x1 x2  x3 x4 predict(lm1234) predict(lm3412)\n1 -7.5 -4 -2  -8  6              -7              -7\n2 -3.5 -3  1 -11  8              -4              -4\n3 -0.5 -2 -1  -4  5              -1              -1\n4  1.5 -1  2  -7  7               2               2\n5  5.0  0  0   0  4               5               5\n6  8.5  1  2  -1  5               8               8\n7 10.5  2 -1   8  1              11              11\n8 13.5  3  1   7  2              14              14\n9 17.5  4 -2  16 -2              17              17\n\n\n\nBoth models agree for estimating the mean at the observed \\(\\mathbf{X}\\) points!"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#out-of-sample",
    "href": "resources/slides/05-BLUE-MVUE.html#out-of-sample",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Out of Sample",
    "text": "Out of Sample\n\nout = data.frame(test.set,\n      Y1234=predict(lm1234, new=test.set),\n      Y3412=predict(lm3412, new=test.set))\nout\n\n  x1 x2 x3 x4 Y1234 Y3412\n1  3  1  7  2    14    14\n2  6  2 14  4    23    47\n3  6  2 14  0    23    23\n4  0  0  0  4     5     5\n5  0  0  0  0     5   -19\n6  1  2  3  4     8    14\n\n\n\nAgreement for cases 1, 3, and 4 only!\nCan we determine that without finding the predictions and comparing?\nConditions for general \\(\\boldsymbol{\\Lambda}\\) or \\(\\boldsymbol{\\lambda}\\) without findingn \\(\\mathbf{B}\\) (\\(\\boldsymbol{\\beta}^T\\))?"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#conditions-for-lue-of-boldsymbollambda",
    "href": "resources/slides/05-BLUE-MVUE.html#conditions-for-lue-of-boldsymbollambda",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Conditions for LUE of \\(\\boldsymbol{\\lambda}\\)",
    "text": "Conditions for LUE of \\(\\boldsymbol{\\lambda}\\)\n\nGM requires that \\(\\boldsymbol{\\lambda}^T = \\mathbf{b}^T\\mathbf{X}\\Leftrightarrow \\boldsymbol{\\lambda}= \\mathbf{X}^T \\mathbf{b}\\) therefore \\(\\boldsymbol{\\lambda}\\in C(\\mathbf{X}^T)\\)\nSuppose we have an arbitrary \\(\\boldsymbol{\\lambda}= \\boldsymbol{\\lambda}_* + \\mathbf{u}\\), where \\(\\boldsymbol{\\lambda}_*  \\in C(\\mathbf{X}^T)\\) and \\(\\mathbf{u}\\in C(\\mathbf{X}^T)^\\perp\\) (orthogonal complement)\nLet \\(\\mathbf{P}_{\\mathbf{X}^T}\\) denote an orthogonal projection onto \\(C(\\mathbf{X}^T)\\) then \\(\\mathbf{I}- \\mathbf{P}_{\\mathbf{X}^T}\\) is an orthogonal projection onto \\(C(\\mathbf{X}^T)^\\perp\\)\n\\((\\mathbf{I}- \\mathbf{P}_{\\mathbf{X}^T})\\boldsymbol{\\lambda}= (\\mathbf{I}- \\mathbf{P}_{\\mathbf{X}^T})\\boldsymbol{\\lambda}_* + (\\mathbf{I}- \\mathbf{P}_{\\mathbf{X}^T})\\mathbf{u}= \\mathbf{0}_p + \\mathbf{u}\\)\nso if \\(\\boldsymbol{\\lambda}\\in C(\\mathbf{X}^T)\\) we will have \\((\\mathbf{I}- \\mathbf{P}_{\\mathbf{X}^T})\\boldsymbol{\\lambda}= \\mathbf{0}_p\\)! (or \\(\\mathbf{P}_{\\mathbf{X}^T} \\boldsymbol{\\lambda}= \\boldsymbol{\\lambda}\\))\nNote this is really just a generalization of Proposition 2.1.6 in Christensen that \\(\\beta_j\\) is not identifiable iff there exist scalars such that \\(\\mathbf{X}_j = \\sum_{i \\neq j} \\mathbf{X}_i \\alpha_i\\)"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#prediction-example-again",
    "href": "resources/slides/05-BLUE-MVUE.html#prediction-example-again",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Prediction Example Again",
    "text": "Prediction Example Again\nFor prediction at a new \\(\\mathbf{x}_*\\), this is implemented in the R package estimability\n\nrequire(\"estimability\" )\ncbind(epredict(lm1234, test.set), epredict(lm3412, test.set))\n\n  [,1] [,2]\n1   14   14\n2   NA   NA\n3   23   23\n4    5    5\n5   NA   NA\n6   NA   NA\n\n\nRows 2, 5, and 6 do not have a unique best linear unbiased estimator, \\(\\mathbf{x}_*^T \\boldsymbol{\\beta}\\)"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#mvue-minimum-variance-unbiased-estimators",
    "href": "resources/slides/05-BLUE-MVUE.html#mvue-minimum-variance-unbiased-estimators",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "MVUE: Minimum Variance Unbiased Estimators",
    "text": "MVUE: Minimum Variance Unbiased Estimators\n\nGauss-Markov Theorem says that OLS has minimum variance in the class of all Linear Unbiased estimators for \\(\\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\) and \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\sigma^2 \\mathbf{I}_n\\)\nRequires just first and second moments\nAdditional assumption of normality and full rank, OLS of \\(\\boldsymbol{\\beta}\\) is the same as MLEs and have minimum variance out of ALL unbiased estimators (MVUE); not just linear estimators (section 2.5 in Christensen)\nrequires Complete Sufficient Statististics and Rao-Blackwell Theorem - next semester in STA732)\nso Best Unbiased Estimators (BUE) not just BLUE!"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#what-about",
    "href": "resources/slides/05-BLUE-MVUE.html#what-about",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "What about ?",
    "text": "What about ?\n\nare there nonlinear estimators that are better than OLS under the assumptions ?\nAnderson (1962) showed OLS is not generally the MVUE with \\(\\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\) and \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\sigma^2 \\mathbf{I}_n\\)\npointed out that linear-plus-quadratic (LPQ) estimators can outperform the OLS estimator for certain error distributions.\nOther assumptions on \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\boldsymbol{\\Sigma}\\)?\n\nGeneralized Least Squares are BLUE (not necessarily equivalent to OLS)\n\nmore recently Hansen (2022) concludes that OLS is BUE over the broader class of linear models with \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}]\\) finite and \\(\\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\)\nlively ongoing debate! - see What Estimators are Unbiased for Linear Models (2023) and references within"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#next-up",
    "href": "resources/slides/05-BLUE-MVUE.html#next-up",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Next Up",
    "text": "Next Up\n\nGLS under assumptions \\(\\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\) and \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\boldsymbol{\\Sigma}\\)\nOblique projections and orthogonality with other inner products on \\(\\mathbb{R}^n\\)\nMLEs in Multivariate Normal setting\nGauss-Markov\n\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "HW/hw-02.html",
    "href": "HW/hw-02.html",
    "title": "Homework 2",
    "section": "",
    "text": "See Gradescope for any updates on due dates."
  },
  {
    "objectID": "HW/hw-02.html#due-1100pm-thurs-sept-12",
    "href": "HW/hw-02.html#due-1100pm-thurs-sept-12",
    "title": "Homework 2",
    "section": "",
    "text": "See Gradescope for any updates on due dates."
  },
  {
    "objectID": "HW/hw-02.html#rstudio",
    "href": "HW/hw-02.html#rstudio",
    "title": "Homework 2",
    "section": "RStudio",
    "text": "RStudio\nYou all should have R and RStudio installed on your computers by now or access to the Department RStudio Server https://rstudio.stat.duke.edu. If you need to setup your local version, first install the latest version of R here: https://cran.rstudio.com - remember to select the right installer for your operating system). Next, install the latest version of RStudio here: https://www.rstudio.com/products/rstudio/download/. Scroll down to the “Installers for Supported Platforms” section and find the right installer for your operating system."
  },
  {
    "objectID": "HW/hw-02.html#r-rnw",
    "href": "HW/hw-02.html#r-rnw",
    "title": "Homework 2",
    "section": "R & RNW",
    "text": "R & RNW\nYou are required to use the .Rnw format to type up this report report. To get started see Knitr with LaTeX article by Karl Broman. Feel free to include images of technical derivations if you are short of time to typeset in LaTeX. The macros.tex file should help with shortcuts. You may need to change some of your Project or Global options to get your document to compile with KnitR. If so please post under Ed in the General category or ask in Lab."
  },
  {
    "objectID": "HW/hw-02.html#getting-started-with-github-classroom",
    "href": "HW/hw-02.html#getting-started-with-github-classroom",
    "title": "Homework 2",
    "section": "Getting started with Github Classroom",
    "text": "Getting started with Github Classroom\nGo to Github classroom to accept the invitation to create a repository for this assignment at HW2\nThis will create a private repo in the STA721-F24 organization on Github with your github user name. Note: You may receive an email invitation to join STA702-F24 on your behalf.\nHit refresh, to see the link for the repo, and then click on it to go to the STA721-F24 organization in Github.\nFollow the instructions in the README in your repo and in the hw2.Rnw file to complete the assignment and push to GitHub. If you encounter issues with Gradescope submission, it is critical that you have pushed your final assignment to GitHub by the due date to validate timestamps."
  },
  {
    "objectID": "HW/hw-02.html#gradescope-submission",
    "href": "HW/hw-02.html#gradescope-submission",
    "title": "Homework 2",
    "section": "Gradescope Submission",
    "text": "Gradescope Submission\nYou MUST submit both your final .Rnw .pdf files to the repo on Github and upload the pdf to Gradescope here: https://www.gradescope.com/courses/843802/assignments\nMake sure to to pdf; ask the TA about knitting to pdf if you cannot figure it out. Be sure to submit under the right assignment entry by the due date!"
  },
  {
    "objectID": "HW/hw-02.html#grading",
    "href": "HW/hw-02.html#grading",
    "title": "Homework 2",
    "section": "Grading",
    "text": "Grading\nTotal: 20 points."
  },
  {
    "objectID": "HW/hw-03.html",
    "href": "HW/hw-03.html",
    "title": "Homework 3",
    "section": "",
    "text": "See Gradescope for any updates on due dates."
  },
  {
    "objectID": "HW/hw-03.html#due-1100pm-thurs-sept-19",
    "href": "HW/hw-03.html#due-1100pm-thurs-sept-19",
    "title": "Homework 3",
    "section": "",
    "text": "See Gradescope for any updates on due dates."
  },
  {
    "objectID": "HW/hw-03.html#rstudio",
    "href": "HW/hw-03.html#rstudio",
    "title": "Homework 3",
    "section": "RStudio",
    "text": "RStudio\nYou all should have R and RStudio installed on your computers by now or access to the Department RStudio Server https://rstudio.stat.duke.edu. If you need to setup your local version, first install the latest version of R here: https://cran.rstudio.com - remember to select the right installer for your operating system). Next, install the latest version of RStudio here: https://www.rstudio.com/products/rstudio/download/. Scroll down to the “Installers for Supported Platforms” section and find the right installer for your operating system."
  },
  {
    "objectID": "HW/hw-03.html#getting-started-with-github-classroom",
    "href": "HW/hw-03.html#getting-started-with-github-classroom",
    "title": "Homework 3",
    "section": "Getting started with Github Classroom",
    "text": "Getting started with Github Classroom\nGo to Github classroom to accept the invitation to create a repository for this assignment at HW3\nThis will create a private repo in the STA721-F24 organization on Github with your github user name. Note: You may receive an email invitation to join STA702-F24 on your behalf.\nHit refresh, to see the link for the repo, and then click on it to go to the STA721-F24 organization in Github.\nFollow the instructions in the README in your repo and in the hw#.Rnw file to complete the assignment and push to GitHub. If you encounter issues with Gradescope submission, it is critical that you have pushed your final assignment to GitHub by the due date to validate timestamps."
  },
  {
    "objectID": "HW/hw-03.html#r-rnw",
    "href": "HW/hw-03.html#r-rnw",
    "title": "Homework 3",
    "section": "R & RNW",
    "text": "R & RNW\nYou are required to use the .Rnw format to type up this report report if there are any R problems. To get started see Knitr with LaTeX article by Karl Broman. Feel free to include images of technical derivations if you are short of time to typeset in LaTeX. The macros.tex file should help with shortcuts. You may need to change some of your Project or Global options to get your document to compile with KnitR. If so please post under Ed in the General category or ask in Lab."
  },
  {
    "objectID": "HW/hw-03.html#gradescope-submission",
    "href": "HW/hw-03.html#gradescope-submission",
    "title": "Homework 3",
    "section": "Gradescope Submission",
    "text": "Gradescope Submission\nYou MUST submit both your final .Rnw .pdf files to the repo on Github and upload the pdf to Gradescope here: https://www.gradescope.com/courses/843802/assignments\nMake sure to to pdf; ask the TA about knitting to pdf if you cannot figure it out. Be sure to submit under the right assignment entry by the due date!"
  },
  {
    "objectID": "HW/hw-03.html#grading",
    "href": "HW/hw-03.html#grading",
    "title": "Homework 3",
    "section": "Grading",
    "text": "Grading\nTotal: 20 points."
  },
  {
    "objectID": "reading/02-mles.html",
    "href": "reading/02-mles.html",
    "title": "MLEs & Projections",
    "section": "",
    "text": "Readings:\n\nChristensen Chapter 1-2, Appendix A, and Appendix B\nSeber & Lee Chapter 3, Appendix B"
  },
  {
    "objectID": "reading/04-BLUE.html",
    "href": "reading/04-BLUE.html",
    "title": "Best Linear Unbiased Estimation",
    "section": "",
    "text": "We will explore properties of MLEs/OLS and linear functionals of them. In particular characterizing the class of linear unbiased estimates and minimimum variances, cumulating with the Guass Markov Theorem for both the full rank case and rank deficient models and establing conditions under which OLS/MLE estimators are the Best Linear Unbiased Estimators under the assumption that the covariance of the errors is spherically symmetric.\nReadings:\n\nChristensen Chapter 1-2, Appendix B\nSeber & Lee Chapter 3, Appendix A & Appendix B"
  },
  {
    "objectID": "reading/05-BLUE-MVUE.html",
    "href": "reading/05-BLUE-MVUE.html",
    "title": "Best Linear Unbiased Estimation in Prediction, MVUEs and BUEs",
    "section": "",
    "text": "We will continue our discussion of OLS/MLE estimators with exploring conditions for when the Best Linear Unbiased Estimators exist, with a special focus on out of sample prediction in the non-full rank case. We will outline the proof for why MLEs are also Minimum Variance Unbiased Estimators or “BUE” out of all unbiased estimators linear or non-linear under the additional assumption of normality of the errors. This opens up the question of what estimators are unbiased for linear models and if there are other nonlinear unbiased estimators that are better than OLS, a topic that has received recent attention.\nReadings:\n\nChristensen Chapter 2, Appendix B\nSeber & Lee Chapter 3\n\nFor the curious\n\nWhat Estimators are Unbiased for Linear Models (2023) and references within\nAnderson, T.W. (1962). Least squares and best unbiased estimates. The Annals of Mathematical Statistics, 33(1): 266–272\nHansen, B.E. (2022) A modern gauss-markov theorem. Econometrica"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": " Schedule",
    "section": "",
    "text": "Please refresh often in case links/content has been updated\n\n\n\n\n\n\n\n\nWeek\nDate\nLesson\nReading\nSlides\nLabs\nHomework\n\n\n\n\nWEEK 1\nTues, Aug 26\nLecture 1: Introduction to Linear Models\n\n\n\n\n\n\n\n\n\n\nThur, Aug 28\nLecture 2: MLEs & Projections\n\n\n\n\n hw-01\n\n\n\n\nFri, Aug 29\n\n\n\n\n\n\n\n\n\n\n\nWEEK 2\nTues, Sept 3\nLecture 3: Rank Deficient Models\n\n\n\n\n\n\n\n\n\n\nThur, Sept 5\nLecture 4: Best Linear Unbiased Estimation and Gauss-Markov Theorem\n\n\n\n\n hw-02\n\n\n\n\nFri, Sept 6\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 3\nTues, Sept 10\nLecture 5: BLUES for Prediction and MVUE\n\n\n\n\n\n\n\n\n\n\nThur, Sept 12\nLecture 6: Generalized Linear Squares\n\n\n\n\n hw-03\n\n\n\n\nFri, Sept 13\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 4\nTues, Sept 17\nLecture 7: Sampling Distributions\n\n\n\n\n\n\n\n\n\n\n\nThur, Sept 19\n\n\n\n\n\n\n\n\n hw-04\n\n\n\n\nFri, Sept 20\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 5\nTues, Sept 24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThur, Sept 26\n\n\n\n\n\n\n\n\n hw-05\n\n\n\n\nFri, Sept 28\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 6\nTues, Oct 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThu, Oct 3\n\n\n\n\n\n\n\n\n hw-06\n\n\n\n\nFri, Oct 4\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 7\nTue, Oct 8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThu, Oct 10\n\n\n\n\n\n\n\n\n hw-07\n\n\n\n\nFri, Oct 11\nReview for Midterm I\n\n\n\n\n\n\n\n\n\n\nWEEK 8\nTue, Oct 15\nNO CLASS FALL BREAK\n\n\n\n\n\n\n\n\n\n\n\n\nThur, Oct 17\nMidterm 1\n\n\n\n\n\n\n\n\n\n\n\n\nFri, Oct 18\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 9\nTue, Oct 22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThur, Oct 24\n\n\n\n\n\n\n\n\n hw-08\n\n\n\n\nFri, Oct 25\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 10\nTues, Oct 29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThur, Oct 31\n\n\n\n\n\n\n\n\n hw-09\n\n\n\n\nFri, Nov 1\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 11\nTues, Nov 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThurs, Nov 7\n\n\n\n\n\n\n\n\n hw-10\n\n\n\n\nFri, Nov 8\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 12\nTues, Nov 12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThurs, Nov 14\nMidterm2\n\n\n\n\n\n\n hw-11\n\n\n\n\nFri, Nov 15\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 12\nTues, Nov 19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThurs, Nov 21\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 13\nTues, Nov 26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThur, Nov 30\nNO CLASS THANKSGIVING\n\n\n\n\n\n\n\n\n\n\nWeek 14\n\n\nGraduate Reading Period\n\n\n\n\n\n\n\n\n\n\nFinals Period\nSunt, Dec 15 9am-12pm (in classroom)"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": " Syllabus",
    "section": "",
    "text": "When in doubt about anything at all, ask questions!!!\n\nPrerequisites\nALL students are expected to be familiar with all the topics covered within the required prerequisites to be in this course. That is - mathematical statistics and probability, linear algebra, and multivariate calculus. Students are also expected to be familiar with R and are encouraged to learn LaTeX during the course.\n\n\nWorkload\nWork hours will include time spent going through the preassigned readings, attending lectures and lab sessions, and doing all graded work.\n\n\nGraded Work\nGraded work for the course will consist of homework assignments, lab exercises, two midterms and a final exam. Regrade requests for problem sets and lab exercises must be done via Gradescope AT MOST 24 hours after grades are released! Regrade requests for quizzes, midterm, and final exams must be done via Gradescope AT MOST 12 hours after grades are released! Always write in complete sentences and show your steps.\nStudents’ final grades will be determined as shown below:\n\nComponent Percentage\n\n\nComponent\nPercentage\n\n\n\n\nHomework\n20%\n\n\nMidterm\n25%\n\n\nMidterm II\n25%\n\n\nParticipation\n5%\n\n\nFinal Exam\n25%\n\n\n\nThere are no make-ups for any of the graded work except for medical or familial emergencies or for reasons approved by the instructor BEFORE the due date. See the instructor in advance of relevant due dates to discuss possible alternatives.\nGrades may be curved at the end of the semester. Cumulative averages of 90% – 100% are guaranteed at least an A-, 80% – 89% at least a B-, and 70% – 79% at least a C-, however the exact ranges for letter grades will be determined at the end of the course.\n\n\nDescriptions of graded work\n\nProblem sets\nHomework will be handed out on a weekly basis. They will be based on both the lectures and labs and will be announced every Thursday or Friday – be sure to check the website regularly! Also, please note that any work that is not legible by the instructor or TAs will not be graded (given a score of 0). Every write-up must be clearly written in full sentences and clear English. Any assignment that is completely unclear to the instructors and/or TAs, may result in a grade of a 0. For programming exercises, we will be using R/knitr with \\(\\LaTeX\\) for preparing assignments using github classroom for data analysis.\nEach student MUST write up and turn in her or his own answers. You are encouraged to talk to each other regarding homework problems or to the instructor/TA. However, the write-up, solution, and code must be entirely your own work. No sharing of solutions or code! The assignments must be submitted on Gradescope under Assignments. Note that you will not be able to make online submissions after the due date, so be sure to submit before or by the Gradescope-specified deadline. You may resubmit, so when in doubt submit work early. In certain situations if there are issues with submissions, the TA may review your GitHub repository prior to the due date.\nSolutions will be curated from student solutions with proper attribution. Every week the TAs will select a representative correct solution for the assigned problems and put them together into one solutions set with each answer being attributed to the student who wrote it. If you would like to OPT OUT of having your homework solutions used for the class solutions, please let the Instructor and TAs know in advance.\nFinally, your lowest homework score will be dropped!\n\n\nLab exercises\nThe objective of the lab assignments is to give you more hands-on experience with Bayesian data analysis. Attend the lab session and learn a concept or two and some R from the TA, and then work on the computational part of the problem sets. Each lab assignment should be submitted in timely fashion. You are REQUIRED to use R/knitr (or R/Rmarkdown in some cases).\n\n\nMidterm Exams\nThere will be two inclass midterm exams. Detailed instructions on the midterm will be made available later but please check dates on the calendar well in advance!\n\n\nFinal Exam\nThere will be a final exam after the reading week. If you miss any quiz or the midterm, your grade will depend more on the final exam score since there are no make-up exams. You cannot miss the final exam! Please check the important dates on the homepage for the date and time of the final before making plans to return home at the end of the semester. Detailed instructions on the final will be made available later.\n\n\n\nLate Submission Policy\n\nno late submission of homework or lab assignments, however we will drop the lowest score in each.\n\n\n\nCourse Topics\nFor a detailed day-by-day list of topics, please refer to the Course Schedule\n\n\nAcademic integrity\nDuke University is a community dedicated to scholarship, leadership, and service and to the principles of honesty, fairness, respect, and accountability. Citizens of this community commit to reflect upon and uphold these principles in all academic and nonacademic endeavors, and to protect and promote a culture of integrity.\nRemember the Duke Community Standard that you have agreed to abide by:\n\nTo uphold the Duke Community Standard:\n\n\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\nCheating or plagiarism on any graded assessments, lying about an illness or absence and other forms of academic dishonesty are a breach of trust with classmates and faculty, violate the Duke Community Standard, and will not be tolerated. Such incidences will result in a 0 grade for all parties involved. Additionally, there may be penalties to your final class grade along with being reported to the Office of Student Conduct. Review the academic dishonesty policies at https://studentaffairs.duke.edu/conduct/z-policies/academic-dishonesty.\n\n\nDiversity & Inclusiveness\nThis course is designed so that students from all backgrounds and perspectives all feel welcome both in and out of class. Please feel free to talk to me (in person or via email) if you do not feel well-served by any aspect of this class, or if some aspect of class is not welcoming or accessible to you. My goal is for you to succeed in this course, therefore, let me know immediately if you feel you are struggling with any part of the course more than you know how to manage. Doing so will not affect your grades, but it will allow me to provide the resources to help you succeed in the course.\n\n\nDisability Statement\nStudents with disabilities who believe that they may need accommodations in the class are encouraged to contact the Student Disabilities Access Office at 919-668-1267 or disabilities@aas.duke.edu as soon as possible to better ensure that such accommodations are implemented in a timely fashion.\n\n\nOther Information\nIt can be a lot more pleasant oftentimes to get one-on-one answers and help. Make use of the teaching team’s office hours, we’re here to help! Do not hesitate to talk to me during office hours or by appointment to discuss a problem set or any aspect of the course. Questions related to course assignments and honesty policy should be directed to me. When the teaching team has announcements for you we will send an email to your Duke email address. Be sure to check your email daily.\nMost of the course components will be held in person, but occasionally may need to be held online using Zoom meetings. If you have any concerns, issues or challenges, let the instructor know as soon as possible. Also, all students are strongly encouraged to rely on the forums in Sakai, for interacting among yourself and asking other students questions. You can also ask the instructor or the TAs questions on there and we will try to respond as soon as possible. If you experience any technical issues with joining or using the forums, let the instructor know.\n\n\nProfessionalism\nTry as much as possible to refrain from texting or using your computer for anything other than coursework during class and labs. Again, the more engaged you are, the quicker you will be able to get through the materials. You are responsible for everything covered in the lecture videos, lecture notes/slides, and in the assigned readings."
  }
]