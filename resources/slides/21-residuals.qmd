---
title: "Residuals and Diagnostics"
author: "Merlise Clyde"
subtitle: "STA 721: Lecture 21"
institute: "Duke University"
format: 
  revealjs:
    theme: [simple, custom.scss]
    slide-number: true
    incremental: true
    scrollable: false
    controls: true
    fragments: true
    preview-links: auto
    smaller: true
    logo: ../../img/icon.png
    footer: <https://sta721-F24.github.io/website/>
    chalkboard: 
      boardmarker-width: 1
      chalk-width: 2
      chalk-effect: 0
    embed-resources: false
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"    
editor: 
  markdown: 
    wrap: 72
execute: 
  echo: false
number-sections: false
filters:
  - custom-numbered-blocks  
custom-numbered-blocks:
  groups:
    thmlike:
      colors: [948bde, 584eab]
      boxstyle: foldbox.simple
      collapse: false
      listin: [mathstuff]
    todos: default  
  classes:
    Theorem:
      group: thmlike
    Corollary:
      group: thmlike
    Conjecture:
      group: thmlike
      collapse: true  
    Definition:
      group: thmlike
      colors: [d999d3, a01793]
    Feature: default
    TODO:
      label: "To do"
      colors: [e7b1b4, 8c3236]
      group: todos
      listin: [stilltodo]
    DONE:
      label: "Done"
      colors: [cce7b1, 86b754]  
      group: todos  
---


```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warm = 1,
  width=72
  )
# Set dpi and height for images
library(knitr)
knitr::opts_chunk$set(fig.height = 2.65, dpi = 300,fig.align='center',fig.show='hold',size='footnotesize', small.mar=TRUE) 
# For nonsese...
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
```

## Linear Model Assumptions
{{< include macros.qmd >}}

Linear Model:
 $$ \Y = \mub + \eps $$  
Assumptions:  
\begin{eqnarray*}
   \mub \in C(\X) & \Leftrightarrow & \mub = \X \b  \\
   \eps  & \sim &  \N(\zero_n, \sigma^2 \I_n) 
\end{eqnarray*}

. . .

Focus on

-  Wrong mean for a case or cases   
-  Cases that influence the estimates of the mean   
-  Wrong distribution for $\eps$   

. . .

If $\mu_i \neq \x_i^T\b$ then expected value of $e_i = Y_i -\hat{Y}_i$
is not zero

## Standardized residuals
- Standardized residuals $e_i/\sqrt{\sigma^2(1 - h_{ii})}$
- $h_{ii}$ is the $i$th diagonal element of the hat matrix $\H = \X(\X^T\X)^{-1}\X^T$ or  _leverage_ 

-  Under correct model standardized residuals have mean 0 and scale 1 
-  plug in the usual unbiased  estimate of $\sigma^2$ 
$$r_i = e_i/\sqrt{\hat{\sigma}^2(1 - h_{ii})}
$$

-  if $h_{ii}$ is close to 1, then $\hat{Y}_i$ is close to
  $Y_i$ (why?!?) so $e_i$ is approximately 0  
-  $\var(e_i)$ is also almost 0 as $h_{ii} \to 1$,  so $e_i \to 0$ with probability 1 
-  if $h_{ii} \approx 1$ $r_i$ may not flag ``outliers''
-  even if $h_{ii}$ is not close to 1, the distribution of $r_i$ is not a $t$  (hard to judge if large $|r_i|$ is unusual)

## Outlier Test for Mean Shift

Test $H_0$: $\mu_i = \x_i^T \b$  versus $H_a$: $\mu_i = \x_i^T\b + \alpha_i$   

-   t-test for testing H$_0$: $\alpha_i = 0$ has $n - p -1$ degrees of freedom  
-  if p-value is
small declare the $i$th case to be an outlier:  $\E[Y_i]$ not given by
$\X \b$  but $\X\b + \delta_i \alpha_i$     
-  Can extend to include multiple $\delta_i$ and $\delta_j$ to test
  that case $i$ and $j$ are both outliers  
-  Extreme case $\mub = \X\b + \I_n \alphav$  all points have their
  own mean!    
-  Need to control for multiple testing without prior reason to expect a case to be an outlier 
(or use a Bayesian approach)

## Predicted Residuals
Estimates without Case (i):
\begin{eqnarray*}
  \bhat_{(i)} & = & (\X_{(i)}^T\X_{(i)})^{-1 }\X_{(i)}^T \Y_{(i)}   \\
 & = & \bhat - \frac{ (\X^T\X)^{-1} \x_i e_i}{ 1 - h_{ii}}   
\end{eqnarray*}

- Predicted residual $e_{(i)} = y_i - \x_i^T \bhat_{(i)} = \frac{e_i}{1 - h_{ii}}$

 
- variance $\var(e_{(i)}) = \frac{\sigma^2}{1 - h_{ii}}$
  
- standardized predicted residual is
$$\frac{e_{(i)}}{\sqrt{\var(e_{(i)})}}  = \frac{e_i/(1 -
  h_{ii})}{\sigma/\sqrt{1 - h_{ii}}} = \frac{e_i}{\sigma \sqrt{1 -
    h_{ii}} }
$$  
these are the same as standardized residual!

::: {.footer}
:::

## How to Calculate $\bhat_{(i)}$

How do we calculate $\bhat_{(i)}$ without case $i$ without refitting the model $n$ times?

- Note:  $\XtX  = \X_{(i)}^T\X_{(i)} + \x_i\x_i^T$  rearrange to get
  $\X_{(i)}^T\X_{(i)} = \X^T\X - \x_i\x_i^T$
- Special Case of Binomial Inverse Theorem or Woodbury Theorem:  (Thm B.56 in Christensen)
$$(\A + \u \v^T)^{-1} = \A^{-1} - \frac{\A^{-1} \u \v^T \A^{-1}}{1 + \v^T \A^{-1} \u}$$
with $\A = \X^T\X$ and $\u = -\x_i$ and $\v = \x_i$

. . .

$$(\X_{(i)}^T\X_{(i)})^{-1} = (\XtX - \x_i\x_i^T)^{-1} = (\XtX)^{-1} + \frac{(\XtX)^{-1} \x_i \x_i^T (\XtX)^{-1}}{1 - \x_i^T (\XtX)^{-1} \x_i}$$
-  use $\X_{i}^T\Y_{i} = \X^T\Y - \x_i y_i$ to get $\bhat_{(i)}$ and other quantities

::: {.footer}
:::

## External estimate of $\sigma^2$
Estimate $\shat_{(i)}$ using data with case $i$ deleted   
    \begin{eqnarray*}
\SSE_{(i)} & = &  \SSE - \frac{e_i^2}{1 - h_{ii}}   \\
\shat_{(i)} = \MSE_{(i)} & = &  \frac{\SSE_{(i)}}{n - p - 1}  \\
    \end{eqnarray*}
    
- Externally Standardized residuals   
$$t_i = \frac{e_{(i)}}{\sqrt{\shat_{(i)}/(1 - h_{ii})}}  =  \frac{y_i
  - \x_i^T \bhat_{(i)}} {\sqrt{\shat_{(i)}/(1 - h_{ii})}}   =  r_i \left(
\frac{ n - p - 1}{n - p - r_i^2}\right)^{1/2}$$   




- May still miss extreme points with high leverage, but will pick up unusual $y_i$'s

## Externally Studentized Residual

- Externally studentized residuals have a $t$ distribution with $n - p - 1$ degrees of freedom:
$$t_i = \frac{e_{(i)}}{\sqrt{\shat_{(i)}/(1 - h_{ii})}}  =  \frac{y_i
  - \x_i^T \bhat_{(i)}} {\sqrt{\shat_{(i)}/(1 - h_{ii})}}  \sim \St(n - p - 1)$$
under the hypothesis that the $i$th case is not an "outlier".



- This externally studentized residual statistic is equivalent to the t-statistic for testing that $\alpha_i$ is zero! 

. . .

(HW)

## Multiple Testing

- without prior reason to suspect an outlier, usually look at the maximum of the $|t_i|$'s
- is the $\max |t_i|$ larger than expected under the null of no outliers? 
- Need distribution of the max of Student $t$ random variables
      (simulation?)
- a conservative approach is the Bonferroni Correction:  For $n$ tests of size
  $\alpha$ the probability of falsely labeling at least one case as an
  outlier is no greater than $n \alpha$; e.g. with 21 cases and
  $\alpha = 0.05$, the probability is no greater than 1.05!
- adjust $\alpha^* = \alpha/n$ so that the probability of falsely
  labeling at least one point an outlier is $\alpha$
- with 21 cases and $\alpha = 0.05$,    
  $\alpha/n = .00238$ so use $\alpha^* = 0.0024$ for each test

## Influence - Cook's Distance

- Cook's Distance  measure of how much predictions change with $i$th
    case deleted  
\begin{eqnarray*}
D_i & =  &\frac{\| \hat{\Y}_{(i)} - \hat{\Y} \|^2}{ p \shat} =
\frac{ (\bhat_{(i)} - \bhat)^T \X^T\X (\bhat_{(i)} - \bhat) }{ p
  \shat}   \\
& = & \frac{r_i^2}{p} \frac{h_{ii}}{ 1 - h_{ii}}  
    \end{eqnarray*}

- Flag cases where $D_i > 1$ or large relative to other cases
 
- Influential Cases are those with extreme leverage or large $r_i^2$

##  Stackloss Data

```{r}
#| label: stackloss
#| out.width: "7in"
#| out.height: "5in"
#| fig.width: 7
#| fig.height: 5

library(MASS)
data(stackloss)
# variables   Air.Flow Water.Temp Acid.Conc. stack.loss
pairs(stackloss[,4:1]) 
```


## Case 21

-   Leverage $0.285$  (compare to $p/n = .19$ ) 
-   p-value $t_{21}$ is $0.0042$ 
-   Bonferroni adjusted p-value is $0.0024$  (not really an outlier?)
-   Cooks' Distance $.69$   
-   Other points?  Masking?  
-   Refit without Case 21 and compare results

. . .

Other analyses have suggested that cases (1, 2, 3, 4, 21) are outliers

- look at `MC3.REG` or `BAS` or robust regression

## Bayesian Outlier Detection


Chaloner &  Brant (1988)
["A Bayesian approach to outlier detection and residual analysis" ](http://biomet.oxfordjournals.org/content/75/4/651.full.pdf+html)  

- provides an approach to identify outliers or surprising variables by looking at the probabilty that the *error* for a case is more than $k$ standard deviations above or below zero.
$$P(|\epsilon_i| > k \sigma \mid \Y)$$

- Cases with a high probability (absolute fixed value of $k$ or relative to a multiplicity correction to determine $k$) are then investigated.  

- find posterior distribution of $\epsilon_i$ given the data and model

- Chaloner and Brant use a reference prior for the analysis $p(\b, \phi) \propto 1/\phi$

- no closed form solution for the probability but can be approximated by MCMC or a one dimensional integral!  see `?BAS::Bayes.outlier`

## Stackloss Data
```{r}
#| echo: TRUE
#| fig.width: 7
#| fig.height: 5
#| out.width: "7in"
#| out.height: "5in"
library(BAS)
stack.lm <- lm(stack.loss ~ ., data = stackloss)
stack.outliers <- BAS::Bayes.outlier(stack.lm, k = 3)
plot(stack.outliers$prob.outlier, 
     type = "h", 
     ylab = "Posterior Probability")
```

## Stackloss Data

Adjust prior probability for multiple testing with sample size of 21
and prior probability of no outliers 0.95

```{r}
#| echo: TRUE
stack.outliers <- BAS::Bayes.outlier(stack.lm, prior.prob = 0.95)



```
:::: {.columns}
::: {.column width="60%"}

cases where posterior probability 
exceeds prior probability

```{r}
#| echo: TRUE

which(stack.outliers$prob.outlier > 
      stack.outliers$prior.prob)
```
```{r}
#| echo: TRUE
#| eval: FALSE

plot(stack.loss~Air.Flow, 
  data = stackloss, pch = 19,
  col = ifelse(
        stack.outliers$prob.outlier > 
        stack.outliers$prior.prob, 
        "red", "blue")
)
```

:::
::: { .column width="40%"}

```{r}
#| echo: FALSE
#| fig.width: 6
#| fig.height: 5
#| out.width: "6in"
#| out.height: "5in"

plot(stack.loss~Air.Flow, data = stackloss, pch = 19,
     col = ifelse(stack.outliers$prob.outlier > stack.outliers$prior.prob, 
                  "red", "blue")
)
```
:::
::::




## To Remove or Not Remove?

-    For suspicious cases, check data sources for errors 
-    Check that points are not outliers/influential because of wrong mean
      function or distributional assumptions (transformations)
-   Investigate need for transformations  (use EDA at several stages) 
-   Influential cases - report results with and without cases
     (results may change - are differences meaningful?)  
-   Outlier test - suggests alternative population for the case(s); if keep in analysis,  will inflate $\shat$ and
     interval estimates   
-   Document how you handle any case deletions - reproducibility!
-   If lots of outliers - consider throwing out the model rather than data 
-   Alternative Model Averaging with Outlier models  
-   Robust Regression Methods - M-estimation, L-estimation, S-estimation, MM-estimation, etc.  or Bayes with heavy tails






