{
  "hash": "5c91a087ebdb22bfb5b7b4b6b7d2a780",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Residuals and Diagnostics\"\nauthor: \"Merlise Clyde\"\nsubtitle: \"STA 721: Lecture 21\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    smaller: true\n    logo: ../../img/icon.png\n    footer: <https://sta721-F24.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"    \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\nnumber-sections: false\nfilters:\n  - custom-numbered-blocks  \ncustom-numbered-blocks:\n  groups:\n    thmlike:\n      colors: [948bde, 584eab]\n      boxstyle: foldbox.simple\n      collapse: false\n      listin: [mathstuff]\n    todos: default  \n  classes:\n    Theorem:\n      group: thmlike\n    Corollary:\n      group: thmlike\n    Conjecture:\n      group: thmlike\n      collapse: true  \n    Definition:\n      group: thmlike\n      colors: [d999d3, a01793]\n    Feature: default\n    TODO:\n      label: \"To do\"\n      colors: [e7b1b4, 8c3236]\n      group: todos\n      listin: [stilltodo]\n    DONE:\n      label: \"Done\"\n      colors: [cce7b1, 86b754]  \n      group: todos  \n---\n\n\n\n\n\n## Linear Model Assumptions\n\n\\usepackage{xcolor}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator{\\sgn}{sgn}\n\\newcommand{\\e}{\\mathbf{e}}\n\\newcommand{\\Mb}{\\mathbf{M}}\n\\renewcommand{\\P}{\\mathbf{P}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\R}{\\textsf{R}}\n\\newcommand{\\mat}[1] {\\mathbf{#1}}\n\\newcommand{\\pen}{\\textsf{pen}}\n\\newcommand{\\E}{\\textsf{E}}\n\\newcommand{\\SE}{\\textsf{SE}}\n\\newcommand{\\SSE}{\\textsf{SSE}}\n\\newcommand{\\RSS}{\\textsf{RSS}}\n\\newcommand{\\FSS}{\\textsf{FSS}}\n\\renewcommand{\\SS}{\\textsf{SS}}\n\\newcommand{\\MSE}{\\textsf{MSE}}\n\\newcommand{\\SSR}{\\textsf{SSR}}\n\\newcommand{\\Be}{\\textsf{Beta}}\n\\newcommand{\\St}{\\textsf{St}}\n\\newcommand{\\Ca}{\\textsf{C}}\n\\newcommand{\\Lv}{\\textsf{LÃ©vy}}\n\\newcommand{\\Exp}{\\textsf{Exp}}\n\\newcommand{\\GDP}{\\textsf{GDP}}\n\\newcommand{\\NcSt}{\\textsf{NcSt}}\n\\newcommand{\\Bin}{\\textsf{Bin}}\n\\newcommand{\\NB}{\\textsf{NegBin}}\n\\newcommand{\\Mult}{\\textsf{MultNom}}\n\\renewcommand{\\NG}{\\textsf{NG}}\n\\newcommand{\\N}{\\textsf{N}}\n\\newcommand{\\Ber}{\\textsf{Ber}}\n\\newcommand{\\Poi}{\\textsf{Poi}}\n\\newcommand{\\Gam}{\\textsf{Gamma}}\n\\newcommand{\\BB}{\\textsf{BB}}\n\\newcommand{\\BF}{\\textsf{BF}}\n\\newcommand{\\Gm}{\\textsf{G}}\n\\newcommand{\\Un}{\\textsf{Unif}}\n\\newcommand{\\Ex}{\\textsf{Exp}}\n\\newcommand{\\DE}{\\textsf{DE}}\n\\newcommand{\\tr}{\\textsf{tr}}\n\\newcommand{\\cF}{{\\cal{F}}}\n\\newcommand{\\cL}{{\\cal{L}}}\n\\newcommand{\\cI}{{\\cal{I}}}\n\\newcommand{\\cB}{{\\cal{B}}}\n\\newcommand{\\cP}{{\\cal{P}}}\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\pperp}{\\mathrel{{\\rlap{$\\,\\perp$}\\perp\\,\\,}}}\n\\newcommand{\\OFP}{(\\Omega,\\cF, \\P)}\n\\newcommand{\\eps}{\\boldsymbol{\\epsilon}}\n\\def\\ehat{\\hat{\\boldsymbol{\\epsilon}}}\n\\newcommand{\\Psib}{\\boldsymbol{\\Psi}}\n\\newcommand{\\Omegab}{\\boldsymbol{\\Omega}}\n\\newcommand{\\1}{\\mathbf{1}_n}\n\\newcommand{\\gap}{\\vspace{8mm}}\n\\newcommand{\\ind}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}}\n\\newcommand{\\iid}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}}\n\\newcommand{\\simiid}{\\ensuremath{\\mathrel{\\mathop{\\sim}\\limits^{\\rm\niid}}}}\n\\newcommand{\\eqindis}{\\mathrel{\\mathop{=}\\limits^{\\rm D}}}\n\\newcommand{\\SSZ}{S_{zz}}\n\\newcommand{\\SZW}{S_{zw}}\n\\newcommand{\\Var}{\\textsf{Var}}\n\\newcommand{\\corr}{\\textsf{corr}}\n\\newcommand{\\cov}{\\textsf{cov}}\n\\newcommand{\\diag}{\\textsf{diag}}\n\\newcommand{\\var}{\\textsf{var}}\n\\newcommand{\\Cov}{\\textsf{Cov}}\n\\newcommand{\\Sam}{{\\cal S}}\n\\newcommand{\\VS}{{\\cal V}}\n\\def\\H{\\mathbf{H}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\tY}{\\tilde{\\mathbf{Y}}}\n\\newcommand{\\Yhat}{\\hat{\\mathbf{Y}}}\n\\newcommand{\\yhat}{\\hat{\\mathbf{y}}}\n\\newcommand{\\Yobs}{\\mathbf{Y}_{{\\cal S}}}\n\\newcommand{\\barYobs}{\\bar{Y}_{{\\cal S}}}\n\\newcommand{\\barYmiss}{\\bar{Y}_{{\\cal S}^c}}\n\\def\\bv{\\mathbf{b}}\n\\def\\X{\\mathbf{X}}\n\\def\\XtX{\\X^T\\X}\n\\def\\tX{\\tilde{\\mathbf{X}}}\n\\def\\x{\\mathbf{x}}\n\\def\\xbar{\\bar{\\mathbf{x}}}\n\\def\\Xbar{\\bar{\\mathbf{X}}}\n\\def\\Xg{\\mathbf{X}_{\\boldsymbol{\\gamma}}}\n\\def\\Ybar{\\bar{\\Y}}\n\\def\\ybar{\\bar{y}}\n\\def\\y{\\mathbf{y}}\n\\def\\Yf{\\mathbf{Y_f}}\n\\def\\W{\\mathbf{W}}\n\\def\\L{\\mathbf{L}}\n\\def\\m{\\mathbf{m}}\n\\def\\n{\\mathbf{n}}\n\\def\\w{\\mathbf{w}}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\z{\\mathbf{z}}\n\\def\\v{\\mathbf{v}}\n\\def\\u{\\mathbf{u}}\n\n\\def\\zero{\\mathbf{0}}\n\\def\\one{\\mathbf{1}}\n\\newcommand{\\taub}{\\boldsymbol{\\tau}}\n\\newcommand{\\betav}{\\boldsymbol{\\beta}}\n\\newcommand{\\alphav}{\\boldsymbol{\\alpha}}\n\\newcommand{\\bfomega}{\\boldsymbol{\\omega}}\n\\newcommand{\\bfchi}{\\boldsymbol{\\chi}}\n\\newcommand{\\bfLambda}{\\boldsymbol{\\Lambda}}\n\\newcommand{\\Lmea}{{\\cal L}} \n\\newcommand{\\scale}{\\bflambda} \n\\newcommand{\\Scale}{\\bfLambda} \n\\newcommand{\\bfscale}{\\bflambda} \n\\newcommand{\\mean}{\\bfchi}\n\\newcommand{\\loc}{\\bfchi}\n\\newcommand{\\bfmean}{\\bfchi}\n\\newcommand{\\bfx}{\\x}\n\\renewcommand{\\k}{g}\n\\newcommand{\\Gen}{{\\cal G}}\n\\newcommand{\\Levy}{L{\\'e}vy}\n\\def\\bfChi    {\\boldsymbol{\\Chi}}\n\\def\\bfOmega  {\\boldsymbol{\\Omega}}\n\\newcommand{\\Po}{\\mbox{\\sf P}}\n\\newcommand{\\A}{\\mathbf{A}}\n\\def\\a{\\mathbf{a}}\n\\def\\K{\\mathbf{K}}\n\\newcommand{\\B}{\\mathbf{B}}\n\\def\\b{\\boldsymbol{\\beta}}\n\\def\\bhat{\\hat{\\boldsymbol{\\beta}}}\n\\def\\btilde{\\tilde{\\boldsymbol{\\beta}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\bg{\\boldsymbol{\\beta_\\gamma}}\n\\def\\bgnot{\\boldsymbol{\\beta_{(-\\gamma)}}}\n\\def\\mub{\\boldsymbol{\\mu}}\n\\def\\tmub{\\tilde{\\boldsymbol{\\mu}}}\n\\def\\muhat{\\hat{\\boldsymbol{\\mu}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\tk{\\boldsymbol{\\theta}_k}\n\\def\\tj{\\boldsymbol{\\theta}_j}\n\\def\\Mk{\\boldsymbol{{\\cal M}}_k}\n\\def\\M{\\boldsymbol{{\\cal M}}}\n\\def\\Mj{\\boldsymbol{{\\cal M}}_j}\n\\def\\Mi{\\boldsymbol{{\\cal M}}_i}\n\\def\\Mg{{\\boldsymbol{{\\cal M}_\\gamma}}}\n\\def\\Mnull{\\boldsymbol{{\\cal M}}_{N}}\n\\def\\gMPM{\\boldsymbol{\\gamma}_{\\text{MPM}}}\n\\def\\gHPM{\\boldsymbol{\\gamma}_{\\text{HPM}}}\n\\def\\Mfull{\\boldsymbol{{\\cal M}}_{F}}\n\\def\\NS{\\boldsymbol{{\\cal N}}}\n\\def\\tg{\\boldsymbol{\\theta}_{\\boldsymbol{\\gamma}}}\n\\def\\g{\\boldsymbol{\\gamma}}\n\\def\\eg{\\boldsymbol{\\eta}_{\\boldsymbol{\\gamma}}}\n\\def\\G{\\mathbf{G}}\n\\def\\cM{\\cal M}\n\\def\\D{\\boldsymbol{\\Delta}}\n\\def\\Dbf{\\mathbf{D}}\n\\def \\shat{{\\hat{\\sigma}}^2}\n\\def\\uv{\\mathbf{u}}\n\\def\\l {\\lambda}\n\\def\\d{\\delta}\n\\def\\deltab{\\mathbf{\\delta}}\n\\def\\Sigmab{\\boldsymbol{\\Sigma}}\n\\def\\Phib{\\boldsymbol{\\Phi}}\n\\def\\Lambdab{\\boldsymbol{\\Lambda}}\n\\def\\lambdab{\\boldsymbol{\\lambda}}\n\\def\\Mg{{\\cal M}_\\gamma}\n\\def\\S{{\\cal{S}}}\n\\def\\Sbf{{\\mathbf{S}}}\n\\def\\qg{p_{\\boldsymbol{\\gamma}}}\n\\def\\pg{p_{\\boldsymbol{\\gamma}}}\n\\def\\T{\\boldsymbol{\\Theta}}\n\\def\\Tb{\\boldsymbol{\\Theta}}\n\\def\\t{\\mathbf{t}}\n\\def\\pause{\\vspace{1mm}}\n\n\n\nLinear Model:\n $$ \\Y = \\mub + \\eps $$  \nAssumptions:  \n\\begin{eqnarray*}\n   \\mub \\in C(\\X) & \\Leftrightarrow & \\mub = \\X \\b  \\\\\n   \\eps  & \\sim &  \\N(\\zero_n, \\sigma^2 \\I_n) \n\\end{eqnarray*}\n\n. . .\n\nFocus on\n\n-  Wrong mean for a case or cases   \n-  Cases that influence the estimates of the mean   \n-  Wrong distribution for $\\eps$   \n\n. . .\n\nIf $\\mu_i \\neq \\x_i^T\\b$ then expected value of $e_i = Y_i -\\hat{Y}_i$\nis not zero\n\n## Standardized residuals\n- Standardized residuals $e_i/\\sqrt{\\sigma^2(1 - h_{ii})}$\n- $h_{ii}$ is the $i$th diagonal element of the hat matrix $\\H = \\X(\\X^T\\X)^{-1}\\X^T$ or  _leverage_ \n\n-  Under correct model standardized residuals have mean 0 and scale 1 \n-  plug in the usual unbiased  estimate of $\\sigma^2$ \n$$r_i = e_i/\\sqrt{\\hat{\\sigma}^2(1 - h_{ii})}\n$$\n\n-  if $h_{ii}$ is close to 1, then $\\hat{Y}_i$ is close to\n  $Y_i$ (why?!?) so $e_i$ is approximately 0  \n-  $\\var(e_i)$ is also almost 0 as $h_{ii} \\to 1$,  so $e_i \\to 0$ with probability 1 \n-  if $h_{ii} \\approx 1$ $r_i$ may not flag ``outliers''\n-  even if $h_{ii}$ is not close to 1, the distribution of $r_i$ is not a $t$  (hard to judge if large $|r_i|$ is unusual)\n\n## Outlier Test for Mean Shift\n\nTest $H_0$: $\\mu_i = \\x_i^T \\b$  versus $H_a$: $\\mu_i = \\x_i^T\\b + \\alpha_i$   \n\n-   t-test for testing H$_0$: $\\alpha_i = 0$ has $n - p -1$ degrees of freedom  \n-  if p-value is\nsmall declare the $i$th case to be an outlier:  $\\E[Y_i]$ not given by\n$\\X \\b$  but $\\X\\b + \\delta_i \\alpha_i$     \n-  Can extend to include multiple $\\delta_i$ and $\\delta_j$ to test\n  that case $i$ and $j$ are both outliers  \n-  Extreme case $\\mub = \\X\\b + \\I_n \\alphav$  all points have their\n  own mean!    \n-  Need to control for multiple testing without prior reason to expect a case to be an outlier \n(or use a Bayesian approach)\n\n## Predicted Residuals\nEstimates without Case (i):\n\\begin{eqnarray*}\n  \\bhat_{(i)} & = & (\\X_{(i)}^T\\X_{(i)})^{-1 }\\X_{(i)}^T \\Y_{(i)}   \\\\\n & = & \\bhat - \\frac{ (\\X^T\\X)^{-1} \\x_i e_i}{ 1 - h_{ii}}   \n\\end{eqnarray*}\n\n- Predicted residual $e_{(i)} = y_i - \\x_i^T \\bhat_{(i)} = \\frac{e_i}{1 - h_{ii}}$\n\n \n- variance $\\var(e_{(i)}) = \\frac{\\sigma^2}{1 - h_{ii}}$\n  \n- standardized predicted residual is\n$$\\frac{e_{(i)}}{\\sqrt{\\var(e_{(i)})}}  = \\frac{e_i/(1 -\n  h_{ii})}{\\sigma/\\sqrt{1 - h_{ii}}} = \\frac{e_i}{\\sigma \\sqrt{1 -\n    h_{ii}} }\n$$  \nthese are the same as standardized residual!\n\n::: {.footer}\n:::\n\n## How to Calculate $\\bhat_{(i)}$\n\nHow do we calculate $\\bhat_{(i)}$ without case $i$ without refitting the model $n$ times?\n\n- Note:  $\\XtX  = \\X_{(i)}^T\\X_{(i)} + \\x_i\\x_i^T$  rearrange to get\n  $\\X_{(i)}^T\\X_{(i)} = \\X^T\\X - \\x_i\\x_i^T$\n- Special Case of Binomial Inverse Theorem or Woodbury Theorem:  (Thm B.56 in Christensen)\n$$(\\A + \\u \\v^T)^{-1} = \\A^{-1} - \\frac{\\A^{-1} \\u \\v^T \\A^{-1}}{1 + \\v^T \\A^{-1} \\u}$$\nwith $\\A = \\X^T\\X$ and $\\u = -\\x_i$ and $\\v = \\x_i$\n\n. . .\n\n$$(\\X_{(i)}^T\\X_{(i)})^{-1} = (\\XtX - \\x_i\\x_i^T)^{-1} = (\\XtX)^{-1} + \\frac{(\\XtX)^{-1} \\x_i \\x_i^T (\\XtX)^{-1}}{1 - \\x_i^T (\\XtX)^{-1} \\x_i}$$\n-  use $\\X_{i}^T\\Y_{i} = \\X^T\\Y - \\x_i y_i$ to get $\\bhat_{(i)}$ and other quantities\n\n::: {.footer}\n:::\n\n## External estimate of $\\sigma^2$\nEstimate $\\shat_{(i)}$ using data with case $i$ deleted   \n    \\begin{eqnarray*}\n\\SSE_{(i)} & = &  \\SSE - \\frac{e_i^2}{1 - h_{ii}}   \\\\\n\\shat_{(i)} = \\MSE_{(i)} & = &  \\frac{\\SSE_{(i)}}{n - p - 1}  \\\\\n    \\end{eqnarray*}\n    \n- Externally Standardized residuals   \n$$t_i = \\frac{e_{(i)}}{\\sqrt{\\shat_{(i)}/(1 - h_{ii})}}  =  \\frac{y_i\n  - \\x_i^T \\bhat_{(i)}} {\\sqrt{\\shat_{(i)}/(1 - h_{ii})}}   =  r_i \\left(\n\\frac{ n - p - 1}{n - p - r_i^2}\\right)^{1/2}$$   \n\n\n\n\n- May still miss extreme points with high leverage, but will pick up unusual $y_i$'s\n\n## Externally Studentized Residual\n\n- Externally studentized residuals have a $t$ distribution with $n - p - 1$ degrees of freedom:\n$$t_i = \\frac{e_{(i)}}{\\sqrt{\\shat_{(i)}/(1 - h_{ii})}}  =  \\frac{y_i\n  - \\x_i^T \\bhat_{(i)}} {\\sqrt{\\shat_{(i)}/(1 - h_{ii})}}  \\sim \\St(n - p - 1)$$\nunder the hypothesis that the $i$th case is not an \"outlier\".\n\n\n\n- This externally studentized residual statistic is equivalent to the t-statistic for testing that $\\alpha_i$ is zero! \n\n. . .\n\n(HW)\n\n## Multiple Testing\n\n- without prior reason to suspect an outlier, usually look at the maximum of the $|t_i|$'s\n- is the $\\max |t_i|$ larger than expected under the null of no outliers? \n- Need distribution of the max of Student $t$ random variables\n      (simulation?)\n- a conservative approach is the Bonferroni Correction:  For $n$ tests of size\n  $\\alpha$ the probability of falsely labeling at least one case as an\n  outlier is no greater than $n \\alpha$; e.g. with 21 cases and\n  $\\alpha = 0.05$, the probability is no greater than 1.05!\n- adjust $\\alpha^* = \\alpha/n$ so that the probability of falsely\n  labeling at least one point an outlier is $\\alpha$\n- with 21 cases and $\\alpha = 0.05$,    \n  $\\alpha/n = .00238$ so use $\\alpha^* = 0.0024$ for each test\n\n## Influence - Cook's Distance\n\n- Cook's Distance  measure of how much predictions change with $i$th\n    case deleted  \n\\begin{eqnarray*}\nD_i & =  &\\frac{\\| \\hat{\\Y}_{(i)} - \\hat{\\Y} \\|^2}{ p \\shat} =\n\\frac{ (\\bhat_{(i)} - \\bhat)^T \\X^T\\X (\\bhat_{(i)} - \\bhat) }{ p\n  \\shat}   \\\\\n& = & \\frac{r_i^2}{p} \\frac{h_{ii}}{ 1 - h_{ii}}  \n    \\end{eqnarray*}\n\n- Flag cases where $D_i > 1$ or large relative to other cases\n \n- Influential Cases are those with extreme leverage or large $r_i^2$\n\n##  Stackloss Data\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](21-residuals_files/figure-revealjs/stackloss-1.png){fig-align='center' width=7in height=5in}\n:::\n:::\n\n\n\n\n## Case 21\n\n-   Leverage $0.285$  (compare to $p/n = .19$ ) \n-   p-value $t_{21}$ is $0.0042$ \n-   Bonferroni adjusted p-value is $0.0024$  (not really an outlier?)\n-   Cooks' Distance $.69$   \n-   Other points?  Masking?  \n-   Refit without Case 21 and compare results\n\n. . .\n\nOther analyses have suggested that cases (1, 2, 3, 4, 21) are outliers\n\n- look at `MC3.REG` or `BAS` or robust regression\n\n## Bayesian Outlier Detection\n\n\nChaloner &  Brant (1988)\n[\"A Bayesian approach to outlier detection and residual analysis\" ](http://biomet.oxfordjournals.org/content/75/4/651.full.pdf+html)  \n\n- provides an approach to identify outliers or surprising variables by looking at the probabilty that the *error* for a case is more than $k$ standard deviations above or below zero.\n$$P(|\\epsilon_i| > k \\sigma \\mid \\Y)$$\n\n- Cases with a high probability (absolute fixed value of $k$ or relative to a multiplicity correction to determine $k$) are then investigated.  \n\n- find posterior distribution of $\\epsilon_i$ given the data and model\n\n- Chaloner and Brant use a reference prior for the analysis $p(\\b, \\phi) \\propto 1/\\phi$\n\n- no closed form solution for the probability but can be approximated by MCMC or a one dimensional integral!  see `?BAS::Bayes.outlier`\n\n## Stackloss Data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(BAS)\nstack.lm <- lm(stack.loss ~ ., data = stackloss)\nstack.outliers <- BAS::Bayes.outlier(stack.lm, k = 3)\nplot(stack.outliers$prob.outlier, \n     type = \"h\", \n     ylab = \"Posterior Probability\")\n```\n\n::: {.cell-output-display}\n![](21-residuals_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=7in height=5in}\n:::\n:::\n\n\n\n## Stackloss Data\n\nAdjust prior probability for multiple testing with sample size of 21\nand prior probability of no outliers 0.95\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstack.outliers <- BAS::Bayes.outlier(stack.lm, prior.prob = 0.95)\n```\n:::\n\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\ncases where posterior probability \nexceeds prior probability\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nwhich(stack.outliers$prob.outlier > \n      stack.outliers$prior.prob)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  4 21\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(stack.loss~Air.Flow, \n  data = stackloss, pch = 19,\n  col = ifelse(\n        stack.outliers$prob.outlier > \n        stack.outliers$prior.prob, \n        \"red\", \"blue\")\n)\n```\n:::\n\n\n\n:::\n::: { .column width=\"40%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](21-residuals_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=6in height=5in}\n:::\n:::\n\n\n:::\n::::\n\n\n\n\n## To Remove or Not Remove?\n\n-    For suspicious cases, check data sources for errors \n-    Check that points are not outliers/influential because of wrong mean\n      function or distributional assumptions (transformations)\n-   Investigate need for transformations  (use EDA at several stages) \n-   Influential cases - report results with and without cases\n     (results may change - are differences meaningful?)  \n-   Outlier test - suggests alternative population for the case(s); if keep in analysis,  will inflate $\\shat$ and\n     interval estimates   \n-   Document how you handle any case deletions - reproducibility!\n-   If lots of outliers - consider throwing out the model rather than data \n-   Alternative Model Averaging with Outlier models  \n-   Robust Regression Methods - M-estimation, L-estimation, S-estimation, MM-estimation, etc.  or Bayes with heavy tails\n\n\n\n\n\n\n",
    "supporting": [
      "21-residuals_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}