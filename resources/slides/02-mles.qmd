---
subtitle: "STA 721: Lecture 2"
title: "Maximum Likelihood Estimation"
author: "Merlise Clyde (clyde@duke.edu)"
institute: "Duke University"
format: 
  revealjs:
    theme: [simple, custom.scss]
    slide-number: true
    incremental: true
    scrollable: false
    controls: true
    fragments: true
    preview-links: auto
    smaller: true
    logo: ../../img/icon.png
    footer: <https://sta721-F24.github.io/website/>
    chalkboard: 
      boardmarker-width: 1
      chalk-width: 2
      chalk-effect: 0
    embed-resources: false
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"   
editor: 
  markdown: 
    wrap: 72
execute: 
  echo: false
number-sections: false
filters:
  - custom-numbered-blocks  
custom-numbered-blocks:
  groups:
    thmlike:
      colors: [948bde, 584eab]
      boxstyle: foldbox.simple
      collapse: false
      listin: [mathstuff]
    todos: default  
  classes:
    Theorem:
      group: thmlike
    Corollary:
      group: thmlike
    Conjecture:
      group: thmlike
      collapse: true  
    Definition:
      group: thmlike
      colors: [d999d3, a01793]
    Feature: 
       numbered: false
    TODO:
      label: "To do"
      colors: [e7b1b4, 8c3236]
      group: todos
      listin: [stilltodo]
    DONE:
      label: "Done"
      colors: [cce7b1, 86b754]  
      group: todos  
---


```{r setup, include=FALSE}
# R options
#  

options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warm = 1,
  width=72
  )
# Set dpi and height for images
library(knitr)
knitr::opts_chunk$set(fig.height = 2.65, dpi = 300,fig.align='center',fig.show='hold',size='footnotesize', small.mar=TRUE) 
# For nonsese...
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
```

## Outline
{{< include macros.qmd >}}

- Likelihood Function
- Projections
- Maximum Likelihood Estimates


. . .

Readings: Christensen Chapter 1-2, Appendix A, and Appendix B

## Normal Model

Take an random vector $\Y \in \bbR^n$ which is observable and decompose  
$$ \Y = \mub + \eps$$ 

- $\mub \in \bbR^n$ (unknown, fixed)  
- $\eps \in \bbR^n$ unobservable error vector (random)

. . .

Usual assumptions? 

- $E[\epsilon_i] = 0 \ \forall i \Leftrightarrow \E[\eps] = \zero$  $\quad \Rightarrow \E[\Y] = \mub$
  (mean vector) 
- $\epsilon_i$ independent with $\Var(\epsilon_i) = \sigma^2$ and
  $\Cov(\epsilon_i, \epsilon_j) = 0$
- Matrix version 
$\Cov[\eps] \equiv \left[ (\E\left[(\epsilon_i -\E[\epsilon_i])(\epsilon_j - \E[\epsilon_j])\right]\right]_{ij} = \sigma^2 \I_n  
\quad \Rightarrow \Cov[\Y] = \sigma^2 \I_n$  (errors are uncorrelated) 
- $\eps_i \iid \N(0, \sigma^2)$  implies that $Y_i \ind \N(\mu_i, \sigma^2)$

## Likelihood Function

The likelihood function for $\mub, \sigma^2$ is proportional to the
sampling distribution of the data 

\begin{eqnarray*}
 \cL(\mub, \sigma^2) & \propto & \prod_{i = 1}^n \frac{1}{\sqrt{(2 \pi
                                 \sigma^2)}} \exp{- \frac{1}{2}
                                 \left\{ \frac{( Y_i
                                 - \mu_i)^2}{\sigma^2} \right\}}
                                 \\
 & \propto & ({2 \pi} \sigma^2)^{-n/2}
 \exp{\left\{ - \frac 1 2  \frac{ \sum_i(Y_i - \mu_i)^2 )}{\sigma^2}
\right\}}   \\
   & \propto & (\sigma^2)^{-n/2}
 \exp{\left\{ - \frac 1 2 \frac{\| \Y - \mub \|^2}{\sigma^2}
\right\}} \\ 
  & \propto &  (2 \pi)^{-n/2}
| \I_n\sigma^2|^{-1/2}
 \exp{\left\{ - \frac 1 2 \frac{\| \Y - \mub \|^2}{\sigma^2}
\right\}}  
\end{eqnarray*}

. . .

Last line is the density of $\Y \sim \N_n\left(\mub, \sigma^2 \I_n\right)$

## MLEs

\frametitle{MLEs}

Find values of $\muhat$ and $\shat$ that maximize the likelihood
$\cL(\mub, \sigma^2)$ for $\mub \in \bbR^n$ and $\sigma^2 \in \bbR^+$
\begin{eqnarray*}
 \cL(\mub, \sigma^2)
    & \propto & (\sigma^2)^{-n/2}
 \exp{\left\{ - \frac 1 2 \frac{\| \Y - \mub \|^2}{\sigma^2}
\right\}} \\ 
 \log(\cL(\mub, \sigma^2) )
   & \propto & -\frac{n}{2} \log(\sigma^2)  - \frac 1 2 \frac{\| \Y - \mub \|^2}{\sigma^2}
 \\ 
\end{eqnarray*}
or equivalently the log likelihood



- Clearly, $\muhat = \Y$ but $\shat = 0$  is outside the parameter space

- If $\mub = \X \b$,  can show that $\bhat = (\X^T\X)^{-1}\X^T\Y$ is the MLE/OLS estimator of $\b$ and $\muhat = \X\bhat$ if $\X$ is full column rank.


- show via projections

## Projections

take any point $\y \in \bbR^n$ and "project" it onto $C(\X) = \M$

- any point already in $\M$ stays the same

- so if $\P_\X$ is a projection onto the column space of $\X$ then for $\m \in C(\X)$ 
$\P_\X \m = \m$

- $\P_\X$ is a linear transformation from $\bbR^n \to \bbR^n$
- maps vectors in $\bbR^n$ into $C(\X)$
- if $\z \in \bbR^n$ then $\P_\X \z = \X \a \in C(\X)$ for some $\a \in \bbR^p$

. . .

::: {.callout-note }

## Example
For $\X \in \bbR^{n \times p}$, rank $p$, $\P_\X = \X(\X^T\X)^{-1}\X$ is a projection onto the $p$ dimensional subspace $\M = C(\X)$

:::




## Idempotent Matrix

What if we project a projection?

- $\P_\X \z = \X \a \in C(\X)$
- $\P_\X \X \a  = \X \a$ 
- since $\P_\X^2 \z =  \P_\X \z$ for all $\z \in \bbR^n$ we have $\P_\X^2 = \P_\X$ 

. . .

::: {.Definition #Projection .unnumbered}

### Projection

For a matrix  $\P$ in $\bbR^{n \times n}$ is a projection matrix if $\P^2 = \P$. 
That is all projections $\P$ are idempotent matrix.
:::

. . .

::: {.callout-note}

## Exercise
For $\X \in \bbR^{n \times p}$, rank $p$, if $\P_\X = \X(\X^T\X)^{-1}\X$ use the definition to show that it is a projection onto the $p$ dimensional subspace $\M = C(\X)$

:::

## Null Space

::: {.Definition #OrthComp .unnumbered}
## Orthogonal Complement
The set of all vectors that are orthogonal to a given subspace $\M$ is called the _orthogonal complement_ of the subspace denoted as $\M^\perp$. Under the usual inner product,
$\M^\perp \equiv \{\n \in \bbR^n \ni \m^T\n = 0 {\text{ for }} \m \in \M\}$
:::

. . .

::: {.Definition #Nullspace .unnumbered}
## Null Space
For a matrix $\A$, the _null space_ of $\A$ is defined as $N(\A) = \{\n \ni \A \n = \zero\}$ 
:::
. . .

::: {.callout-note}
## Exercise

Show that  $C(\X)^\perp$ (the _orthogonal complement_ of $C(\X)$) is the _null space_ of $\X^T$, $\, N(\X^T)$.
:::

## Orthogonal Projection

::: {.Definition #orthproj .unnumbered}
## Orthogonal Projections
For a vector space $\VS$ with an inner product $\langle \x, \y \rangle$ for $\x, \y \in \VS$, $\x$ and $\y$ are orthogonal if $\langle \x, \y \rangle = 0$. A projection $\P$ is an _orthogonal projection_ onto a subspace $\M$ of $\VS$ if for any $\m \in \VS$, $\P \m = \m$ and for any $\n \in \M^\perp$, $\P \n = \zero$.

The null space of $\P$ is the orthogonal complement of $\M$
:::
. . .

For $\bbR^N$ with the inner product, $\langle \x, \y \rangle = \x^T\y$, $\P$ is an orthogonal projection onto $\M$ if $\P$ is a projection  ($\P^2 = \P$) and it is symmetric ($\P = \P^T$)
 
. . .

::: {.callout-note}
## Exercise

Show that $\P_\X$ is an orthogonal projection on $C(\X)$.
:::


## Decompsition

- For any $\y \in \bbR^n$, we can write it uniquely as a vector 
$$ \y = \m + \n, \quad \m \in \M \quad \n \in \M^\perp$$ 

- write $\y = \P \y + (\y - \P \y ) = \P \y + (\I - \P)\y$

- claim that if $\P$ is an orthogonal projection, $(\I - \P)$ is an orthogonal projection onto $\M^\perp$

- if $\n \in \M^\perp$, then $(\I - \P)\n = \n - \P \n = \n$

## Back to MLEs

- $\Y \sim \N(\mub, \sigma^2 \I_n)$ with
    $\mub = \X \b$  and $\X$ full column rank 

- Claim: Maximum Likelihood Estimator (MLE) of $\mub$ is
    $\P_\X \Y$  

- Log Likelihood: 
$$ \log \cL(\mub, \sigma^2) =
-\frac{n}{2} \log(\sigma^2)
  - \frac 1 2 \frac{\| \Y - \mub \|^2}{\sigma^2}
$$
- Decompose $\Y = \P_\X \Y + (\I - \P_\X) \Y$  
- Use $\P_\X \mub = \mub$  
- Simplify $\| \Y - \mub \|^2$

## Expand
\begin{eqnarray*}
    \| \Y - \mub \|^2 & = & \|  { (\I- \P_\X) \Y + \P_x \Y} -
    \mub \|^2  \\
  & = & \| (\I - \P_\X) \Y + \P_x \Y - {\P_\X}\mub \|^2  \\
  & = & {\|(\I -\P_\x)}\Y +  {\P_\X}(\Y  - \mub)
  \|^2 \\
 & = & {\|(\I -\P_\x)\Y \|^2} +  {\|
   {\P_\X}(\Y  - \mub) \|^2}  + {\small{2 (\Y -
\mub)^T \P_\X^T (\I - \P_\X) \Y }}\\ 
 & = & \|(\I -\P_\x)\Y \|^2 +  \| {\P_\X}(\Y  - \mub) \|^2 + {0} 
 \\
 & = & \|(\I -\P_\x)\Y \|^2 +  \| {\P_\X}\Y  - \mub \|^2
  \end{eqnarray*}  
  
  . . .
  
Crossproduct term is zero:
\begin{eqnarray*}
  \P_\X^T (\I - \P_\X) & = &  \P_\X (\I - \P_\X)  \\
  & = &  \P_\X - \P_\X\P_\X  \\
 & = &  \P_\X - \P_\X   \\
& = & \zero
\end{eqnarray*}

## Log Likelihood
Substitute decomposition into log likelihood
\begin{eqnarray*}
 \log \cL(\mub, \sigma^2)  & = &
-\frac{n}{2} \log(\sigma^2) - \frac 1 2 \frac{\| \Y - \mub \|^2}{\sigma^2}  \\
  & = & -\frac{n}{2} \log(\sigma^2)  - \frac 1 2 \left( \frac{\|(\I - \P_\X)
  \Y \|^2}{\sigma^2} + \frac{\| \P_\X \Y - \mub\|^2 } {\sigma^2} \right)   \\
 & = &  \underbrace { -\frac{n}{2} \log(\sigma^2)  - \frac 1 2  \frac{\|(\I - \P_\X)
  \Y \|^2}{\sigma^2} }  +  \underbrace{- \frac 1 2  \frac{\| \P_\X \Y -
  \mub\|^2 } {\sigma^2}}   \\
 & = &  \text{ constant with respect to } \mub  \qquad  \leq 0
\end{eqnarray*}   

- Maximize with respect to $\mub$ for each $\sigma^2$ 

- RHS is largest when $\mub = \P_\X \Y$  for any choice of $\sigma^2$
$$\therefore \quad \muhat = \P_\X \Y$$
is the MLE of $\mub$  (fitted values $\Yhat = \P_\X \Y$)

## MLE of $\b$
$$\cL(\mub, \sigma^2)   =  -\frac{n}{2} \log(\sigma^2)  - \frac 1 2 \left( \frac{\|(\I - \P_\X)
  \Y \|^2}{\sigma^2} + \frac{\| \P_\X \Y - \mub\|^2 } {\sigma^2} \right)$$

. . .

Rewrite as likeloood function for $\b, \sigma^2$:
$$\cL(\b, \sigma^2 )  =  -\frac{n}{2} \log(\sigma^2)  - \frac 1 2 \left( \frac{\|(\I - \P_\X)
  \Y \|^2}{\sigma^2} + \frac{\| \P_\X \Y - \X\b\|^2 } {\sigma^2}
\right)$$


. . .

- Similar argument to show that RHS is maximized by minimizing $$\| \P_\X
\Y - \X\b\|^2$$ \pause
- Therefore $\bhat$ is  a MLE of $\b$ if and only if satisfies
$$\P_\X \Y = \X \bhat$$ 
- If $\X^T\X$ is full rank, the MLE of $\b$ is $(\X^T\X)^{-1}\X^T\Y = \bhat$

## MLE of $\sigma^2$

- Plug-in MLE of $\muhat$ for $\mub$ 
$$ \log \cL(\muhat, \sigma^2)  =   -\frac{n}{2} \log \sigma^2 - \frac 1 2
\frac{\| (\I - \P_\X) \Y \|^2  }{\sigma^2}$$
- Differentiate  with respect to $\sigma^2$ 
$$\frac{\partial \, \log \cL(\muhat, \sigma^2)}{\partial \, \sigma^2} =  -\frac{n}{2} \frac{1}{\sigma^2}  +  \frac 1 2
\| (\I - \P_\X) \Y \|^2 \left(\frac{1}{\sigma^2}\right)^2 $$
- Set derivative to zero and solve for MLE
\begin{eqnarray*}
0 & = &  -\frac{n}{2} \frac{1}{\shat}  +  \frac 1 2
\| (\I - \P_\X) \Y \|^2 \left(\frac{1}{\shat}\right)^2  \\
\frac{n}{2} \shat & = & \frac 1 2
\| (\I - \P_\X) \Y \|^2 \\
\shat & = & \frac{\| (\I - \P_\X) \Y \|^2}{n}
\end{eqnarray*}

## MLE Estimate of $\sigma^2$
Maximum Likelihood Estimate of $\sigma^2$
\begin{eqnarray*}
    \shat & = & \frac{\| (\I - \P_\X) \Y \|^2}{n} \\
      & = & \frac{\Y^T(\I - \P_\X)^T(\I-\P_\X) \Y }{n} \\
 & = & \frac{ \Y^T(\I - \P_\X) \Y}{n} \\
 & = & \frac{\e^T\e} {n} 
  \end{eqnarray*}
where $\e = (\I - \P_\X)\Y$  are the *residuals* from the regression of $\Y$
on $\X$
