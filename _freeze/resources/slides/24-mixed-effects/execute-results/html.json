{
  "hash": "461c809e96ec92103adb469e16291dfc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear Mixed Effects Models\"\nauthor: \"Merlise Clyde\"\nsubtitle: \"STA721: Lecture 24\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    smaller: true\n    logo: ../../img/icon.png\n    footer: <https://sta721-F24.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"    \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\nnumber-sections: false\nfilters:\n  - custom-numbered-blocks  \ncustom-numbered-blocks:\n  groups:\n    thmlike:\n      colors: [948bde, 584eab]\n      boxstyle: foldbox.simple\n      collapse: false\n      listin: [mathstuff]\n    todos: default  \n  classes:\n    Theorem:\n      group: thmlike\n    Corollary:\n      group: thmlike\n    Conjecture:\n      group: thmlike\n      collapse: true  \n    Definition:\n      group: thmlike\n      colors: [d999d3, a01793]\n    Feature: default\n    TODO:\n      label: \"To do\"\n      colors: [e7b1b4, 8c3236]\n      group: todos\n      listin: [stilltodo]\n    DONE:\n      label: \"Done\"\n      colors: [cce7b1, 86b754]  \n      group: todos  \n---\n\n\n\n\n\n## Random Effects Regression\n\n\\usepackage{xcolor}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator{\\sgn}{sgn}\n\\newcommand{\\e}{\\mathbf{e}}\n\\newcommand{\\Mb}{\\mathbf{M}}\n\\renewcommand{\\P}{\\mathbf{P}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\R}{\\textsf{R}}\n\\newcommand{\\mat}[1] {\\mathbf{#1}}\n\\newcommand{\\pen}{\\textsf{pen}}\n\\newcommand{\\E}{\\textsf{E}}\n\\newcommand{\\SE}{\\textsf{SE}}\n\\newcommand{\\SSE}{\\textsf{SSE}}\n\\newcommand{\\SST}{\\textsf{SST}}\n\\newcommand{\\RSS}{\\textsf{RSS}}\n\\newcommand{\\FSS}{\\textsf{FSS}}\n\\renewcommand{\\SS}{\\textsf{SS}}\n\\newcommand{\\MSE}{\\textsf{MSE}}\n\\newcommand{\\SSR}{\\textsf{SSR}}\n\\newcommand{\\Be}{\\textsf{Beta}}\n\\newcommand{\\St}{\\textsf{St}}\n\\newcommand{\\Ca}{\\textsf{C}}\n\\newcommand{\\Lv}{\\textsf{LÃ©vy}}\n\\newcommand{\\Exp}{\\textsf{Exp}}\n\\newcommand{\\GDP}{\\textsf{GDP}}\n\\newcommand{\\NcSt}{\\textsf{NcSt}}\n\\newcommand{\\Bin}{\\textsf{Bin}}\n\\newcommand{\\NB}{\\textsf{NegBin}}\n\\newcommand{\\Mult}{\\textsf{MultNom}}\n\\renewcommand{\\NG}{\\textsf{NG}}\n\\newcommand{\\N}{\\textsf{N}}\n\\newcommand{\\Ber}{\\textsf{Ber}}\n\\newcommand{\\Poi}{\\textsf{Poi}}\n\\newcommand{\\Gam}{\\textsf{Gamma}}\n\\newcommand{\\BB}{\\textsf{BB}}\n\\newcommand{\\BF}{\\textsf{BF}}\n\\newcommand{\\Gm}{\\textsf{G}}\n\\newcommand{\\Un}{\\textsf{Unif}}\n\\newcommand{\\Ex}{\\textsf{Exp}}\n\\newcommand{\\DE}{\\textsf{DE}}\n\\newcommand{\\tr}{\\textsf{tr}}\n\\newcommand{\\cF}{{\\cal{F}}}\n\\newcommand{\\cL}{{\\cal{L}}}\n\\newcommand{\\cI}{{\\cal{I}}}\n\\newcommand{\\cB}{{\\cal{B}}}\n\\newcommand{\\cP}{{\\cal{P}}}\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\pperp}{\\mathrel{{\\rlap{$\\,\\perp$}\\perp\\,\\,}}}\n\\newcommand{\\OFP}{(\\Omega,\\cF, \\P)}\n\\newcommand{\\eps}{\\boldsymbol{\\epsilon}}\n\\def\\ehat{\\hat{\\boldsymbol{\\epsilon}}}\n\\newcommand{\\Psib}{\\boldsymbol{\\Psi}}\n\\newcommand{\\Omegab}{\\boldsymbol{\\Omega}}\n\\newcommand{\\1}{\\mathbf{1}_n}\n\\newcommand{\\gap}{\\vspace{8mm}}\n\\newcommand{\\ind}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}}\n\\newcommand{\\iid}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}}\n\\newcommand{\\simiid}{\\ensuremath{\\mathrel{\\mathop{\\sim}\\limits^{\\rm\niid}}}}\n\\newcommand{\\eqindis}{\\mathrel{\\mathop{=}\\limits^{\\rm D}}}\n\\newcommand{\\SSZ}{S_{zz}}\n\\newcommand{\\SZW}{S_{zw}}\n\\newcommand{\\Var}{\\textsf{Var}}\n\\newcommand{\\corr}{\\textsf{corr}}\n\\newcommand{\\cov}{\\textsf{cov}}\n\\newcommand{\\diag}{\\textsf{diag}}\n\\newcommand{\\var}{\\textsf{var}}\n\\newcommand{\\Cov}{\\textsf{Cov}}\n\\newcommand{\\Sam}{{\\cal S}}\n\\newcommand{\\VS}{{\\cal V}}\n\\def\\H{\\mathbf{H}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\tY}{\\tilde{\\mathbf{Y}}}\n\\newcommand{\\Yhat}{\\hat{\\mathbf{Y}}}\n\\newcommand{\\yhat}{\\hat{\\mathbf{y}}}\n\\newcommand{\\Yobs}{\\mathbf{Y}_{{\\cal S}}}\n\\newcommand{\\barYobs}{\\bar{Y}_{{\\cal S}}}\n\\newcommand{\\barYmiss}{\\bar{Y}_{{\\cal S}^c}}\n\\def\\bv{\\mathbf{b}}\n\\def\\X{\\mathbf{X}}\n\\def\\XtX{\\X^T\\X}\n\\def\\tX{\\tilde{\\mathbf{X}}}\n\\def\\x{\\mathbf{x}}\n\\def\\xbar{\\bar{\\mathbf{x}}}\n\\def\\Xbar{\\bar{\\mathbf{X}}}\n\\def\\Xg{\\mathbf{X}_{\\boldsymbol{\\gamma}}}\n\\def\\Ybar{\\bar{\\Y}}\n\\def\\ybar{\\bar{y}}\n\\def\\y{\\mathbf{y}}\n\\def\\Yf{\\mathbf{Y_f}}\n\\def\\W{\\mathbf{W}}\n\\def\\L{\\mathbf{L}}\n\\def\\m{\\mathbf{m}}\n\\def\\n{\\mathbf{n}}\n\\def\\w{\\mathbf{w}}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\z{\\mathbf{z}}\n\\def\\v{\\mathbf{v}}\n\\def\\u{\\mathbf{u}}\n\n\\def\\zero{\\mathbf{0}}\n\\def\\one{\\mathbf{1}}\n\\newcommand{\\taub}{\\boldsymbol{\\tau}}\n\\newcommand{\\betav}{\\boldsymbol{\\beta}}\n\\newcommand{\\alphav}{\\boldsymbol{\\alpha}}\n\\newcommand{\\bfomega}{\\boldsymbol{\\omega}}\n\\newcommand{\\bfchi}{\\boldsymbol{\\chi}}\n\\newcommand{\\bfLambda}{\\boldsymbol{\\Lambda}}\n\\newcommand{\\Lmea}{{\\cal L}} \n\\newcommand{\\scale}{\\bflambda} \n\\newcommand{\\Scale}{\\bfLambda} \n\\newcommand{\\bfscale}{\\bflambda} \n\\newcommand{\\mean}{\\bfchi}\n\\newcommand{\\loc}{\\bfchi}\n\\newcommand{\\bfmean}{\\bfchi}\n\\newcommand{\\bfx}{\\x}\n\\renewcommand{\\k}{g}\n\\newcommand{\\Gen}{{\\cal G}}\n\\newcommand{\\Levy}{L{\\'e}vy}\n\\def\\bfChi    {\\boldsymbol{\\Chi}}\n\\def\\bfOmega  {\\boldsymbol{\\Omega}}\n\\newcommand{\\Po}{\\mbox{\\sf P}}\n\\newcommand{\\A}{\\mathbf{A}}\n\\def\\a{\\mathbf{a}}\n\\def\\K{\\mathbf{K}}\n\\newcommand{\\B}{\\mathbf{B}}\n\\def\\b{\\boldsymbol{\\beta}}\n\\def\\bhat{\\hat{\\boldsymbol{\\beta}}}\n\\def\\btilde{\\tilde{\\boldsymbol{\\beta}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\thetab{\\boldsymbol{\\theta}}\n\\def\\bg{\\boldsymbol{\\beta_\\gamma}}\n\\def\\bgnot{\\boldsymbol{\\beta_{(-\\gamma)}}}\n\\def\\mub{\\boldsymbol{\\mu}}\n\\def\\tmub{\\tilde{\\boldsymbol{\\mu}}}\n\\def\\muhat{\\hat{\\boldsymbol{\\mu}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\tk{\\boldsymbol{\\theta}_k}\n\\def\\tj{\\boldsymbol{\\theta}_j}\n\\def\\Mk{\\boldsymbol{{\\cal M}}_k}\n\\def\\M{\\boldsymbol{{\\cal M}}}\n\\def\\Mj{\\boldsymbol{{\\cal M}}_j}\n\\def\\Mi{\\boldsymbol{{\\cal M}}_i}\n\\def\\Mg{{\\boldsymbol{{\\cal M}_\\gamma}}}\n\\def\\Mnull{\\boldsymbol{{\\cal M}}_{N}}\n\\def\\gMPM{\\boldsymbol{\\gamma}_{\\text{MPM}}}\n\\def\\gHPM{\\boldsymbol{\\gamma}_{\\text{HPM}}}\n\\def\\Mfull{\\boldsymbol{{\\cal M}}_{F}}\n\\def\\NS{\\boldsymbol{{\\cal N}}}\n\\def\\tg{\\boldsymbol{\\theta}_{\\boldsymbol{\\gamma}}}\n\\def\\g{\\boldsymbol{\\gamma}}\n\\def\\eg{\\boldsymbol{\\eta}_{\\boldsymbol{\\gamma}}}\n\\def\\G{\\mathbf{G}}\n\\def\\cM{\\cal M}\n\\def\\D{\\boldsymbol{\\Delta}}\n\\def\\Dbf{\\mathbf{D}}\n\\def \\shat{{\\hat{\\sigma}}^2}\n\\def\\uv{\\mathbf{u}}\n\\def\\l {\\lambda}\n\\def\\d{\\delta}\n\\def\\deltab{\\mathbf{\\delta}}\n\\def\\Sigmab{\\boldsymbol{\\Sigma}}\n\\def\\Phib{\\boldsymbol{\\Phi}}\n\\def\\Lambdab{\\boldsymbol{\\Lambda}}\n\\def\\lambdab{\\boldsymbol{\\lambda}}\n\\def\\Mg{{\\cal M}_\\gamma}\n\\def\\S{{\\cal{S}}}\n\\def\\Sbf{{\\mathbf{S}}}\n\\def\\qg{p_{\\boldsymbol{\\gamma}}}\n\\def\\pg{p_{\\boldsymbol{\\gamma}}}\n\\def\\T{\\boldsymbol{\\Theta}}\n\\def\\Tb{\\boldsymbol{\\Theta}}\n\\def\\t{\\mathbf{t}}\n\\def\\pause{\\vspace{1mm}}\n\n\n\n- Easy to extend from random means by groups to random group level coefficients:\n$$\\begin{align*}Y_{ij} & = \\tb^T_j \\x_{ij}+ \\epsilon_{ij} \\\\\n\\epsilon_{ij}   & \\iid  \\N(0, \\sigma^2) \n\\end{align*}\n$$\n- $\\tb_j$ is a $p \\times 1$ vector regression coefficients for group $j$\n- $\\x_{ij}$ is a $p \\times 1$ vector of predictors for group $j$\n\n- If we view the groups as exchangeable, describe across group heterogeneity by\n$$\\tb_j \\iid \\N(\\b, \\Sigmab)$$\n- $\\b$, $\\Sigmab$ and $\\sigma^2$ are population parameters to be estimated.\n\n- Designed to accommodate correlated data due to nested/hierarchical structure/repeated measurements: \nstudents w/in schools; patients w/in hospitals; additional covariates\n\n \n## Linear Mixed Effects Models\n\n- We can write $\\tb = \\b + \\alphav_j$ with $\\alphav_j \\iid \\N(\\zero, \\Sigmab)$\n\n- Substituting, we can rewrite model\n$$\\begin{align*}Y_{ij} & = \\b^T \\x_{ij}+ \\alphav_j^T \\x_{ij} + \\epsilon_{ij}, \\qquad\n\\epsilon_{ij}  \\overset{iid}{\\sim}  \\N(0, \\sigma^2) \\\\\n\\alphav_j & \\overset{iid}{\\sim} \\N(\\zero_p, \\Sigmab)\n\\end{align*}$$\n\n\n- Fixed effects contribution $\\b$ is constant across groups \n\n \n\n- Random effects are $\\alphav_j$ as they vary across groups  \n\n- called **mixed effects** as we have both fixed and random effects in the regression model\n\n## More General Model\n- No reason for the fixed effects and random effect covariates to be the same\n$$\\begin{align*}Y_{ij} & = \\b^T \\x_{ij}+ \\alphav_j^T \\z_{ij} + \\epsilon_{ij}, \\qquad\n\\epsilon_{ij}  \\iid  \\N(0, \\sigma^2) \\\\\n\\alphav_j & {\\iid} \\N(\\zero_q, \\Sigmab)\n\\end{align*}$$\n\n- dimension of $\\x_{ij}$ $p \\times 1$\n\n- dimension of $\\z_{ij}$ $q \\times 1$\n\n- may or may not be overlapping\n\n- $\\x_{ij}$ could include predictors that are constant across all $i$ in group $j$. (can't estimate if they are in $\\z_{ij}$)\n\n\n- features of school $j$ that vary across schools but are constant within a school\n \n\n## Marginal Distribution of Data\n\n- Express observed data as vectors for each group $j$:  $(\\Y_j, \\X_j, \\Z_j)$ where  $\\Y_j$ is $n_j \\times 1$, $\\X_j$ is $n_j \\times d$ and $\\Z_j$ is $n_j \\times q$;\n \n\n- Group Specific Model (1):\n$$\\begin{align}\\Y_j  & = \\X_j \\b + \\Z_j \\alphav_j + \\eps_j, \\qquad\n\\eps_j  \\sim \\N(\\zero_{n_j}, \\sigma^2 \\I_{n_j})\\\\\n\\alphav_j & \\iid \\N(\\zero_p, \\Sigmab)\n\\end{align}$$\n\n- Population Mean $\\E[\\Y_j] = \\E[\\X_j \\b + \\Z_j \\alphav_j + \\eps_j] = \\X_j \\b$\n\n- Covariance $\\Cov[\\Y_j] = \\Var[\\X_j \\b + \\Z_j \\alphav_j + \\eps_j] = \\Z_j \\Sigmab \\Z_j^T + \\sigma^2 \\I_{n_j}$\n$$\\Y_j \\mid  \\b, \\Sigmab, \\sigma^2 \\ind \\N(\\X_j \\b, \\Z_j \\Sigmab \\Z_j^T + \\sigma^2 \\I_{n_j})$$\n- write out as a big regression model stacking $\\Y$\n$\\X$ and $\\Z$ and $\\eps = (\\eps_1, \\ldots, \\eps_J)$\n$$\\Y = \\X\\b + \\Z \\alphav + \\eps$$\n\n\n\n## GLS Estimation \n\n\nMarginal Model\n$$\\Y \\mid  \\b, \\Sigmab, \\sigma^2 \\ind \\N(\\X \\b, \\Z \\Sigmab \\Z^T + \\sigma^2 \\I_{n})$$\n\n- Define covariance of $\\Y$  to be $\\V = \\Z \\Sigmab \\Z^T + \\sigma^2 \\I_{n}$\n\n- Use GLS conditional on $\\Sigmab, \\sigma^2$ to estimate $\\b$:\n$$\\b = (\\X^T \\V^{-1} \\X)^{-1} \\X^T \\V^{-1} \\Y$$\n- since $\\V$ has unknown parameters, typical practice (non-Bayes) is to use an estimate of $\\V$, and replace $\\V$ by $\\hat{\\V}$. (MLE, Methods of Moments, REML)\n\n- frequentist random effects models arose from analysis of variance models so generally\nsome simplification in $\\Sigmab$!\n\n## One Way Anova Random Effects Model\n\n- Consider Balance data so that $n_1 = n_2 = \\cdots = n_J = r$ and $n = rJ$\n- design matrix $\\X = \\one_n$ \n- covariance for random effects is $\\Sigmab = \\sigma^2_{\\alpha} \\I_J$\n- matrix $\\Z$ is $n \\times J$ \n$$\\Z = \\begin{pmatrix} \\one_r & \\zero & \\cdots & \\zero \\\\\n\\zero & \\one_r & \\cdots & \\zero \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\zero & \\zero & \\cdots & \\one_r\n\\end{pmatrix}$$\n\n- Covariance \n$$\\V =  \\sigma^2 \\I_{n} + \\Z \\Sigmab \\Z^T = \\sigma^2 \\I_{n} + \\sigma^2_{\\alpha} \\Z \\Z^T = \\sigma^2 \\I_{n} + \\sigma^2_{\\alpha} r\\P_\\Z$$\n\n## MLEs for One-Way Random Effects Model\n\n- Model $\\Y = \\one_n \\beta + \\eps$ with $\\eps \\sim \\N(\\zero, \\V)$\n- Since $C(\\V\\X) \\subset C(\\X)$, the GLS of $\\b$ is the same as the OLS of $\\beta$ in this case\n$$\\hat{\\beta} = \\ybar_{..} = \\sum_{j=1}^J \\sum_{i=1}^r y_{ij}/n$$\n\n- We need the determinant and inverse of $\\V$ to get the MLEs for $\\sigma^2$ and $\\sigma^2_{\\alpha}$\n- note that $\\V$ is block diagonal with blocks \n$\\sigma^2 \\I_r + \\sigma^2_{\\alpha} r \\P_{\\one_r}$ (use eigenvalues based on svd of $\\P_{\\one_r}$ and $\\I_r$)\n- determinant of $\\V$ is the product of determinants of blocks \n$\\sigma{^{2}}^n (1 + r \\sigma^2_{\\alpha}/\\sigma^2)^J$\n\n\n- find inverse of $\\V$ via Woodbury identity (or svd of projections/eigenvalues)\n$$\\V^{-1} = \\frac{1}{\\sigma^2} \\left( \\I_n - \\frac{r \\sigma^2_{\\alpha}}\n{\\sigma^2 + r \\sigma^2_{\\alpha}}\\P_{\\Z} \\right)$$\n\n## Log likelihood \n\n- plug in $\\hat{\\beta}$\n\\begin{align*}\n\\log L(\\sigma^2, \\sigma^2_{\\alpha}) & = - \\frac{1}{2}\\log{|V|} - \\frac{1}{2}\n(\\Y - \\one_n \\ybar)^T\\V^{-1} (\\Y - \\one_n\\ybar)\\\\\n& = - \\frac{1}{2}\\log{|V|} - \\frac{1}{2}\n\\Y^T(\\I - \\P_{\\one_n})\\V^{-1} (\\I - \\P_{\\one_n})\\Y \\\\\n& = - \\frac{J(r - 1)}{2}\\log{\\sigma^2} - \\frac{J}{2} \\log(\\sigma^2 + r \\sigma^2_{\\alpha}) \\\\\n& \\ - \\frac{1}{2 \\sigma^2} \\left( \\Y^T(\\I - \\P_{\\one_n})( \\I_n - \\frac{r \\sigma^2_{\\alpha}}\n{\\sigma^2 + r \\sigma^2_{\\alpha}}\\P_{\\Z})(\\I - \\P_{\\one_n})\\Y \\right) \\\\\n& = - \\frac{J(r - 1)}{2}\\log{\\sigma^2} - \\frac{J}{2} \\log(\\sigma^2 + r \\sigma^2_{\\alpha}) \\\\\n& \\ -  \\frac{1}{2\\sigma^2} \\left( \\Y^T(\\I - \\P_{\\one_n})\\left(\\frac{ \\sigma^2\\I_n + r \\sigma^2_{\\alpha} (\\I_n - \\P_{\\Z})}\n{\\sigma^2 + r \\sigma^2_{\\alpha}}\\right)(\\I - \\P_{\\one_n})\\Y \\right)\n\\end{align*}\n\n## MLEs\n\n- Simplify using Properties of Projections; ie $(\\I_n - \\P_{\\one_n})(\\I_n - \\P_{\\Z})$ to rewrite in terms of familiar $\\SSE = \\Y^T(\\I - \\P_{\\Z})\\Y$ and $\\SST = \\Y^T(\\P_{\\Z} - \\P_{\\one_n})\\Y$ based on the fixed effects one-way anova model\n\n- take derivatives and solve for MLEs (some alegebra involved!)\n\n\n- MLE of $\\sigma^2$ is $\\hat{\\sigma}^2 = \\MSE = \\SSE/(n - J)$\n\n- MLE of $\\sigma^2_{\\alpha}$ is \n$$\\hat{\\sigma}^2_{\\alpha} = \\frac{\\frac{\\SST}{J} - \\MSE}{n}$$\nbut this is true only if $\\MSE < \\SST/J$ otherwise the mode is on the boundary and  $\\hat{\\sigma}^2_{\\alpha} = 0$ \n\n## Comments\n\nFor the One-Way model (and HW) we can find MLEs in closed form - but several approaches to simplify the algebra\n\n- steps outlined here (via the stacked approach - more general)\n\n- treating the response as a matrix and using the matrix normal distribution with the \n  mean function and covariance via Kronecker transformations (lab)\n  \n  - extends to other balanced ANOVA models\n  \n- simplify the problem based on summary statistics - i.e.  the distributions in terms of $\\SSE$. (Gamma) and the sample means  (Normal) and integrate out random effects  (Approach in Box & Hill for Bayesian solution)\n\n  - easiest imho for the one-way model\n\n. . .\n\nFor more general problems we may need iterative methods to find MLEs (alternating between conditional MLE of $\\b$ and MLE of $\\Sigmab$)  (Gauss-Siedel optimization)\n\n## Best Linear Prediction\n\nGiven a linear model with $\\E[Y^*] = \\X \\b$ with or without correlation structure, we can *predict* a new observation $Y^*$ at $\\x$ as $\\x^T \\bhat$ where $\\bhat$ is the OLS or GLS of $\\b$.\n\n- but if $Y^*$ and $\\Y$ are correlated we can do better!\n\n. . .\n\n::: {.Theorem  .unnumbered}\n## Christensen 6.3.4; Sec 12.2\nLet $\\Y$ and $Y^*$ be random variables with the following moments\n\\begin{align*}\n\\E[\\Y]  =  \\X \\b & \\quad & \\E[\\Y^*]  = \\x^T\\b \\\\\n\\Var[\\Y]  =  \\V  & \\quad & \\Cov[\\Y, Y^*]  = \\psi \n\\end{align*}\n\nThen the best linear predictor of $Y^*$ given $\\Y$ is \n$$\\E[Y^* \\mid \\Y] = \\x^T \\bhat + \\delta(\\Y - \\X \\bhat)$$ \nwhere $\\delta = \\V^{-1}\\psi$\n\n\n\n:::\n\n## Best Linear Unbiased Prediction\n\nTo go from BLPs to BLUPs we need to estimate the unknown parameters in the model $\\b$\n\n- replacing $\\b$ by $\\bhat$ in the BLP gives the Best Linear Unbiased Predictor (BLUP) of $Y^*$ given $\\Y$ (see Christensen 12.2 for details and proof)\n\n- if the $y_i$ are uncorrelated, the BLUP is the same as the BLUE, $\\x^T \\bhat$\n\n- what about other linear combinations?  $\\lambdab^T\\alphav$?\n- $\\E[\\lambdab^T\\alphav] = 0$ so we can let $Y^* = \\lambdab^T\\alphav$ with $\\x = 0$\n\\begin{align*} \n\\psi & = \\Cov[\\Y, Y^*] = \\Cov(\\Z\\alphav + \\eps, \\lambdab^T\\alphav) \\\\\n     & = \\Cov(\\Z\\alphav, \\lambdab^T\\alphav) = \\Z\\Sigmab\\lambdab\n\\end{align*}\n\n- the BLUP of $\\lambdab^T\\alphav$ given $\\Y$ is \n$$\\delta_*^T(\\Y - \\X\\bhat)$$\nwhere $\\delta_* = \\V^{-1}\\psi = \\V^{-1}\\Z\\Sigmab\\lambdab$ \n\n::: {.footer}\n:::\n\n## Mixed Model Equations via Bayes Rule\n\nThe *mixed model equations* are the normal equations for the mixed effects model and provide both BLUEs and BLUPs\n\n- Consider the model\n\\begin{align*}\n\\Y & \\sim \\N(\\W\\thetab, \\sigma^2 \\I_n) \\\\\n\\W & = [\\X, \\Z] \\\\\n\\thetab & = [\\b^T, \\alphav^T] \n\\end{align*}\n\n- estimate $\\thetab$ using Bayes with the prior $\\thetab \\sim \\N(\\zero, \\Omega)$\nwhere \n$\\Omega = \\begin{pmatrix} \\I_p  /\\kappa & \\zero \\\\ \\zero & \\Sigmab \\end{pmatrix}$\n- posterior mean of $\\thetab$\n$$\\hat{\\thetab} =  \\begin{pmatrix} \\bhat \\\\ \\hat{\\alphav} \\end{pmatrix} =\n\\begin{pmatrix} \\XtX/\\sigma^2 + \\kappa \\I_p & \\X^T\\Z/\\sigma^2  \\\\\n\\Z^T\\X/\\sigma^2   &   \\Z^T\\Z + \\Sigmab^{-1} \\end{pmatrix}^{-1} \n\\begin{pmatrix} \\X^T\\Y/\\sigma^2 \\\\ \\Z^T\\Y/\\sigma^2 \\end{pmatrix}$$\n\n## BLUEs and BLUPs via Bayes Rule\n\n- take the limiting prior with $\\kappa \\rightarrow 0$ and $\\Sigmab \\rightarrow \\zero$ to get the mixed model equations\n\n-  The BLUE of $\\b$ and BLUP of $\\alphav$ satisfy the limiting form of the posterior mean of $\\thetab$\n$$\\hat{\\thetab} =  \\begin{pmatrix} \\bhat \\\\ \\hat{\\alphav} \\end{pmatrix} =\n\\begin{pmatrix} \\XtX/\\sigma^2  & \\X^T\\Z/\\sigma^2  \\\\\n\\Z^T\\X/\\sigma^2   &   \\Z^T\\Z + \\Sigmab^{-1} \\end{pmatrix}^{-1} \n\\begin{pmatrix} \\X^T\\Y/\\sigma^2 \\\\ \\Z^T\\Y/\\sigma^2 \\end{pmatrix}$$\n\n- see Christensen Sec 12.3 for details\n\n- the mixed model equations have computational advantages over the usual GLS espression for $\\b$ as it avoids inverting $\\V$ $n \\times n$ and instead we are inverting $p + q$ matrix!\n\n- related to spatial kriging  and Gaussian Process Regression\n\n\n## Other Questions   \n\n- How do you decide what is a random effect or fixed effect?\n\n- Design structure is often important\n\n\n- What if the means are not normal?  Extensions to Generalized Linear Models\n\n- what if random effects are not normal? (Mixtures of Normals,  Bayes...)\n\n- more examples in Case Studies next semester! \n\n- for more in depth treatment take STA 610 \n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}