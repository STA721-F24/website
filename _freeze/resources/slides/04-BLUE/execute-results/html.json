{
  "hash": "c279014c9bdc73c4d803fdaf5f2427ca",
  "result": {
    "engine": "knitr",
    "markdown": "---\nsubtitle: \"STA 721: Lecture 4\"\ntitle: \"Best Linear Unbiased Estimators\"\nauthor: \"Merlise Clyde (clyde@duke.edu)\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    smaller: true\n    logo: ../../img/icon.png\n    footer: <https://sta721-F24.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"   \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\nnumber-sections: false\nfilters:\n  - custom-numbered-blocks  \ncustom-numbered-blocks:\n  groups:\n    thmlike:\n      colors: [948bde, 584eab]\n      boxstyle: foldbox.simple\n      collapse: false\n      listin: [mathstuff]\n    todos: default  \n  classes:\n    Theorem:\n      group: thmlike\n      numbered: false\n    Lemma:\n      group: thmlike\n      numbered: false\n    Corollary:\n      group: thmlike\n      numbered: false\n    Proof:\n      group: thmlike\n      numbered: false\n      collapse: true  \n    Definition:\n      group: thmlike\n      numbered: false\n      colors: [d999d3, a01793]\n    Feature: \n       numbered: false\n    TODO:\n      label: \"To do\"\n      colors: [e7b1b4, 8c3236]\n      group: todos\n      listin: [stilltodo]\n    DONE:\n      label: \"Done\"\n      colors: [cce7b1, 86b754]  \n      group: todos  \n---\n\n\n\n\n\n## Outline\n\n\\usepackage{xcolor}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator{\\sgn}{sgn}\n\\newcommand{\\e}{\\mathbf{e}}\n\\newcommand{\\Mb}{\\mathbf{M}}\n\\renewcommand{\\P}{\\mathbf{P}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\R}{\\textsf{R}}\n\\newcommand{\\mat}[1] {\\mathbf{#1}}\n\\newcommand{\\E}{\\textsf{E}}\n\\newcommand{\\SE}{\\textsf{SE}}\n\\newcommand{\\SSE}{\\textsf{SSE}}\n\\newcommand{\\RSS}{\\textsf{RSS}}\n\\newcommand{\\FSS}{\\textsf{FSS}}\n\\renewcommand{\\SS}{\\textsf{SS}}\n\\newcommand{\\MSE}{\\textsf{MSE}}\n\\newcommand{\\SSR}{\\textsf{SSR}}\n\\newcommand{\\Be}{\\textsf{Beta}}\n\\newcommand{\\St}{\\textsf{St}}\n\\newcommand{\\Ca}{\\textsf{C}}\n\\newcommand{\\Lv}{\\textsf{LÃ©vy}}\n\\newcommand{\\Exp}{\\textsf{Exp}}\n\\newcommand{\\GDP}{\\textsf{GDP}}\n\\newcommand{\\NcSt}{\\textsf{NcSt}}\n\\newcommand{\\Bin}{\\textsf{Bin}}\n\\newcommand{\\NB}{\\textsf{NegBin}}\n\\newcommand{\\Mult}{\\textsf{MultNom}}\n\\renewcommand{\\NG}{\\textsf{NG}}\n\\newcommand{\\N}{\\textsf{N}}\n\\newcommand{\\Ber}{\\textsf{Ber}}\n\\newcommand{\\Poi}{\\textsf{Poi}}\n\\newcommand{\\Gam}{\\textsf{Gamma}}\n\\newcommand{\\BB}{\\textsf{BB}}\n\\newcommand{\\BF}{\\textsf{BF}}\n\\newcommand{\\Gm}{\\textsf{G}}\n\\newcommand{\\Un}{\\textsf{Unif}}\n\\newcommand{\\Ex}{\\textsf{Exp}}\n\\newcommand{\\DE}{\\textsf{DE}}\n\\newcommand{\\tr}{\\textsf{tr}}\n\\newcommand{\\cF}{{\\cal{F}}}\n\\newcommand{\\cL}{{\\cal{L}}}\n\\newcommand{\\cI}{{\\cal{I}}}\n\\newcommand{\\cB}{{\\cal{B}}}\n\\newcommand{\\cP}{{\\cal{P}}}\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\pperp}{\\mathrel{{\\rlap{$\\,\\perp$}\\perp\\,\\,}}}\n\\newcommand{\\OFP}{(\\Omega,\\cF, \\P)}\n\\newcommand{\\eps}{\\boldsymbol{\\epsilon}}\n\\newcommand{\\Psib}{\\boldsymbol{\\Psi}}\n\\newcommand{\\1}{\\mathbf{1}_n}\n\\newcommand{\\gap}{\\vspace{8mm}}\n\\newcommand{\\ind}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}}\n\\newcommand{\\iid}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}}\n\\newcommand{\\simiid}{\\ensuremath{\\mathrel{\\mathop{\\sim}\\limits^{\\rm\niid}}}}\n\\newcommand{\\eqindis}{\\mathrel{\\mathop{=}\\limits^{\\rm D}}}\n\\newcommand{\\SSZ}{S_{zz}}\n\\newcommand{\\SZW}{S_{zw}}\n\\newcommand{\\Var}{\\textsf{Var}}\n\\newcommand{\\corr}{\\textsf{corr}}\n\\newcommand{\\cov}{\\textsf{cov}}\n\\newcommand{\\diag}{\\textsf{diag}}\n\\newcommand{\\var}{\\textsf{var}}\n\\newcommand{\\Cov}{\\textsf{Cov}}\n\\newcommand{\\Sam}{{\\cal S}}\n\\newcommand{\\VS}{{\\cal V}}\n\\def\\H{\\mathbf{H}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\tY}{\\tilde{\\mathbf{Y}}}\n\\newcommand{\\Yhat}{\\hat{\\mathbf{Y}}}\n\\newcommand{\\yhat}{\\hat{\\mathbf{y}}}\n\\newcommand{\\Yobs}{\\mathbf{Y}_{{\\cal S}}}\n\\newcommand{\\barYobs}{\\bar{Y}_{{\\cal S}}}\n\\newcommand{\\barYmiss}{\\bar{Y}_{{\\cal S}^c}}\n\\def\\bv{\\mathbf{b}}\n\\def\\X{\\mathbf{X}}\n\\def\\XtX{\\X^T\\X}\n\\def\\tX{\\tilde{\\mathbf{X}}}\n\\def\\x{\\mathbf{x}}\n\\def\\xbar{\\bar{\\mathbf{x}}}\n\\def\\Xbar{\\bar{\\mathbf{X}}}\n\\def\\Xg{\\mathbf{X}_{\\boldsymbol{\\gamma}}}\n\\def\\Ybar{\\bar{\\Y}}\n\\def\\ybar{\\bar{y}}\n\\def\\y{\\mathbf{y}}\n\\def\\Yf{\\mathbf{Y_f}}\n\\def\\W{\\mathbf{W}}\n\\def\\L{\\mathbf{L}}\n\\def\\m{\\mathbf{m}}\n\\def\\n{\\mathbf{n}}\n\\def\\w{\\mathbf{w}}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\z{\\mathbf{z}}\n\\def\\v{\\mathbf{v}}\n\\def\\u{\\mathbf{u}}\n\n\\def\\zero{\\mathbf{0}}\n\\def\\one{\\mathbf{1}}\n\\newcommand{\\taub}{\\boldsymbol{\\tau}}\n\\newcommand{\\betav}{\\boldsymbol{\\beta}}\n\\newcommand{\\alphav}{\\boldsymbol{\\alpha}}\n\\newcommand{\\bfomega}{\\boldsymbol{\\omega}}\n\\newcommand{\\bfchi}{\\boldsymbol{\\chi}}\n\\newcommand{\\bfLambda}{\\boldsymbol{\\Lambda}}\n\\newcommand{\\Lmea}{{\\cal L}} \n\\newcommand{\\scale}{\\bflambda} \n\\newcommand{\\Scale}{\\bfLambda} \n\\newcommand{\\bfscale}{\\bflambda} \n\\newcommand{\\mean}{\\bfchi}\n\\newcommand{\\loc}{\\bfchi}\n\\newcommand{\\bfmean}{\\bfchi}\n\\newcommand{\\bfx}{\\x}\n\\renewcommand{\\k}{g}\n\\newcommand{\\Gen}{{\\cal G}}\n\\newcommand{\\Levy}{L{\\'e}vy}\n\\def\\bfChi    {\\boldsymbol{\\Chi}}\n\\def\\bfOmega  {\\boldsymbol{\\Omega}}\n\\newcommand{\\Po}{\\mbox{\\sf P}}\n\\newcommand{\\A}{\\mathbf{A}}\n\\def\\a{\\mathbf{a}}\n\\def\\K{\\mathbf{K}}\n\\newcommand{\\B}{\\mathbf{B}}\n\\def\\b{\\boldsymbol{\\beta}}\n\\def\\bhat{\\hat{\\boldsymbol{\\beta}}}\n\\def\\btilde{\\tilde{\\boldsymbol{\\beta}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\bg{\\boldsymbol{\\beta_\\gamma}}\n\\def\\bgnot{\\boldsymbol{\\beta_{(-\\gamma)}}}\n\\def\\mub{\\boldsymbol{\\mu}}\n\\def\\tmub{\\tilde{\\boldsymbol{\\mu}}}\n\\def\\muhat{\\hat{\\boldsymbol{\\mu}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\tk{\\boldsymbol{\\theta}_k}\n\\def\\tj{\\boldsymbol{\\theta}_j}\n\\def\\Mk{\\boldsymbol{{\\cal M}}_k}\n\\def\\M{\\boldsymbol{{\\cal M}}}\n\\def\\Mj{\\boldsymbol{{\\cal M}}_j}\n\\def\\Mi{\\boldsymbol{{\\cal M}}_i}\n\\def\\Mg{{\\boldsymbol{{\\cal M}_\\gamma}}}\n\\def\\Mnull{\\boldsymbol{{\\cal M}}_{N}}\n\\def\\gMPM{\\boldsymbol{\\gamma}_{\\text{MPM}}}\n\\def\\gHPM{\\boldsymbol{\\gamma}_{\\text{HPM}}}\n\\def\\Mfull{\\boldsymbol{{\\cal M}}_{F}}\n\\def\\NS{\\boldsymbol{{\\cal N}}}\n\\def\\tg{\\boldsymbol{\\theta}_{\\boldsymbol{\\gamma}}}\n\\def\\g{\\boldsymbol{\\gamma}}\n\\def\\eg{\\boldsymbol{\\eta}_{\\boldsymbol{\\gamma}}}\n\\def\\G{\\mathbf{G}}\n\\def\\cM{\\cal M}\n\\def\\D{\\boldsymbol{\\Delta}}\n\\def\\Dbf{\\mathbf{D}}\n\\def \\shat{{\\hat{\\sigma}}^2}\n\\def\\uv{\\mathbf{u}}\n\\def\\l {\\lambda}\n\\def\\d{\\delta}\n\\def\\deltab{\\mathbf{\\delta}}\n\\def\\Sigmab{\\boldsymbol{\\Sigma}}\n\\def\\Phib{\\boldsymbol{\\Phi}}\n\\def\\Lambdab{\\boldsymbol{\\Lambda}}\n\\def\\lambdab{\\boldsymbol{\\lambda}}\n\\def\\Mg{{\\cal M}_\\gamma}\n\\def\\S{{\\cal{S}}}\n\\def\\Sbf{{\\mathbf{S}}}\n\\def\\qg{p_{\\boldsymbol{\\gamma}}}\n\\def\\pg{p_{\\boldsymbol{\\gamma}}}\n\\def\\T{\\boldsymbol{\\Theta}}\n\\def\\Tb{\\boldsymbol{\\Theta}}\n\\def\\t{\\mathbf{t}}\n\n\n\n- Characterizing Linear Unbiased Estimators\n- Gauss-Markov Theorem\n- Best Linear Unbiased Estimators\n\n\n\n. . .\n\nReadings: \n - Christensen Chapter 1-2 and Appendix B\n - Seber & Lee Chapter 3\n\n\n\n## Full Rank Case\n\n- Model: $\\Y = \\mub + \\eps$ \n\n- Minimal Assumptions: \n  - Mean $\\mub \\in C(\\X)$ for $\\X \\in \\bbR^{n \\times p}$\n  - Errors $\\E[\\eps] = \\zero_n$\n\n. . .\n\n::: {.Definition .unnumbered}\n## Linear Unbiased Estimators (LUEs)\nAn estimator $\\btilde$ is a **Linear Unbiased Estimator** (LUE) of $\\b$ if\n\n1) linearity: $\\btilde = \\A \\Y$ for $\\A \\in \\bbR^{p \\times n}$\n2) unbiasedness: $\\E[\\btilde] = \\b$ for all $\\b \\in \\bbR^p$\n\n\n:::\n\n. . .\n\nThe class of linear unbiased estimators is the same for every model with parameter space $\\b \\in \\bbR^p$ and $P \\in \\cal{P}$, for any collection $\\cal{P}$ of mean-zero distributions over $\\bbR ^n$.\n\n## Linear Unbiased Estimators (LUEs)\n\n- Let $\\N$ be an ONB for $\\NS = \\M^\\perp = N(\\X^T)$: \n  - $\\N^T\\m =  \\N^T\\X\\bv = \\zero  \\quad \\forall \\m =\\X \\bv \\in \\M$\n  - $\\N^T\\N = \\I_{n-p}$\n\n. . .\n\nConsider another linear estimator $\\btilde = \\A \\Y$\n\n- Difference between $\\btilde$ and $\\bhat$ (OLS/MLE):\n  \\begin{align*}\n    \\deltab = \\btilde - \\bhat & = \\left(\\A - (\\XtX)^{-1}\\X^T \\right)\\Y \\\\\n                    & \\equiv \\H^T \\Y\n  \\end{align*}\n\n-  Since both $\\btilde$ and $\\bhat$ are unbiased, $\\E[\\deltab] = \\zero_p \\quad \\forall \\b \\in \\bbR^p$\n$$\\zero_p = \\E[\\H^T \\Y] = \\H^T \\X \\b \\quad \\forall \\b \\in \\bbR^p$$\n- $\\X^T \\H = \\zero$ so each column of $\\H$ is in $\\M^\\perp \\equiv \\NS$\n\n::: footer\n:::\n\n## LUEs continued\n\n\nSince each column of $\\H$ is in $\\NS$\nthere exists a $\\G \\in \\bbR^{p \\times (n-p)} \\ni \\H = \\N \\G^T$\n \n . . .\n \n Rewriting $\\deltab = \\btilde - \\bhat$:\n \\begin{align*}\n \\btilde & = \\bhat + \\deltab \\\\\n         & = \\bhat + \\H^T\\Y \\\\\n         & = \\bhat + \\G \\N^T\\Y\n \\end{align*}\n\n- therefore $\\btilde$ is linear and unbiased:\n \\begin{align*}\n \\E[\\btilde] & = \\E[\\bhat + \\G \\N^T\\Y] \\\\\n             & =  \\b + \\E[\\G \\N^T\\X\\b] \\\\ \n             & = \\b\n \\end{align*} \n  \n## Characterization of LUEs\n\nSummary of previous results:\n\n::: {.Theorem .unnumbered}\nAn estimator $\\btilde$ is a linear unbiased estimator of $\\b$ in a linear statistical model if and only if \n$$\\btilde = \\bhat + \\H^T\\Y$$\nfor some $\\H \\in \\bbR^{n \\times p}$ such that $\\X^T \\H = \\zero$ or equivalently for some $\\G \\in \\bbR^{p \\times (n-p)}$\n$$\\btilde = \\bhat + \\G \\N^T\\Y$$\n:::\n\n## Numerical \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# X is model matrix; Y is response\n  p = ncol(X)\n  n = nrow(X)\n  G = matrix(rnorm(p*(n-p)), nrow=p, ncol=n-p)\n  H = MASS::Null(X) %*% t(G)\n  btilde = bhat + t(H) %*% Y\n```\n:::\n\n\n\ninfinite number of LUEs!\n\n## LUEs via Generalized Inverses\n\n\n\nLet $\\btilde = \\A \\Y$ be a LUE in the statistical linear model $\\Y = \\X \\b + \\eps$ with $\\X$ full column rank $p$\n\\begin{align*}\n\\E[\\btilde] & = \\E[\\A \\Y] \\\\\n            & = \\A \\E[\\Y] \\\\\n            & = \\A \\X \\b \\quad \\forall \\b \\in \\bbR^p\n\\end{align*}\n\n- Must have $\\A \\X = \\I_p$ ($\\A$ is a generalized inverse of $\\X$) \n- $\\X \\X^- \\X = \\X$\n- one generalized inverse is $\\X_{MP}^- = (\\XtX)^{-1}\\X^T$ \n- $\\X_{MP}^- = (\\XtX)^{-1}\\X^T = \\V \\D^{-1} \\U^T$ (using SVD of $\\X = \\U \\D \\V^T$)\n- $\\A$ is a generalized inverse of $\\X$ iff $\\A = \\X_{MP}^- + \\H^T$ for \n$\\H \\in \\bbR^{n \\times p} \\ni \\H^T \\U = \\zero$\n- $\\A \\Y = (\\X_{MP}^- + \\H^T)\\Y = \\bhat +  \\H^T \\Y$\n\n## Best Linear Unbiased Estimators\n\n- the distribution of values of any unbiased estimator is _centered_ around $\\b$\n\n- out of the infinite number of LUEs is there one that is more _concentrated_ around $\\b$?\n\n- is there an unbiased estimator that has a lower variance than all other unbiased estimators?\n\n- Recall variance-covariance matrix of a random vector $\\Z$ with mean $\\tb$\n\\begin{align*}\n\\Cov[\\Z]      & \\equiv \\E[(\\Z - \\tb)(\\Z - \\tb)^T] \\\\\n\\Cov[\\Z]_{ij} &  =     \\E[(z_i - \\theta_i)(z_j - \\theta_j)]\n\\end{align*}\n\n. . .\n\n::: {.callout-note title=\"Lemma\" appearance=\"default\"}\nLet $\\A \\in \\bbR^{q \\times p}$ and $\\bv \\in \\bbR^q$ with $\\Z$ a random vector in $\\bbR^p$ then\n$$\\Cov[\\A \\Z + \\bv] = \\A \\Cov[\\Z] \\A^T \\ge 0$$\n:::\n\n## Variance of Linear Unbiased Estimators\n\nLet's look at the variance of any LUE under assumption $\\Cov[\\eps] = \\sigma^2 \\I_n$\n\n- for $\\bhat = (\\XtX)^{-1} \\X^T\\Y = \\b + (\\XtX)^{-1} \\X^T\\eps$\n\\begin{align*}\n\\Cov[\\bhat] & = \\Cov[\\b + (\\XtX)^{-1} \\X^T\\eps] \\\\ \n            & =  (\\XtX)^{-1} \\X^T\\Cov[\\eps] \\X (\\XtX)^{-1} \\\\\n            & = \\sigma^2 (\\XtX)^{-1} \\X^T\\X (\\XtX)^{-1} \\\\\n            & = \\sigma^2 (\\XtX)^{-1}\n\\end{align*}\n\n- Covariance is increasing in $\\sigma^2$ and generally decreasing in $n$\n- Rewrite $\\XtX$ as  $\\XtX = \\sum_{i=1}^n \\x_i \\x_i^T$ (a sum of $n$ outer-products)\n\n## Variance of Arbitrary LUE\n\n- for $\\btilde = \\left((\\XtX)^{-1} \\X^T + \\H^T \\right)\\Y = \\b + \\left((\\XtX)^{-1} \\X^T + \\H^T \\right)\\eps$\n- recall $\\X_{MP}^- \\equiv  (\\XtX)^{-1} \\X^T$\n\\begin{align*}\n\\Cov[\\btilde] & = \\Cov[\\left(\\X_{MP}^- + \\H^T \\right)\\eps]  \\\\\n              & = \\sigma^2 \\left(\\X_{MP}^- + \\H^T \\right)\\left(\\X_{MP}^- + \\H^T \\right)^T \\\\\n              & = \\sigma^2\\left( \\X_{MP}^-(\\X_{MP}^-)^T + \\X_{MP}^-\\H +\n                  \\H^T (\\X_{MP}^-)^T + \\H^T \\H \\right) \\\\\n              & =   \\sigma^2\\left( (\\XtX)^{-1} +  \\H^T \\H \\right)\n\\end{align*}\n\n- Cross-product term $\\H^T(\\X_{MP}^-)^T = \\H^T\\X (\\XtX)^{-1} =  \\zero$\n\n- Therefor the $\\Cov[\\btilde] = \\Cov[\\bhat] + \\H^T\\H$\n\n- the sum of a positive definite matrix plus a positive semi-definite matrix\n\n\n\n## Gauss-Markov Theorem\n\nIs $\\Cov[\\btilde] \\ge \\Cov[\\bhat]$ in some sense?\n\n. . .\n\n::: {.Definition .unnumbered}\n## Loewner Ordering\nFor two positive semi-definite matrices $\\Sigmab_1$ and $\\Sigmab_2$, we say that $\\Sigmab_1 > \\Sigmab_2$ if $\\Sigmab_1 - \\Sigmab_2$ is positive definite, $\\x^T(\\Sigmab_1 - \\Sigmab_2)\\x) > 0$, and $\\Sigmab_1 \\ge \\Sigmab_2$ if $\\Sigmab_1 - \\Sigmab_2$ is positive semi-definite, $\\x^T(\\Sigmab_1 - \\Sigmab_2)\\x) \\ge 0$\n:::\n\n- Since $\\Cov[\\btilde] - \\Cov[\\bhat] = \\H^T\\H$, we have that $\\Cov[\\btilde] \\ge \\Cov[\\bhat]$\n\n. . .\n\n::: {.Theorem .unnumbered}\n## Gauss-Markov \nLet $\\btilde$ be a linear unbiased estimator of $\\b$ in a linear model where $\\E[\\Y] = \\X\\b, \\b \\in \\bbR^p$, $\\X$ rank $p$, and $\\Cov[\\Y] = \\sigma^2\\I_n, \\sigma^2 > 0$. Then\n$\\Cov[\\btilde] \\ge \\Cov[\\bhat]$ where $\\bhat$ is the OLS estimator and is the **Best Linear Unbiased Estimator** (BLUE) of $\\b$.\n:::\n\n## {#slide13-id data-menu-title=\"GM Theorem and Proof\"}\n::: {.Theorem}\n## Gauss-Markov Theorem (Classic)\nFor $\\Y = \\mub + \\eps$, with $\\mub \\in \\M$, $\\E[\\eps]= \\zero_n$ and $\\Cov[\\eps] =\\sigma^2 \\I_n$ and $\\P$ the orthogonal projection onto $\\M$,  $\\P\\Y = \\muhat$ is the BLUE of $\\mub$  out of the class of LUEs $\\A\\Y$ where $\\E[\\A \\Y] = \\mub$, $\\A \\in \\bbR^{n \\times n}$ equality iff $\\A = \\P$\n:::\n\n::: Proof\n- write $\\A = \\P + \\H^T$ so $\\H^T = \\A - \\P$\n- since $\\A\\mub = \\mub$, $\\H^T\\mu = \\zero_n$ for $\\mu \\in \\M$ and $\\H^T \\P = \\P\\H = \\zero$ so columns of $\\H \\in \\M^\\perp$.\n\n. . .\n\n\\begin{align*}\n\\E[\\|\\A\\Y - \\mub\\|^2]  & = \\E[\\|\\A(\\Y - \\mub)\\|^2]  = \\E[\\|\\P(\\Y - \\mub) + \\H ^T(\\Y - \\mub)\\|^2]  \\\\\n& = \\E[\\|\\P(\\Y - \\mub)\\|^2] + \\underbrace{\\E[\\|\\H^T(\\Y - \\mub)\\|^2]} + \\underbrace{2\\E[(\\H^T(\\Y - \\mu))^T\\P(\\Y - \\mub)]} \\\\\n& \\hspace{4.35in} \\ge 0 \\quad \\quad + \\quad \\quad {\\text{cross-product is 0}}\\\\\n& \\ge \\E[\\|\\P(\\Y - \\mub)\\|^2]\n\\end{align*}\n:::\n\n::: Footer\n:::\n\n## Estimation of Linear Functionals of $\\mub$\nIf $\\P\\Y = \\muhat$ is the BLUE of $\\mub$, is $\\B\\P\\Y = \\B\\muhat$ the BLUE of $\\B\\mub$?\n\n. . .\n\nYes! Similar proof as above to show that out of the class of LUEs $\\A\\Y$ of $\\B\\mub$ where $\\A \\in \\bbR^{d \\times n}$ that\n$$\\E[\\|\\A\\Y - \\B\\mub\\|^2] \\ge \\E[\\|\\B\\P\\Y - \\B\\mub\\|^2]$$ \nwith equality iff $\\A = \\B\\P$.\n\n. . .\n\nWhat about linear functionals of $\\b$, $\\Lambdab^T \\b$, for $\\X$ rank $r \\le p$?\n\n- $\\bhat$ is not unique if $r < p$ even though $\\muhat$ is unique ($\\bhat$ is not BLUE)\n- Since $\\B\\mub = \\B\\X\\b$ is always identifiable, the only linear functions of $\\b$ that are identifiable and can be estimated uniquely are functions of $\\X\\b$, i.e. estimates in the form $\\Lambdab^T \\b = \\B\\X\\b$ or $\\Lambdab = \\X^T \\B^T$.\n- columns of $\\Lambdab$ must be in the $C(\\X^T)$\n- detailed discussion and proof in Christensen Ch. 2 for scalar functionals\n$\\lambda^T\\beta$.\n\n## BLUE of $\\Lambdab \\b$\n\nIf $\\Lambdab^T= \\B\\X$ for some matrix $\\B$ then\n\n- $\\E[\\B\\P\\Y] = \\E[\\Lambdab \\bhat] = \\Lambdab^T\\b$\n- The unique OLS estimate of $\\Lambdab^T\\b$ is $\\Lambdab^T\\bhat$\n- $\\B\\P\\Y = \\Lambdab^T\\bhat$ is the BLUE of $\\\\Lambdab^T\\b$\n$$\\E[\\|\\B\\P\\Y - \\B\\mub\\|^2]  \\le \\E[\\|\\A\\Y - \\B\\mub\\|^2] \\LeftRightarrow\n\\E[\\|\\Lambdab^T\\bhat - \\Lambdab^T\\b)\\|^2]  \\le \\E[\\|\\L^T\\bhat - \\Lambdab^T\\b\\|^2]$$\nfor LUE $\\A\\Y$ and $\\L^T\\bhat$ of $\\Lambdab^T\\b$\n\n\n## Proof of Cross-Product\nLet $\\Dbf = \\H \\P$ and write\n\\begin{align*}\n\\E[(\\H^T(\\Y - \\mu))^T\\P(\\Y - \\mub)] & = \\E[(\\Y - \\mu))^T\\H\\P(\\Y - \\mub)] \\\\\n & = \\E[(\\Y - \\mu))^T\\Dbf(\\Y - \\mub)]\n\\end{align*}\n\n. . .\n\n\\begin{align*}\n\\E[(\\Y - \\mu))^T\\Dbf(\\Y - \\mub)] = & \\E[\\tr(\\Y - \\mu))^T\\Dbf(\\Y - \\mub))]  \\\\\n = & \\E[\\tr(\\Dbf(\\Y - \\mub)(\\Y - \\mu)^T)] \\\\\n = & \\tr(\\E[\\Dbf(\\Y - \\mub)(\\Y - \\mu)^T]) \\\\\n = & \\tr(\\Dbf\\E[(\\Y - \\mub)(\\Y - \\mu)^T]) \\\\\n  = & \\sigma^2 \\tr(\\Dbf \\I_n)\\\\\n\\end{align*}\n\n. . .\n\nSince $\\tr(\\Dbf) = \\tr(\\H\\P) = \\tr (\\P\\H)$ we can conclude that the cross-product term is zero.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}