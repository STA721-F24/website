[
  {
    "objectID": "reading/01-introduction.html",
    "href": "reading/01-introduction.html",
    "title": "Lecture 1 Readings",
    "section": "",
    "text": "links to eBooks are on the Home page of the website or Resources page."
  },
  {
    "objectID": "reading/01-introduction.html#introduction-to-linear-models",
    "href": "reading/01-introduction.html#introduction-to-linear-models",
    "title": "Lecture 1 Readings",
    "section": "Introduction to Linear Models",
    "text": "Introduction to Linear Models\n\nreview the course website and syllabus for policies"
  },
  {
    "objectID": "reading/01-introduction.html#vector-spaces",
    "href": "reading/01-introduction.html#vector-spaces",
    "title": "Lecture 1 Readings",
    "section": "Vector Spaces",
    "text": "Vector Spaces\nChristensen: Read\n\nChapter 1: pages 1-3\nAppendix A in Christensen pages 411-413\nAppendix B section B.1\n\nSee also Seber & Lee Chapter 1."
  },
  {
    "objectID": "reading/10-james-stein.html",
    "href": "reading/10-james-stein.html",
    "title": "James-Stein Estimation",
    "section": "",
    "text": "We discuss properties of the James-Stein shrinkage estimator and its relationship to Empirical Bayes in the case of ridge regression and the \\(g\\)-prior in regression with orthonormal predictors. While the James-Stein and positive-part James-Stein shrinkage estimators dominate the MLE/OLS estimator, they are not admissible.\nReadings:\n\nChristensen Chapter 15\nSeber & Lee Chapter 12\nJames and Stein (1961) “Estimation with Quadratic Loss”\nEfron & Morris (1973) “Stein’s Estimation Rule and its Competitors”\nRobert, C. (2007) “The Bayesian Choice” Chapter 5"
  },
  {
    "objectID": "reading/11-shrinkage.html",
    "href": "reading/11-shrinkage.html",
    "title": "Shrinkage and Hierarchical Bayes",
    "section": "",
    "text": "We discuss properties of the shrinkage estimators that provide both shrinkage and selection from frequentist and Bayesian paradigms.\nReadings:\n\nSeber & Lee Chapter 12\nTibshirani (1996) “Regression Shrinkage and Selection via the Lasso”\nPark & Casella (2008) “The Bayesian Lasso”\nHans (2010) “Model uncertainty and variable selection in Bayesian lasso regression”\nCarvalho, Polson & Scott (2009) “Handling Sparsity via the Horseshoe”\nArmagan, Dunson & Lee (2013) “Generalized Double Pareto Shrinkage”\nRobert, C. (2007) “The Bayesian Choice” Chapter 5"
  },
  {
    "objectID": "reading/09-bayes-freq-risk.html",
    "href": "reading/09-bayes-freq-risk.html",
    "title": "Bayesian and Frequentist Risk",
    "section": "",
    "text": "We study frequentist properties of Bayes estimators of the regression coefficients under the \\(g\\)-prior and independent normal prior that leads to ridge regression. In particular, there are values of the hper-parameters in the prior that lead to Bayes estimators dominating OLS in terms of MSE.\nReadings:\n\nChristensen Chapter 2.9 and Chapter 15\nSeber & Lee Chapter 3.12, 10.7.3 and 12\nHoerl, A.E. and Kennard, R.W. (1970) Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1):55–67\nZellner, A. (1986) On assessing prior distributions and Bayesian regression analysis with \\(g\\)-prior distributions. In Bayesian Inference and Decision Techniques: Essays in Honor of Bruno de Finetti, eds. P. K. Goel and A. Zellner, 233–243. Amsterdam: North-Holland"
  },
  {
    "objectID": "reading/07-sampling.html",
    "href": "reading/07-sampling.html",
    "title": "Sampling Dsistributions & Distribution Theory",
    "section": "",
    "text": "Readings:\n\nChristensen Chapter 1, 2.91 and Appendix C\nSeber & Lee Chapter 3.3 3.5"
  },
  {
    "objectID": "reading/06-GLS.html",
    "href": "reading/06-GLS.html",
    "title": "Generalize Least Squares, MLEs, MVUEs and BUEs",
    "section": "",
    "text": "We will develop GLS/MLE estimators when the covariance of the errors is is not a scale multiple of the identity matrix, and establish conditions for when the OLS and GLS are equivalent. Inthe case that the errors have a multivatiate normal distribution, the MLEs are equivalent to GLS and are not only the BLUE but are also Minimum Variance Unbiased Estimators or “BUE” out of all unbiased estimators linear or non-linear.\nReadings:\n\nChristensen Chapter 2 and 10 Appendix B\nSeber & Lee Chapter 3"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": " Resources",
    "section": "",
    "text": "Primary Textbooks\nThese textbooks are great resources for some of the topics we will cover.\n\nPlane Answers to Complex Questions, Ronald Christensen. eBook in Duke Library\nLinear Regression Analysis, George A.F Seber and Alan J. Lee eBook in Duke Library. Duke Library is aware the link is broken\nThe Linear Model and Hypothesis, George A. F. Seber eBook in Duke Library\n\n\n\nSupplementary Textbooks on Linear/Matrix Algebra\n\nGilbert Strang’s Online Course at MIT\nVideo Lectures\n\nIntroduction to Linear Algebra. Strang, Gilbert. 4th ed. Wellesley, MA: Wellesley-Cambridge Press, 2009. ISBN: 9780980232714. Buy @ Amazon\n\nMatrix Algebra from a Statistician’s Perspective. Harville, David A. eBook in Duke Library\n\n\n\nR and R Markdown Resources\nQuarto/R Markdown/LaTeX can be used to create high quality reports and presentations with embedded chunks of R code and LaTeX equations! You are required to use Quarto in RStudio to type up your homework assignments that involve Data Analysis/Simulation for this course, but you are welcome to use any word processor of your choice for those. To learn more about Quarto/R Markdown and for other resources for programming in R, see the links below.\n\nUsing R in Quarto for Documents\nR for Data Science (by Hadley Wickham & Garrett Grolemund)\nIntroduction to R Markdown (Article by Garrett Grolemund)\nIntroduction to R Markdown (Slides by Andrew Cho)\nR Markdown Cheat Sheet\nData Visualization with ggplot2 Cheat Sheet\nOther Useful Cheat Sheets\nA very (very!) basic R Markdown template\n\n\n\nLaTeX\nYou may also use LaTeX to type up your assignments. You may find it easier to create your TeX and LaTeX documents using online editors such as Overleaf (simply create a free account and you are good to go!). However, that need not be the case. If you prefer to create them locally/offline on your personal computers, you will need to download a TeX distribution (the most popular choices are MiKTeX for Windows and MacTeX for macOS) plus an editor (I personally prefer TeXstudio but feel free to download any editor of your choice). Follow the links below for some options, and to also learn how to use LaTeX.\n\nLearn LaTeX in 30 minutes\nChoosing a LaTeX Compiler.\n\n\n\nInteresting Articles\nI will add articles I find interesting below. These are articles I find useful as supplementary readings for topics covered in class, or as good sources that cover concepts I think you should know, but which we may not have time to cover. I strongly suggest you find time to (at the very least) take a “quick peek” at each article.\n\nEfron, B., 1986. Why isn’t everyone a Bayesian?. The American Statistician, 40(1), pp. 1-5.\nGelman, A., 2008. Objections to Bayesian statistics. Bayesian Analysis, 3(3), pp. 445-449.\nDiaconis, P., 1977. Finite forms of de Finetti’s theorem on exchangeability. Synthese, 36(2), pp. 271-281.\nGelman, A., Meng, X. L. and Stern, H., 1996. Posterior predictive assessment of model fitness via realized discrepancies. Statistica sinica, pp. 733-760.\nDunson, D. B., 2018. Statistics in the big data era: Failures of the machine. Statistics & Probability Letters, 136, pp. 4-9."
  },
  {
    "objectID": "resources/slides/01-introduction.html#introduction-to-sta721",
    "href": "resources/slides/01-introduction.html#introduction-to-sta721",
    "title": "Introduction to STA721",
    "section": "Introduction to STA721",
    "text": "Introduction to STA721\n\nCourse: Theory and Application of linear models from both a frequentist (classical) and Bayesian perspective\nPrerequisites: linear algebra and a mathematical statistics course covering likelihoods and distribution theory (normal, t, F, chi-square, gamma distributions)\nIntroduce R programming as needed in the lab\nIntroduce Bayesian methods, but assume that you are co-registered in 702 or have taken it previously\nmore info on Course website https://sta721-F24.github.io/website/\n\nschedule and slides, HW, etc\ncritical dates (Midterms and Finals)\noffice hours\n\nCanvas for grades, email, announcements\n\n\nPlease let me know if there are broken links for slides, etc!"
  },
  {
    "objectID": "resources/slides/01-introduction.html#notation",
    "href": "resources/slides/01-introduction.html#notation",
    "title": "Introduction to STA721",
    "section": "Notation",
    "text": "Notation\n\nscalors are \\(a\\) (italics or math italics)\nvectors are in bold lower case, \\(\\mathbf{a}\\), with the exception of random variables\nall vectors are column vectors \\[\\mathbf{a}= \\left[\\begin{array}{c}\n    a_1 \\\\\n    a_2 \\\\\n    \\vdots \\\\\n    a_n\n     \\end{array} \\right]\n\\]\n\n\\(\\mathbf{1}_n\\) is a \\(n \\times 1\\) vector of all ones\n\ninner product \\(\\langle    \\mathbf{a}, \\mathbf{a}\\rangle = \\mathbf{a}^T\\mathbf{a}= \\|\\mathbf{a}\\|^2 = \\sum_{i=1}^n a_i^2\\); \\(\\langle    \\mathbf{a}, \\mathbf{b}\\rangle = \\mathbf{a}^T\\mathbf{b}\\)\nlength or norm of \\(\\mathbf{a}\\) is \\(\\|\\mathbf{a}\\|\\)"
  },
  {
    "objectID": "resources/slides/01-introduction.html#matrices",
    "href": "resources/slides/01-introduction.html#matrices",
    "title": "Introduction to STA721",
    "section": "Matrices",
    "text": "Matrices\n\nMatrices are represented in bold \\(\\mathbf{A}= (a_{ij})\\) \\[\\mathbf{A}= \\left[\\begin{array}{cccc}\n    a_{11} & a_{12} & \\cdots & a_{1m}  \\\\\n    a_{21} & a_{22} & \\cdots & a_{2m} \\\\\n    \\vdots & \\vdots & \\vdots & \\vdots\\\\\n    a_{n1} & a_{n2} & \\cdots & a_{nm}\n     \\end{array} \\right]\n\\]\n\nidentity matrix \\(\\mathbf{I}_n\\) square matrix with diagonal elements 1 and off diagonal 0\ntrace: if \\(\\mathbf{A}\\) is \\(n \\times m\\) \\(\\textsf{tr}(\\mathbf{A}) = \\sum_i^{\\max n,m } a_{ii}\\)\ndeterminant: for \\(\\mathbf{A}\\) is \\(n \\times n\\) then the determinant is \\(\\det(A)\\)\ninverse: if \\(\\mathbf{A}\\) is nonsingular \\(\\mathbf{A}&gt; 0\\), then its inverse is \\(\\mathbf{A}^{-1}\\)"
  },
  {
    "objectID": "resources/slides/01-introduction.html#statistical-models",
    "href": "resources/slides/01-introduction.html#statistical-models",
    "title": "Introduction to STA721",
    "section": "Statistical Models",
    "text": "Statistical Models\nOhm’s Law: \\(Y\\) is voltage across a resistor of \\(r\\) ohms and \\(X\\) is the amperes of the current through the resistor (in theory) \\[Y = rX\\]\n\nSimple linear regression for observational data \\[Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\text{  for  } i = 1,\n\\ldots, n\\]\nRewrite in vectors: \\[\\begin{eqnarray*}\n\\left[\n\\begin{array}{c}  y_1 \\\\ \\vdots \\\\  y_n \\end{array}\n\\right]   =  &\n\\left[ \\begin{array}{c}  1 \\\\ \\vdots \\\\ 1 \\end{array}  \\right]   \\beta_0 +\n\\left[ \\begin{array}{c}  x_1 \\\\ \\vdots \\\\  x_n \\end{array}\n\\right] \\beta_1 +\n\\left[ \\begin{array}{c}  \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n  \\end{array}\n\\right]\n=  &\n\\left[ \\begin{array}{cc}  1 &  x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_n\\end{array}  \\right]\n\\left[ \\begin{array}{c}  \\beta_0  \\\\  \\beta_1 \\end{array}\n\\right] +\n\\left[ \\begin{array}{c}  \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n  \\end{array}\n\\right] \\\\\n\\\\\n\\mathbf{Y}= & \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "resources/slides/01-introduction.html#nonlinear-models",
    "href": "resources/slides/01-introduction.html#nonlinear-models",
    "title": "Introduction to STA721",
    "section": "Nonlinear Models",
    "text": "Nonlinear Models\nGravitational Law: \\(F = \\alpha/d^\\beta\\) where \\(d\\) is distance between 2 objects and \\(F\\) is the force of gravity between them\n\nlog transformations \\[\\log(F) = \\log(\\alpha) - \\beta \\log(d)\\]\ncompare to noisy experimental data \\(Y_i =\\log(F_i)\\) observed at \\(x_i = \\log(d_i)\\)\nwrite \\(\\mathbf{X}= [\\mathbf{1}_n \\, \\mathbf{x}]\\)\n\\(\\boldsymbol{\\beta}= (\\log(\\alpha), -\\beta)^T\\)\nmodel with additive error on log scale \\(\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{e}\\)\ntest if \\(\\beta = 2\\)\nerror assumptions?"
  },
  {
    "objectID": "resources/slides/01-introduction.html#intrinsically-nonlinear-models",
    "href": "resources/slides/01-introduction.html#intrinsically-nonlinear-models",
    "title": "Introduction to STA721",
    "section": "Intrinsically Nonlinear Models",
    "text": "Intrinsically Nonlinear Models\nRegression function may be an intrinsically nonlinear function of \\(t_i\\) (time) and parameters \\(\\boldsymbol{\\theta}\\) \\[Y_i = f(t_i, \\boldsymbol{\\theta}) + \\epsilon_i\\]"
  },
  {
    "objectID": "resources/slides/01-introduction.html#quadratic-linear-regression",
    "href": "resources/slides/01-introduction.html#quadratic-linear-regression",
    "title": "Introduction to STA721",
    "section": "Quadratic Linear Regression",
    "text": "Quadratic Linear Regression\nTaylor’s Theorem: \\[f(t_i, \\boldsymbol{\\theta}) = f(t_0, \\boldsymbol{\\theta}) + (t_i - t_0) f'(t_0, \\boldsymbol{\\theta}) + (t_i - t_0)^2\n\\frac{f^{''}(t_0, \\boldsymbol{\\theta})}{2}  + R(t_i, \\boldsymbol{\\theta})\\]\n\n\\[Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i \\text{  for  } i = 1, \\ldots, n\\]\n\n\nRewrite in vectors: \\[\\begin{eqnarray*}\n\\left[\n\\begin{array}{c}  y_1 \\\\ \\vdots \\\\  y_n \\end{array}\n  \\right]   =  &\n\\left[ \\begin{array}{ccc}  1 &  x_1 & x_1^2 \\\\ \\vdots & \\vdots \\\\ 1 &\n     x_n &  x_n^2\\end{array}  \\right]\n\\left[ \\begin{array}{c}  \\beta_0  \\\\  \\beta_1 \\\\ \\beta_2 \\end{array}\n\\right] +\n\\left[ \\begin{array}{c}  \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n  \\end{array}\n\\right] \\\\\n& \\\\\n\\mathbf{Y}= & \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\n\\end{eqnarray*}\\]\n\n\nQuadratic in \\(x\\), but linear in \\(\\beta\\)’s - how do we know this model is adequate?"
  },
  {
    "objectID": "resources/slides/01-introduction.html#kernel-regression-nonparametric",
    "href": "resources/slides/01-introduction.html#kernel-regression-nonparametric",
    "title": "Introduction to STA721",
    "section": "Kernel Regression (NonParametric)",
    "text": "Kernel Regression (NonParametric)\n\\[y_i =  \\beta_0 + \\sum_{j = 1}^J \\beta_j e^{-\\lambda (x_i - k_j)^d} + \\epsilon_i \\text{  for  } i = 1, \\ldots, n\\] where \\(k_j\\) are kernel locations and \\(\\lambda\\) is a smoothing parameter \\[\\begin{eqnarray*}\n\\left[\n\\begin{array}{c}  y_1 \\\\ \\vdots \\\\  y_n \\end{array}\n  \\right]   =  &\n\\left[ \\begin{array}{cccc}  1 &  e^{-\\lambda (x_1 - k_1)^d} &\n     \\ldots &  e^{-\\lambda (x_1 - k_J)^d}  \\\\\n     \\vdots & \\vdots & & \\vdots \\\\ 1 & e^{-\\lambda (x_n - k_1)^d} &  \\ldots & e^{-\\lambda (x_n - k_J)^d} \\end{array}  \\right]\n\\left[ \\begin{array}{c}  \\beta_0  \\\\  \\beta_1 \\\\\\vdots \\\\ \\beta_J \\end{array}\n\\right] +\n\\left[ \\begin{array}{c}  \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n  \\end{array}\n\\right] \\\\\n& \\\\\n\\mathbf{Y}= & \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\n\\end{eqnarray*}\\]\n\nLinear in \\(\\beta\\) given \\(\\lambda\\) and \\(k_1, \\ldots k_J\\)\nLearn \\(\\lambda\\), \\(k_1, \\ldots k_J\\) and \\(J\\)"
  },
  {
    "objectID": "resources/slides/01-introduction.html#hierarchical-models",
    "href": "resources/slides/01-introduction.html#hierarchical-models",
    "title": "Introduction to STA721",
    "section": "Hierarchical Models",
    "text": "Hierarchical Models\n\n\n\n\n\n\n\n\n\n\n\n\n\neach line represent individual sample trajectories\ncorrelation between an individual’s measurements\nsimilarities within groups\ndifferences among groups?\nallow individual regressions for each individual ?\nadd more structure?"
  },
  {
    "objectID": "resources/slides/01-introduction.html#linear-regression-models",
    "href": "resources/slides/01-introduction.html#linear-regression-models",
    "title": "Introduction to STA721",
    "section": "Linear Regression Models",
    "text": "Linear Regression Models\nResponse \\(Y_i\\) and \\(p\\) predictors \\(x_{i1}, x_{i2}, \\dots x_ip\\) \\[Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\ldots \\beta_{p}\n  x_{ip} + \\epsilon_i\\]\n\nDesign matrix \\[\\mathbf{X}=\n\\left[\\begin{array}{cccc}\n1 & x_{11} & \\ldots & x_{1p} \\\\\n1 & x_{21}  & \\ldots & x_{2p} \\\\\n\\vdots & \\vdots  & \\vdots & \\vdots \\\\\n1 & x_{n1} & \\ldots & x_{np} \\\\\n\\end{array} \\right] = \\left[ \\begin{array}{cc}\n1 & \\mathbf{x}_1^T  \\\\\n\\vdots & \\vdots \\\\\n1 & \\mathbf{x}_n^T\n\\end{array} \\right] =\n\\left[\\begin{array}{cccc}\n\\mathbf{1}_n & \\mathbf{X}_1 & \\mathbf{X}_2 \\cdots \\mathbf{X}_p\n\\end{array} \\right]\n\\]\nmatrix version \\[\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\epsilon\\] what should go into \\(\\mathbf{X}\\) and do we need all columns of \\(\\mathbf{X}\\) for inference about \\(\\mathbf{Y}\\)?"
  },
  {
    "objectID": "resources/slides/01-introduction.html#linear-model",
    "href": "resources/slides/01-introduction.html#linear-model",
    "title": "Introduction to STA721",
    "section": "Linear Model",
    "text": "Linear Model\n\n\\(\\mathbf{Y}= \\mathbf{X}\\, \\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\)\n\\(\\mathbf{Y}\\) (\\(n \\times 1\\)) vector of random response (observe \\(\\mathbf{y}\\)); \\(\\mathbf{Y}, \\mathbf{y}\\in \\mathbb{R}^n\\)\n\\(\\mathbf{X}\\) (\\(n \\times p\\)) design matrix (observe)\n\\(\\boldsymbol{\\beta}\\) (\\(p \\times 1\\)) vector of coefficients (unknown)\n\\(\\boldsymbol{\\epsilon}\\) (\\(n \\times 1\\)) vector of “errors” (unobservable)\n\n\nGoals:\n\nWhat goes into \\(\\mathbf{X}\\)? (model building, model selection - post-selection inference?)\nWhat if multiple models are “good”? (model averaging or ensembles) \nWhat about the future? (Prediction)\nUncertainty Quantification - assumptions about \\(\\boldsymbol{\\epsilon}\\)\n\n\n\nAll models are wrong, but some may be useful (George Box)"
  },
  {
    "objectID": "resources/slides/01-introduction.html#ordinary-least-squares",
    "href": "resources/slides/01-introduction.html#ordinary-least-squares",
    "title": "Introduction to STA721",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\nGoal: Find the best fitting “line” or “hyper-plane” that minimizes \\[\\sum_i  (Y_i - \\mathbf{x}_i^T \\boldsymbol{\\beta})^2 = (\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}) = \\| \\mathbf{Y}-\n\\mathbf{X}\\boldsymbol{\\beta}\\|^2 \\]\n\nOptimization problem - seek \\(\\boldsymbol{\\beta}\\ni \\mathbf{X}\\boldsymbol{\\beta}\\) is close to \\(\\mathbf{Y}\\) in squared error\nMay over-fit \\(\\Rightarrow\\) add other criteria that provide a penalty Penalized Least Squares\nRobustness to extreme points \\(\\Rightarrow\\) replace quadratic loss with other functions\n\nno notion of uncertainty of estimates\n\nno structure of problem (repeated measures on individual, randomization restrictions, etc)\n\n\nNeed Distribution Assumptions of \\(\\mathbf{Y}\\) (or \\(\\boldsymbol{\\epsilon}\\)) for testing and uncertainty measures \\(\\Rightarrow\\) Likelihood and Bayesian inference"
  },
  {
    "objectID": "resources/slides/01-introduction.html#random-vectors",
    "href": "resources/slides/01-introduction.html#random-vectors",
    "title": "Introduction to STA721",
    "section": "Random Vectors",
    "text": "Random Vectors\n\nLet \\(Y_1, \\ldots Y_n\\) be random variables in \\(\\mathbb{R}\\) Then \\[\\mathbf{Y}\\equiv\n\\left[ \\begin{array}{c}\nY_1 \\\\\n\\vdots \\\\\nY_n\n\\end{array} \\right]\\] is a random vector in \\(\\mathbb{R}^n\\)\nExpectations of random vectors are defined element-wise: \\[\\textsf{E}[\\mathbf{Y}] \\equiv\n\\textsf{E}\\left[ \\begin{array}{c}\nY_1 \\\\\n\\vdots \\\\\nY_n\n\\end{array} \\right] \\equiv\n\\left[ \\begin{array}{c}\n\\textsf{E}[Y_1] \\\\\n\\vdots \\\\\n\\textsf{E}[Y_n]\n\\end{array} \\right] =\n\\left[ \\begin{array}{c}\n\\mu_1 \\\\\n\\vdots \\\\\n\\mu_n\n\\end{array} \\right]\n\\equiv \\boldsymbol{\\mu}\\in \\mathbb{R}^n\n\\] where mean or expected value \\(\\textsf{E}[Y_i] = \\mu_i\\)"
  },
  {
    "objectID": "resources/slides/01-introduction.html#model-space",
    "href": "resources/slides/01-introduction.html#model-space",
    "title": "Introduction to STA721",
    "section": "Model Space",
    "text": "Model Space\nWe will work with inner product spaces: a vector spaces, say \\(\\mathbb{R}^n\\) equipped with an inner product \\(\\langle \\mathbf{x},\\mathbf{y}\\rangle \\equiv \\mathbf{x}^T\\mathbf{y}, \\quad \\mathbf{x}, \\mathbf{y}\\in \\mathbb{R}^n\\)\n\n\nDefinition: SubspaceA set \\(\\boldsymbol{{\\cal M}}\\) is a subspace of \\(\\mathbb{R}^n\\) if is a subset of \\(\\mathbb{R}^n\\) and also a vector space.\nThat is, if \\(\\mathbf{x}_1 \\in \\boldsymbol{{\\cal M}}\\) and \\(\\mathbf{x}_2 \\in \\boldsymbol{{\\cal M}}\\), then \\(b_1\\mathbf{x}_1 + b_2 \\mathbf{x}_2 \\in \\boldsymbol{{\\cal M}}\\) for all \\(b_1, b_2 \\in \\mathbb{R}\\)\n\n\n\n\n\nDefinition: Column SpaceThe column space of \\(\\mathbf{X}\\) is \\(C(\\mathbf{X}) = \\mathbf{X}\\boldsymbol{\\beta}\\) for \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\)\n\n\n\n\nIf \\(\\mathbf{X}\\) is full column rank, then the columns of \\(\\mathbf{X}\\) form a basis for \\(C(\\mathbf{X})\\) and \\(C(\\mathbf{X})\\) is a p-dimensional subspace of \\(\\mathbb{R}^n\\)\n\n\nIf we have just a single model matrix \\(\\mathbf{X}\\), then the subspace \\(\\boldsymbol{{\\cal M}}\\) is the model space."
  },
  {
    "objectID": "resources/slides/01-introduction.html#philosophy",
    "href": "resources/slides/01-introduction.html#philosophy",
    "title": "Introduction to STA721",
    "section": "Philosophy",
    "text": "Philosophy\n\nfor many problems frequentist and Bayesian methods will give similar answers (more a matter of taste in interpretation)\n\nFor small problems, Bayesian methods allow us to incorporate prior information which provides better calibrated answers\n\nfor problems with complex designs and/or missing data Bayesian methods are often easier to implement (do not need to rely on asymptotics) \n\nFor problems involving hypothesis testing or model selection frequentist and Bayesian methods can be strikingly different.\n\nFrequentist methods often faster (particularly with “big data”) so great for exploratory analysis and for building a “data-sense”\n\nBayesian methods sit on top of Frequentist Likelihood\n\nGoemetric perspective important in both!\n\n\nImportant to understand advantages and problems of each perspective!\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#outline",
    "href": "resources/slides/04-BLUE.html#outline",
    "title": "Best Linear Unbiased Estimators",
    "section": "Outline",
    "text": "Outline\n\nCharacterizing Linear Unbiased Estimators\nGauss-Markov Theorem\nBest Linear Unbiased Estimators\n\n\nReadings: - Christensen Chapter 1-2 and Appendix B - Seber & Lee Chapter 3"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#full-rank-case",
    "href": "resources/slides/04-BLUE.html#full-rank-case",
    "title": "Best Linear Unbiased Estimators",
    "section": "Full Rank Case",
    "text": "Full Rank Case\n\nModel: \\(\\mathbf{Y}= \\boldsymbol{\\mu}+ \\boldsymbol{\\epsilon}\\)\nMinimal Assumptions:\n\nMean \\(\\boldsymbol{\\mu}\\in C(\\mathbf{X})\\) for \\(\\mathbf{X}\\in \\mathbb{R}^{n \\times p}\\)\nErrors \\(\\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\)\n\n\n\n\nDefinition: Linear Unbiased Estimators (LUEs)\nAn estimator \\(\\tilde{\\boldsymbol{\\beta}}\\) is a Linear Unbiased Estimator (LUE) of \\(\\boldsymbol{\\beta}\\) if\n\nlinearity: \\(\\tilde{\\boldsymbol{\\beta}}= \\mathbf{A}\\mathbf{Y}\\) for \\(\\mathbf{A}\\in \\mathbb{R}^{p \\times n}\\)\nunbiasedness: \\(\\textsf{E}[\\tilde{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\\) for all \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\)\n\n\n\n\n\nThe class of linear unbiased estimators is the same for every model with parameter space \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\) and \\(P \\in \\cal{P}\\), for any collection \\(\\cal{P}\\) of mean-zero distributions over \\(\\mathbb{R}^n\\)."
  },
  {
    "objectID": "resources/slides/04-BLUE.html#linear-unbiased-estimators-lues-1",
    "href": "resources/slides/04-BLUE.html#linear-unbiased-estimators-lues-1",
    "title": "Best Linear Unbiased Estimators",
    "section": "Linear Unbiased Estimators (LUEs)",
    "text": "Linear Unbiased Estimators (LUEs)\n\nLet \\(\\textsf{N}\\) be an ONB for \\(\\boldsymbol{{\\cal N}}= \\boldsymbol{{\\cal M}}^\\perp = N(\\mathbf{X}^T)\\):\n\n\\(\\textsf{N}^T\\mathbf{m}=  \\textsf{N}^T\\mathbf{X}\\mathbf{b}= \\mathbf{0}\\quad \\forall \\mathbf{m}=\\mathbf{X}\\mathbf{b}\\in \\boldsymbol{{\\cal M}}\\)\n\\(\\textsf{N}^T\\textsf{N}= \\mathbf{I}_{n-p}\\)\n\n\n\nConsider another linear estimator \\(\\tilde{\\boldsymbol{\\beta}}= \\mathbf{A}\\mathbf{Y}\\)\n\nDifference between \\(\\tilde{\\boldsymbol{\\beta}}\\) and \\(\\hat{\\boldsymbol{\\beta}}\\) (OLS/MLE): \\[\\begin{align*}\n  \\mathbf{\\delta}= \\tilde{\\boldsymbol{\\beta}}- \\hat{\\boldsymbol{\\beta}}& = \\left(\\mathbf{A}- (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\right)\\mathbf{Y}\\\\\n                  & \\equiv \\mathbf{H}^T \\mathbf{Y}\n\\end{align*}\\]\nSince both \\(\\tilde{\\boldsymbol{\\beta}}\\) and \\(\\hat{\\boldsymbol{\\beta}}\\) are unbiased, \\(\\textsf{E}[\\mathbf{\\delta}] = \\mathbf{0}_p \\quad \\forall \\boldsymbol{\\beta}\\in \\mathbb{R}^p\\) \\[\\mathbf{0}_p = \\textsf{E}[\\mathbf{H}^T \\mathbf{Y}] = \\mathbf{H}^T \\mathbf{X}\\boldsymbol{\\beta}\\quad \\forall \\boldsymbol{\\beta}\\in \\mathbb{R}^p\\]\n\\(\\mathbf{X}^T \\mathbf{H}= \\mathbf{0}\\) so each column of \\(\\mathbf{H}\\) is in \\(\\boldsymbol{{\\cal M}}^\\perp \\equiv \\boldsymbol{{\\cal N}}\\)"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#lues-continued",
    "href": "resources/slides/04-BLUE.html#lues-continued",
    "title": "Best Linear Unbiased Estimators",
    "section": "LUEs continued",
    "text": "LUEs continued\nSince each column of \\(\\mathbf{H}\\) is in \\(\\boldsymbol{{\\cal N}}\\) there exists a \\(\\mathbf{G}\\in \\mathbb{R}^{p \\times (n-p)} \\ni \\mathbf{H}= \\textsf{N}\\mathbf{G}^T\\)\n\nRewriting \\(\\mathbf{\\delta}= \\tilde{\\boldsymbol{\\beta}}- \\hat{\\boldsymbol{\\beta}}\\): \\[\\begin{align*}\n\\tilde{\\boldsymbol{\\beta}}& = \\hat{\\boldsymbol{\\beta}}+ \\mathbf{\\delta}\\\\\n         & = \\hat{\\boldsymbol{\\beta}}+ \\mathbf{H}^T\\mathbf{Y}\\\\\n         & = \\hat{\\boldsymbol{\\beta}}+ \\mathbf{G}\\textsf{N}^T\\mathbf{Y}\n\\end{align*}\\]\n\ntherefore \\(\\tilde{\\boldsymbol{\\beta}}\\) is linear and unbiased: \\[\\begin{align*}\n\\textsf{E}[\\tilde{\\boldsymbol{\\beta}}] & = \\textsf{E}[\\hat{\\boldsymbol{\\beta}}+ \\mathbf{G}\\textsf{N}^T\\mathbf{Y}] \\\\\n           & =  \\textsf{E}[\\hat{\\boldsymbol{\\beta}}] + \\textsf{E}[\\mathbf{G}\\textsf{N}^T\\mathbf{Y}] \\\\\n           & =  \\boldsymbol{\\beta}+ \\mathbf{G}\\textsf{N}^T\\mathbf{X}\\boldsymbol{\\beta}\\\\\n           & = \\boldsymbol{\\beta}\n\\end{align*}\\]"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#characterization-of-lues",
    "href": "resources/slides/04-BLUE.html#characterization-of-lues",
    "title": "Best Linear Unbiased Estimators",
    "section": "Characterization of LUEs",
    "text": "Characterization of LUEs\nSummary of previous results:\n\nTheoremAn estimator \\(\\tilde{\\boldsymbol{\\beta}}\\) is a linear unbiased estimator of \\(\\boldsymbol{\\beta}\\) in a linear statistical model if and only if \\[\\tilde{\\boldsymbol{\\beta}}= \\hat{\\boldsymbol{\\beta}}+ \\mathbf{H}^T\\mathbf{Y}\\] for some \\(\\mathbf{H}\\in \\mathbb{R}^{n \\times p}\\) such that \\(\\mathbf{X}^T \\mathbf{H}= \\mathbf{0}\\) or equivalently for some \\(\\mathbf{G}\\in \\mathbb{R}^{p \\times (n-p)}\\) \\[\\tilde{\\boldsymbol{\\beta}}= \\hat{\\boldsymbol{\\beta}}+ \\mathbf{G}\\textsf{N}^T\\mathbf{Y}\\]"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#numerical",
    "href": "resources/slides/04-BLUE.html#numerical",
    "title": "Best Linear Unbiased Estimators",
    "section": "Numerical",
    "text": "Numerical\n\n# X is model matrix; Y is response\n  p = ncol(X)\n  n = nrow(X)\n  G = matrix(rnorm(p*(n-p)), nrow=p, ncol=n-p)\n  H = MASS::Null(X) %*% t(G)\n  btilde = bhat + t(H) %*% Y\n\ninfinite number of LUEs!"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#lues-via-generalized-inverses",
    "href": "resources/slides/04-BLUE.html#lues-via-generalized-inverses",
    "title": "Best Linear Unbiased Estimators",
    "section": "LUEs via Generalized Inverses",
    "text": "LUEs via Generalized Inverses\nLet \\(\\tilde{\\boldsymbol{\\beta}}= \\mathbf{A}\\mathbf{Y}\\) be a LUE in the statistical linear model \\(\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\) with \\(\\mathbf{X}\\) full column rank \\(p\\) \\[\\begin{align*}\n\\textsf{E}[\\tilde{\\boldsymbol{\\beta}}] & = \\textsf{E}[\\mathbf{A}\\mathbf{Y}] \\\\\n            & = \\mathbf{A}\\textsf{E}[\\mathbf{Y}] \\\\\n            & = \\mathbf{A}\\mathbf{X}\\boldsymbol{\\beta}\\quad \\forall \\boldsymbol{\\beta}\\in \\mathbb{R}^p\n\\end{align*}\\]\n\nMust have \\(\\mathbf{A}\\mathbf{X}= \\mathbf{I}_p\\) (\\(\\mathbf{A}\\) is a generalized inverse of \\(\\mathbf{X}\\))\n\\(\\mathbf{X}\\mathbf{X}^- \\mathbf{X}= \\mathbf{X}\\)\none generalized inverse is \\(\\mathbf{X}_{MP}^- = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\)\n\\(\\mathbf{X}_{MP}^- = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T = \\mathbf{V}\\boldsymbol{\\Delta}^{-1} \\mathbf{U}^T\\) (using SVD of \\(\\mathbf{X}= \\mathbf{U}\\boldsymbol{\\Delta}\\mathbf{V}^T\\))\n\\(\\mathbf{A}\\) is a generalized inverse of \\(\\mathbf{X}\\) iff \\(\\mathbf{A}= \\mathbf{X}_{MP}^- + \\mathbf{H}^T\\) for \\(\\mathbf{H}\\in \\mathbb{R}^{n \\times p} \\ni \\mathbf{H}^T \\mathbf{U}= \\mathbf{0}\\)\n\\(\\mathbf{A}\\mathbf{Y}= (\\mathbf{X}_{MP}^- + \\mathbf{H}^T)\\mathbf{Y}= \\hat{\\boldsymbol{\\beta}}+  \\mathbf{H}^T \\mathbf{Y}\\)"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#best-linear-unbiased-estimators",
    "href": "resources/slides/04-BLUE.html#best-linear-unbiased-estimators",
    "title": "Best Linear Unbiased Estimators",
    "section": "Best Linear Unbiased Estimators",
    "text": "Best Linear Unbiased Estimators\n\nthe distribution of values of any unbiased estimator is centered around \\(\\boldsymbol{\\beta}\\)\nout of the infinite number of LUEs is there one that is more concentrated around \\(\\boldsymbol{\\beta}\\)?\nis there an unbiased estimator that has a lower variance than all other unbiased estimators?\nRecall variance-covariance matrix of a random vector \\(\\mathbf{Z}\\) with mean \\(\\boldsymbol{\\theta}\\) \\[\\begin{align*}\n\\textsf{Cov}[\\mathbf{Z}]      & \\equiv \\textsf{E}[(\\mathbf{Z}- \\boldsymbol{\\theta})(\\mathbf{Z}- \\boldsymbol{\\theta})^T] \\\\\n\\textsf{Cov}[\\mathbf{Z}]_{ij} &  =     \\textsf{E}[(z_i - \\theta_i)(z_j - \\theta_j)]\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\nLemma\n\n\nLet \\(\\mathbf{A}\\in \\mathbb{R}^{q \\times p}\\) and \\(\\mathbf{b}\\in \\mathbb{R}^q\\) with \\(\\mathbf{Z}\\) a random vector in \\(\\mathbb{R}^p\\) then \\[\\textsf{Cov}[\\mathbf{A}\\mathbf{Z}+ \\mathbf{b}] = \\mathbf{A}\\textsf{Cov}[\\mathbf{Z}] \\mathbf{A}^T \\ge 0\\]"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#variance-of-linear-unbiased-estimators",
    "href": "resources/slides/04-BLUE.html#variance-of-linear-unbiased-estimators",
    "title": "Best Linear Unbiased Estimators",
    "section": "Variance of Linear Unbiased Estimators",
    "text": "Variance of Linear Unbiased Estimators\nLet’s look at the variance of any LUE under assumption \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\sigma^2 \\mathbf{I}_n\\)\n\nfor \\(\\hat{\\boldsymbol{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{Y}= \\boldsymbol{\\beta}+ (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\boldsymbol{\\epsilon}\\) \\[\\begin{align*}\n\\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}] & = \\textsf{Cov}[\\boldsymbol{\\beta}+ (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\boldsymbol{\\epsilon}] \\\\\n          & =  (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\textsf{Cov}[\\boldsymbol{\\epsilon}] \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n          & = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n          & = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1}\n\\end{align*}\\]\nCovariance is increasing in \\(\\sigma^2\\) and generally decreasing in \\(n\\)\nRewrite \\(\\mathbf{X}^T\\mathbf{X}\\) as \\(\\mathbf{X}^T\\mathbf{X}= \\sum_{i=1}^n \\mathbf{x}_i \\mathbf{x}_i^T\\) (a sum of \\(n\\) outer-products)"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#variance-of-arbitrary-lue",
    "href": "resources/slides/04-BLUE.html#variance-of-arbitrary-lue",
    "title": "Best Linear Unbiased Estimators",
    "section": "Variance of Arbitrary LUE",
    "text": "Variance of Arbitrary LUE\n\nfor \\(\\tilde{\\boldsymbol{\\beta}}= \\left((\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T + \\mathbf{H}^T \\right)\\mathbf{Y}= \\boldsymbol{\\beta}+ \\left((\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T + \\mathbf{H}^T \\right)\\boldsymbol{\\epsilon}\\)\nrecall \\(\\mathbf{X}_{MP}^- \\equiv  (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\) \\[\\begin{align*}\n\\textsf{Cov}[\\tilde{\\boldsymbol{\\beta}}] & = \\textsf{Cov}[\\left(\\mathbf{X}_{MP}^- + \\mathbf{H}^T \\right)\\boldsymbol{\\epsilon}]  \\\\\n            & = \\sigma^2 \\left(\\mathbf{X}_{MP}^- + \\mathbf{H}^T \\right)\\left(\\mathbf{X}_{MP}^- + \\mathbf{H}^T \\right)^T \\\\\n            & = \\sigma^2\\left( \\mathbf{X}_{MP}^-(\\mathbf{X}_{MP}^-)^T + \\mathbf{X}_{MP}^-\\mathbf{H}+\n                \\mathbf{H}^T (\\mathbf{X}_{MP}^-)^T + \\mathbf{H}^T \\mathbf{H}\\right) \\\\\n            & =   \\sigma^2\\left( (\\mathbf{X}^T\\mathbf{X})^{-1} +  \\mathbf{H}^T \\mathbf{H}\\right)\n\\end{align*}\\]\nCross-product term \\(\\mathbf{H}^T(\\mathbf{X}_{MP}^-)^T = \\mathbf{H}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} =  \\mathbf{0}\\)\nTherefor the \\(\\textsf{Cov}[\\tilde{\\boldsymbol{\\beta}}] = \\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}] + \\mathbf{H}^T\\mathbf{H}\\)\nthe sum of a positive definite matrix plus a positive semi-definite matrix"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#gauss-markov-theorem",
    "href": "resources/slides/04-BLUE.html#gauss-markov-theorem",
    "title": "Best Linear Unbiased Estimators",
    "section": "Gauss-Markov Theorem",
    "text": "Gauss-Markov Theorem\nIs \\(\\textsf{Cov}[\\tilde{\\boldsymbol{\\beta}}] \\ge \\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}]\\) in some sense?\n\n\nDefinition: Loewner OrderingFor two positive semi-definite matrices \\(\\boldsymbol{\\Sigma}_1\\) and \\(\\boldsymbol{\\Sigma}_2\\), we say that \\(\\boldsymbol{\\Sigma}_1 &gt; \\boldsymbol{\\Sigma}_2\\) if \\(\\boldsymbol{\\Sigma}_1 - \\boldsymbol{\\Sigma}_2\\) is positive definite, \\(\\mathbf{x}^T(\\boldsymbol{\\Sigma}_1 - \\boldsymbol{\\Sigma}_2)\\mathbf{x}) &gt; 0\\), and \\(\\boldsymbol{\\Sigma}_1 \\ge \\boldsymbol{\\Sigma}_2\\) if \\(\\boldsymbol{\\Sigma}_1 - \\boldsymbol{\\Sigma}_2\\) is positive semi-definite, \\(\\mathbf{x}^T(\\boldsymbol{\\Sigma}_1 - \\boldsymbol{\\Sigma}_2)\\mathbf{x}) \\ge 0\\)\n\n\n\nSince \\(\\textsf{Cov}[\\tilde{\\boldsymbol{\\beta}}] - \\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}] = \\mathbf{H}^T\\mathbf{H}\\), we have that \\(\\textsf{Cov}[\\tilde{\\boldsymbol{\\beta}}] \\ge \\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}]\\)\n\n\n\n\nTheorem: Gauss-MarkovLet \\(\\tilde{\\boldsymbol{\\beta}}\\) be a linear unbiased estimator of \\(\\boldsymbol{\\beta}\\) in a linear model where \\(\\textsf{E}[\\mathbf{Y}] = \\mathbf{X}\\boldsymbol{\\beta}, \\boldsymbol{\\beta}\\in \\mathbb{R}^p\\), \\(\\mathbf{X}\\) rank \\(p\\), and \\(\\textsf{Cov}[\\mathbf{Y}] = \\sigma^2\\mathbf{I}_n, \\sigma^2 &gt; 0\\). Then \\(\\textsf{Cov}[\\tilde{\\boldsymbol{\\beta}}] \\ge \\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}]\\) where \\(\\hat{\\boldsymbol{\\beta}}\\) is the OLS estimator and is the Best Linear Unbiased Estimator (BLUE) of \\(\\boldsymbol{\\beta}\\)."
  },
  {
    "objectID": "resources/slides/04-BLUE.html#slide13-id",
    "href": "resources/slides/04-BLUE.html#slide13-id",
    "title": "Best Linear Unbiased Estimators",
    "section": "",
    "text": "Theorem: Gauss-Markov Theorem (Classic)For \\(\\mathbf{Y}= \\boldsymbol{\\mu}+ \\boldsymbol{\\epsilon}\\), with \\(\\boldsymbol{\\mu}\\in \\boldsymbol{{\\cal M}}\\), \\(\\textsf{E}[\\boldsymbol{\\epsilon}]= \\mathbf{0}_n\\) and \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] =\\sigma^2 \\mathbf{I}_n\\) and \\(\\mathbf{P}\\) the orthogonal projection onto \\(\\boldsymbol{{\\cal M}}\\), \\(\\mathbf{P}\\mathbf{Y}= \\hat{\\boldsymbol{\\mu}}\\) is the BLUE of \\(\\boldsymbol{\\mu}\\) out of the class of LUEs \\(\\mathbf{A}\\mathbf{Y}\\) where \\(\\textsf{E}[\\mathbf{A}\\mathbf{Y}] = \\boldsymbol{\\mu}\\), \\(\\mathbf{A}\\in \\mathbb{R}^{n \\times n}\\) equality iff \\(\\mathbf{A}= \\mathbf{P}\\)\n\n\n\nProof\n\nwrite \\(\\mathbf{A}= \\mathbf{P}+ \\mathbf{H}^T\\) so \\(\\mathbf{H}^T = \\mathbf{A}- \\mathbf{P}\\)\nsince \\(\\mathbf{A}\\boldsymbol{\\mu}= \\boldsymbol{\\mu}\\), \\(\\mathbf{H}^T\\mu = \\mathbf{0}_n\\) for \\(\\mu \\in \\boldsymbol{{\\cal M}}\\) and \\(\\mathbf{H}^T \\mathbf{P}= \\mathbf{P}\\mathbf{H}= \\mathbf{0}\\) (columns of \\(\\mathbf{H}\\in \\boldsymbol{{\\cal M}}^\\perp\\)) \\[\\begin{align*}\n\\textsf{E}[\\|\\mathbf{A}\\mathbf{Y}- \\boldsymbol{\\mu}\\|^2]  & =  \\textsf{E}[\\|\\mathbf{P}(\\mathbf{Y}- \\boldsymbol{\\mu}) + \\mathbf{H}^T(\\mathbf{Y}- \\boldsymbol{\\mu})\\|^2]  \\\\\n& = \\textsf{E}[\\|\\mathbf{P}(\\mathbf{Y}- \\boldsymbol{\\mu})\\|^2] + \\underbrace{\\textsf{E}[\\|\\mathbf{H}^T(\\mathbf{Y}- \\boldsymbol{\\mu})\\|^2]} + \\underbrace{{\\text{cross-product}}} \\\\\n& \\hspace{4.35in} \\ge 0 \\quad  + \\hspace{1.25in} 0\\\\\n& \\ge \\textsf{E}[\\|\\mathbf{P}(\\mathbf{Y}- \\boldsymbol{\\mu})\\|^2]\n\\end{align*}\\]\nCross-product is \\(2\\textsf{E}[(\\mathbf{H}^T(\\mathbf{Y}- \\mu))^T\\mathbf{P}(\\mathbf{Y}- \\boldsymbol{\\mu})] = 0\\) (see last slide)"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#estimation-of-linear-functionals-of-boldsymbolmu",
    "href": "resources/slides/04-BLUE.html#estimation-of-linear-functionals-of-boldsymbolmu",
    "title": "Best Linear Unbiased Estimators",
    "section": "Estimation of Linear Functionals of \\(\\boldsymbol{\\mu}\\)",
    "text": "Estimation of Linear Functionals of \\(\\boldsymbol{\\mu}\\)\nIf \\(\\mathbf{P}\\mathbf{Y}= \\hat{\\boldsymbol{\\mu}}\\) is the BLUE of \\(\\boldsymbol{\\mu}\\), is \\(\\mathbf{B}\\mathbf{P}\\mathbf{Y}= \\mathbf{B}\\hat{\\boldsymbol{\\mu}}\\) the BLUE of \\(\\mathbf{B}\\boldsymbol{\\mu}\\)?\n\nYes! Similar proof as above to show that out of the class of LUEs \\(\\mathbf{A}\\mathbf{Y}\\) of \\(\\mathbf{B}\\boldsymbol{\\mu}\\) where \\(\\mathbf{A}\\in \\mathbb{R}^{d \\times n}\\) that \\[\\textsf{E}[\\|\\mathbf{A}\\mathbf{Y}- \\mathbf{B}\\boldsymbol{\\mu}\\|^2] \\ge \\textsf{E}[\\|\\mathbf{B}\\mathbf{P}\\mathbf{Y}- \\mathbf{B}\\boldsymbol{\\mu}\\|^2]\\] with equality iff \\(\\mathbf{A}= \\mathbf{B}\\mathbf{P}\\).\n\n\nWhat about linear functionals of \\(\\boldsymbol{\\beta}\\), \\(\\boldsymbol{\\Lambda}^T \\boldsymbol{\\beta}\\), for \\(\\mathbf{X}\\) rank \\(r \\le p\\)?\n\n\\(\\hat{\\boldsymbol{\\beta}}\\) is not unique if \\(r &lt; p\\) even though \\(\\hat{\\boldsymbol{\\mu}}\\) is unique (\\(\\hat{\\boldsymbol{\\beta}}\\) is not BLUE)\nSince \\(\\mathbf{B}\\boldsymbol{\\mu}= \\mathbf{B}\\mathbf{X}\\boldsymbol{\\beta}\\) is always identifiable, the only linear functions of \\(\\boldsymbol{\\beta}\\) that are identifiable and can be estimated uniquely are functions of \\(\\mathbf{X}\\boldsymbol{\\beta}\\), i.e. estimates in the form \\(\\boldsymbol{\\Lambda}^T \\boldsymbol{\\beta}= \\mathbf{B}\\mathbf{X}\\boldsymbol{\\beta}\\) or \\(\\boldsymbol{\\Lambda}= \\mathbf{X}^T \\mathbf{B}^T\\).\ncolumns of \\(\\boldsymbol{\\Lambda}\\) must be in the \\(C(\\mathbf{X}^T)\\)\ndetailed discussion and proof in Christensen Ch. 2 for scalar functionals \\(\\lambda^T\\beta\\)."
  },
  {
    "objectID": "resources/slides/04-BLUE.html#blue-of-boldsymbollambdat-boldsymbolbeta",
    "href": "resources/slides/04-BLUE.html#blue-of-boldsymbollambdat-boldsymbolbeta",
    "title": "Best Linear Unbiased Estimators",
    "section": "BLUE of \\(\\boldsymbol{\\Lambda}^T \\boldsymbol{\\beta}\\)",
    "text": "BLUE of \\(\\boldsymbol{\\Lambda}^T \\boldsymbol{\\beta}\\)\nIf \\(\\boldsymbol{\\Lambda}^T= \\mathbf{B}\\mathbf{X}\\) for some matrix \\(\\mathbf{B}\\) then\n\n\\(\\textsf{E}[\\mathbf{B}\\mathbf{P}\\mathbf{Y}] = \\textsf{E}[\\boldsymbol{\\Lambda}^T \\hat{\\boldsymbol{\\beta}}] = \\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\)\nThe unique OLS estimate of \\(\\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\) is \\(\\boldsymbol{\\Lambda}^T\\hat{\\boldsymbol{\\beta}}\\)\n\\(\\mathbf{B}\\mathbf{P}\\mathbf{Y}= \\boldsymbol{\\Lambda}^T\\hat{\\boldsymbol{\\beta}}\\) is the BLUE of \\(\\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\) \\[\\begin{align*}\n& \\textsf{E}[\\|\\mathbf{B}\\mathbf{P}\\mathbf{Y}- \\mathbf{B}\\boldsymbol{\\mu}\\|^2]  \\le \\textsf{E}[\\|\\mathbf{A}\\mathbf{Y}- \\mathbf{B}\\boldsymbol{\\mu}\\|^2] \\\\\n\\Leftrightarrow & \\\\\n& \\textsf{E}[\\|\\boldsymbol{\\Lambda}^T\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta})\\|^2]  \\le \\textsf{E}[\\|\\mathbf{L}^T\\tilde{\\boldsymbol{\\beta}}- \\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\|^2]\n\\end{align*}\\] for LUE \\(\\mathbf{A}\\mathbf{Y}= \\mathbf{L}^T\\tilde{\\boldsymbol{\\beta}}\\) of \\(\\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\)\nProof proceeds as the classic case."
  },
  {
    "objectID": "resources/slides/04-BLUE.html#proof-of-cross-product",
    "href": "resources/slides/04-BLUE.html#proof-of-cross-product",
    "title": "Best Linear Unbiased Estimators",
    "section": "Proof of Cross-Product",
    "text": "Proof of Cross-Product\nLet \\(\\mathbf{D}= \\mathbf{H}\\mathbf{P}\\) and write \\[\\begin{align*}\n\\textsf{E}[(\\mathbf{H}^T(\\mathbf{Y}- \\mu))^T\\mathbf{P}(\\mathbf{Y}- \\boldsymbol{\\mu})] & = \\textsf{E}[(\\mathbf{Y}- \\mu))^T\\mathbf{H}\\mathbf{P}(\\mathbf{Y}- \\boldsymbol{\\mu})] \\\\\n& = \\textsf{E}[(\\mathbf{Y}- \\mu))^T\\mathbf{D}(\\mathbf{Y}- \\boldsymbol{\\mu})]\n\\end{align*}\\]\n\n\\[\\begin{align*}\n\\textsf{E}[(\\mathbf{Y}- \\mu))^T\\mathbf{D}(\\mathbf{Y}- \\boldsymbol{\\mu})] = & \\textsf{E}[\\textsf{tr}(\\mathbf{Y}- \\mu))^T\\mathbf{D}(\\mathbf{Y}- \\boldsymbol{\\mu}))]  \\\\\n= & \\textsf{E}[\\textsf{tr}(\\mathbf{D}(\\mathbf{Y}- \\boldsymbol{\\mu})(\\mathbf{Y}- \\mu)^T)] \\\\\n= & \\textsf{tr}(\\textsf{E}[\\mathbf{D}(\\mathbf{Y}- \\boldsymbol{\\mu})(\\mathbf{Y}- \\mu)^T]) \\\\\n= & \\textsf{tr}(\\mathbf{D}\\textsf{E}[(\\mathbf{Y}- \\boldsymbol{\\mu})(\\mathbf{Y}- \\mu)^T]) \\\\\n  = & \\sigma^2 \\textsf{tr}(\\mathbf{D}\\mathbf{I}_n)\\\\\n\\end{align*}\\]\n\n\nSince \\(\\textsf{tr}(\\mathbf{D}) = \\textsf{tr}(\\mathbf{H}\\mathbf{P}) = \\textsf{tr}(\\mathbf{P}\\mathbf{H})\\) we can conclude that the cross-product term is zero.\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/02-mles.html#outline",
    "href": "resources/slides/02-mles.html#outline",
    "title": "Maximum Likelihood Estimation",
    "section": "Outline",
    "text": "Outline\n\nLikelihood Function\nProjections\nMaximum Likelihood Estimates\n\n\nReadings: Christensen Chapter 1-2, Appendix A, and Appendix B"
  },
  {
    "objectID": "resources/slides/02-mles.html#normal-model",
    "href": "resources/slides/02-mles.html#normal-model",
    "title": "Maximum Likelihood Estimation",
    "section": "Normal Model",
    "text": "Normal Model\nTake an random vector \\(\\mathbf{Y}\\in \\mathbb{R}^n\\) which is observable and decompose\n\\[ \\mathbf{Y}= \\boldsymbol{\\mu}+ \\boldsymbol{\\epsilon}\\]\n\n\\(\\boldsymbol{\\mu}\\in \\mathbb{R}^n\\) (unknown, fixed)\n\n\\(\\boldsymbol{\\epsilon}\\in \\mathbb{R}^n\\) unobservable error vector (random)\n\n\nUsual assumptions?\n\n\\(E[\\epsilon_i] = 0 \\ \\forall i \\Leftrightarrow \\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}\\) \\(\\quad \\Rightarrow \\textsf{E}[\\mathbf{Y}] = \\boldsymbol{\\mu}\\) (mean vector)\n\\(\\epsilon_i\\) independent with \\(\\textsf{Var}(\\epsilon_i) = \\sigma^2\\) and \\(\\textsf{Cov}(\\epsilon_i, \\epsilon_j) = 0\\)\nMatrix version \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] \\equiv \\left[ (\\textsf{E}\\left[(\\epsilon_i -\\textsf{E}[\\epsilon_i])(\\epsilon_j - \\textsf{E}[\\epsilon_j])\\right]\\right]_{ij} = \\sigma^2 \\mathbf{I}_n\n\\quad \\Rightarrow \\textsf{Cov}[\\mathbf{Y}] = \\sigma^2 \\mathbf{I}_n\\) (errors are uncorrelated)\n\\(\\boldsymbol{\\epsilon}_i \\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}\\textsf{N}(0, \\sigma^2)\\) implies that \\(Y_i \\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}\\textsf{N}(\\mu_i, \\sigma^2)\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#likelihood-function",
    "href": "resources/slides/02-mles.html#likelihood-function",
    "title": "Maximum Likelihood Estimation",
    "section": "Likelihood Function",
    "text": "Likelihood Function\nThe likelihood function for \\(\\boldsymbol{\\mu}, \\sigma^2\\) is proportional to the sampling distribution of the data\n\\[\\begin{eqnarray*}\n{\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2) & \\propto & \\prod_{i = 1}^n \\frac{1}{\\sqrt{(2 \\pi\n                                 \\sigma^2)}} \\exp{- \\frac{1}{2}\n                                 \\left\\{ \\frac{( Y_i\n                                 - \\mu_i)^2}{\\sigma^2} \\right\\}}\n                                 \\\\\n& \\propto & ({2 \\pi} \\sigma^2)^{-n/2}\n\\exp{\\left\\{ - \\frac 1 2  \\frac{ \\sum_i(Y_i - \\mu_i)^2 )}{\\sigma^2}\n\\right\\}}   \\\\\n   & \\propto & (\\sigma^2)^{-n/2}\n\\exp{\\left\\{ - \\frac 1 2 \\frac{\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2}{\\sigma^2}\n\\right\\}} \\\\\n  & \\propto &  (2 \\pi)^{-n/2}\n| \\mathbf{I}_n\\sigma^2|^{-1/2}\n\\exp{\\left\\{ - \\frac 1 2 \\frac{\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2}{\\sigma^2}\n\\right\\}}  \n\\end{eqnarray*}\\]\n\nLast line is the density of \\(\\mathbf{Y}\\sim \\textsf{N}_n\\left(\\boldsymbol{\\mu}, \\sigma^2 \\mathbf{I}_n\\right)\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#mles",
    "href": "resources/slides/02-mles.html#mles",
    "title": "Maximum Likelihood Estimation",
    "section": "MLEs",
    "text": "MLEs\nFind values of \\(\\hat{\\boldsymbol{\\mu}}\\) and \\({\\hat{\\sigma}}^2\\) that maximize the likelihood \\({\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2)\\) for \\(\\boldsymbol{\\mu}\\in \\mathbb{R}^n\\) and \\(\\sigma^2 \\in \\mathbb{R}^+\\) \\[\\begin{eqnarray*}\n{\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2)\n    & \\propto & (\\sigma^2)^{-n/2}\n\\exp{\\left\\{ - \\frac 1 2 \\frac{\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2}{\\sigma^2}\n\\right\\}} \\\\\n\\log({\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2) )\n   & \\propto & -\\frac{n}{2} \\log(\\sigma^2)  - \\frac 1 2 \\frac{\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2}{\\sigma^2}\n\\\\\n\\end{eqnarray*}\\] or equivalently the log likelihood\n\nClearly, \\(\\hat{\\boldsymbol{\\mu}}= \\mathbf{Y}\\) but \\({\\hat{\\sigma}}^2= 0\\) is outside the parameter space\nIf \\(\\boldsymbol{\\mu}= \\mathbf{X}\\boldsymbol{\\beta}\\), can show that \\(\\hat{\\boldsymbol{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) is the MLE/OLS estimator of \\(\\boldsymbol{\\beta}\\) and \\(\\hat{\\boldsymbol{\\mu}}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\) if \\(\\mathbf{X}\\) is full column rank.\nshow via projections"
  },
  {
    "objectID": "resources/slides/02-mles.html#projections",
    "href": "resources/slides/02-mles.html#projections",
    "title": "Maximum Likelihood Estimation",
    "section": "Projections",
    "text": "Projections\ntake any point \\(\\mathbf{y}\\in \\mathbb{R}^n\\) and “project” it onto \\(C(\\mathbf{X}) = \\boldsymbol{{\\cal M}}\\)\n\nany point already in \\(\\boldsymbol{{\\cal M}}\\) stays the same\nso if \\(\\mathbf{P}_\\mathbf{X}\\) is a projection onto the column space of \\(\\mathbf{X}\\) then for \\(\\mathbf{m}\\in C(\\mathbf{X})\\) \\(\\mathbf{P}_\\mathbf{X}\\mathbf{m}= \\mathbf{m}\\)\n\\(\\mathbf{P}_\\mathbf{X}\\) is a linear transformation from \\(\\mathbb{R}^n \\to \\mathbb{R}^n\\)\nmaps vectors in \\(\\mathbb{R}^n\\) into \\(C(\\mathbf{X})\\)\nif \\(\\mathbf{z}\\in \\mathbb{R}^n\\) then \\(\\mathbf{P}_\\mathbf{X}\\mathbf{z}= \\mathbf{X}\\mathbf{a}\\in C(\\mathbf{X})\\) for some \\(\\mathbf{a}\\in \\mathbb{R}^p\\)\n\n\n\n\n\n\n\n\nExample\n\n\nFor \\(\\mathbf{X}\\in \\mathbb{R}^{n \\times p}\\), rank \\(p\\), \\(\\mathbf{P}_\\mathbf{X}= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}\\) is a projection onto the \\(p\\) dimensional subspace \\(\\boldsymbol{{\\cal M}}= C(\\mathbf{X})\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#idempotent-matrix",
    "href": "resources/slides/02-mles.html#idempotent-matrix",
    "title": "Maximum Likelihood Estimation",
    "section": "Idempotent Matrix",
    "text": "Idempotent Matrix\nWhat if we project a projection?\n\n\\(\\mathbf{P}_\\mathbf{X}\\mathbf{z}= \\mathbf{X}\\mathbf{a}\\in C(\\mathbf{X})\\)\n\\(\\mathbf{P}_\\mathbf{X}\\mathbf{X}\\mathbf{a}= \\mathbf{X}\\mathbf{a}\\)\nsince \\(\\mathbf{P}_\\mathbf{X}^2 \\mathbf{z}=  \\mathbf{P}_\\mathbf{X}\\mathbf{z}\\) for all \\(\\mathbf{z}\\in \\mathbb{R}^n\\) we have \\(\\mathbf{P}_\\mathbf{X}^2 = \\mathbf{P}_\\mathbf{X}\\)\n\n\n\nDefinition: ProjectionFor a matrix \\(\\mathbf{P}\\) in \\(\\mathbb{R}^{n \\times n}\\) is a projection matrix if \\(\\mathbf{P}^2 = \\mathbf{P}\\). That is all projections \\(\\mathbf{P}\\) are idempotent matrix.\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\nFor \\(\\mathbf{X}\\in \\mathbb{R}^{n \\times p}\\), rank \\(p\\), if \\(\\mathbf{P}_\\mathbf{X}= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}\\) use the definition to show that it is a projection onto the \\(p\\) dimensional subspace \\(\\boldsymbol{{\\cal M}}= C(\\mathbf{X})\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#null-space",
    "href": "resources/slides/02-mles.html#null-space",
    "title": "Maximum Likelihood Estimation",
    "section": "Null Space",
    "text": "Null Space\n\nDefinition: Orthogonal ComplementThe set of all vectors that are orthogonal to a given subspace \\(\\boldsymbol{{\\cal M}}\\) is called the orthogonal complement of the subspace denoted as \\(\\boldsymbol{{\\cal M}}^\\perp\\). Under the usual inner product, \\(\\boldsymbol{{\\cal M}}^\\perp \\equiv \\{\\mathbf{n}\\in \\mathbb{R}^n \\ni \\mathbf{m}^T\\mathbf{n}= 0 {\\text{ for }} \\mathbf{m}\\in \\boldsymbol{{\\cal M}}\\}\\)\n\n\n\n\nDefinition: Null SpaceFor a matrix \\(\\mathbf{A}\\), the null space of \\(\\mathbf{A}\\) is defined as \\(N(\\mathbf{A}) = \\{\\mathbf{n}\\ni \\mathbf{A}\\mathbf{n}= \\mathbf{0}\\}\\)\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\nShow that \\(C(\\mathbf{X})^\\perp\\) (the orthogonal complement of \\(C(\\mathbf{X})\\)) is the null space of \\(\\mathbf{X}^T\\), \\(\\, N(\\mathbf{X}^T)\\)."
  },
  {
    "objectID": "resources/slides/02-mles.html#orthogonal-projection",
    "href": "resources/slides/02-mles.html#orthogonal-projection",
    "title": "Maximum Likelihood Estimation",
    "section": "Orthogonal Projection",
    "text": "Orthogonal Projection\n\nDefinition: Orthogonal ProjectionsFor a vector space \\({\\cal V}\\) with an inner product \\(\\langle \\mathbf{x}, \\mathbf{y}\\rangle\\) for \\(\\mathbf{x}, \\mathbf{y}\\in {\\cal V}\\), \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are orthogonal if \\(\\langle \\mathbf{x}, \\mathbf{y}\\rangle = 0\\). A projection \\(\\mathbf{P}\\) is an orthogonal projection onto a subspace \\(\\boldsymbol{{\\cal M}}\\) of \\({\\cal V}\\) if for any \\(\\mathbf{m}\\in {\\cal V}\\), \\(\\mathbf{P}\\mathbf{m}= \\mathbf{m}\\) and for any \\(\\mathbf{n}\\in \\boldsymbol{{\\cal M}}^\\perp\\), \\(\\mathbf{P}\\mathbf{n}= \\mathbf{0}\\).\nThe null space of \\(\\mathbf{P}\\) is the orthogonal complement of \\(\\boldsymbol{{\\cal M}}\\)\n\n\n\nFor \\(\\mathbb{R}^N\\) with the inner product, \\(\\langle \\mathbf{x}, \\mathbf{y}\\rangle = \\mathbf{x}^T\\mathbf{y}\\), \\(\\mathbf{P}\\) is an orthogonal projection onto \\(\\boldsymbol{{\\cal M}}\\) if \\(\\mathbf{P}\\) is a projection (\\(\\mathbf{P}^2 = \\mathbf{P}\\)) and it is symmetric (\\(\\mathbf{P}= \\mathbf{P}^T\\))\n\n\n\n\n\n\n\n\nExercise\n\n\nShow that \\(\\mathbf{P}_\\mathbf{X}\\) is an orthogonal projection on \\(C(\\mathbf{X})\\)."
  },
  {
    "objectID": "resources/slides/02-mles.html#decompsition",
    "href": "resources/slides/02-mles.html#decompsition",
    "title": "Maximum Likelihood Estimation",
    "section": "Decompsition",
    "text": "Decompsition\n\nFor any \\(\\mathbf{y}\\in \\mathbb{R}^n\\), we can write it uniquely as a vector \\[ \\mathbf{y}= \\mathbf{m}+ \\mathbf{n}, \\quad \\mathbf{m}\\in \\boldsymbol{{\\cal M}}\\quad \\mathbf{n}\\in \\boldsymbol{{\\cal M}}^\\perp\\]\nwrite \\(\\mathbf{y}= \\mathbf{P}\\mathbf{y}+ (\\mathbf{y}- \\mathbf{P}\\mathbf{y}) = \\mathbf{P}\\mathbf{y}+ (\\mathbf{I}- \\mathbf{P})\\mathbf{y}\\)\nclaim that if \\(\\mathbf{P}\\) is an orthogonal projection, \\((\\mathbf{I}- \\mathbf{P})\\) is an orthogonal projection onto \\(\\boldsymbol{{\\cal M}}^\\perp\\)\nif \\(\\mathbf{n}\\in \\boldsymbol{{\\cal M}}^\\perp\\), then \\((\\mathbf{I}- \\mathbf{P})\\mathbf{n}= \\mathbf{n}- \\mathbf{P}\\mathbf{n}= \\mathbf{n}\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#back-to-mles",
    "href": "resources/slides/02-mles.html#back-to-mles",
    "title": "Maximum Likelihood Estimation",
    "section": "Back to MLEs",
    "text": "Back to MLEs\n\n\\(\\mathbf{Y}\\sim \\textsf{N}(\\boldsymbol{\\mu}, \\sigma^2 \\mathbf{I}_n)\\) with \\(\\boldsymbol{\\mu}= \\mathbf{X}\\boldsymbol{\\beta}\\) and \\(\\mathbf{X}\\) full column rank\nClaim: Maximum Likelihood Estimator (MLE) of \\(\\boldsymbol{\\mu}\\) is \\(\\mathbf{P}_\\mathbf{X}\\mathbf{Y}\\)\nLog Likelihood: \\[ \\log {\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2) =\n-\\frac{n}{2} \\log(\\sigma^2)\n- \\frac 1 2 \\frac{\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2}{\\sigma^2}\n\\]\nDecompose \\(\\mathbf{Y}= \\mathbf{P}_\\mathbf{X}\\mathbf{Y}+ (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\)\n\nUse \\(\\mathbf{P}_\\mathbf{X}\\boldsymbol{\\mu}= \\boldsymbol{\\mu}\\)\n\nSimplify \\(\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#expand",
    "href": "resources/slides/02-mles.html#expand",
    "title": "Maximum Likelihood Estimation",
    "section": "Expand",
    "text": "Expand\n\\[\\begin{eqnarray*}\n    \\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2 & = & \\|  { (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}+ \\mathbf{P}_x \\mathbf{Y}} -\n    \\boldsymbol{\\mu}\\|^2  \\\\\n  & = & \\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}+ \\mathbf{P}_x \\mathbf{Y}- {\\mathbf{P}_\\mathbf{X}}\\boldsymbol{\\mu}\\|^2  \\\\\n  & = & {\\|(\\mathbf{I}-\\mathbf{P}_\\mathbf{x})}\\mathbf{Y}+  {\\mathbf{P}_\\mathbf{X}}(\\mathbf{Y}- \\boldsymbol{\\mu})\n  \\|^2 \\\\\n& = & {\\|(\\mathbf{I}-\\mathbf{P}_\\mathbf{x})\\mathbf{Y}\\|^2} +  {\\|\n   {\\mathbf{P}_\\mathbf{X}}(\\mathbf{Y}- \\boldsymbol{\\mu}) \\|^2}  + {\\small{2 (\\mathbf{Y}-\n\\boldsymbol{\\mu})^T \\mathbf{P}_\\mathbf{X}^T (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}}}\\\\\n& = & \\|(\\mathbf{I}-\\mathbf{P}_\\mathbf{x})\\mathbf{Y}\\|^2 +  \\| {\\mathbf{P}_\\mathbf{X}}(\\mathbf{Y}- \\boldsymbol{\\mu}) \\|^2 + {0}\n\\\\\n& = & \\|(\\mathbf{I}-\\mathbf{P}_\\mathbf{x})\\mathbf{Y}\\|^2 +  \\| {\\mathbf{P}_\\mathbf{X}}\\mathbf{Y}- \\boldsymbol{\\mu}\\|^2\n  \\end{eqnarray*}\\]\n\nCrossproduct term is zero: \\[\\begin{eqnarray*}\n  \\mathbf{P}_\\mathbf{X}^T (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) & = &  \\mathbf{P}_\\mathbf{X}(\\mathbf{I}- \\mathbf{P}_\\mathbf{X})  \\\\\n  & = &  \\mathbf{P}_\\mathbf{X}- \\mathbf{P}_\\mathbf{X}\\mathbf{P}_\\mathbf{X}\\\\\n& = &  \\mathbf{P}_\\mathbf{X}- \\mathbf{P}_\\mathbf{X}\\\\\n& = & \\mathbf{0}\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "resources/slides/02-mles.html#log-likelihood",
    "href": "resources/slides/02-mles.html#log-likelihood",
    "title": "Maximum Likelihood Estimation",
    "section": "Log Likelihood",
    "text": "Log Likelihood\nSubstitute decomposition into log likelihood \\[\\begin{eqnarray*}\n\\log {\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2)  & = &\n-\\frac{n}{2} \\log(\\sigma^2) - \\frac 1 2 \\frac{\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2}{\\sigma^2}  \\\\\n  & = & -\\frac{n}{2} \\log(\\sigma^2)  - \\frac 1 2 \\left( \\frac{\\|(\\mathbf{I}- \\mathbf{P}_\\mathbf{X})\n  \\mathbf{Y}\\|^2}{\\sigma^2} + \\frac{\\| \\mathbf{P}_\\mathbf{X}\\mathbf{Y}- \\boldsymbol{\\mu}\\|^2 } {\\sigma^2} \\right)   \\\\\n& = &  \\underbrace { -\\frac{n}{2} \\log(\\sigma^2)  - \\frac 1 2  \\frac{\\|(\\mathbf{I}- \\mathbf{P}_\\mathbf{X})\n  \\mathbf{Y}\\|^2}{\\sigma^2} }  +  \\underbrace{- \\frac 1 2  \\frac{\\| \\mathbf{P}_\\mathbf{X}\\mathbf{Y}-\n  \\boldsymbol{\\mu}\\|^2 } {\\sigma^2}}   \\\\\n& = &  \\text{ constant with respect to } \\boldsymbol{\\mu}\\qquad  \\leq 0\n\\end{eqnarray*}\\]\n\nMaximize with respect to \\(\\boldsymbol{\\mu}\\) for each \\(\\sigma^2\\)\nRHS is largest when \\(\\boldsymbol{\\mu}= \\mathbf{P}_\\mathbf{X}\\mathbf{Y}\\) for any choice of \\(\\sigma^2\\) \\[\\therefore \\quad \\hat{\\boldsymbol{\\mu}}= \\mathbf{P}_\\mathbf{X}\\mathbf{Y}\\] is the MLE of \\(\\boldsymbol{\\mu}\\) (fitted values \\(\\hat{\\mathbf{Y}}= \\mathbf{P}_\\mathbf{X}\\mathbf{Y}\\))"
  },
  {
    "objectID": "resources/slides/02-mles.html#mle-of-boldsymbolbeta",
    "href": "resources/slides/02-mles.html#mle-of-boldsymbolbeta",
    "title": "Maximum Likelihood Estimation",
    "section": "MLE of \\(\\boldsymbol{\\beta}\\)",
    "text": "MLE of \\(\\boldsymbol{\\beta}\\)\n\\[{\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2)   =  -\\frac{n}{2} \\log(\\sigma^2)  - \\frac 1 2 \\left( \\frac{\\|(\\mathbf{I}- \\mathbf{P}_\\mathbf{X})\n  \\mathbf{Y}\\|^2}{\\sigma^2} + \\frac{\\| \\mathbf{P}_\\mathbf{X}\\mathbf{Y}- \\boldsymbol{\\mu}\\|^2 } {\\sigma^2} \\right)\\]\n\nRewrite as likeloood function for \\(\\boldsymbol{\\beta}, \\sigma^2\\): \\[{\\cal{L}}(\\boldsymbol{\\beta}, \\sigma^2 )  =  -\\frac{n}{2} \\log(\\sigma^2)  - \\frac 1 2 \\left( \\frac{\\|(\\mathbf{I}- \\mathbf{P}_\\mathbf{X})\n  \\mathbf{Y}\\|^2}{\\sigma^2} + \\frac{\\| \\mathbf{P}_\\mathbf{X}\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}\\|^2 } {\\sigma^2}\n\\right)\\]\n\n\n\nSimilar argument to show that RHS is maximized by minimizing \\[\\| \\mathbf{P}_\\mathbf{X}\n\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}\\|^2\\] \nTherefore \\(\\hat{\\boldsymbol{\\beta}}\\) is a MLE of \\(\\boldsymbol{\\beta}\\) if and only if satisfies \\[\\mathbf{P}_\\mathbf{X}\\mathbf{Y}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\]\nIf \\(\\mathbf{X}^T\\mathbf{X}\\) is full rank, the MLE of \\(\\boldsymbol{\\beta}\\) is \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}= \\hat{\\boldsymbol{\\beta}}\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#mle-of-sigma2",
    "href": "resources/slides/02-mles.html#mle-of-sigma2",
    "title": "Maximum Likelihood Estimation",
    "section": "MLE of \\(\\sigma^2\\)",
    "text": "MLE of \\(\\sigma^2\\)\n\nPlug-in MLE of \\(\\hat{\\boldsymbol{\\mu}}\\) for \\(\\boldsymbol{\\mu}\\) \\[ \\log {\\cal{L}}(\\hat{\\boldsymbol{\\mu}}, \\sigma^2)  =   -\\frac{n}{2} \\log \\sigma^2 - \\frac 1 2\n\\frac{\\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\|^2  }{\\sigma^2}\\]\nDifferentiate with respect to \\(\\sigma^2\\) \\[\\frac{\\partial \\, \\log {\\cal{L}}(\\hat{\\boldsymbol{\\mu}}, \\sigma^2)}{\\partial \\, \\sigma^2} =  -\\frac{n}{2} \\frac{1}{\\sigma^2}  +  \\frac 1 2\n\\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\|^2 \\left(\\frac{1}{\\sigma^2}\\right)^2 \\]\nSet derivative to zero and solve for MLE \\[\\begin{eqnarray*}\n0 & = &  -\\frac{n}{2} \\frac{1}{{\\hat{\\sigma}}^2}  +  \\frac 1 2\n\\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\|^2 \\left(\\frac{1}{{\\hat{\\sigma}}^2}\\right)^2  \\\\\n\\frac{n}{2} {\\hat{\\sigma}}^2& = & \\frac 1 2\n\\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\|^2 \\\\\n{\\hat{\\sigma}}^2& = & \\frac{\\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\|^2}{n}\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "resources/slides/02-mles.html#mle-estimate-of-sigma2",
    "href": "resources/slides/02-mles.html#mle-estimate-of-sigma2",
    "title": "Maximum Likelihood Estimation",
    "section": "MLE Estimate of \\(\\sigma^2\\)",
    "text": "MLE Estimate of \\(\\sigma^2\\)\nMaximum Likelihood Estimate of \\(\\sigma^2\\) \\[\\begin{eqnarray*}\n    {\\hat{\\sigma}}^2& = & \\frac{\\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\|^2}{n} \\\\\n      & = & \\frac{\\mathbf{Y}^T(\\mathbf{I}- \\mathbf{P}_\\mathbf{X})^T(\\mathbf{I}-\\mathbf{P}_\\mathbf{X}) \\mathbf{Y}}{n} \\\\\n& = & \\frac{ \\mathbf{Y}^T(\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}}{n} \\\\\n& = & \\frac{\\mathbf{e}^T\\mathbf{e}} {n}\n  \\end{eqnarray*}\\] where \\(\\mathbf{e}= (\\mathbf{I}- \\mathbf{P}_\\mathbf{X})\\mathbf{Y}\\) are the residuals from the regression of \\(\\mathbf{Y}\\) on \\(\\mathbf{X}\\)\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#outline",
    "href": "resources/slides/03-non-full-rank.html#outline",
    "title": "Rank Deficient Models",
    "section": "Outline",
    "text": "Outline\n\nRank Deficient Models\nGeneralized Inverses, Projections and MLEs/OLS\nClass of Unbiased Estimators\n\n\nReadings: - Christensen Chapter 2 and Appendix B - Seber & Lee Chapter 3"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#geometric-view",
    "href": "resources/slides/03-non-full-rank.html#geometric-view",
    "title": "Rank Deficient Models",
    "section": "Geometric View",
    "text": "Geometric View"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#non-full-rank-case",
    "href": "resources/slides/03-non-full-rank.html#non-full-rank-case",
    "title": "Rank Deficient Models",
    "section": "Non-Full Rank Case",
    "text": "Non-Full Rank Case\n\nModel: \\(\\mathbf{Y}= \\boldsymbol{\\mu}+ \\boldsymbol{\\epsilon}\\)\nAssumption: \\(\\boldsymbol{\\mu}\\in C(\\mathbf{X})\\) for \\(\\mathbf{X}\\in \\mathbb{R}^{n \\times p}\\)\nWhat if the rank of \\(\\mathbf{X}\\), \\(r(\\mathbf{X}) \\equiv r \\ne p\\)?\nStill have result that the OLS/MLE solution satisfies \\[\\mathbf{P}_\\mathbf{X}\\mathbf{Y}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\]\nHow can we characterize \\(\\mathbf{P}\\) and \\(\\hat{\\boldsymbol{\\beta}}\\) in this case? 2 cases\n\n\n\n\\(p \\le n\\), \\(r(\\mathbf{X}) \\ne p\\) \\(\\Rightarrow r(\\mathbf{X}) &lt; p\\)\n\\(p \\gt n\\), \\(r(\\mathbf{X}) \\ne p\\)\n\n\n\nFocus on the first case for OLS/MLE for now…"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#model-space",
    "href": "resources/slides/03-non-full-rank.html#model-space",
    "title": "Rank Deficient Models",
    "section": "Model Space",
    "text": "Model Space\n\n\\(\\boldsymbol{{\\cal M}}= C(\\mathbf{X})\\) is an \\(r\\)-dimensional subspace of \\(\\mathbb{R}^n\\)\n\\(\\boldsymbol{{\\cal M}}\\) has an \\((n - r)\\)-dimensional orthogonal complement \\(\\boldsymbol{{\\cal N}}\\)\neach \\(\\mathbf{y}\\in \\mathbb{R}^n\\) has a unique representation as \\[ \\mathbf{y}= \\hat{\\mathbf{y}}+ \\mathbf{e}\\] for \\(\\hat{\\mathbf{y}}\\in \\boldsymbol{{\\cal M}}\\) and \\(\\mathbf{e}\\in \\boldsymbol{{\\cal N}}\\)\n\\(\\hat{\\mathbf{y}}\\) is the orthogonal projection of \\(\\mathbf{y}\\) onto \\(\\boldsymbol{{\\cal M}}\\) and is the OLS/MLE estimate of \\(\\boldsymbol{\\mu}\\) that satisfies \\[\\mathbf{P}_\\mathbf{X}\\mathbf{y}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}= \\hat{\\mathbf{y}}\\]\n\\(\\mathbf{X}^T\\mathbf{X}\\) is not invertible so need another way to represent \\(\\mathbf{P}_\\mathbf{X}\\) and \\(\\hat{\\boldsymbol{\\beta}}\\)"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#spectral-decomposition-sd",
    "href": "resources/slides/03-non-full-rank.html#spectral-decomposition-sd",
    "title": "Rank Deficient Models",
    "section": "Spectral Decomposition (SD)",
    "text": "Spectral Decomposition (SD)\nEvery symmetric \\(n \\times n\\) matrix, \\({\\mathbf{S}}\\), has an eigen decomposition \\({\\mathbf{S}}= \\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{U}^T\\)\n\n\\(\\boldsymbol{\\Lambda}\\) is a diagonal matrix with eigenvalues \\((\\lambda_1, \\ldots, \\lambda_n)\\) of \\({\\mathbf{S}}\\)\n\\(\\mathbf{U}\\) is a \\(n \\times n\\) orthogonal matrix \\(\\mathbf{U}^T\\mathbf{U}= \\mathbf{U}\\mathbf{U}^T = \\mathbf{I}_n\\) ( \\(\\mathbf{U}^{-1} = \\mathbf{U}^T\\))\nthe columns of \\(\\mathbf{U}\\) from an Orthonormal Basis (ONB) for \\(\\mathbb{R}^n\\)\nthe columns of \\(\\mathbf{U}\\) associated with non-zero eigenvalues form an ONB for \\(C({\\mathbf{S}})\\)\nthe number of non-zero eigenvalues is the rank of \\({\\mathbf{S}}\\)\nthe columns of \\(\\mathbf{U}\\) associated with zero eigenvalues form an ONB for \\(C({\\mathbf{S}})^\\perp\\)\n\\({\\mathbf{S}}^d = \\mathbf{U}\\boldsymbol{\\Lambda}^d \\mathbf{U}^T\\) (matrix powers)"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#positive-definite-and-non-negative-definite-matrices",
    "href": "resources/slides/03-non-full-rank.html#positive-definite-and-non-negative-definite-matrices",
    "title": "Rank Deficient Models",
    "section": "Positive Definite and Non-Negative Definite Matrices",
    "text": "Positive Definite and Non-Negative Definite Matrices\n\nDefinition: B.21 Positive Definite and Non-Negative DefiniteA symmetric matrix \\({\\mathbf{S}}\\) is positive definite (\\({\\mathbf{S}}\\gt 0\\)) if and only if \\(\\mathbf{x}^T{\\mathbf{S}}\\mathbf{x}&gt; 0\\) for \\(\\mathbf{x}\\in \\mathbb{R}^n\\), \\(\\mathbf{x}\\ne \\mathbf{0}_n\\), and positive semi-definite or non-negative definite (\\({\\mathbf{S}}\\ge 0\\)) if and only if \\(\\mathbf{x}^T{\\mathbf{S}}\\mathbf{x}\\ge 0\\) for \\(\\mathbf{x}\\in \\mathbb{R}^n\\), \\(\\mathbf{x}\\ne \\mathbf{0}_n\\)\n\n\n\n\n\n\n\n\n\nExercise\n\n\nShow that a symmetric matrix \\({\\mathbf{S}}\\) is positive definite if and only if its eigenvalues are all strictly greater than zero, and positive semi-definite if all the eigenvalues are non-negative."
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#projections",
    "href": "resources/slides/03-non-full-rank.html#projections",
    "title": "Rank Deficient Models",
    "section": "Projections",
    "text": "Projections\nLet \\(\\mathbf{P}\\) be an orthogonal projection matrix onto \\(\\boldsymbol{{\\cal M}}\\), then\n\nthe eigenvalues of \\(\\mathbf{P}\\), \\(\\lambda_i\\), are either zero or one\nthe trace of \\(\\mathbf{P}\\) is the rank of \\(\\mathbf{P}\\)\nthe dimension of the subspace that \\(\\mathbf{P}\\) projects onto is the rank of \\(\\mathbf{P}\\)\nthe columns of \\(\\mathbf{U}_r = [u_1, u_2, \\ldots u_r]\\) form an ONB for the \\(C(\\mathbf{P})\\)\nthe projection \\(\\mathbf{P}\\) has the representation \\(\\mathbf{P}= \\mathbf{U}_r \\mathbf{U}_r^T = \\sum_{i = 1}^r u_i u_i^T\\) (the sum of \\(r\\) rank \\(1\\) projections)\nthe projection \\(\\mathbf{I}_n - \\mathbf{P}= \\mathbf{I}- \\mathbf{U}_r \\mathbf{U}_r^T = \\mathbf{U}_\\perp \\mathbf{U}_\\perp^T\\) where \\(\\mathbf{U}_\\perp = [u_{r+1}, \\ldots u_n]\\) is an orthogonal projection onto \\(\\boldsymbol{{\\cal N}}\\)\n\n\nMLE/OLS:\n\n\\(\\mathbf{P}_X \\mathbf{y}= \\mathbf{U}_r \\mathbf{U}_r^T \\mathbf{y}= \\mathbf{U}_r \\tilde{\\boldsymbol{\\beta}}\\)\nClaim \\(\\tilde{\\boldsymbol{\\beta}}\\) is a MLE/OLS estimate of \\(\\boldsymbol{\\beta}\\) where \\(\\tilde{\\mathbf{X}}= \\mathbf{U}_r\\)."
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#singular-value-decomposition-connections-to-spectral-decompositions",
    "href": "resources/slides/03-non-full-rank.html#singular-value-decomposition-connections-to-spectral-decompositions",
    "title": "Rank Deficient Models",
    "section": "Singular Value Decomposition & Connections to Spectral Decompositions",
    "text": "Singular Value Decomposition & Connections to Spectral Decompositions\nA matrix \\(\\mathbf{X}\\in \\mathbb{R}^{n \\times p}\\), \\(p \\le n\\) has a singular value decomposition \\[\\mathbf{X}= \\mathbf{U}_p \\mathbf{D}\\mathbf{V}^T\\]\n\n\\(\\mathbf{U}_p\\) is a \\(n \\times p\\) matrix with the first \\(p\\) eigenvectors in \\(\\mathbf{U}\\) associated with the \\(p\\) largest eigenvectors of \\(\\mathbf{X}\\mathbf{X}^T = \\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{U}^T\\) with \\(\\mathbf{U}_p^T\\mathbf{U}_p = I_p\\) but \\(\\mathbf{U}_p \\mathbf{U}_p^T \\ne \\mathbf{I}_n\\) (or \\(\\mathbf{P}_p\\))\n\\(\\mathbf{V}\\) is a \\(p \\times p\\) orthogonal matrix associated with the \\(p\\) eigenvectors of \\(\\mathbf{X}^T\\mathbf{X}= \\mathbf{V}\\boldsymbol{\\Lambda}_p \\mathbf{V}^T\\) where \\(\\boldsymbol{\\Lambda}_p\\) is the diagonal matrix of eigenvalues associated with the \\(p\\) largest eigenvalues of \\(\\boldsymbol{\\Lambda}\\)\n\\(\\mathbf{D}\\) = \\(\\boldsymbol{\\Lambda}_p^{1/2}\\) are the singular values\nif \\(\\mathbf{X}\\) has rank \\(r &lt; p\\), then \\(C(\\mathbf{X}) = C(\\mathbf{U}_p) = C(\\mathbf{U}_r)\\), where \\(\\mathbf{U}_r\\) are the eigenvectors of \\(\\mathbf{U}\\) or \\(\\mathbf{U}_p\\) associated with the non-zero eigenvalues.\n\\(\\mathbf{U}_r\\) is an ONB for \\(C(X)\\)"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#mleols-for-non-full-rank-case",
    "href": "resources/slides/03-non-full-rank.html#mleols-for-non-full-rank-case",
    "title": "Rank Deficient Models",
    "section": "MLE/OLS for non-full rank case",
    "text": "MLE/OLS for non-full rank case\n\nif \\(\\mathbf{X}^T\\mathbf{X}\\) is invertible, \\(\\mathbf{P}_X = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\) and \\(\\hat{\\boldsymbol{\\beta}}\\) is the unique estimator that satisfies \\(\\mathbf{P}_\\mathbf{X}\\mathbf{y}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\) or \\(\\hat{\\boldsymbol{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{y}\\)\nif \\(\\mathbf{X}^T\\mathbf{X}\\) is not invertible, replace \\(\\mathbf{X}\\) by \\(\\tilde{\\mathbf{X}}\\) that is rank \\(r\\)\nor represent \\(\\mathbf{P}_\\mathbf{X}= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-} \\mathbf{X}^T\\) where \\((\\mathbf{X}^T\\mathbf{X})^{-}\\) is a generalized inverse of \\(\\mathbf{X}^T\\mathbf{X}\\) and \\(\\hat{\\boldsymbol{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-}\\mathbf{X}^T \\mathbf{y}\\)"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#generalized-inverses",
    "href": "resources/slides/03-non-full-rank.html#generalized-inverses",
    "title": "Rank Deficient Models",
    "section": "Generalized Inverses",
    "text": "Generalized Inverses\n\nDefinition: Generalized-Inverse (B.36)A generalized inverse of any matrix \\(\\mathbf{A}\\): \\(\\mathbf{A}^{-}\\) satisfies \\(\\mathbf{A}\\mathbf{A}^- \\mathbf{A}= \\mathbf{A}\\)\n\n\n\nA generalized inverse of \\(\\mathbf{A}\\) symmetric always exists!\n\n\n\nTheorem: Christensen B.39If \\(\\mathbf{G}_1\\) and \\(\\mathbf{G}_2\\) are generalized inverses of \\(\\mathbf{A}\\) then \\(\\mathbf{G}_1 \\mathbf{A}\\mathbf{G}_2\\) is also a generalized inverse of \\(\\mathbf{A}\\)\n\n\n\nif \\(\\mathbf{A}\\) is symmetric, then \\(\\mathbf{A}^-\\) need not be!"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#orthogonal-projections-in-general",
    "href": "resources/slides/03-non-full-rank.html#orthogonal-projections-in-general",
    "title": "Rank Deficient Models",
    "section": "Orthogonal Projections in General",
    "text": "Orthogonal Projections in General\n\n\n\n\n\n\n\nLemma B.43\n\n\nIf \\(\\mathbf{G}\\) and \\(\\mathbf{H}\\) are generalized inverses of \\(\\mathbf{X}^T\\mathbf{X}\\) then \\[\\begin{align*}\n\\mathbf{X}\\mathbf{G}\\mathbf{X}^T \\mathbf{X}& = \\mathbf{X}\\mathbf{H}\\mathbf{X}^T \\mathbf{X}= \\mathbf{X}\\\\\n\\mathbf{X}\\mathbf{G}\\mathbf{X}^T & = \\mathbf{X}\\mathbf{H}\\mathbf{X}^T\n\\end{align*}\\]\n\n\n\n\n\n\nTheorem: B.44\\(\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^-\\mathbf{X}^T\\) is an orthogonal projection onto \\(C(\\mathbf{X})\\).\n\n\n\n\n\nWe need to show that (i) \\(\\mathbf{P}\\mathbf{m}= \\mathbf{m}\\) for \\(\\mathbf{m}\\in C(\\mathbf{X})\\) and (ii) \\(\\mathbf{P}\\mathbf{n}= 0\\) for \\(\\mathbf{n}\\in C(\\mathbf{X})^\\perp\\).\n\nFor \\(\\mathbf{m}\\in C(\\mathbf{X})\\), write \\(\\mathbf{m}= \\mathbf{X}\\mathbf{b}\\). Then \\(\\mathbf{P}\\mathbf{m}= \\mathbf{P}\\mathbf{X}\\mathbf{b}= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^-\\mathbf{X}^T \\mathbf{X}\\mathbf{b}\\) and by Lemma B43, we have that \\(\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^-\\mathbf{X}^T \\mathbf{X}\\mathbf{b}= \\mathbf{X}\\mathbf{b}= \\mathbf{m}\\)\nFor \\(\\mathbf{n}\\perp C(\\mathbf{X})\\), \\(\\mathbf{P}\\mathbf{n}= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^-\\mathbf{X}^T \\mathbf{n}= \\mathbf{0}_n\\) as \\(C(\\mathbf{X})^\\perp = N(\\mathbf{X}^T)\\)."
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#mles-ols",
    "href": "resources/slides/03-non-full-rank.html#mles-ols",
    "title": "Rank Deficient Models",
    "section": "MLEs & OLS",
    "text": "MLEs & OLS\nMLE/OLS satisfies\n\n\\(\\mathbf{P}\\mathbf{y}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\)\n\\(\\mathbf{P}\\mathbf{y}= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^-\\mathbf{X}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\) (does not depend on choice of generalized inverse)\n\\(\\hat{\\boldsymbol{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^-\\mathbf{X}^T \\mathbf{y}\\)\n\\(\\hat{\\boldsymbol{\\beta}}\\) is not unique - does depend on choice of generalized inverse unless \\(\\mathbf{X}\\) is full rank"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#moore-penrose-generalized-inverse",
    "href": "resources/slides/03-non-full-rank.html#moore-penrose-generalized-inverse",
    "title": "Rank Deficient Models",
    "section": "Moore-Penrose Generalized Inverse:",
    "text": "Moore-Penrose Generalized Inverse:\n\nDecompose symmetric \\(\\mathbf{A}= \\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{U}^T\\) (i.e \\(\\mathbf{X}^T\\mathbf{X}\\))\n\\(\\mathbf{A}^-_{MP} = \\mathbf{U}\\boldsymbol{\\Lambda}^- \\mathbf{U}^T\\)\n\n\\(\\boldsymbol{\\Lambda}^-\\) is diagonal with \\[ \\lambda_i^- = \\left\\{\n\\begin{array}{l}\n1/\\lambda_i \\text{ if } \\lambda_i \\neq 0 \\\\\n0 \\quad \\, \\text{  if } \\lambda_i = 0\n\\end{array}\n\\right.\\]\n\nSymmetric \\(\\mathbf{A}^-_{MP} = (\\mathbf{A}^-_{MP})^T\\)\n\nReflexive \\(\\mathbf{A}^-_{MP}\\mathbf{A}\\mathbf{A}^-_{MP} = \\mathbf{A}^-_{MP}\\)\n\n\n\nShow that \\(\\mathbf{A}_{MP}^-\\) is a generalized inverse of \\(\\mathbf{A}\\)\nCan you construct another generalized inverse of \\(\\mathbf{X}^T\\mathbf{X}\\) ?\nCan you find the Moore-Penrose generalized inverse of \\(\\mathbf{X}\\in \\mathbb{R}^{n \\times p}\\)?"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#properties-of-ols-full-rank-case",
    "href": "resources/slides/03-non-full-rank.html#properties-of-ols-full-rank-case",
    "title": "Rank Deficient Models",
    "section": "Properties of OLS (full rank case)",
    "text": "Properties of OLS (full rank case)\nHow good is \\(\\hat{\\boldsymbol{\\beta}}\\) as an estimator of \\(\\beta\\)\n\n\\(\\hat{\\boldsymbol{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{Y}=  (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{X}\\boldsymbol{\\beta}+ (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}= \\boldsymbol{\\beta}+  (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}\\)\ndon’t know \\(\\boldsymbol{\\epsilon}\\), but can talk about behavior on average over\n\ndifferent runs of an experiment\ndifferent samples from a population\ndifferent values of \\(\\boldsymbol{\\epsilon}\\)\n\nwith minimal assumption \\(\\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\), \\[\\begin{align*}\n\\textsf{E}[\\hat{\\boldsymbol{\\beta}}] & = \\textsf{E}[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}+ (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}]\\\\\n& = \\boldsymbol{\\beta}+ (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\textsf{E}[\\boldsymbol{\\epsilon}] \\\\\n& = \\boldsymbol{\\beta}\n\\end{align*}\\]\nBias of \\(\\hat{\\boldsymbol{\\beta}}\\), \\(\\text{Bias}[\\hat{\\boldsymbol{\\beta}}] =  \\textsf{E}[\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}] = \\mathbf{0}_p\\)\n\\(\\hat{\\boldsymbol{\\beta}}\\) is an unbiased estimator of \\(\\boldsymbol{\\beta}\\) if \\(\\boldsymbol{\\mu}\\in C(\\mathbf{X})\\)"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#class-of-unbiased-estimators",
    "href": "resources/slides/03-non-full-rank.html#class-of-unbiased-estimators",
    "title": "Rank Deficient Models",
    "section": "Class of Unbiased Estimators",
    "text": "Class of Unbiased Estimators\nClass of linear statistical models: \\[\\begin{align*}\n\\mathbf{Y}& = \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\\\\n\\boldsymbol{\\epsilon}& \\sim P \\\\\nP & \\in \\cal{P}\n\\end{align*}\\]\n\nAn estimator \\(\\tilde{\\boldsymbol{\\beta}}\\) is unbiased for \\(\\boldsymbol{\\beta}\\) if \\(\\textsf{E}_P[\\tilde{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\\quad \\forall \\boldsymbol{\\beta}\\in \\mathbb{R}^p\\) and \\(P \\in \\cal{P}\\)\n\n\nExamples:\n\n\n\\(\\cal{P}_1= \\{P = \\textsf{N}(\\mathbf{0}_n ,\\mathbf{I}_n)\\}\\)\n\n\n\\(\\cal{P}_2 = \\{P = \\textsf{N}(\\mathbf{0}_n ,\\sigma^2 \\mathbf{I}_n), \\sigma^2 &gt;0\\}\\)\n\n\n\\(\\cal{P}_3 = \\{P = \\textsf{N}(\\mathbf{0}_n ,\\boldsymbol{\\Sigma}), \\boldsymbol{\\Sigma}\\in \\cal{{\\cal{S}}}^+ \\}\\) (\\(\\cal{{\\cal{S}}}^+\\) is the set of all \\(n \\times n\\) symmetric positive definite matrices.)\n\n\n\\(\\cal{P}_4\\) is the set of distributions with \\(\\textsf{E}_P[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\) and \\(\\textsf{E}_P[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] \\gt 0\\)\n\n\n\\(\\cal{P}_5\\) is the set of distributions with \\(\\textsf{E}_P[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\) and \\(\\textsf{E}_P[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] \\ge 0\\)"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#linear-unbiased-estimation",
    "href": "resources/slides/03-non-full-rank.html#linear-unbiased-estimation",
    "title": "Rank Deficient Models",
    "section": "Linear Unbiased Estimation",
    "text": "Linear Unbiased Estimation\n\n\n\n\n\n\n\nExercise\n\n\n\nExplain why an estimator that is unbiased for the model with parameter space \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\) and \\(P \\in  \\cal{P}_{k+1}\\) is unbiased for the model with parameter space \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\) and \\(P \\in  \\cal{P}_{k}\\) .\nFind an estimator that is unbiased for \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\) and \\(P \\in  \\cal{P}_{1}\\) that but is biased for \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\) and \\(P \\in  \\cal{P}_{2}\\).\n\n\n\n\n\n\nRestrict attention to linear unbiased estimators\n\n\n\nDefinition: Linear Unbiased Estimators (LUEs)\nAn estimator \\(\\tilde{\\boldsymbol{\\beta}}\\) is a Linear Unbiased Estimator (LUE) of \\(\\boldsymbol{\\beta}\\) if\n\nlinearity: \\(\\tilde{\\boldsymbol{\\beta}}= \\mathbf{A}\\mathbf{Y}\\) for \\(\\mathbf{A}\\in \\mathbb{R}^{p \\times n}\\)\nunbiasedness: \\(\\textsf{E}[\\tilde{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\\) for all \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\)\n\n\n\n\n\n\nAre there other LUEs besides the OLS/MLE estimator?\nWhich is “best”? (and in what sense?)\n\n\n\n\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/09-bayes-freq-risk.html#outline",
    "href": "resources/slides/09-bayes-freq-risk.html#outline",
    "title": "Bayesian Estimation and Frequentist Risk",
    "section": "Outline",
    "text": "Outline\n\nFrequentist Risk of Bayes estimators\nBayes and Penalized Loss Functions\nGeneralized Ridge Regression\nHierarchical Bayes and Other Penalties\n\n\nReadings:\n\nChristensen Chapter 2.9 and Chapter 15\nSeber & Lee Chapter 10.7.3 and Chapter 12"
  },
  {
    "objectID": "resources/slides/09-bayes-freq-risk.html#frequentist-risk-of-bayes-estimators",
    "href": "resources/slides/09-bayes-freq-risk.html#frequentist-risk-of-bayes-estimators",
    "title": "Bayesian Estimation and Frequentist Risk",
    "section": "Frequentist Risk of Bayes Estimators",
    "text": "Frequentist Risk of Bayes Estimators\nQuadratic loss for estimating \\(\\boldsymbol{\\beta}\\) using estimator \\(\\mathbf{a}\\) \\[ L(\\boldsymbol{\\beta}, \\mathbf{a}) =  ( \\boldsymbol{\\beta}- \\mathbf{a})^T(\\boldsymbol{\\beta}-\\mathbf{a})\\]\n\nConsider our expected loss (before we see the data) of taking an ``action’’ \\(\\mathbf{a}\\) (i.e. reporting \\(\\mathbf{a}\\) as the estimate of \\(\\boldsymbol{\\beta}\\)) \\[ \\textsf{E}_{\\mathbf{Y}\\mid \\boldsymbol{\\beta}}[L(\\boldsymbol{\\beta}, \\mathbf{a})] =  \\textsf{E}_{\\mathbf{Y}\\mid \\boldsymbol{\\beta}}[( \\boldsymbol{\\beta}- \\mathbf{a})^T(\\boldsymbol{\\beta}-\\mathbf{a})]\\] where the expectation is over the data \\(\\mathbf{Y}\\) given the true value of \\(\\boldsymbol{\\beta}\\)."
  },
  {
    "objectID": "resources/slides/09-bayes-freq-risk.html#expectation-of-quadratic-forms",
    "href": "resources/slides/09-bayes-freq-risk.html#expectation-of-quadratic-forms",
    "title": "Bayesian Estimation and Frequentist Risk",
    "section": "Expectation of Quadratic Forms",
    "text": "Expectation of Quadratic Forms\n\nTheorem: Christensen Thm 1.3.2If \\(\\mathbf{W}\\) is a random variable with mean \\(\\boldsymbol{\\mu}\\) and covariance matrix \\(\\boldsymbol{\\Sigma}\\) then \\[\\textsf{E}[\\mathbf{W}^T\\mathbf{A}\\mathbf{W}] = \\textsf{tr}(\\mathbf{A}\\boldsymbol{\\Sigma}) + \\boldsymbol{\\mu}^T\\mathbf{A}\\boldsymbol{\\mu}\\]\n\n\n\n\nProof\\[\\begin{eqnarray*}\n(\\mathbf{W}- \\boldsymbol{\\mu})^T\\mathbf{A}(\\mathbf{W}- \\boldsymbol{\\mu}) & = & \\mathbf{W}^T\\mathbf{A}\\mathbf{W}- 2\\boldsymbol{\\mu}^T\\mathbf{A}\\mathbf{W}+ \\boldsymbol{\\mu}^T\\mathbf{A}\\boldsymbol{\\mu}\\\\\n\\textsf{E}[(\\mathbf{W}- \\boldsymbol{\\mu})^T\\mathbf{A}(\\mathbf{W}- \\boldsymbol{\\mu})] & = & \\textsf{E}[\\mathbf{W}^T\\mathbf{A}\\mathbf{W}]  - 2 \\boldsymbol{\\mu}^T\\mathbf{A}\\textsf{E}[\\mathbf{W}] + \\boldsymbol{\\mu}^T\\mathbf{A}\\boldsymbol{\\mu}\n\\end{eqnarray*}\\]\nRearranging we have \\[\\textsf{E}[\\mathbf{W}^T\\mathbf{A}\\mathbf{W}] =  \\textsf{E}[(\\mathbf{W}- \\boldsymbol{\\mu})^T\\mathbf{A}(\\mathbf{W}- \\boldsymbol{\\mu})] + \\boldsymbol{\\mu}^T\\mathbf{A}\\boldsymbol{\\mu}\\]"
  },
  {
    "objectID": "resources/slides/09-bayes-freq-risk.html#steps-to-evaluate-frequentist-risk",
    "href": "resources/slides/09-bayes-freq-risk.html#steps-to-evaluate-frequentist-risk",
    "title": "Bayesian Estimation and Frequentist Risk",
    "section": "Steps to Evaluate Frequentist Risk",
    "text": "Steps to Evaluate Frequentist Risk\n\nMSE: \\(\\textsf{E}_\\mathbf{Y}[( \\boldsymbol{\\beta}- \\mathbf{a})^T(\\boldsymbol{\\beta}-\\mathbf{a}) = \\textsf{tr}(\\boldsymbol{\\Sigma}_\\mathbf{a}) + (\\boldsymbol{\\beta}- \\textsf{E}_{\\mathbf{Y}\\mid \\boldsymbol{\\beta}}[\\mathbf{a}])^T(\\boldsymbol{\\beta}- \\textsf{E}_{\\mathbf{Y}\\mid \\boldsymbol{\\beta}}[\\mathbf{a}])\\)\nBias of \\(\\mathbf{a}\\): \\(\\textsf{E}_{\\mathbf{Y}\\mid \\boldsymbol{\\beta}}[\\mathbf{a}- \\boldsymbol{\\beta}] = \\textsf{E}_{\\mathbf{Y}\\mid \\boldsymbol{\\beta}}[\\mathbf{a}] - \\boldsymbol{\\beta}\\)\nCovariance of \\(\\mathbf{a}\\): \\(\\textsf{Cov}_{\\mathbf{Y}\\mid \\boldsymbol{\\beta}}[\\mathbf{a}- \\textsf{E}[\\mathbf{a}]\\)\nMultivariate analog of MSE = Bias\\(^2\\) + Variance in the univariate case"
  },
  {
    "objectID": "resources/slides/09-bayes-freq-risk.html#mean-square-error-of-ols-estimator",
    "href": "resources/slides/09-bayes-freq-risk.html#mean-square-error-of-ols-estimator",
    "title": "Bayesian Estimation and Frequentist Risk",
    "section": "Mean Square Error of OLS Estimator",
    "text": "Mean Square Error of OLS Estimator\n\nMSE of OLS \\(\\textsf{E}_\\mathbf{Y}[( \\boldsymbol{\\beta}- \\hat{\\boldsymbol{\\beta}})^T(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})\\)\nOLS is unbiased os mean of \\(\\boldsymbol{\\beta}- \\hat{\\boldsymbol{\\beta}}\\) is \\(\\mathbf{0}_p\\)\ncovariance is \\(\\textsf{Cov}[\\boldsymbol{\\beta}- \\hat{\\boldsymbol{\\beta}}] = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1}\\) \\[\\begin{eqnarray*}\n\\textsf{MSE}(\\boldsymbol{\\beta}) \\equiv \\textsf{E}_\\mathbf{Y}[( \\boldsymbol{\\beta}- \\hat{\\boldsymbol{\\beta}})^T(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}}) & = &\\sigma^2\n\\textsf{tr}[(\\mathbf{X}^T\\mathbf{X})^{-1}]  \\\\\n& = & \\sigma^2 \\textsf{tr}\\mathbf{U}\\Lambda^{-1} \\mathbf{U}^T \\\\\n& = & \\sigma^2 \\sum_{j=1}^p \\lambda_j^{-1}\n\\end{eqnarray*}\\] where \\(\\lambda_j\\) are eigenvalues of \\(\\mathbf{X}^T\\mathbf{X}\\).\nIf smallest \\(\\lambda_j \\to 0\\) then MSE \\(\\to \\infty\\)"
  },
  {
    "objectID": "resources/slides/09-bayes-freq-risk.html#mean-square-error-using-the-g-prior",
    "href": "resources/slides/09-bayes-freq-risk.html#mean-square-error-using-the-g-prior",
    "title": "Bayesian Estimation and Frequentist Risk",
    "section": "Mean Square Error using the \\(g\\)-prior",
    "text": "Mean Square Error using the \\(g\\)-prior\n\nposterior mean is \\(\\hat{\\boldsymbol{\\beta}}_g = \\frac{g}{1+g} \\hat{\\boldsymbol{\\beta}}\\) (minimizes Bayes risk under squared error loss)\nbias of \\(\\hat{\\boldsymbol{\\beta}}_g\\): \\[\\begin{align*}\n\\textsf{E}_{\\mathbf{Y}\\mid \\boldsymbol{\\beta}}[\\boldsymbol{\\beta}- \\hat{\\boldsymbol{\\beta}}_g] & = \\boldsymbol{\\beta}\\left(1 - \\frac{g}{1+g}\\right)  = \\frac{1}{1+g} \\boldsymbol{\\beta}\n\\end{align*}\\]\ncovariance of \\(\\hat{\\boldsymbol{\\beta}}_g\\): \\(\\textsf{Cov}(\\hat{\\boldsymbol{\\beta}}_g) = \\frac{g^2}{(1+g)^2} \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1}\\)\nMSE of \\(\\hat{\\boldsymbol{\\beta}}_g\\): \\[\\begin{align*}\n\\textsf{MSE}(\\boldsymbol{\\beta}) = \\frac{g^2}{(1+g)^2} \\sigma^2 \\textsf{tr}(\\mathbf{X}^T\\mathbf{X})^{-1} + \\frac{1}{(1+g)^2} \\|\\boldsymbol{\\beta}\\|^2 \\\\\n= \\frac{1}{(1+g)^2} \\left( g^2 \\sigma^2 \\sum_{j=1}^p\\lambda_j^{-1} + \\|\\boldsymbol{\\beta}\\|^2 \\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "resources/slides/09-bayes-freq-risk.html#can-bayes-estimators-have-smaller-mse-than-ols",
    "href": "resources/slides/09-bayes-freq-risk.html#can-bayes-estimators-have-smaller-mse-than-ols",
    "title": "Bayesian Estimation and Frequentist Risk",
    "section": "Can Bayes Estimators have smaller MSE than OLS?",
    "text": "Can Bayes Estimators have smaller MSE than OLS?\n\nMSE of OLS is \\(\\textsf{E}_\\mathbf{Y}[( \\boldsymbol{\\beta}- \\hat{\\boldsymbol{\\beta}})^T(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}}) = \\sigma^2\n  \\textsf{tr}[(\\mathbf{X}^T\\mathbf{X})^{-1}]\\) (OLS has minimum MSE under squared error loss out of all unbiased estimators)\nMSE of \\(g\\)-prior estimator is\n\\[\\textsf{MSE}_g(\\boldsymbol{\\beta}) = \\frac{1}{(1+g)^2} \\left( g^2 \\sigma^2 \\textsf{tr}[(\\mathbf{X}^T\\mathbf{X})^{-1}] + \\|\\boldsymbol{\\beta}\\|^2 \\right)\\]\nfor fixed \\(\\boldsymbol{\\beta}\\), what values of \\(g\\) is the MSE of \\(\\hat{\\boldsymbol{\\beta}}_g\\) lower than that of \\(\\hat{\\boldsymbol{\\beta}}\\)?\nfor fixed \\(g\\), what values of \\(\\boldsymbol{\\beta}\\) is the MSE of \\(\\hat{\\boldsymbol{\\beta}}_g\\) lower than that of \\(\\hat{\\boldsymbol{\\beta}}\\)?\nis there a value of \\(g\\) that minimizes the MSE of \\(\\hat{\\boldsymbol{\\beta}}_g\\)?\nwhat is the MSE of \\(\\hat{\\boldsymbol{\\beta}}_g\\) under the “optimal” \\(g\\)?\nis the MSE of \\(\\hat{\\boldsymbol{\\beta}}_g\\) using the “optimal” \\(g\\) always lower than that of \\(\\hat{\\boldsymbol{\\beta}}\\)?"
  },
  {
    "objectID": "resources/slides/09-bayes-freq-risk.html#mean-square-error-under-ridge-priors",
    "href": "resources/slides/09-bayes-freq-risk.html#mean-square-error-under-ridge-priors",
    "title": "Bayesian Estimation and Frequentist Risk",
    "section": "Mean Square Error under Ridge Priors",
    "text": "Mean Square Error under Ridge Priors\n\nMSE with OLS and \\(g\\)-prior estimators depend on the eigenvalues of \\(\\mathbf{X}^T\\mathbf{X}\\) and can be infinite if the smallest eigenvalue is zero.\nRidge regression estimator \\(\\hat{\\boldsymbol{\\beta}}_\\kappa = (\\mathbf{X}^T\\mathbf{X}+  \\kappa \\mathbf{I}_p)^{-1} \\mathbf{X}^T\\mathbf{Y}\\) has finite MSE for all \\(\\kappa &gt; 0\\). (\\(k = 0\\) is OLS)\nMSE of Ridge estimator \\(\\textsf{E}_{\\mathbf{Y}\\mid \\boldsymbol{\\beta}}[( \\boldsymbol{\\beta}- \\hat{\\boldsymbol{\\beta}}_\\kappa)^T(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}}_\\kappa) = \\textsf{E}[(\\boldsymbol{\\alpha}- \\mathbf{a})^T(\\boldsymbol{\\alpha}- \\mathbf{a})]\\)\nbias of \\(a_j = \\frac{\\lambda_j}{\\lambda_j + \\kappa} \\hat{\\alpha}_j\\) is \\(\\frac{\\kappa}{\\lambda_j + \\kappa} {\\alpha}_j\\)\nvariance \\(a_j = \\sigma^2 \\frac{\\lambda_j^2}{(\\lambda_j + \\kappa)^2}\\) \\[\\textsf{MSE}_R = \\sigma^2 \\sum_{j=1}^p \\frac{\\lambda_j^2}{(\\lambda_j + \\kappa)^2} + \\sum_{j=1}^p \\frac{\\kappa^2}{(\\lambda_j + \\kappa)^2} \\alpha_j^2\\]\ncan show that the deriviate of the \\(\\textsf{MSE}_R\\) with respect to \\(\\kappa\\) is negative at \\(k = 0\\) so that there exists a \\(\\kappa\\) so the MSE of the Ridge estimator is always less than that of OLS."
  },
  {
    "objectID": "resources/slides/09-bayes-freq-risk.html#penalized-regression",
    "href": "resources/slides/09-bayes-freq-risk.html#penalized-regression",
    "title": "Bayesian Estimation and Frequentist Risk",
    "section": "Penalized Regression",
    "text": "Penalized Regression\n\nRidge regression is a special case of penalized regression where the penalty is \\(\\kappa \\|\\boldsymbol{\\beta}\\|^2\\) for some \\(\\kappa &gt; 0\\). (let \\(\\kappa^* = \\kappa/\\phi\\))\nposterior mode maximizes the posterior density or log posterior density \\[\\begin{align*}\n\\hat{\\boldsymbol{\\beta}}_R = \\arg \\max_{\\boldsymbol{\\beta}} \\cal{L}(\\boldsymbol{\\beta}) & = \\log p(\\boldsymbol{\\beta}\\mid \\mathbf{Y}) \\propto \\log p(\\mathbf{Y}\\mid \\boldsymbol{\\beta}) + \\log p(\\boldsymbol{\\beta}) \\\\\n& = -\\frac{\\phi}{2} \\|\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}\\|^2 - \\frac{\\kappa}{2} \\|\\boldsymbol{\\beta}\\|^2 \\\\\n& = -\\frac{\\phi}{2} \\left( \\|\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}\\|^2 + \\kappa^* \\|\\boldsymbol{\\beta}\\|^2 \\right)\n\\end{align*}\\]\nmaximizing the posterior mode is equivalent to minimizing the penalized loss function \\[\\begin{align*}\n\\hat{\\boldsymbol{\\beta}}_R & =  \\arg \\max_{\\boldsymbol{\\beta}} -\\left(\\|\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}\\|^2 + \\kappa^* \\|\\boldsymbol{\\beta}\\|^2 \\right) \\\\\n& = \\arg \\min_{\\boldsymbol{\\beta}} \\left(\\|\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}\\|^2 + \\kappa^* \\|\\boldsymbol{\\beta}\\|^2 \\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "resources/slides/09-bayes-freq-risk.html#scaling-and-centering",
    "href": "resources/slides/09-bayes-freq-risk.html#scaling-and-centering",
    "title": "Bayesian Estimation and Frequentist Risk",
    "section": "Scaling and Centering",
    "text": "Scaling and Centering\nNote: usually use Ridge regression after centering and scaling the columns of \\(\\mathbf{X}\\) so that the penalty is the same for all variables. \\(\\mathbf{Y}_c = (\\mathbf{I}- \\mathbf{P}_1) \\mathbf{Y}\\) and \\(X_c\\) the centered and standardized \\(\\mathbf{X}\\) matrix\n\nalternatively as a prior, we are assuming that the \\(\\boldsymbol{\\beta}_j\\) are iid \\(\\textsf{N}(0, 1/\\kappa^*)\\) so that the prior for \\(\\boldsymbol{\\beta}\\) is \\(\\textsf{N}(\\mathbf{0}_p, \\mathbf{I}_p/\\kappa^* )\\)\nif the units/scales of the variables are different, then the variance or penality should be different for each variable.\nstandardizing the \\(\\mathbf{X}\\) so that \\(\\mathbf{X}_c^T\\mathbf{X}_c\\) is a constant times the correlation matrix of \\(\\mathbf{X}\\) ensures that all \\(\\boldsymbol{\\beta}\\)’s have the same scale\ncentering the data forces the intercept to be 0 (so no shrinkage or penality)"
  },
  {
    "objectID": "resources/slides/09-bayes-freq-risk.html#alternative-motivation",
    "href": "resources/slides/09-bayes-freq-risk.html#alternative-motivation",
    "title": "Bayesian Estimation and Frequentist Risk",
    "section": "Alternative Motivation",
    "text": "Alternative Motivation\n\nIf \\(\\hat{\\boldsymbol{\\beta}}\\) is unconstrained expect high variance with nearly singular \\(\\mathbf{X}_c\\) \nControl how large coefficients may grow \\[\\arg \\min_{\\boldsymbol{\\beta}} (\\mathbf{Y}_c - \\mathbf{X}_c \\boldsymbol{\\beta})^T (\\mathbf{Y}_c - \\mathbf{X}_c\\boldsymbol{\\beta})\\] subject to \\[ \\sum \\beta_j^2 \\le t\\]\nEquivalent Quadratic Programming Problem \\[\\hat{\\boldsymbol{\\beta}}_{R} = \\arg \\min_{\\boldsymbol{\\beta}} \\| \\mathbf{Y}_c - \\mathbf{X}_c \\boldsymbol{\\beta}\\|^2 + \\kappa^* \\|\\boldsymbol{\\beta}\\|^2\\]\ndifferent approaches to selecting \\(\\kappa^*\\) from frequentist ane Bayesian perspectives"
  },
  {
    "objectID": "resources/slides/09-bayes-freq-risk.html#plot-of-constrained-problem",
    "href": "resources/slides/09-bayes-freq-risk.html#plot-of-constrained-problem",
    "title": "Bayesian Estimation and Frequentist Risk",
    "section": "Plot of Constrained Problem",
    "text": "Plot of Constrained Problem"
  },
  {
    "objectID": "resources/slides/09-bayes-freq-risk.html#generalized-ridge-regression",
    "href": "resources/slides/09-bayes-freq-risk.html#generalized-ridge-regression",
    "title": "Bayesian Estimation and Frequentist Risk",
    "section": "Generalized Ridge Regression",
    "text": "Generalized Ridge Regression\n\nrather than a common penalty for all variables, consider a different penalty for each variable\nas a prior, we are assuming that the \\(\\boldsymbol{\\beta}_j\\) are iid \\(\\textsf{N}(0, \\frac{\\kappa_j}{\\phi})\\) so that the prior for \\(\\boldsymbol{\\beta}\\) is \\(\\textsf{N}(\\mathbf{0}_p, \\phi^{-1} \\mathbf{K}^{-1})\\) where \\(\\mathbf{K}= \\textsf{diag}(\\kappa_1, \\ldots, \\kappa_p)\\)\nhard enough to choose a single penalty, how to choose \\(p\\) penalties?\nplace independent priors on each of the \\(\\kappa_j\\)’s\na hierarchical Bayes model\nif we can integrate out the \\(\\kappa_j\\)’s we have a new prior for \\(\\beta_j\\)\nthis leads to a new penalty!\nexamples include the Lasso (Double Exponential Prior) and Double Pareto Priors\n\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/07-sampling.html#outline",
    "href": "resources/slides/07-sampling.html#outline",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Outline",
    "text": "Outline\n\ndistributions of \\(\\hat{\\boldsymbol{\\beta}}\\), \\(\\hat{\\mathbf{Y}}\\), \\(\\hat{\\boldsymbol{\\epsilon}}\\) under normality\nUnbiased Estimation of \\(\\sigma^2\\)\nsampling distribution of \\(\\hat{\\sigma^2}\\)\nindependence\n\n\nReadings:\n\nChristensen Chapter 1, 2.91 and Appendix C\nSeber & Lee Chapter 3.3 - 3.5"
  },
  {
    "objectID": "resources/slides/07-sampling.html#multivariate-normal",
    "href": "resources/slides/07-sampling.html#multivariate-normal",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Multivariate Normal",
    "text": "Multivariate Normal\nUnder the linear model \\(\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\), \\(\\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\) and \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\sigma^2 \\mathbf{I}_n\\), we had\n\n\\(\\textsf{E}[\\hat{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\\)\n\\(\\textsf{E}[\\hat{\\mathbf{Y}}] = \\mathbf{P}_\\mathbf{X}\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}\\)\n\\(\\textsf{E}[\\hat{\\boldsymbol{\\epsilon}}] = (\\mathbf{I}_n - \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}= \\mathbf{0}_n\\)\ndistributions if \\(\\epsilon_i \\sim \\textsf{N}(0, \\sigma^2)\\)?\n\n\nFor a \\(d\\) dimensional multivariate normal random vector, we write \\(\\mathbf{Y}\\sim \\textsf{N}_d(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\)\n\n\\(\\textsf{E}[\\mathbf{Y}] = \\boldsymbol{\\mu}\\): \\(d\\) dimensional vector with means \\(E[Y_j]\\)\n\\(\\textsf{Cov}[\\mathbf{Y}] = \\boldsymbol{\\Sigma}\\): \\(d \\times d\\) matrix with diagonal elements that are the variances of \\(Y_j\\) and off diagonal elements that are the covariances \\(\\textsf{E}[(Y_j - \\mu_j)(Y_k - \\mu_k)]\\)\nIf \\(\\boldsymbol{\\Sigma}\\) is positive definite (\\(\\mathbf{x}'\\boldsymbol{\\Sigma}\\mathbf{x}&gt; 0\\) for any \\(\\mathbf{x}\\ne\n  0\\) in \\(\\mathbb{R}^d\\)) then \\(\\mathbf{Y}\\) has a density\\(^\\dagger\\) \\[p(\\mathbf{Y}) = (2 \\pi)^{-d/2} |\\boldsymbol{\\Sigma}|^{-1/2} \\exp(-\\frac{1}{2}(\\mathbf{Y}- \\boldsymbol{\\mu})^T\n\\boldsymbol{\\Sigma}^{-1} (\\mathbf{Y}- \\boldsymbol{\\mu}))\\]\n\n\n\\(\\dagger\\) density with respect to Lebesgue measure on \\(\\mathbb{R}^d\\)"
  },
  {
    "objectID": "resources/slides/07-sampling.html#transformations-of-normal-random-variables",
    "href": "resources/slides/07-sampling.html#transformations-of-normal-random-variables",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Transformations of Normal Random Variables",
    "text": "Transformations of Normal Random Variables\nIf \\(\\mathbf{Y}\\sim \\textsf{N}_n(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) then for \\(\\mathbf{A}\\) \\(m \\times n\\) \\[\\mathbf{A}\\mathbf{Y}\\sim \\textsf{N}_m(\\mathbf{A}\\boldsymbol{\\mu}, \\mathbf{A}\\boldsymbol{\\Sigma}\\mathbf{A}^T)\\]\n\n\n\\(\\hat{\\boldsymbol{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\sim \\textsf{N}(\\boldsymbol{\\beta}, \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1})\\)\n\\(\\hat{\\mathbf{Y}}= \\mathbf{P}_\\mathbf{X}\\mathbf{Y}\\sim \\textsf{N}(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2 \\mathbf{P}_\\mathbf{X})\\)\n\\(\\hat{\\boldsymbol{\\epsilon}}= (\\mathbf{I}_n - \\mathbf{P}_\\mathbf{X})\\mathbf{Y}\\sim \\textsf{N}(\\mathbf{0}, \\sigma^2 (\\mathbf{I}_n - \\mathbf{P}_\\mathbf{X}))\\)\n\n\n\n\\(\\mathbf{A}\\boldsymbol{\\Sigma}\\mathbf{A}^T\\) does not have to be positive definite!"
  },
  {
    "objectID": "resources/slides/07-sampling.html#singular-case",
    "href": "resources/slides/07-sampling.html#singular-case",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Singular Case",
    "text": "Singular Case\nIf the covariance is singular then there is no density (on \\(\\mathbb{R}^n\\)), but claim that \\(\\mathbf{Y}\\) still has a multivariate normal distribution!\n\n\nDefinition: Multivariate Normal\\(\\mathbf{Y}\\in \\mathbb{R}^n\\) has a multivariate normal distribution \\(\\textsf{N}(\\boldsymbol{\\mu},\n  \\boldsymbol{\\Sigma})\\) if for any \\(\\mathbf{v}\\in \\mathbb{R}^n\\) \\(\\mathbf{v}^T\\mathbf{Y}\\) has a univariate normal distribution with mean \\(\\mathbf{v}^T\\boldsymbol{\\mu}\\) and variance \\(\\mathbf{v}^T\\boldsymbol{\\Sigma}\\mathbf{v}\\)\n\n\n\n\n\nProofUse moment generating or characteristic functions which uniquely characterize distribution to show that \\(\\mathbf{v}^T\\mathbf{Y}\\) has a univariate normal distribution.\n\n\n\nboth \\(\\hat{\\mathbf{Y}}\\) and \\(\\hat{\\boldsymbol{\\epsilon}}\\) have multivariate normal distributions even though they do not have densities! (singular distributions)"
  },
  {
    "objectID": "resources/slides/07-sampling.html#distribution-of-mle-of-sigma2",
    "href": "resources/slides/07-sampling.html#distribution-of-mle-of-sigma2",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Distribution of MLE of \\(\\sigma^2\\)",
    "text": "Distribution of MLE of \\(\\sigma^2\\)\nRecall we found the MLE of \\(\\sigma^2\\) \\[{\\hat{\\sigma}}^2= \\frac{\\hat{\\boldsymbol{\\epsilon}}^T\\hat{\\boldsymbol{\\epsilon}}} {n}\\]\n\nlet \\(\\textsf{RSS}= \\| \\hat{\\boldsymbol{\\epsilon}}\\|^2 = \\hat{\\boldsymbol{\\epsilon}}^T\\hat{\\boldsymbol{\\epsilon}}\\)\nthen \\[\\begin{align*}\n\\| \\hat{\\boldsymbol{\\epsilon}}\\|^2  & = \\hat{\\boldsymbol{\\epsilon}}^T\\hat{\\boldsymbol{\\epsilon}}\\\\\n             & = \\boldsymbol{\\epsilon}^T(\\mathbf{I}_n - \\mathbf{P}_\\mathbf{X})^T (\\mathbf{I}_n - \\mathbf{P}_\\mathbf{X}) \\boldsymbol{\\epsilon}\\\\\n             & = \\boldsymbol{\\epsilon}^T(\\mathbf{I}_n - \\mathbf{P}_\\mathbf{X}) \\boldsymbol{\\epsilon}\\\\\n             & = \\boldsymbol{\\epsilon}^N \\textsf{N}\\textsf{N}^T \\boldsymbol{\\epsilon}\\\\\n             & = \\mathbf{e}^T\\mathbf{e}\n\\end{align*}\\]\n\\(\\textsf{N}\\) is the matrix of the \\((n - p)\\) eigen vectors from the spectral decomposition of \\((\\mathbf{I}_n - \\mathbf{P}_\\mathbf{X})\\) associated with the non-zero eigen-values."
  },
  {
    "objectID": "resources/slides/07-sampling.html#distribution-of-textsfrss",
    "href": "resources/slides/07-sampling.html#distribution-of-textsfrss",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Distribution of \\(\\textsf{RSS}\\)",
    "text": "Distribution of \\(\\textsf{RSS}\\)\nSince \\(\\boldsymbol{\\epsilon}\\sim \\textsf{N}(\\mathbf{0}_n, \\sigma^2 \\mathbf{I}_n)\\) and \\(\\textsf{N}\\in \\mathbb{R}^{n \\times (n - p)}\\), \\[\\textsf{N}^T \\boldsymbol{\\epsilon}= \\mathbf{e}\\sim \\textsf{N}(\\mathbf{0}_{n - p}, \\sigma^2\\textsf{N}^T\\textsf{N}) = \\textsf{N}(\\mathbf{0}_{n - p}, \\sigma^2\\mathbf{I}_{n - p} )\\]\n\n\\[\\begin{align*}\n\\textsf{RSS}& =  \\sum_{i = 1}^{n-p} e_i^2 \\\\\n     & \\mathrel{\\mathop{=}\\limits^{\\rm D}}\\sum_{i = 1}^{n-p} (\\sigma z_i)^2  \\quad \\text{ where } \\mathbf{Z}\\sim \\textsf{N}(\\mathbf{0}_{n-p}, \\mathbf{I}_{n-p}) \\\\\n     & = \\sigma^2 \\sum_{i = 1}^{n-p} z_i^2 \\\\\n     &\\mathrel{\\mathop{=}\\limits^{\\rm D}}\\sigma^2 \\chi^2_{n-p}\n     \n\\end{align*}\\]\n\n\nBackground Theory: If \\(\\mathbf{Z}\\sim \\textsf{N}_d(\\mathbf{0}_d, \\mathbf{I}_d)\\), then \\(\\mathbf{Z}^T\\mathbf{Z}\\sim \\chi^2_{d}\\)"
  },
  {
    "objectID": "resources/slides/07-sampling.html#unbiased-estimate-of-sigma2",
    "href": "resources/slides/07-sampling.html#unbiased-estimate-of-sigma2",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Unbiased Estimate of \\(\\sigma^2\\)",
    "text": "Unbiased Estimate of \\(\\sigma^2\\)\n\nExpected value of a \\(\\chi^2_d\\) random variable is \\(d\\) (the degrees of freedom)\n\\(\\textsf{E}[\\textsf{RSS}] = \\textsf{E}[\\sigma^2 \\chi^2_{n-p}] = \\sigma^2 (n-p)\\)\nthe expected value of the MLE is \\[{\\hat{\\sigma}}^2= \\textsf{E}[\\textsf{RSS}]/n = \\sigma^2 \\frac{(n-p)}{n}\\] so is biased\nan unbiased estimator of \\(\\sigma^2\\), is \\(s^2 = \\textsf{RSS}/(n-p)\\)\nnote: we can find the expectation of \\({\\hat{\\sigma}}^2\\) or \\(s^2\\) based on the covariance of \\(\\boldsymbol{\\epsilon}\\) without assuming normality by exploiting properties of the trace."
  },
  {
    "objectID": "resources/slides/07-sampling.html#distribution-of-hatboldsymbolbeta",
    "href": "resources/slides/07-sampling.html#distribution-of-hatboldsymbolbeta",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Distribution of \\(\\hat{\\boldsymbol{\\beta}}\\)",
    "text": "Distribution of \\(\\hat{\\boldsymbol{\\beta}}\\)\n\\(\\hat{\\boldsymbol{\\beta}}\\sim \\textsf{N}\\left(\\boldsymbol{\\beta}, \\sigma^2( \\mathbf{X}^T\\mathbf{X})^{-1}\\right)\\)\n\ndo not know \\(\\sigma^2\\)\nNeed a distribution that does not depend on unknown parameters for deriving confidence intervals and hypothesis tests for \\(\\boldsymbol{\\beta}\\).\nwhat if we plug in \\(s^2\\) or \\({\\hat{\\sigma}}^2\\) for \\(\\sigma^2\\)?\nwon’t be multivariate normal\nneed to reflect uncertainty in estimating \\(\\sigma^2\\)\nfirst show that \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(s^2\\) are independent"
  },
  {
    "objectID": "resources/slides/07-sampling.html#independence-of-hatboldsymbolbeta-and-s2",
    "href": "resources/slides/07-sampling.html#independence-of-hatboldsymbolbeta-and-s2",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Independence of \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(s^2\\)",
    "text": "Independence of \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(s^2\\)\nIf the distribution of \\(\\mathbf{Y}\\) is normal, then \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(s^2\\) are statistically independent.\n\nThe derivation of this result basically has three steps:\n\n\\(\\hat{\\boldsymbol{\\beta}}\\) and \\(\\hat{\\boldsymbol{\\epsilon}}\\) or \\(\\mathbf{e}\\) have zero covariance\n\\(\\hat{\\boldsymbol{\\beta}}\\) and \\(\\hat{\\boldsymbol{\\epsilon}}\\) or \\(\\mathbf{e}\\) are independent\nConclude \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(\\textsf{RSS}\\) (or \\(s^2\\)) are independent\n\n\n\nStep 1:\n\n\n\\[\\begin{align*}\n\\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}, \\hat{\\boldsymbol{\\epsilon}}] & = \\textsf{E}[(\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}) \\hat{\\boldsymbol{\\epsilon}}^T] \\\\\n                   & = \\textsf{E}[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T (\\mathbf{I}- \\mathbf{P}_\\mathbf{X})] \\\\\n                   & = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\\\\n                   & = \\mathbf{0}\n\\end{align*}\\]"
  },
  {
    "objectID": "resources/slides/07-sampling.html#zero-covariance-leftrightarrow-independence-in-multivariate-normals",
    "href": "resources/slides/07-sampling.html#zero-covariance-leftrightarrow-independence-in-multivariate-normals",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Zero Covariance \\(\\Leftrightarrow\\) Independence in Multivariate Normals",
    "text": "Zero Covariance \\(\\Leftrightarrow\\) Independence in Multivariate Normals\nStep 2: \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(\\hat{\\boldsymbol{\\epsilon}}\\) are independent\n\nTheorem: Zero Correlation and IndependenceFor a random vector \\(\\mathbf{W}\\sim \\textsf{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) partitioned as \\[\n\\mathbf{W}= \\left[\n  \\begin{array}{c}\n\\mathbf{W}_1  \\\\ \\mathbf{W}_2 \\end{array} \\right]  \\sim \\textsf{N}\\left( \\left[\n  \\begin{array}{c} \\boldsymbol{\\mu}_1  \\\\ \\boldsymbol{\\mu}_2 \\end{array} \\right],\n  \\left[ \\begin{array}{cc}\n\\boldsymbol{\\Sigma}_{11} &  \\boldsymbol{\\Sigma}_{12}  \\\\\n\\boldsymbol{\\Sigma}_{21} & \\boldsymbol{\\Sigma}_{22} \\end{array} \\right]\n\\right)\n\\]\nthen \\(\\textsf{Cov}(\\mathbf{W}_1, \\mathbf{W}_2) = \\boldsymbol{\\Sigma}_{12} = \\boldsymbol{\\Sigma}_{21}^T = \\mathbf{0}\\) if and only if \\(\\mathbf{W}_1\\) and \\(\\mathbf{W}_2\\) are independent."
  },
  {
    "objectID": "resources/slides/07-sampling.html#proof-independence-implies-zero-covariance",
    "href": "resources/slides/07-sampling.html#proof-independence-implies-zero-covariance",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Proof: Independence implies Zero Covariance",
    "text": "Proof: Independence implies Zero Covariance\nEasy direction\n\n\\(\\textsf{Cov}[\\mathbf{W}_1, \\mathbf{W}_2] = \\textsf{E}[(\\mathbf{W}_1 - \\boldsymbol{\\mu}_1)(\\mathbf{W}_2 - \\boldsymbol{\\mu}_2)^T]\\)\nsince they are independent \\[\\begin{align*}\n\\textsf{Cov}[\\mathbf{W}_1, \\mathbf{W}_2] & = \\textsf{E}[(\\mathbf{W}_1 - \\boldsymbol{\\mu}_1)] \\textsf{E}[(\\mathbf{W}_2 - \\boldsymbol{\\mu}_2)^T] \\\\\n& = \\mathbf{0}\\mathbf{0}^T \\\\\n& = \\mathbf{0}\n\\end{align*}\\]\n\n\nso \\(\\mathbf{W}_1\\) and \\(\\mathbf{W}_2\\) are uncorrelated"
  },
  {
    "objectID": "resources/slides/07-sampling.html#zero-covariance-implies-independence",
    "href": "resources/slides/07-sampling.html#zero-covariance-implies-independence",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Zero Covariance Implies Independence",
    "text": "Zero Covariance Implies Independence\n\nProof\nAssume \\(\\boldsymbol{\\Sigma}_{12} = \\mathbf{0}\\):\n\nChoose an \\[\\mathbf{A}= \\left[\n\\begin{array}{ll}\n  \\mathbf{A}_1 & \\mathbf{0}\\\\\n  \\mathbf{0}& \\mathbf{A}_2\n\\end{array}\n\\right]\\] such that \\(\\mathbf{A}_1 \\mathbf{A}_1^T = \\boldsymbol{\\Sigma}_{11}\\), \\(\\mathbf{A}_2 \\mathbf{A}_2^T = \\boldsymbol{\\Sigma}_{22}\\)\nPartition\n\\[ \\mathbf{Z}= \\left[\n\\begin{array}{c}\n  \\mathbf{Z}_1 \\\\ \\mathbf{Z}_2\n\\end{array}\n\\right] \\sim \\textsf{N}\\left(\n\\left[\n\\begin{array}{c}\n  \\mathbf{0}_1 \\\\ \\mathbf{0}_2\n\\end{array}\n\\right],\n\\left[\n\\begin{array}{ll}\n  \\mathbf{I}_1 &\\mathbf{0}\\\\\n\\mathbf{0}& \\mathbf{I}_2\n\\end{array}\n\\right]\n\\right)  \\text{ and } \\boldsymbol{\\mu}= \\left[\n\\begin{array}{c}\n  \\boldsymbol{\\mu}_1 \\\\ \\boldsymbol{\\mu}_2\n\\end{array}\n\\right]\\]\nthen \\(\\mathbf{W}\\mathrel{\\mathop{=}\\limits^{\\rm D}}\\mathbf{A}\\mathbf{Z}+ \\boldsymbol{\\mu}\\sim  \\textsf{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\)"
  },
  {
    "objectID": "resources/slides/07-sampling.html#step-3",
    "href": "resources/slides/07-sampling.html#step-3",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Step 3:",
    "text": "Step 3:\nShow \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(\\textsf{RSS}\\) are independent\n\n\\(\\hat{\\boldsymbol{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{Y}\\) and \\(\\hat{\\boldsymbol{\\epsilon}}= (\\mathbf{I}- \\mathbf{P}_\\mathbf{X})\\mathbf{Y}\\) are independent\nfunctions of independent random variables are independent so \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(\\textsf{RSS}= \\hat{\\boldsymbol{\\epsilon}}^T\\hat{\\boldsymbol{\\epsilon}}\\) are independent\nso \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(s^2 = \\textsf{RSS}/(n-p)\\) are independent\n\n\nThis result will be critical for creating confidence regions and intervals for \\(\\boldsymbol{\\beta}\\) and linear combinations of \\(\\boldsymbol{\\beta}\\), \\(\\lambda^T \\boldsymbol{\\beta}\\) as well as testing hypotheses"
  },
  {
    "objectID": "resources/slides/07-sampling.html#next-class",
    "href": "resources/slides/07-sampling.html#next-class",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Next Class",
    "text": "Next Class\n\nshrinkage estimators\nBayes and penalized loss functions\n\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/06-GLS.html#outline",
    "href": "resources/slides/06-GLS.html#outline",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Outline",
    "text": "Outline\n\nGeneral Least Squares and MLEs\nGauss-Markov Theorem & BLUEs\nMVUE\n\n\nReadings:\n\nChristensen Chapter 2 and 10 (Appendix B as needed)\nSeber & Lee Chapter 3"
  },
  {
    "objectID": "resources/slides/06-GLS.html#other-error-distributions",
    "href": "resources/slides/06-GLS.html#other-error-distributions",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Other Error Distributions",
    "text": "Other Error Distributions\nModel:\n\\[\\begin{align} \\mathbf{Y}& = \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\quad\n              \\textsf{E}[\\boldsymbol{\\epsilon}]  = \\mathbf{0}_n \\\\\n              \\textsf{Cov}[\\boldsymbol{\\epsilon}] & = \\sigma^2 \\mathbf{V}\n\\end{align}\\] where \\(\\sigma^2\\) is a scalar and \\(\\mathbf{V}\\) is a \\(n \\times n\\) symmetric matrix\n\nExamples:\n\nHeteroscedasticity: \\(\\mathbf{V}\\) is a diagonal matrix with \\([\\mathbf{V}]_{ii} =  v_i\\)\n\n\\(v_{i} = 1/n_i\\) if \\(y_i\\) is the mean of \\(n_i\\) observations\nsurvey weights or propogation of measurement errors in physics models\n\nCorrelated data:\n\ntime series; first order auto-regressive model with equally spaced data \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\sigma^2 \\mathbf{V}\\), where \\(v_{ij} = \\rho^{|i−j|}\\).\n\nHierarchical models with random effects"
  },
  {
    "objectID": "resources/slides/06-GLS.html#ols-under-a-general-covariance",
    "href": "resources/slides/06-GLS.html#ols-under-a-general-covariance",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "OLS under a General Covariance",
    "text": "OLS under a General Covariance\n\nIs it still unbiased? What’s its variance? Is it still the BLUE?\nUnbiasedness of \\(\\hat{\\boldsymbol{\\beta}}\\) \\[\\begin{align}\n\\textsf{E}[\\hat{\\boldsymbol{\\beta}}] & = \\textsf{E}[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{Y}] \\\\\n        & = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\textsf{E}[\\mathbf{Y}] =  (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\textsf{E}[\\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}] \\\\\n        & = \\boldsymbol{\\beta}+ \\mathbf{0}_p = \\boldsymbol{\\beta}\n\\end{align}\\]\nCovariance of \\(\\hat{\\boldsymbol{\\beta}}\\) \\[\\begin{align}\n\\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}] & = \\textsf{Cov}[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{Y}] \\\\\n       & = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\textsf{Cov}[\\mathbf{Y}]  \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n       & = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T  \\mathbf{V}\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\n\\end{align}\\]\nNot necessarily \\(\\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1}\\) unless \\(\\mathbf{V}\\) has a special form"
  },
  {
    "objectID": "resources/slides/06-GLS.html#gls-via-whitening",
    "href": "resources/slides/06-GLS.html#gls-via-whitening",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "GLS via Whitening",
    "text": "GLS via Whitening\nTransform the data and reduce problem to one we have solved!\n\nFor \\(\\mathbf{V}&gt; 0\\) use the Spectral Decomposition \\[\\mathbf{V}= \\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{U}^T = \\mathbf{U}\\boldsymbol{\\Lambda}^{1/2} \\boldsymbol{\\Lambda}^{1/2} \\mathbf{U}^T\\]\ndefine the symmetric square root of \\(\\mathbf{V}\\) as \\[\\mathbf{V}^{1/2} \\equiv \\mathbf{U}\\boldsymbol{\\Lambda}^{1/2} \\mathbf{U}^T\\]\ntransform model: \\[\\begin{align*}\n\\mathbf{V}^{-1/2} \\mathbf{Y}& = \\mathbf{V}^{-1/2} \\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{V}^{-1/2}\\boldsymbol{\\epsilon}\\\\\n\\tilde{\\mathbf{Y}} & = \\tilde{\\mathbf{X}} \\boldsymbol{\\beta}+ \\tilde{\\boldsymbol{\\epsilon}}\n\\end{align*}\\]\nSince \\(\\textsf{Cov}[\\tilde{\\boldsymbol{\\epsilon}}] = \\sigma^2\\mathbf{V}^{-1/2} \\mathbf{V}\\mathbf{V}^{-1/2} = \\sigma^2 \\mathbf{I}_n\\), we know that \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\equiv (\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}})^{-1} \\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{Y}}\\) is the BLUE for \\(\\boldsymbol{\\beta}\\) based on \\(\\tilde{\\mathbf{Y}}\\) (\\(\\mathbf{X}\\) full rank)"
  },
  {
    "objectID": "resources/slides/06-GLS.html#gls",
    "href": "resources/slides/06-GLS.html#gls",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "GLS",
    "text": "GLS\n\nIf \\(\\mathbf{V}\\) is known, then \\(\\tilde{\\mathbf{Y}}\\) and \\(\\mathbf{Y}\\) are known linear transformations of each other\nany estimator of \\(\\boldsymbol{\\beta}\\) that is linear in \\(\\mathbf{Y}\\) is linear in \\(\\tilde{\\mathbf{Y}}\\) and vice versa from previous results\n\\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\) is the BLUE of \\(\\boldsymbol{\\beta}\\) based on either \\(\\tilde{\\mathbf{Y}}\\) or \\(\\mathbf{Y}\\)!\nSubstituting back, we have \\[\\begin{align}\n\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}& =  (\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}})^{-1} \\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{Y}}\\\\\n       & = (\\mathbf{X}^T \\mathbf{V}^{-1/2}\\mathbf{V}^{-1/2} \\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{V}^{-1/2}\\mathbf{V}^{-1/2}\\mathbf{Y}\\\\\n       & = (\\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{Y}\n\\end{align}\\] which is the Generalized Least Squares Estimator of \\(\\boldsymbol{\\beta}\\)\n\n\n\nExercise: Weighted RegressionConsider the model \\(\\mathbf{Y}= \\beta \\mathbf{x}+ \\boldsymbol{\\epsilon}\\) where \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}]\\) is a known diagonal matrix \\(\\mathbf{V}\\). Write out the GLS estimator in terms of sums and interpret."
  },
  {
    "objectID": "resources/slides/06-GLS.html#gls-of-boldsymbolmu-full-rank-casedagger",
    "href": "resources/slides/06-GLS.html#gls-of-boldsymbolmu-full-rank-casedagger",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "GLS of \\(\\boldsymbol{\\mu}\\) (Full Rank Case)\\(^{\\dagger}\\)",
    "text": "GLS of \\(\\boldsymbol{\\mu}\\) (Full Rank Case)\\(^{\\dagger}\\)\n\nthe OLS/MLE of \\(\\boldsymbol{\\mu}\\in C(\\mathbf{X})\\) with transformed variables is \\[\\begin{align*}\n\\mathbf{P}_{\\tilde{\\mathbf{X}}} \\tilde{\\mathbf{Y}}& = \\tilde{\\mathbf{X}}\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\\\\n\\tilde{\\mathbf{X}}\\left(\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}\\right)^{-1}\\tilde{\\mathbf{X}}^T \\tilde{\\mathbf{Y}}& = \\tilde{\\mathbf{X}}\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\\\\n\\mathbf{V}^{-1/2} \\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-1}\\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{Y}& = \\mathbf{V}^{-1/2} \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\end{align*}\\]\nsince \\(\\mathbf{V}\\) is positive definite, multiple thru by \\(\\mathbf{V}^{1/2}\\), to show that \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\) is a GLS/MLE estimator of \\(\\boldsymbol{\\beta}\\) iff \\[\\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-1}\\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{Y}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\]\nIs \\(\\mathbf{P}_\\mathbf{V}\\equiv \\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-1}\\mathbf{X}^T \\mathbf{V}^{-1}\\) a projection onto \\(C(\\mathbf{X})\\)? Is it an orthogonal projection onto \\(C(\\mathbf{X})\\)?\n\n\n\\(\\dagger\\) if \\(\\mathbf{X}\\) is not full rank replace \\(\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-1}\\) with \\(\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-}\\)"
  },
  {
    "objectID": "resources/slides/06-GLS.html#projections",
    "href": "resources/slides/06-GLS.html#projections",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Projections",
    "text": "Projections\nWe want to show that \\(\\mathbf{P}_\\mathbf{V}\\equiv \\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-1}\\mathbf{X}^T \\mathbf{V}^{-1}\\) is a projection onto \\(C(\\mathbf{X})\\)\n\nfrom the definition of \\(\\mathbf{P}_\\mathbf{V}\\) it follows that \\(\\mathbf{m}\\in C(\\mathbf{P}_\\mathbf{v})\\) implies that \\(\\mathbf{m}= \\mathbf{P}_\\mathbf{V}\\mathbf{m}= \\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{m}\\) so \\(C(\\mathbf{P}_\\mathbf{V}) \\subset C(\\mathbf{X})\\)\nsince \\(\\mathbf{P}_\\tilde{\\mathbf{X}}\\) is a projection onto \\(C(\\tilde{\\mathbf{X}})\\) we have \\[\\begin{align*}\n\\mathbf{P}_{\\tilde{\\mathbf{X}}} \\tilde{\\mathbf{X}}& = \\tilde{\\mathbf{X}}\\\\\n\\tilde{\\mathbf{X}}\\left(\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}\\right)^{-1}\\tilde{\\mathbf{X}}^T \\tilde{\\mathbf{X}}& = \\tilde{\\mathbf{X}}\\\\\n\\mathbf{V}^{-1/2} \\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-1}\\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{X}& = \\mathbf{V}^{-1/2} \\mathbf{X}\\\\\n\\mathbf{V}^{-1/2} \\mathbf{P}_\\mathbf{V}\\mathbf{X}& = \\mathbf{V}^{-1/2} \\mathbf{X}\n\\end{align*}\\]\nWe can multiply both sides by \\(\\mathbf{V}^{1/2} &gt; 0\\), so that \\(\\mathbf{P}_\\mathbf{V}\\mathbf{X}= \\mathbf{X}\\)\nfor \\(\\mathbf{m}\\in C(\\mathbf{X})\\), \\(\\mathbf{P}_\\mathbf{V}\\mathbf{m}= \\mathbf{m}\\) and \\(C(\\mathbf{X}) \\subset C(\\mathbf{P}_\\mathbf{V})\\)\n\\(\\quad \\quad \\therefore C(\\mathbf{P}_\\mathbf{V}) = C(\\mathbf{X})\\) so that \\(\\mathbf{P}_\\mathbf{V}\\) is a projection onto \\(C(\\mathbf{X})\\)"
  },
  {
    "objectID": "resources/slides/06-GLS.html#oblique-projections",
    "href": "resources/slides/06-GLS.html#oblique-projections",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Oblique Projections",
    "text": "Oblique Projections\n\nProposition: ProjectionThe \\(n \\times n\\) matrix \\(\\mathbf{P}_\\mathbf{V}\\equiv \\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-1}\\mathbf{X}^T \\mathbf{V}^{-1}\\) is a projection onto the \\(C(\\mathbf{X})\\)\n\n\n\nShow that \\(\\mathbf{P}_\\mathbf{V}^2 = \\mathbf{P}_\\mathbf{V}\\) (idempotent)\nevery vector \\(\\mathbf{y}\\in \\mathbb{R}^n\\) may be written as \\(\\mathbf{y}= \\mathbf{m}+ \\mathbf{n}\\) where \\(\\mathbf{P}_\\mathbf{v}\\mathbf{y}= \\mathbf{m}\\) and \\((\\mathbf{I}_n - \\mathbf{P}_\\mathbf{v})\\mathbf{y}= \\mathbf{n}\\) where \\(\\mathbf{m}\\in C(\\mathbf{P}_\\mathbf{V})\\) and \\(\\mathbf{u}\\in N(\\mathbf{P}_\\mathbf{V})\\)\nIs \\(\\mathbf{P}_\\mathbf{V}\\) an orthogonal projection onto \\(C(\\mathbf{X})\\) for the inner product space \\((\\mathbb{R}^n, \\langle \\mathbf{v}, \\mathbf{u}\\rangle = \\mathbf{v}^T\\mathbf{u})\\)?\n\n\n\nDefinition: Oblique ProjectionFor the inner product space \\((\\mathbb{R}^n, \\langle \\mathbf{v}, \\mathbf{u}\\rangle = \\mathbf{v}^T\\mathbf{u})\\), a projection \\(\\mathbf{P}\\) that is not an orthogonal projection is called an oblique projection"
  },
  {
    "objectID": "resources/slides/06-GLS.html#loss-function",
    "href": "resources/slides/06-GLS.html#loss-function",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Loss Function",
    "text": "Loss Function\nThe GLS estimator minimizes the following generalized squared error loss: \\[\\begin{align}\n\\| \\tilde{\\mathbf{Y}}- \\tilde{\\mathbf{X}}\\boldsymbol{\\beta}\\|^2 & = (\\tilde{\\mathbf{Y}}- \\tilde{\\mathbf{X}}\\boldsymbol{\\beta})^T(\\tilde{\\mathbf{Y}}- \\tilde{\\mathbf{X}}\\boldsymbol{\\beta}) \\\\\n                    & = (\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta})^T \\mathbf{V}^{-1/2}\\mathbf{V}^{-1/2}(\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}) \\\\\n                    & = (\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta})^T \\mathbf{V}^{-1}(\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta})  \\\\\n                    & = \\| \\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}\\|^2_{\\mathbf{V}^{-1}}\n\\end{align}\\] where we can change the inner product to be \\[\\langle \\mathbf{u}, \\mathbf{v}\\rangle_{\\mathbf{V}^{-1}} \\equiv \\mathbf{u}^T\\mathbf{V}^{-1} \\mathbf{v}\\]"
  },
  {
    "objectID": "resources/slides/06-GLS.html#orthogonality-in-an-inner-product-space",
    "href": "resources/slides/06-GLS.html#orthogonality-in-an-inner-product-space",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Orthogonality in an Inner Product Space",
    "text": "Orthogonality in an Inner Product Space\n\nDefinition: Orthogonal ProjectonFor an inner product space, (\\(\\mathbb{R}^n, \\langle , \\rangle\\)). The projection \\(\\mathbf{P}\\) is an orthogonal projection if for every vector \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) in \\(\\mathbb{R}^n\\), \\[\n  \\langle \\mathbf{P}\\mathbf{x}, (\\mathbf{I}_n -\\mathbf{P})\\mathbf{y}\\rangle = \\langle (\\mathbf{I}_n - \\mathbf{P}) \\mathbf{x},\\mathbf{P}\\mathbf{y}\\rangle = 0\n\\] Equivalently: \\[\n  \\langle \\mathbf{x},\\mathbf{P}\\mathbf{y}\\rangle = \\langle \\mathbf{P}\\mathbf{x}, \\mathbf{P}\\mathbf{y}\\rangle =\\langle \\mathbf{P}\\mathbf{x},\\mathbf{y}\\rangle\n\\]\n\n\n\nExerciseShow that \\(\\mathbf{P}_\\mathbf{V}\\) is an orthogonal projection under the inner product \\(\\langle \\mathbf{x}, \\mathbf{y}\\rangle_{\\mathbf{V}^{-1}} \\equiv \\mathbf{x}^T\\mathbf{V}^{-1} \\mathbf{y}\\)"
  },
  {
    "objectID": "resources/slides/06-GLS.html#variance-of-gls",
    "href": "resources/slides/06-GLS.html#variance-of-gls",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Variance of GLS",
    "text": "Variance of GLS\n\nVariance of the GLS estimator \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}=  (\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})^{−1}\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{Y}\\) is much simpler \\[\\begin{align}\n\\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}] & = (\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})^{−1}\\mathbf{X}^T\\mathbf{V}^{−1}\\textsf{Cov}[\\mathbf{Y}]\\mathbf{V}^{−1}\\mathbf{X}(\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})^{−1} \\\\\n& = (\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})^{−1}\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{V}\\mathbf{V}^{−1}\\mathbf{X}(\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})^{−1} \\\\\n& = \\sigma^2(\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})^{−1}(\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})(\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})^{−1} \\\\\n& = \\sigma^2(\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})^{−1}\n\\end{align}\\]\n\n\n\nTheorem: Gauss-Markov-AitkinLet \\(\\tilde{\\boldsymbol{\\beta}}\\) be a linear unbiased estimator of \\(\\boldsymbol{\\beta}\\) and \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\) be the GLS estimator of \\(\\boldsymbol{\\beta}\\) in the linear model \\(\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\) with \\(\\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\) and \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\sigma^2 \\mathbf{V}\\) with \\(\\mathbf{X}\\) and \\(\\mathbf{V}&gt;0\\) known. Then \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\) is the BLUE where \\[\\textsf{Cov}[\\tilde{\\boldsymbol{\\beta}}] \\ge  \\sigma^2 (\\mathbf{X}^T\\mathbf{V}^{-1} \\mathbf{X})^{-1} = \\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}] \\]"
  },
  {
    "objectID": "resources/slides/06-GLS.html#when-will-ols-and-gls-be-equal",
    "href": "resources/slides/06-GLS.html#when-will-ols-and-gls-be-equal",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "When will OLS and GLS be Equal?",
    "text": "When will OLS and GLS be Equal?\n\nFor what covariance matrices \\(\\mathbf{V}\\) will the OLS and GLS estimators be the same?\nFiguring this out can help us understand why the GLS estimator has a lower variance in general.\n\n\n\nTheoremThe estimators \\(\\hat{\\boldsymbol{\\beta}}\\) (OLS) and \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\) (GLS) are the same for all \\(\\mathbf{Y}\\in \\mathbb{R}^n\\) iff \\[\\mathbf{V}= \\mathbf{X}\\boldsymbol{\\Psi}\\mathbf{X}^T + \\mathbf{H}\\boldsymbol{\\Phi}\\mathbf{H}^T\\] for some positive definite matrices \\(\\boldsymbol{\\Psi}\\) and \\(\\boldsymbol{\\Phi}\\) and a matrix \\(\\mathbf{H}\\) such that \\(\\mathbf{H}^T \\mathbf{X}= \\mathbf{0}\\)."
  },
  {
    "objectID": "resources/slides/06-GLS.html#outline-of-proof",
    "href": "resources/slides/06-GLS.html#outline-of-proof",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Outline of Proof",
    "text": "Outline of Proof\nWe need to show that \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\) are the same for all \\(\\mathbf{Y}\\). Since both \\(\\mathbf{P}\\) and \\(\\mathbf{P}_\\mathbf{V}\\) are projections onto \\(C(\\mathbf{X})\\), \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\) will be the same iff \\(\\mathbf{P}_\\mathbf{V}\\) is an orthogonal projection onto \\(C(\\mathbf{X})\\) so that \\(\\mathbf{P}_\\mathbf{V}\\mathbf{n}= 0\\) for \\(\\mathbf{n}\\in C(\\mathbf{X})^\\perp\\) (they have the same null spaces)\n\nShow that \\(C(\\mathbf{X}) = C(\\mathbf{V}\\mathbf{X})\\) iff \\(\\mathbf{V}\\) can be written as \\[\\mathbf{V}= \\mathbf{X}\\boldsymbol{\\Psi}\\mathbf{X}^T + \\mathbf{H}\\boldsymbol{\\Phi}\\mathbf{H}^T\\] (Show \\(C(\\mathbf{V}\\mathbf{X}) \\subset C( \\mathbf{X})\\) iff \\(\\mathbf{V}\\) has the above form and since the two subspaces have the same rank \\(C(\\mathbf{X}) = C(\\mathbf{V}\\mathbf{X})\\)\nShow that \\(C(\\mathbf{X}) = C(\\mathbf{V}^{-1} \\mathbf{X})\\) iff \\(C(\\mathbf{X}) = C(\\mathbf{V}\\mathbf{X})\\)\nShow that \\(C(\\mathbf{X})^\\perp = C(\\mathbf{V}^{-1} \\mathbf{X})^\\perp\\) iff \\(C(\\mathbf{X}) = C(\\mathbf{V}^{-1} \\mathbf{X})\\)\nShow that \\(\\mathbf{n}\\in C(\\mathbf{X})^\\perp\\) iff \\(\\mathbf{n}\\in C(\\mathbf{V}^{-1}\\mathbf{X})^\\perp\\) so \\(\\mathbf{P}_\\mathbf{V}\\mathbf{n}= 0\\)\n\n\nSee Proposition 2.7.5 and Proof in Christensen"
  },
  {
    "objectID": "resources/slides/06-GLS.html#some-intuition",
    "href": "resources/slides/06-GLS.html#some-intuition",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Some Intuition",
    "text": "Some Intuition\nFor the linear model \\(\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\) with \\(\\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\) and \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\sigma^2 \\mathbf{V}\\), we can always write\n\\[\\begin{align} \\boldsymbol{\\epsilon}& = \\mathbf{P}\\boldsymbol{\\epsilon}+ (\\mathbf{I}- \\mathbf{P})\\boldsymbol{\\epsilon}\\\\\n                   & = \\boldsymbol{\\epsilon}_\\mathbf{X}+ \\boldsymbol{\\epsilon}_N   \n\\end{align}\\]\n\nwe can recover \\(\\boldsymbol{\\epsilon}_N\\) from the data \\(\\mathbf{Y}\\) but not \\(\\boldsymbol{\\epsilon}_\\mathbf{X}\\): \\[\\begin{align} \\mathbf{P}\\mathbf{Y}& = \\mathbf{P}( \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}_\\mathbf{X}+ \\boldsymbol{\\epsilon}_n )\\\\\n                   & =  \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}_\\mathbf{X}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\\\\n    (\\mathbf{I}_n - \\mathbf{P}) \\mathbf{Y}& =  \\boldsymbol{\\epsilon}_N = \\hat{\\boldsymbol{\\epsilon}} = \\mathbf{e}\n\\end{align}\\]\nCan \\(\\boldsymbol{\\epsilon}_\\textsf{N}\\) help us estimate \\(\\mathbf{X}\\boldsymbol{\\beta}\\)? What if \\(\\boldsymbol{\\epsilon}_N\\) could tell us something about \\(\\boldsymbol{\\epsilon}_X\\)?\nYes if they were highly correlated! But if they were independent or uncorrelated then knowing \\(\\boldsymbol{\\epsilon}_\\textsf{N}\\) doesn’t help us!"
  },
  {
    "objectID": "resources/slides/06-GLS.html#intuition-continued",
    "href": "resources/slides/06-GLS.html#intuition-continued",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Intuition Continued",
    "text": "Intuition Continued\n\nFor what matrices are \\(\\boldsymbol{\\epsilon}_\\mathbf{X}\\) and \\(\\boldsymbol{\\epsilon}_N\\) uncorrelated?\nUnder \\(\\mathbf{V}= \\mathbf{I}_n\\): \\[\\begin{align}\n\\textsf{E}[\\boldsymbol{\\epsilon}_X \\boldsymbol{\\epsilon}_N] & = \\mathbf{P}\\textsf{E}[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T](\\mathbf{I}-\\mathbf{P}) \\\\\n                & = \\sigma^2 \\mathbf{P}(\\mathbf{I}- \\mathbf{P}) = \\mathbf{0}\n\\end{align}\\] so they are uncorrelated\nFor the \\(\\mathbf{V}\\) in the theorem, introduce\n\n\\(\\mathbf{Z}_\\mathbf{X}\\) where \\(\\textsf{E}[\\mathbf{Z}_\\mathbf{X}]= \\mathbf{0}_n\\) and \\(\\textsf{Cov}[\\mathbf{Z}_\\mathbf{X}] = \\boldsymbol{\\Psi}\\)\n\\(\\mathbf{Z}_\\textsf{N}\\) where \\(\\textsf{E}[\\mathbf{Z}_\\textsf{N}]= \\mathbf{0}_n\\) and \\(\\textsf{Cov}[\\mathbf{Z}_\\textsf{N}] = \\boldsymbol{\\Phi}\\)\n\\(\\mathbf{Z}_\\mathbf{X}\\) and \\(\\mathbf{Z}_\\textsf{N}\\) are uncorrelated, \\(\\textsf{E}[\\mathbf{Z}_\\mathbf{X}\\mathbf{Z}_\\textsf{N}] = \\mathbf{0}\\)\n\\(\\boldsymbol{\\epsilon}= \\mathbf{X}\\mathbf{Z}_\\mathbf{X}+ \\mathbf{H}\\mathbf{Z}_\\textsf{N}\\) so that \\(\\boldsymbol{\\epsilon}\\) has the desired mean and covariance \\(\\mathbf{V}\\) in the theorem"
  },
  {
    "objectID": "resources/slides/06-GLS.html#intuition-continued-1",
    "href": "resources/slides/06-GLS.html#intuition-continued-1",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Intuition Continued",
    "text": "Intuition Continued\nAs a consequence we have\n\n\\(\\boldsymbol{\\epsilon}_\\mathbf{X}= \\mathbf{P}\\boldsymbol{\\epsilon}= \\mathbf{X}\\mathbf{Z}_\\mathbf{X}\\)\n\\(\\boldsymbol{\\epsilon}_\\textsf{N}= (\\mathbf{I}_n - \\mathbf{P})\\boldsymbol{\\epsilon}= \\mathbf{H}\\mathbf{Z}_\\textsf{N}\\)\n\\(\\boldsymbol{\\epsilon}_\\mathbf{X}\\) and \\(\\boldsymbol{\\epsilon}_\\textsf{N}\\) are uncorrelated \\[\\begin{align}\n\\textsf{E}[\\boldsymbol{\\epsilon}_\\mathbf{X}\\boldsymbol{\\epsilon}_\\textsf{N}] & = \\textsf{E}[\\mathbf{X}\\mathbf{Z}_\\mathbf{X}\\mathbf{Z}_\\textsf{N}^T \\mathbf{H}^T] \\\\\n                  & = \\mathbf{X}\\mathbf{0}\\mathbf{H}^T \\\\\n                  & = \\mathbf{0}\n\\end{align}\\]\nso that \\(\\boldsymbol{\\epsilon}_\\mathbf{X}\\) and \\(\\boldsymbol{\\epsilon}_\\textsf{N}\\) are uncorrelated with \\(\\mathbf{V}= \\mathbf{X}\\boldsymbol{\\Psi}\\mathbf{X}^T + \\mathbf{H}\\boldsymbol{\\Phi}\\mathbf{H}\\) ^T$\nAlternative Statement of Theorem: \\(\\hat{\\boldsymbol{\\beta}}= \\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\) for all \\(\\mathbf{Y}\\) under \\(\\textsf{Cov}[\\mathbf{Y}] = \\sigma^2 \\mathbf{V}\\) iff \\(\\mathbf{P}\\mathbf{Y}\\) and \\((\\mathbf{I}- \\mathbf{P})\\mathbf{Y}\\) are uncorrelated"
  },
  {
    "objectID": "resources/slides/06-GLS.html#equivalence-of-gls-estimators",
    "href": "resources/slides/06-GLS.html#equivalence-of-gls-estimators",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Equivalence of GLS estimators",
    "text": "Equivalence of GLS estimators\nThe following corollary to the theorem establishes when two GLS estimators for different \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}]\\) are equivalent :\n\nCorollarySuppose \\(\\mathbf{V}= \\mathbf{X}\\boldsymbol{\\Psi}\\mathbf{X}^ T + \\boldsymbol{\\Phi}\\mathbf{H}\\boldsymbol{\\Omega}\\mathbf{H}^T \\boldsymbol{\\Phi}\\). Then \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}= \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{\\Phi}\\)\n\n\n\nCan you construct an equivalent representation based on zero correlation of \\(\\mathbf{P}_\\boldsymbol{\\Phi}\\mathbf{Y}\\) and \\((\\mathbf{I}_n - \\mathbf{P}_\\boldsymbol{\\Phi})\\mathbf{Y}\\) when \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\sigma^2 \\mathbf{V}?\\)\n\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "HW/hw-03.html",
    "href": "HW/hw-03.html",
    "title": "Homework 3",
    "section": "",
    "text": "See Gradescope for any updates on due dates."
  },
  {
    "objectID": "HW/hw-03.html#due-1100pm-thurs-sept-19",
    "href": "HW/hw-03.html#due-1100pm-thurs-sept-19",
    "title": "Homework 3",
    "section": "",
    "text": "See Gradescope for any updates on due dates."
  },
  {
    "objectID": "HW/hw-03.html#rstudio",
    "href": "HW/hw-03.html#rstudio",
    "title": "Homework 3",
    "section": "RStudio",
    "text": "RStudio\nYou all should have R and RStudio installed on your computers by now or access to the Department RStudio Server https://rstudio.stat.duke.edu. If you need to setup your local version, first install the latest version of R here: https://cran.rstudio.com - remember to select the right installer for your operating system). Next, install the latest version of RStudio here: https://www.rstudio.com/products/rstudio/download/. Scroll down to the “Installers for Supported Platforms” section and find the right installer for your operating system."
  },
  {
    "objectID": "HW/hw-03.html#getting-started-with-github-classroom",
    "href": "HW/hw-03.html#getting-started-with-github-classroom",
    "title": "Homework 3",
    "section": "Getting started with Github Classroom",
    "text": "Getting started with Github Classroom\nGo to Github classroom to accept the invitation to create a repository for this assignment at HW3\nThis will create a private repo in the STA721-F24 organization on Github with your github user name. Note: You may receive an email invitation to join STA702-F24 on your behalf.\nHit refresh, to see the link for the repo, and then click on it to go to the STA721-F24 organization in Github.\nFollow the instructions in the README in your repo and in the hw#.Rnw file to complete the assignment and push to GitHub. If you encounter issues with Gradescope submission, it is critical that you have pushed your final assignment to GitHub by the due date to validate timestamps."
  },
  {
    "objectID": "HW/hw-03.html#r-rnw",
    "href": "HW/hw-03.html#r-rnw",
    "title": "Homework 3",
    "section": "R & RNW",
    "text": "R & RNW\nYou are required to use the .Rnw format to type up this report report if there are any R problems. To get started see Knitr with LaTeX article by Karl Broman. Feel free to include images of technical derivations if you are short of time to typeset in LaTeX. The macros.tex file should help with shortcuts. You may need to change some of your Project or Global options to get your document to compile with KnitR. If so please post under Ed in the General category or ask in Lab."
  },
  {
    "objectID": "HW/hw-03.html#gradescope-submission",
    "href": "HW/hw-03.html#gradescope-submission",
    "title": "Homework 3",
    "section": "Gradescope Submission",
    "text": "Gradescope Submission\nYou MUST submit both your final .Rnw .pdf files to the repo on Github and upload the pdf to Gradescope here: https://www.gradescope.com/courses/843802/assignments\nMake sure to to pdf; ask the TA about knitting to pdf if you cannot figure it out. Be sure to submit under the right assignment entry by the due date!"
  },
  {
    "objectID": "HW/hw-03.html#grading",
    "href": "HW/hw-03.html#grading",
    "title": "Homework 3",
    "section": "Grading",
    "text": "Grading\nTotal: 20 points."
  },
  {
    "objectID": "HW/hw-02.html",
    "href": "HW/hw-02.html",
    "title": "Homework 2",
    "section": "",
    "text": "See Gradescope for any updates on due dates."
  },
  {
    "objectID": "HW/hw-02.html#due-1100pm-thurs-sept-12",
    "href": "HW/hw-02.html#due-1100pm-thurs-sept-12",
    "title": "Homework 2",
    "section": "",
    "text": "See Gradescope for any updates on due dates."
  },
  {
    "objectID": "HW/hw-02.html#rstudio",
    "href": "HW/hw-02.html#rstudio",
    "title": "Homework 2",
    "section": "RStudio",
    "text": "RStudio\nYou all should have R and RStudio installed on your computers by now or access to the Department RStudio Server https://rstudio.stat.duke.edu. If you need to setup your local version, first install the latest version of R here: https://cran.rstudio.com - remember to select the right installer for your operating system). Next, install the latest version of RStudio here: https://www.rstudio.com/products/rstudio/download/. Scroll down to the “Installers for Supported Platforms” section and find the right installer for your operating system."
  },
  {
    "objectID": "HW/hw-02.html#r-rnw",
    "href": "HW/hw-02.html#r-rnw",
    "title": "Homework 2",
    "section": "R & RNW",
    "text": "R & RNW\nYou are required to use the .Rnw format to type up this report report. To get started see Knitr with LaTeX article by Karl Broman. Feel free to include images of technical derivations if you are short of time to typeset in LaTeX. The macros.tex file should help with shortcuts. You may need to change some of your Project or Global options to get your document to compile with KnitR. If so please post under Ed in the General category or ask in Lab."
  },
  {
    "objectID": "HW/hw-02.html#getting-started-with-github-classroom",
    "href": "HW/hw-02.html#getting-started-with-github-classroom",
    "title": "Homework 2",
    "section": "Getting started with Github Classroom",
    "text": "Getting started with Github Classroom\nGo to Github classroom to accept the invitation to create a repository for this assignment at HW2\nThis will create a private repo in the STA721-F24 organization on Github with your github user name. Note: You may receive an email invitation to join STA702-F24 on your behalf.\nHit refresh, to see the link for the repo, and then click on it to go to the STA721-F24 organization in Github.\nFollow the instructions in the README in your repo and in the hw2.Rnw file to complete the assignment and push to GitHub. If you encounter issues with Gradescope submission, it is critical that you have pushed your final assignment to GitHub by the due date to validate timestamps."
  },
  {
    "objectID": "HW/hw-02.html#gradescope-submission",
    "href": "HW/hw-02.html#gradescope-submission",
    "title": "Homework 2",
    "section": "Gradescope Submission",
    "text": "Gradescope Submission\nYou MUST submit both your final .Rnw .pdf files to the repo on Github and upload the pdf to Gradescope here: https://www.gradescope.com/courses/843802/assignments\nMake sure to to pdf; ask the TA about knitting to pdf if you cannot figure it out. Be sure to submit under the right assignment entry by the due date!"
  },
  {
    "objectID": "HW/hw-02.html#grading",
    "href": "HW/hw-02.html#grading",
    "title": "Homework 2",
    "section": "Grading",
    "text": "Grading\nTotal: 20 points."
  },
  {
    "objectID": "HW/hw-04.html",
    "href": "HW/hw-04.html",
    "title": "Homework 4",
    "section": "",
    "text": "See Gradescope for any updates on due dates.\nCreate your repo from GitHub Classroom"
  },
  {
    "objectID": "HW/hw-04.html#due-1100pm-thurs-sept-27",
    "href": "HW/hw-04.html#due-1100pm-thurs-sept-27",
    "title": "Homework 4",
    "section": "",
    "text": "See Gradescope for any updates on due dates.\nCreate your repo from GitHub Classroom"
  },
  {
    "objectID": "HW/hw-04.html#rstudio",
    "href": "HW/hw-04.html#rstudio",
    "title": "Homework 4",
    "section": "RStudio",
    "text": "RStudio\nYou all should have R and RStudio installed on your computers by now or access to the Department RStudio Server https://rstudio.stat.duke.edu. If you need to setup your local version, first install the latest version of R here: https://cran.rstudio.com - remember to select the right installer for your operating system). Next, install the latest version of RStudio here: https://www.rstudio.com/products/rstudio/download/. Scroll down to the “Installers for Supported Platforms” section and find the right installer for your operating system."
  },
  {
    "objectID": "HW/hw-04.html#getting-started-with-github-classroom",
    "href": "HW/hw-04.html#getting-started-with-github-classroom",
    "title": "Homework 4",
    "section": "Getting started with Github Classroom",
    "text": "Getting started with Github Classroom\nGo to Github classroom to accept the invitation to create a repository for this assignment.\nThis will create a private repo in the STA721-F24 organization on Github with your github user name. Note: You may receive an email invitation to join STA702-F24 on your behalf.\nHit refresh, to see the link for the repo, and then click on it to go to the STA721-F24 organization in Github.\nFollow the instructions in the README in your repo and in the hw#.Rnw file to complete the assignment and push to GitHub. If you encounter issues with Gradescope submission, it is critical that you have pushed your final assignment to GitHub by the due date to validate timestamps."
  },
  {
    "objectID": "HW/hw-04.html#r-rnw",
    "href": "HW/hw-04.html#r-rnw",
    "title": "Homework 4",
    "section": "R & RNW",
    "text": "R & RNW\nYou are required to use the .Rnw format to type up this report report if there are any R problems. To get started see Knitr with LaTeX article by Karl Broman. Feel free to include images of technical derivations if you are short of time to typeset in LaTeX. The macros.tex file should help with shortcuts. You may need to change some of your Project or Global options to get your document to compile with KnitR. If so please post under Ed in the General category or ask in Lab."
  },
  {
    "objectID": "HW/hw-04.html#gradescope-submission",
    "href": "HW/hw-04.html#gradescope-submission",
    "title": "Homework 4",
    "section": "Gradescope Submission",
    "text": "Gradescope Submission\nYou MUST submit both your final .Rnw .pdf files to the repo on Github and upload the pdf to Gradescope here: https://www.gradescope.com/courses/843802/assignments\nMake sure to to pdf; ask the TA about knitting to pdf if you cannot figure it out. Be sure to submit under the right assignment entry by the due date!"
  },
  {
    "objectID": "HW/hw-04.html#grading",
    "href": "HW/hw-04.html#grading",
    "title": "Homework 4",
    "section": "Grading",
    "text": "Grading\nTotal: 20 points."
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "STA721: Linear Models (Fall 2024)",
    "section": "Course Description",
    "text": "Course Description\nThis course introduces students to linear models and extensions for model building from the frequentist (OLS/MLE and penalized likelihoods) and Bayesian paradigms, with an emphasis on a geometric perspective. Course topics include optimal estimation and prediction, distributional assumptions and model checking, hypothesis testing and model selection including Bayes factors and intrinsic Bayes factors, and Bayesian Model Averaging. Students should have a strong background in matrix algebra and distribution theory. Co-requisite: STA 602L, 702L or equivalent."
  },
  {
    "objectID": "index.html#course-info",
    "href": "index.html#course-info",
    "title": "STA721: Linear Models (Fall 2024)",
    "section": "Course Info",
    "text": "Course Info\n\nTextbooks\n\n\n\nTextbook\nOrdering Information\n\n\n\n\n\nPlane Answers to Complex Questions Ronald Christensen (2011) 4th Edition Springer-Verlag, NY. The textbook is freely available as an eBook thru the Duke Library. You’re welcomed to read on screen or print it out. If you prefer a paperback version you can buy it at the cost of printing from Springer or purchase a hardback version at your favorite vendor.\n\n\n\nLinear Regression Analysis, George A.F Seber and Alan J. Lee (2003) 2nd Edition, Wiley eBook in Duke Library. Duke Library is aware the link to the ebook is broken See the\n\n\n\nCanvas site for a pdf version until the links are fixed.\n\n\nLecture\n   Tuesday and Thursday\n   1:25pm - 2:40pm\n   Old Chemistry 123 \n\n\nLabs\n   Fridays\n  10:05am - 11:20pm\n   LINK 088 (Clasroom 4) \n\n\nFinal Exam\n   December 15\n   9:00am - 12:00pm\n   Old Chemistry 123 \n\n\nInstructional Team and Office Hours\n\n\n\nRole\nName\nEmail\nOffice Hours\nLocation\n\n\n\n\nInstructor\nDr Merlise Clyde\n\nTues 2:45 - 3:45  or by appointment \n223E Old Chem\n\n\nTA\nBongjung Sung\n\nMon 9:30-11:30am\n203B Old Chem"
  },
  {
    "objectID": "index.html#course-topics",
    "href": "index.html#course-topics",
    "title": "STA721: Linear Models (Fall 2024)",
    "section": "Course Topics",
    "text": "Course Topics\nCourse topics will be drawn (but subject to change) from\n\nMotivation for Studying Linear Models as Foundation\nRandom Vectors and Matrices\nMultivariate Normal Distribution Theory\nConditional Normal Distribution Theory\nLinear Models via Coordinate free representations (examples)\nMaximum Likelihood Estimation & Projections\nInterval Estimation: Distribution of Quadratic Forms\nGauss-Markov Theorem & Optimality of OLS/GLS\nFormulation of Bayesian Inference\nSubjective and Default Priors\nRelated Shrinkage Methods and Penalized Likelihoods (Ridge regression, lasso)\nModel Selection (comparison of classical and Bayesian approaches)\nBayes Factors\nBayesian Model Averaging\nModel Checking: Residual Analysis & Diagnostics\nRobust Methods for Outliers\nGeneralized Linear Model\nHierarchical Models\n\nPlease check the website for updates, slides and current readings."
  },
  {
    "objectID": "HW/hw-01.html",
    "href": "HW/hw-01.html",
    "title": "Homework 1",
    "section": "",
    "text": "See Gradescope for any updates on due dates."
  },
  {
    "objectID": "HW/hw-01.html#due-1100pm-thurs-sept-5",
    "href": "HW/hw-01.html#due-1100pm-thurs-sept-5",
    "title": "Homework 1",
    "section": "",
    "text": "See Gradescope for any updates on due dates."
  },
  {
    "objectID": "HW/hw-01.html#rstudio",
    "href": "HW/hw-01.html#rstudio",
    "title": "Homework 1",
    "section": "RStudio",
    "text": "RStudio\nYou all should have R and RStudio installed on your computers by now or access to the Department RStudio Server https://rstudio.stat.duke.edu. If you need to setup your local version, first install the latest version of R here: https://cran.rstudio.com - remember to select the right installer for your operating system). Next, install the latest version of RStudio here: https://www.rstudio.com/products/rstudio/download/. Scroll down to the “Installers for Supported Platforms” section and find the right installer for your operating system."
  },
  {
    "objectID": "HW/hw-01.html#r-quarto",
    "href": "HW/hw-01.html#r-quarto",
    "title": "Homework 1",
    "section": "R & Quarto",
    "text": "R & Quarto\nYou are required to use the .qmd format to type up this report report. To get started see technical writing with Quarto. Feel free to include images of technical derivations if you are short of time to typeset in LaTeX. The macros.tex file should help with shortcuts."
  },
  {
    "objectID": "HW/hw-01.html#getting-started-with-github-classroom",
    "href": "HW/hw-01.html#getting-started-with-github-classroom",
    "title": "Homework 1",
    "section": "Getting started with Github Classroom",
    "text": "Getting started with Github Classroom\nGo to Github classroom to accept the invitation to create a repository for this assignment at HW1\nThis will create a private repo in the STA721-F24 organization on Github with your github user name. Note: You may receive an email invitation to join STA702-F24 on your behalf.\nHit refresh, to see the link for the repo, and then click on it to go to the STA721-F24 organization in Github.\nFollow the instructions in the README in your repo and in the hw1.qmd file to complete the assignment and push to GitHub. If you encounter issues with Gradescope submission, it is critical that you have pushed your final assignment to GitHub by the due date to validate timestamps."
  },
  {
    "objectID": "HW/hw-01.html#gradescope-submission",
    "href": "HW/hw-01.html#gradescope-submission",
    "title": "Homework 1",
    "section": "Gradescope Submission",
    "text": "Gradescope Submission\nYou MUST submit both your final .qmd .pdf files to the repo on Github and upload the pdf to Gradescope here: https://www.gradescope.com/courses/843802/assignments\nMake sure to Render to pdf; ask the TA about knitting to pdf if you cannot figure it out. Be sure to submit under the right assignment entry by the due date!"
  },
  {
    "objectID": "HW/hw-01.html#grading",
    "href": "HW/hw-01.html#grading",
    "title": "Homework 1",
    "section": "Grading",
    "text": "Grading\nTotal: 20 points."
  },
  {
    "objectID": "HW/hw-05.html",
    "href": "HW/hw-05.html",
    "title": "Homework 5",
    "section": "",
    "text": "See Gradescope for any updates on due dates.\nCreate your repo for the HW from GitHub Classroom"
  },
  {
    "objectID": "HW/hw-05.html#due-1000pm-fri-oct-4",
    "href": "HW/hw-05.html#due-1000pm-fri-oct-4",
    "title": "Homework 5",
    "section": "",
    "text": "See Gradescope for any updates on due dates.\nCreate your repo for the HW from GitHub Classroom"
  },
  {
    "objectID": "HW/hw-05.html#rstudio",
    "href": "HW/hw-05.html#rstudio",
    "title": "Homework 5",
    "section": "RStudio",
    "text": "RStudio\nYou all should have R and RStudio installed on your computers by now or access to the Department RStudio Server https://rstudio.stat.duke.edu. If you need to setup your local version, first install the latest version of R here: https://cran.rstudio.com - remember to select the right installer for your operating system). Next, install the latest version of RStudio here: https://www.rstudio.com/products/rstudio/download/. Scroll down to the “Installers for Supported Platforms” section and find the right installer for your operating system."
  },
  {
    "objectID": "HW/hw-05.html#getting-started-with-github-classroom",
    "href": "HW/hw-05.html#getting-started-with-github-classroom",
    "title": "Homework 5",
    "section": "Getting started with Github Classroom",
    "text": "Getting started with Github Classroom\nGo to Github classroom to accept the invitation to create a repository for this assignment.\nThis will create a private repo in the STA721-F24 organization on Github with your github user name. Note: You may receive an email invitation to join STA702-F24 on your behalf.\nHit refresh, to see the link for the repo, and then click on it to go to the STA721-F24 organization in Github.\nFollow the instructions in the README in your repo and in the hw#.Rnw file to complete the assignment and push to GitHub. If you encounter issues with Gradescope submission, it is critical that you have pushed your final assignment to GitHub by the due date to validate timestamps."
  },
  {
    "objectID": "HW/hw-05.html#r-rnw",
    "href": "HW/hw-05.html#r-rnw",
    "title": "Homework 5",
    "section": "R & RNW",
    "text": "R & RNW\nYou are required to use the .Rnw format to type up this report report if there are any R problems. To get started see Knitr with LaTeX article by Karl Broman. Feel free to include images of technical derivations if you are short of time to typeset in LaTeX. The macros.tex file should help with shortcuts. You may need to change some of your Project or Global options to get your document to compile with KnitR. If so please post under Ed in the General category or ask in Lab."
  },
  {
    "objectID": "HW/hw-05.html#gradescope-submission",
    "href": "HW/hw-05.html#gradescope-submission",
    "title": "Homework 5",
    "section": "Gradescope Submission",
    "text": "Gradescope Submission\nYou MUST submit both your final .Rnw .pdf files to the repo on Github and upload the pdf to Gradescope here: https://www.gradescope.com/courses/843802/assignments\nMake sure to to pdf; ask the TA about knitting to pdf if you cannot figure it out. Be sure to submit under the right assignment entry by the due date!"
  },
  {
    "objectID": "HW/hw-05.html#grading",
    "href": "HW/hw-05.html#grading",
    "title": "Homework 5",
    "section": "Grading",
    "text": "Grading\nTotal: 20 points."
  },
  {
    "objectID": "resources/slides/example.html",
    "href": "resources/slides/example.html",
    "title": "Custom blocks and crossreferencing",
    "section": "",
    "text": "Custom blocks and crossreferencing\nWith this filter, you can define custom div classes (environments) that come with numbering, such as theorems, examples, exercises. The filter supports output formats pdf and html.\n\n\nNumbering\nNumbering is (currently) within section for single documents, or within chapter for books. Grouped classes share the same counter, and the same default style.\nNumbered custom blocks can be cross-referenced with \\ref.\nDefault numbering can be switched off for the whole class by setting the numbered to false, or for an individual block by adding the class unnumbered.\nCrossreferences my need a re-run to update.\n\n\nBoxes can be nested\nHowever, inner boxes are not numbered – it would be hard to put them in a sequence with outer boxes anyway.\n\n\n\nThe default style for custom divs is foldbox: a collapsible similar to quarto’s callouts, with a collapse button at the bottom that makes it easier collapse long boxes, and box open to the right. It comes with the variant foldbox.simple, with closed box and no additional close button. (needs a fix for the moment)\n\n\nCustom styles\n\ncreate an API for user defined block styles\nprovide an example\nand documentation\n\n\n\nCustom list of blocks\nGenerate .qmd files that contains a list of selected block classes, intermitted with headers from the document for easy customization and commenting. This way, one can make a list of all definitions, examples, or {theorems, propositions and lemmas} etc., edit it later and attach to the main document. If you edit, make sure to rename the autogenerated list first, otherwise it will get overwritten in the next run and all work is lost …\nCurrently, you need to give a key listin for any class or group of classes that should appear in a list of things. The value of this key is an array of names, also if only one list is to be generated. These names are turned into files list-of-name.qmd. I am considering replacing the yaml interface by a sub-key to custom-numbered-classes. This would allow to define arbitrary classes that can be attached to any custom div block, such as .important.\n\n\n\nPseudomath examples\n\n\nF\\(\\alpha\\)ncybox\nA box is called f\\(\\alpha\\)ncybox if it looks quite fancy.\nIn this context, by fancy we mean that the title of the box appears as a clickable button when rendered as html, where clickable implies that it throws a small shadow that becomes bigger when hovering over it.\n\n\n\nBy Definition \\(\\ref{fancy}\\), foldboxes are fancyboxes.\n\n\nStudents are lured into clicking on the title and unfolding the fancybox.\n\n\nThis extension has been written by a teacher who hopes that students read the course notes…\n\nTheorem \\(\\ref{TeacherHopes}\\) is suggested by Conjecture \\(\\ref{TeachersHope}\\), but it cannot be proven theoretically. It does give rise to more conjectures, though.\n\nThe teacher mentioned in Theorem \\(\\ref{TeacherHopes}\\) is a statistician who got addicted to quarto due to James J Balamuta’s web-r extension, and desparately wanted to have a common counter for theorem and alike. She got also convinced that everything is possible in quarto by the many nice extensions from Shafayet Khan Shafee."
  },
  {
    "objectID": "resources/slides/template.html#outline",
    "href": "resources/slides/template.html#outline",
    "title": "template",
    "section": "Outline",
    "text": "Outline\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/08-bayes.html#outline",
    "href": "resources/slides/08-bayes.html#outline",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Outline",
    "text": "Outline\n\nReadings:\n\nChristensen Chapter 2.9 and Chapter 15\nSeber & Lee Chapter 3.12"
  },
  {
    "objectID": "resources/slides/08-bayes.html#bayes-estimation",
    "href": "resources/slides/08-bayes.html#bayes-estimation",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Bayes Estimation",
    "text": "Bayes Estimation\nModel \\(\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\) with \\(\\boldsymbol{\\epsilon}\\sim \\textsf{N}(\\mathbf{0}_n , \\sigma^2\n  \\mathbf{I}_n)\\) is equivalent to \\[\n\\mathbf{Y}\\sim \\textsf{N}(\\mathbf{X}\\boldsymbol{\\beta}, \\mathbf{I}_n/\\phi)\n\\]\n\n\\(\\phi = 1/\\sigma^2\\) is the precision of the data.\nwe might expect \\(\\boldsymbol{\\beta}\\) to be close to some vector \\(\\mathbf{b}_0\\)\nrepresent this a priori with a Prior Distribution for \\(\\boldsymbol{\\beta}\\), e.g. \\[\\boldsymbol{\\beta}\\sim \\textsf{N}(\\mathbf{b}_0, \\boldsymbol{\\Phi}_0^{-1})\\]\n\\(\\mathbf{b}_0\\) is the prior mean and \\(\\boldsymbol{\\Phi}_0\\) is the prior precision of \\(\\boldsymbol{\\beta}\\) that captures how close \\(\\boldsymbol{\\beta}\\) is to \\(\\mathbf{b}_0\\)\nSimilarly, we could represent prior uncertainty about \\(\\sigma\\), \\(\\sigma^2\\) or equivalently \\(\\phi\\) with a probability distribution\nfor now treat \\(\\phi\\) as fixed"
  },
  {
    "objectID": "resources/slides/08-bayes.html#bayesian-inference",
    "href": "resources/slides/08-bayes.html#bayesian-inference",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Bayesian Inference",
    "text": "Bayesian Inference\n\nonce we see data \\(\\mathbf{Y}\\), Bayesian inference proceeds by updating prior beliefs\nrepresented by the posterior distribution of \\(\\boldsymbol{\\beta}\\) which is the conditional distribution of \\(\\boldsymbol{\\beta}\\) given the data \\(\\mathbf{Y}\\) (and \\(\\phi\\) for now)\nPosterior \\(p(\\boldsymbol{\\beta}\\mid \\mathbf{Y}, \\phi)\\) \\[p(\\boldsymbol{\\beta}\\mid \\mathbf{Y}) = \\frac{p(\\mathbf{Y}\\mid \\boldsymbol{\\beta}, \\phi) p(\\boldsymbol{\\beta}\\mid \\phi)}{c}\\]\n\\(c\\) is a constant so that the posterior density integrates to \\(1\\) \\[c = \\int_{\\mathbb{R}^p} p(\\mathbf{Y}\\mid \\boldsymbol{\\beta}, \\phi) p(\\boldsymbol{\\beta}\\mid \\phi) d\\, \\boldsymbol{\\beta}\\equiv p(\\mathbf{Y})\\]\nsince \\(c\\) is a constant that doesn’t depend on \\(\\boldsymbol{\\beta}\\) just ignore\nwork with density up to constant of proportionality"
  },
  {
    "objectID": "resources/slides/08-bayes.html#posterior-density",
    "href": "resources/slides/08-bayes.html#posterior-density",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Posterior Density",
    "text": "Posterior Density\nPosterior for \\(\\boldsymbol{\\beta}\\) is \\(p(\\boldsymbol{\\beta}\\mid \\mathbf{Y}) \\propto p(\\mathbf{Y}\\mid \\boldsymbol{\\beta}, \\phi) p(\\boldsymbol{\\beta}\\mid \\phi)\\)\n\nLikelihood for \\(\\boldsymbol{\\beta}\\) is proportional to \\(p(\\mathbf{Y}\\mid \\boldsymbol{\\beta}, \\phi\\))\n\n\n\\[\\begin{align*} p(\\mathbf{Y}\\mid \\boldsymbol{\\beta}, \\phi) & = (2 \\pi)^{-n/2} |\\mathbf{I}_n / \\phi |^{-1/2}\n\\exp\\left\\{-\\frac{1}{2} \\left( (\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta})^T \\phi \\mathbf{I}_n (\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}) \\right) \\right\\} \\\\\n& \\propto \\exp\\left\\{-\\frac{1}{2} \\left( \\phi \\mathbf{Y}^T\\mathbf{Y}- 2 \\boldsymbol{\\beta}^T \\phi \\mathbf{X}^T\\mathbf{Y}+ \\phi \\boldsymbol{\\beta}\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\\right) \\right\\}\n\\end{align*}\\]\n\nsimilarly expand prior \\[\\begin{align*}\np(\\boldsymbol{\\beta}\\mid \\phi) & = (2 \\pi)^{-p/2} |\\boldsymbol{\\Phi}_0^{-1}|^{-1/2}\n\\exp\\left\\{-\\frac{1}{2} \\left( (\\boldsymbol{\\beta}- \\mathbf{b}_0)^T \\boldsymbol{\\Phi}_0 (\\boldsymbol{\\beta}- \\mathbf{b}_0) \\right) \\right\\} \\\\\n& \\propto \\exp\\left\\{-\\frac{1}{2} \\left(  \\mathbf{b}_0^T \\boldsymbol{\\Phi}_0\\mathbf{b}_0 - 2 \\boldsymbol{\\beta}^T\\boldsymbol{\\Phi}_0 \\mathbf{b}_0 + \\boldsymbol{\\beta}\\boldsymbol{\\Phi}_0 \\boldsymbol{\\beta}\\right) \\right\\}\n\\end{align*}\\]"
  },
  {
    "objectID": "resources/slides/08-bayes.html#posterior-steps",
    "href": "resources/slides/08-bayes.html#posterior-steps",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Posterior Steps",
    "text": "Posterior Steps\n\nExpand quadratics and regroup terms \\[\\begin{align*}\np(\\boldsymbol{\\beta}\\mid \\mathbf{Y}, \\phi)\n& \\propto e^{\\left\\{-\\frac{1}{2} \\left( \\phi \\boldsymbol{\\beta}\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\beta}\\boldsymbol{\\Phi}_0 \\boldsymbol{\\beta}- 2(\\phi \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{Y}+ \\boldsymbol{\\beta}^T\\boldsymbol{\\Phi}_0 \\mathbf{b}_0)  + \\phi \\mathbf{Y}^T\\mathbf{Y}+ \\mathbf{b}_0^T \\boldsymbol{\\Phi}_0\\mathbf{b}_0 \\right) \\right\\} } \\\\\n&  \\propto \\exp\\left\\{-\\frac{1}{2} \\left( \\boldsymbol{\\beta}( \\phi\\mathbf{X}^T\\mathbf{X}+ \\boldsymbol{\\Phi}_0) \\boldsymbol{\\beta}- 2 \\boldsymbol{\\beta}^T(\\phi \\mathbf{X}^T\\mathbf{Y}+ \\boldsymbol{\\Phi}_0 \\mathbf{b}_0)  \\right) \\right\\}  \n\\end{align*}\\]\n\n\nKernel of a Multivariate Normal\n\nRead off posterior precision from Quadratic in \\(\\boldsymbol{\\beta}\\)\nRead off posterior precision \\(\\times\\) posterior mean from Linear term in \\(\\boldsymbol{\\beta}\\)\n\nwill need to complete the quadratic in the posterior mean\\(^{\\dagger}\\)\n\n\n\\(\\dagger\\) necessary to keep track of all terms for \\(\\phi\\) when we do not condition on \\(\\phi\\)"
  },
  {
    "objectID": "resources/slides/08-bayes.html#posterior-precision-and-covariance",
    "href": "resources/slides/08-bayes.html#posterior-precision-and-covariance",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Posterior Precision and Covariance",
    "text": "Posterior Precision and Covariance\n\\[ p(\\boldsymbol{\\beta}\\mid \\mathbf{Y}, \\phi)  \\propto \\exp\\left\\{-\\frac{1}{2} \\left( \\boldsymbol{\\beta}( \\phi\\mathbf{X}^T\\mathbf{X}+ \\boldsymbol{\\Phi}_0) \\boldsymbol{\\beta}- 2 \\boldsymbol{\\beta}^T(\\phi \\mathbf{X}^T\\mathbf{Y}+ \\boldsymbol{\\Phi}_0 \\mathbf{b}_0)  \\right) \\right\\}\n\\]\n\nPosterior Precision \\[\\boldsymbol{\\Phi}_n \\equiv \\phi\\mathbf{X}^T\\mathbf{X}+ \\boldsymbol{\\Phi}_0\\]\nsum of data precision and prior precision\nposterior Covariance \\[\\textsf{Cov}[\\boldsymbol{\\beta}\\mid \\mathbf{Y}, \\phi] = \\boldsymbol{\\Phi}_n^{-1} = (\\phi\\mathbf{X}^T\\mathbf{X}+ \\boldsymbol{\\Phi}_0)^{-1}\\]\nif \\(\\boldsymbol{\\Phi}_0\\) is full rank, then \\(\\textsf{Cov}[\\boldsymbol{\\beta}\\mid \\mathbf{Y}, \\phi]\\) is full rank even if \\(\\mathbf{X}^T\\mathbf{X}\\) is not"
  },
  {
    "objectID": "resources/slides/08-bayes.html#posterior-mean-updating",
    "href": "resources/slides/08-bayes.html#posterior-mean-updating",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Posterior Mean Updating",
    "text": "Posterior Mean Updating\n\\[\\begin{align*} p(\\boldsymbol{\\beta}\\mid \\mathbf{Y}, \\phi) & \\propto \\exp\\left\\{\\frac{1}{2} \\left( \\boldsymbol{\\beta}( \\phi\\mathbf{X}^T\\mathbf{X}+ \\boldsymbol{\\Phi}_0) \\boldsymbol{\\beta}- 2 \\boldsymbol{\\beta}^T(\\phi \\mathbf{X}^T\\mathbf{Y}+ \\boldsymbol{\\Phi}_0 \\mathbf{b}_0)  \\right) \\right\\} \\\\\n& \\propto \\exp\\left\\{\\frac{1}{2} \\left( \\boldsymbol{\\beta}( \\phi\\mathbf{X}^T\\mathbf{X}+ \\boldsymbol{\\Phi}_0) \\boldsymbol{\\beta}- 2 \\boldsymbol{\\beta}^T\\boldsymbol{\\Phi}_n \\boldsymbol{\\Phi}_n^{-1}(\\phi \\mathbf{X}^T\\mathbf{Y}+ \\boldsymbol{\\Phi}_0 \\mathbf{b}_0)  \\right) \\right\\} \\\\\n\\end{align*}\\]\n\nposterior mean \\(\\mathbf{b}_n\\) \\[\\begin{align*}\n\\mathbf{b}_n & \\equiv \\boldsymbol{\\Phi}_n^{-1} (\\phi \\mathbf{X}^T\\mathbf{Y}+ \\boldsymbol{\\Phi}_0 \\mathbf{b}_0 ) \\\\\n    & = (\\phi\\mathbf{X}^T\\mathbf{X}+ \\boldsymbol{\\Phi}_0)^{-1}\\left(\\phi (\\mathbf{X}^T\\mathbf{X}) (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{Y}+ \\boldsymbol{\\Phi}_0 \\mathbf{b}_0 \\right) \\\\\n    & = (\\phi\\mathbf{X}^T\\mathbf{X}+ \\boldsymbol{\\Phi}_0)^{-1} \\left( \\phi (\\mathbf{X}^T\\mathbf{X}) \\hat{\\boldsymbol{\\beta}}+ \\boldsymbol{\\Phi}_0 \\mathbf{b}_0 \\right)\n\\end{align*}\\]\na precision weighted linear combination of MLE and prior mean\nfirst expression useful if \\(\\mathbf{X}\\) is not full rank!"
  },
  {
    "objectID": "resources/slides/08-bayes.html#notes",
    "href": "resources/slides/08-bayes.html#notes",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Notes",
    "text": "Notes\nPosterior is a Multivariate Normal \\(p(\\boldsymbol{\\beta}\\mid \\mathbf{Y}, \\phi) \\sim \\textsf{N}(\\mathbf{b}_n, \\boldsymbol{\\Phi}_n^{-1})\\)\n\nposterior mean: \\(\\mathbf{b}_n  =  \\boldsymbol{\\Phi}_n^{-1} (\\phi \\mathbf{X}^T\\mathbf{Y}+ \\boldsymbol{\\Phi}_0 \\mathbf{b}_0 )\\)\nposterior precision: \\(\\boldsymbol{\\Phi}_n = \\phi\\mathbf{X}^T\\mathbf{X}+ \\boldsymbol{\\Phi}_0\\)\nthe posterior precision (inverse posterior variance) is the sum of the prior precision and the data precision.\nthe posterior mean is a linear combination of MLE/OLS and prior mean\nif the prior precision \\(\\boldsymbol{\\Phi}_n\\) is very large compared to the data precision \\(\\phi \\mathbf{X}^T\\mathbf{X}\\), the posterior mean will be close to the prior mean \\(\\mathbf{b}_0\\).\nif the prior precision \\(\\boldsymbol{\\Phi}_n\\) is very small compared to the data precision \\(\\phi \\mathbf{X}^T\\mathbf{X}\\), the posterior mean will be close to the MLE/OLS estimator.\ndata precision will generally be increasing with sample size"
  },
  {
    "objectID": "resources/slides/08-bayes.html#bayes-estimators",
    "href": "resources/slides/08-bayes.html#bayes-estimators",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Bayes Estimators",
    "text": "Bayes Estimators\nA Bayes estimator is a potential value of \\(\\boldsymbol{\\beta}\\) that is obtained from the posterior distribution in some principled way.\n\nStandard estimators include\n\nthe posterior mean estimator, which is the minimizer of the Bayes risk under squared error loss\nthe maximum a posteriori (MAP) estimator, the value \\(\\boldsymbol{\\beta}\\) that maximizes the posterior density (or log posterior density)\n\nThe first estimator is based on principles from classical decision theory, whereas the second can be related to penalized likelihood estimation.\nin the case of linear regression they turn out to be the same estimator!"
  },
  {
    "objectID": "resources/slides/08-bayes.html#bayes-estimator-under-squared-error-loss",
    "href": "resources/slides/08-bayes.html#bayes-estimator-under-squared-error-loss",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Bayes Estimator under Squared Error Loss",
    "text": "Bayes Estimator under Squared Error Loss\n\nthe Frequentist Risk \\(R(\\beta, \\delta) \\equiv \\textsf{E}_{\\mathbf{Y}\\mid \\boldsymbol{\\beta}}[\\| \\delta(\\mathbf{Y})− \\boldsymbol{\\beta}\\|^2]\\) is the expected loss of decision \\(\\delta\\) for a given \\(\\boldsymbol{\\beta}\\)\n\n\n\nDefinition: Bayes Rule and Bayes RiskThe Bayes rule under squared error loss is the function of \\(\\mathbf{Y}\\), \\(\\delta^*(\\mathbf{Y})\\), that minimizes the Bayes risk \\(B(p_\\boldsymbol{\\beta}, \\delta)\\) \\[\\delta^*(\\mathbf{Y}) =  \\arg \\min_{\\delta \\in \\cal{D}} B(p_\\boldsymbol{\\beta}, \\delta)\\]\n\\[B(p_\\boldsymbol{\\beta}, \\delta) = \\textsf{E}_\\boldsymbol{\\beta}R(\\boldsymbol{\\beta}, \\delta) = \\textsf{E}_{\\boldsymbol{\\beta}} \\textsf{E}_{\\mathbf{Y}\\mid \\boldsymbol{\\beta}}[\\| \\delta(\\mathbf{Y})− \\boldsymbol{\\beta}\\|^2]\\] where the expectation is with respect to the prior distribution, \\(p_\\boldsymbol{\\beta}\\), over \\(\\boldsymbol{\\beta}\\) and the conditional distribution of \\(\\mathbf{Y}\\) given \\(\\boldsymbol{\\beta}\\)"
  },
  {
    "objectID": "resources/slides/08-bayes.html#bayes-estimators-1",
    "href": "resources/slides/08-bayes.html#bayes-estimators-1",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Bayes Estimators",
    "text": "Bayes Estimators\n\nDefinition: Bayes ActionThe Bayes Action is the action \\(a \\in {\\cal{A}}\\) that minimizes the posterior expected loss: \\[ \\delta_B^*(\\mathbf{Y}) = \\arg \\min_{\\delta \\in \\cal{D}} E_{\\boldsymbol{\\beta}\\mid \\mathbf{Y}} [\\| \\delta − \\boldsymbol{\\beta}\\|^2]\n\\]\n\n\n\ncan show that the Bayes action that minimizes the posterior expected loss is the posterior mean \\(\\boldsymbol{\\beta}_n = (\\phi \\mathbf{X}^T\\mathbf{X}+ \\boldsymbol{\\Phi}_0)^{-1}(\\phi \\mathbf{X}^T\\mathbf{Y}+ \\boldsymbol{\\Phi}_0 \\mathbf{b}_0\\) and is also the Bayes rule.\ndifferent values of \\(\\mathbf{b}_0\\) and \\(\\boldsymbol{\\Phi}_0\\) will lead to different Bayes estimators as will different prior distributions besides the Normal\ntake \\(\\mathbf{b}_0 = \\mathbf{0}\\); Bayes estimators are often referred to as shrinkage estimators \\[\\boldsymbol{\\beta}_n = \\left( \\mathbf{X}^T\\mathbf{X}+ \\boldsymbol{\\Phi}_0/\\phi  \\right)^{-1}  \\mathbf{X}^T\\mathbf{Y}\\] as they shrink the MLE/OLS estimator towards \\(\\mathbf{0}\\)"
  },
  {
    "objectID": "resources/slides/08-bayes.html#prior-choice",
    "href": "resources/slides/08-bayes.html#prior-choice",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Prior Choice",
    "text": "Prior Choice\nOne of the most common priors for the normal linear model is the g-prior of Zellner (1986) where \\(\\boldsymbol{\\Phi}_0 = \\frac{\\phi}{g} \\mathbf{X}^T\\mathbf{X}\\) \\[\\boldsymbol{\\beta}\\mid \\phi, g \\sim \\textsf{N}(\\mathbf{0}, g/\\phi (\\mathbf{X}^T\\mathbf{X})^{-1})\\]\n\n\\[\\begin{align*}\n\\mathbf{b}_n & = \\left( \\mathbf{X}^T\\mathbf{X}+ \\frac{\\phi}{g} \\frac{\\mathbf{X}^T\\mathbf{X}}{\\phi} \\right)^{-1} \\mathbf{X}^T\\mathbf{Y}\\\\\n  & = \\left( \\mathbf{X}^T\\mathbf{X}+ \\frac{1}{g} \\mathbf{X}^T\\mathbf{X}\\right)^{-1} \\mathbf{X}^T\\mathbf{Y}\\\\\n  & = \\left( \\frac{1 +g}{g} \\mathbf{X}^T\\mathbf{X}\\right)^{-1} \\mathbf{X}^T\\mathbf{Y}\\\\\n  & = \\frac{g}{1+g} \\hat{\\boldsymbol{\\beta}}\n\\end{align*}\\]\n\n\\(g\\) controls the amount of shrinkage where all of the MLEs are shrunk to zero by the same fraction \\(g/(1+g)\\)"
  },
  {
    "objectID": "resources/slides/08-bayes.html#another-common-choice",
    "href": "resources/slides/08-bayes.html#another-common-choice",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Another Common Choice",
    "text": "Another Common Choice\n\nanother common choice is the independent prior \\[\\boldsymbol{\\beta}\\mid \\phi \\sim \\textsf{N}(\\mathbf{0}, \\boldsymbol{\\Phi}_0^{-1})\\] where \\(\\boldsymbol{\\Phi}_0 = \\phi \\kappa \\mathbf{I}_b\\) for some \\(\\kappa&gt; 0\\)\nthe posterior mean is \\[\\begin{align*}\n\\boldsymbol{\\beta}_n & = (\\mathbf{X}^T\\mathbf{X}+ \\kappa \\mathbf{I})^{-1} \\mathbf{X}^T\\mathbf{Y}\\\\\n   & =  (\\mathbf{X}^T\\mathbf{X}+ \\kappa \\mathbf{I})^{-1} \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\end{align*}\\]\nthis is also a shrinkage estimator but the amount of shrinkage is different for the different components of \\(\\mathbf{b}_n\\) depending on the eigenvalues of \\(\\mathbf{X}^T\\mathbf{X}\\)\neasiest to see this via an orthogonal rotation of the model"
  },
  {
    "objectID": "resources/slides/08-bayes.html#rotated-regression",
    "href": "resources/slides/08-bayes.html#rotated-regression",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Rotated Regression",
    "text": "Rotated Regression\n\nUse the singular value decomposition of \\(\\mathbf{X}= \\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{V}^T\\) and multiply thru by \\(\\mathbf{U}^T\\) to get the rotated model \\[\\begin{align*}\n\\mathbf{U}^T \\mathbf{Y}& =  \\boldsymbol{\\Lambda}\\mathbf{V}^T\\boldsymbol{\\beta}+ \\mathbf{U}^T\\boldsymbol{\\epsilon}\\\\\n\\tilde{\\mathbf{Y}}& = \\boldsymbol{\\Lambda}\\boldsymbol{\\alpha}+ \\tilde{\\boldsymbol{\\epsilon}}\n\\end{align*}\\] where \\(\\boldsymbol{\\alpha}= \\mathbf{V}^T\\boldsymbol{\\beta}\\) and \\(\\tilde{\\boldsymbol{\\epsilon}} = \\mathbf{U}^T\\boldsymbol{\\epsilon}\\)\nthe induced prior is still \\(\\boldsymbol{\\alpha}\\mid \\phi \\sim \\textsf{N}(\\mathbf{0}, (\\phi \\kappa)^{-1} \\mathbf{I})\\)\nthe posterior mean of \\(\\boldsymbol{\\alpha}\\) is \\[\\begin{align*}\n\\mathbf{a}& =  (\\boldsymbol{\\Lambda}^2 + \\kappa \\mathbf{I})^{-1} \\boldsymbol{\\Lambda}^2 \\hat{\\boldsymbol{\\alpha}}\\\\\na_j & = \\frac{\\lambda_j^2}{\\lambda_j^2 + \\kappa} \\hat{\\alpha}_j\n\\end{align*}\\]\nsets to zero the components of the OLS solution where eigenvalues are zero!"
  },
  {
    "objectID": "resources/slides/08-bayes.html#connections-to-frequentist-estimators",
    "href": "resources/slides/08-bayes.html#connections-to-frequentist-estimators",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Connections to Frequentist Estimators",
    "text": "Connections to Frequentist Estimators\n\nThe posterior mean under this independent prior is the same as the classic ridge regression estimator of Hoerl and\nthe variance of \\(\\hat{\\alpha}_j\\) is \\(\\sigma^2/\\lambda_j^2\\) while the variance of \\(a_j\\) is \\(\\sigma^2/(\\lambda_j^2 + \\kappa)\\)\nclearly components of \\(\\boldsymbol{\\alpha}\\) with small eigenvalues will have large variances\nridge regression keeps those components from “blowing up” by shrinking them towards zero and having a finite variance\nrotate back to get the ridge estimator for \\(\\boldsymbol{\\beta}\\), \\(\\hat{\\boldsymbol{\\beta}}_R = \\mathbf{V}\\mathbf{a}\\)\nridge regression applies a high degree of shrinkage to the “parts” (linear combinations) of \\(\\boldsymbol{\\beta}\\) that have high variability, and a low degree of shrinkage to the parts that are well-estimated.\nturns out there always exists a value of \\(\\kappa\\) that will improve over OLS!\n\nUnfortunately no closed form solution except in orthogonal regression and then it depends on the unknown \\(\\|\\boldsymbol{\\beta}\\|^2\\)!"
  },
  {
    "objectID": "resources/slides/08-bayes.html#next-class",
    "href": "resources/slides/08-bayes.html#next-class",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Next Class",
    "text": "Next Class\n\nFrequentist risk of Bayes estimators\nBayes and penalized loss functions\n\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#outline",
    "href": "resources/slides/05-BLUE-MVUE.html#outline",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Outline",
    "text": "Outline\n\nGauss-Markov Theorem for non-full rank \\(\\mathbf{X}\\) (recap)\nBest Linear Unbiased Estimators for Prediction\nMVUE\nDiscussion of recent papers on Best Unbiased Estimators beyond linearity\n\n\nReadings:\n\nChristensen Chapter 2 (Appendix B as needed)\nSeber & Lee Chapter 3\nFor the curious:\n\nAndersen (1962) Least squares and best unbiased estimates\nHansen (2022) A modern gauss-markov theorem\nWhat Estimators are Unbiased for Linear Models (2023) and references within"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#identifiability",
    "href": "resources/slides/05-BLUE-MVUE.html#identifiability",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Identifiability",
    "text": "Identifiability\n\nDefinition: Identifiable\\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) are identifiable if the distribution of \\(\\mathbf{Y}\\), \\(f_\\mathbf{Y}(\\mathbf{y};\n\\boldsymbol{\\beta}_1, \\sigma^2_1) = f_\\mathbf{Y}(\\mathbf{y};\n\\boldsymbol{\\beta}_2, \\sigma^2_2)\\) implies that \\((\\boldsymbol{\\beta}_1, \\sigma^2_1)^T =  (\\boldsymbol{\\beta}_2, \\sigma^2_2)^T\\)\n\n\n\nFor linear models, equivalent definition is that \\(\\boldsymbol{\\beta}\\) is identifiable if for any \\(\\boldsymbol{\\beta}_1\\) and \\(\\boldsymbol{\\beta}_2\\), \\(\\mu(\\boldsymbol{\\beta}_1)  = \\mu(\\boldsymbol{\\beta}_2)\\) or \\(\\mathbf{X}\\boldsymbol{\\beta}_1 =\\mathbf{X}\\boldsymbol{\\beta}_2\\) implies that \\(\\boldsymbol{\\beta}_1 = \\boldsymbol{\\beta}_2\\).\nIf \\(r(\\mathbf{X}) = p\\) then \\(\\boldsymbol{\\beta}\\) is identifiable\nIf \\(\\mathbf{X}\\) is not full rank, there exists \\(\\boldsymbol{\\beta}_1 \\neq \\boldsymbol{\\beta}_2\\), but \\(\\mathbf{X}\\boldsymbol{\\beta}_1 =\n\\mathbf{X}\\boldsymbol{\\beta}_2\\) and hence \\(\\boldsymbol{\\beta}\\) is not identifiable!\nidentifiable linear functions of \\(\\boldsymbol{\\beta}\\), \\(\\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\) that have an unbiased estimator are historically referred to as estimable in linear models."
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#blue-of-boldsymbollambdat-boldsymbolbeta",
    "href": "resources/slides/05-BLUE-MVUE.html#blue-of-boldsymbollambdat-boldsymbolbeta",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "BLUE of \\(\\boldsymbol{\\Lambda}^T \\boldsymbol{\\beta}\\)",
    "text": "BLUE of \\(\\boldsymbol{\\Lambda}^T \\boldsymbol{\\beta}\\)\nIf \\(\\boldsymbol{\\Lambda}^T= \\mathbf{B}\\mathbf{X}\\) for some matrix \\(\\mathbf{B}\\) (or \\(\\boldsymbol{\\Lambda}= \\mathbf{X}^T\\mathbf{B}\\) then\n\n\\(\\textsf{E}[\\mathbf{B}\\mathbf{P}\\mathbf{Y}] = \\textsf{E}[\\mathbf{B}\\mathbf{X}\\hat{\\boldsymbol{\\beta}}] = \\textsf{E}[\\boldsymbol{\\Lambda}^T \\hat{\\boldsymbol{\\beta}}] = \\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\)\nidentifiable as it is a function of \\(\\boldsymbol{\\mu}\\), linear and unbiased\nThe unique OLS estimate of \\(\\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\) is \\(\\boldsymbol{\\Lambda}^T\\hat{\\boldsymbol{\\beta}}\\)\n\\(\\mathbf{B}\\mathbf{P}\\mathbf{Y}= \\boldsymbol{\\Lambda}^T\\hat{\\boldsymbol{\\beta}}\\) is the BLUE of \\(\\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\) \\[\\begin{align*}\n& \\textsf{E}[\\|\\mathbf{B}\\mathbf{P}\\mathbf{Y}- \\mathbf{B}\\boldsymbol{\\mu}\\|^2]  \\le \\textsf{E}[\\|\\mathbf{A}\\mathbf{Y}- \\mathbf{B}\\boldsymbol{\\mu}\\|^2] \\\\\n\\Leftrightarrow & \\\\\n& \\textsf{E}[\\|\\boldsymbol{\\Lambda}^T\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta})\\|^2]  \\le \\textsf{E}[\\|\\mathbf{L}^T\\tilde{\\boldsymbol{\\beta}}- \\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\|^2]\n\\end{align*}\\] for LUE \\(\\mathbf{A}\\mathbf{Y}= \\mathbf{L}^T\\tilde{\\boldsymbol{\\beta}}\\) of \\(\\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\)"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#non-identifiable-example",
    "href": "resources/slides/05-BLUE-MVUE.html#non-identifiable-example",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Non-Identifiable Example",
    "text": "Non-Identifiable Example\nOne-way ANOVA model \\[\\mu_{ij} = \\mu + \\tau_j \\qquad \\boldsymbol{\\mu}= (\n    \\mu_{11}, \\ldots,\\mu_{n_1 1},\\mu_{12},\\ldots, \\mu_{n_2,2},\\ldots, \\mu_{1J},\n\\ldots,\n\\mu_{n_J J})^T \\]\n\nLet \\(\\boldsymbol{\\beta}_{1} = (\\mu, \\tau_1, \\ldots, \\tau_J)^T\\)\nLet \\(\\boldsymbol{\\beta}_{2} = (\\mu - 42, \\tau_1 + 42, \\ldots, \\tau_J + 42)^T\\)\nThen \\(\\boldsymbol{\\mu}_{1} = \\boldsymbol{\\mu}_{2}\\) even though \\(\\boldsymbol{\\beta}_1 \\neq \\boldsymbol{\\beta}_2\\)\n\\(\\boldsymbol{\\beta}\\) is not identifiable\nyet \\(\\boldsymbol{\\mu}\\) is identifiable, where \\(\\boldsymbol{\\mu}= \\mathbf{X}\\boldsymbol{\\beta}\\) (a linear combination of \\(\\boldsymbol{\\beta}\\))"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#lues-of-individual-beta_j",
    "href": "resources/slides/05-BLUE-MVUE.html#lues-of-individual-beta_j",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "LUEs of Individual \\(\\beta_j\\)",
    "text": "LUEs of Individual \\(\\beta_j\\)\n\nProposition: Christensen 2.1.6For \\(\\boldsymbol{\\mu}= \\mathbf{X}\\boldsymbol{\\beta}= \\sum_j \\mathbf{X}_j \\beta_j\\) \\(\\beta_j\\) is not identifiable if and only if there exists \\(\\alpha_j\\) such that \\(\\mathbf{X}_j = \\sum_{i \\neq j} \\mathbf{X}_i \\alpha_i\\)\n\n\n\nOne-way Anova Model: \\(Y_{ij} = \\mu + \\tau_j + \\epsilon_{ij}\\) \\[\\boldsymbol{\\mu}=  \\left[\n    \\begin{array}{lllll}\n\\mathbf{1}_{n_1} & \\mathbf{1}_{n_1} & \\mathbf{0}_{n_1} &  \\ldots & \\mathbf{0}_{n_1} \\\\\n\\mathbf{1}_{n_2} & \\mathbf{0}_{n_2} & \\mathbf{1}_{n_2} &  \\ldots & \\mathbf{0}_{n_2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbf{1}_{n_J} & \\mathbf{0}_{n_J} & \\mathbf{0}_{n_J} &  \\ldots & \\mathbf{1}_{n_J} \\\\\n    \\end{array} \\right]\n\\left(   \\begin{array}{l}\n      \\mu \\\\\n      \\tau_1 \\\\\n   \\tau_2 \\\\\n\\vdots \\\\\n\\tau_J\n    \\end{array} \\right)\n\\]\n\nAre any parameters \\(\\mu\\) or \\(\\tau_j\\) identifiable?"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#examples-of-boldsymbollambda-of-interest",
    "href": "resources/slides/05-BLUE-MVUE.html#examples-of-boldsymbollambda-of-interest",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Examples of \\(\\boldsymbol{\\lambda}\\) of Interest:",
    "text": "Examples of \\(\\boldsymbol{\\lambda}\\) of Interest:\n\nA \\(j\\)th element of \\(\\boldsymbol{\\beta}\\): \\(\\boldsymbol{\\lambda}= (0, 0, \\ldots,1, 0, \\ldots, 0)^T\\), \\[\\boldsymbol{\\lambda}^T\\boldsymbol{\\beta}= \\beta_j\\]\nDifference between two treatements: \\(\\tau_1 - \\tau_2\\): \\(\\boldsymbol{\\lambda}= (0, 1, -1, \\ldots, 0, \\ldots, 0)^T\\), \\[\\boldsymbol{\\lambda}^T\\boldsymbol{\\beta}= \\tau_1 - \\tau_2\\]\nEstimation at observed \\(\\mathbf{x}_i\\): \\(\\boldsymbol{\\lambda}= \\mathbf{x}_i\\) \\[\\mu_i = \\mathbf{x}_i^T \\boldsymbol{\\beta}\\]\nEstimation or prediction at a new point \\(\\mathbf{x}_*\\): \\(\\boldsymbol{\\lambda}= \\mathbf{x}_*\\), \\[\\mu_* = \\mathbf{x}_*^T \\boldsymbol{\\beta}\\]"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#another-non-full-rank-example",
    "href": "resources/slides/05-BLUE-MVUE.html#another-non-full-rank-example",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Another Non-Full Rank Example",
    "text": "Another Non-Full Rank Example\n\nx1 = -4:4\nx2 = c(-2, 1, -1, 2, 0, 2, -1, 1, -2)\nx3 = 3*x1  -2*x2\nx4 = x2 - x1 + 4\nY = 1+x1+x2+x3+x4 + c(-.5,.5,.5,-.5,0,.5,-.5,-.5,.5)\ndev.set = data.frame(Y, x1, x2, x3, x4)\n\n# Order 1\nlm1234 = lm(Y ~ x1 + x2 + x3 + x4, data=dev.set)\nround(coefficients(lm1234), 4)\n\n(Intercept)          x1          x2          x3          x4 \n          5           3           0          NA          NA \n\n# Order 2\nlm3412 = lm(Y ~ x3 + x4 + x1 + x2, data = dev.set)\nround(coefficients(lm3412), 4)\n\n(Intercept)          x3          x4          x1          x2 \n        -19           3           6          NA          NA"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#in-sample-predictions",
    "href": "resources/slides/05-BLUE-MVUE.html#in-sample-predictions",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "In Sample Predictions",
    "text": "In Sample Predictions\n\ncbind(dev.set, predict(lm1234), predict(lm3412))\n\n     Y x1 x2  x3 x4 predict(lm1234) predict(lm3412)\n1 -7.5 -4 -2  -8  6              -7              -7\n2 -3.5 -3  1 -11  8              -4              -4\n3 -0.5 -2 -1  -4  5              -1              -1\n4  1.5 -1  2  -7  7               2               2\n5  5.0  0  0   0  4               5               5\n6  8.5  1  2  -1  5               8               8\n7 10.5  2 -1   8  1              11              11\n8 13.5  3  1   7  2              14              14\n9 17.5  4 -2  16 -2              17              17\n\n\n\nBoth models agree for estimating the mean at the observed \\(\\mathbf{X}\\) points!"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#out-of-sample",
    "href": "resources/slides/05-BLUE-MVUE.html#out-of-sample",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Out of Sample",
    "text": "Out of Sample\n\nout = data.frame(test.set,\n      Y1234=predict(lm1234, new=test.set),\n      Y3412=predict(lm3412, new=test.set))\nout\n\n  x1 x2 x3 x4 Y1234 Y3412\n1  3  1  7  2    14    14\n2  6  2 14  4    23    47\n3  6  2 14  0    23    23\n4  0  0  0  4     5     5\n5  0  0  0  0     5   -19\n6  1  2  3  4     8    14\n\n\n\nAgreement for cases 1, 3, and 4 only!\nCan we determine that without finding the predictions and comparing?\nConditions for general \\(\\boldsymbol{\\Lambda}\\) or \\(\\boldsymbol{\\lambda}\\) without findingn \\(\\mathbf{B}\\) (\\(\\boldsymbol{\\beta}^T\\))?"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#conditions-for-lue-of-boldsymbollambda",
    "href": "resources/slides/05-BLUE-MVUE.html#conditions-for-lue-of-boldsymbollambda",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Conditions for LUE of \\(\\boldsymbol{\\lambda}\\)",
    "text": "Conditions for LUE of \\(\\boldsymbol{\\lambda}\\)\n\nGM requires that \\(\\boldsymbol{\\lambda}^T = \\mathbf{b}^T\\mathbf{X}\\Leftrightarrow \\boldsymbol{\\lambda}= \\mathbf{X}^T \\mathbf{b}\\) therefore \\(\\boldsymbol{\\lambda}\\in C(\\mathbf{X}^T)\\)\nSuppose we have an arbitrary \\(\\boldsymbol{\\lambda}= \\boldsymbol{\\lambda}_* + \\mathbf{u}\\), where \\(\\boldsymbol{\\lambda}_*  \\in C(\\mathbf{X}^T)\\) and \\(\\mathbf{u}\\in C(\\mathbf{X}^T)^\\perp\\) (orthogonal complement)\nLet \\(\\mathbf{P}_{\\mathbf{X}^T}\\) denote an orthogonal projection onto \\(C(\\mathbf{X}^T)\\) then \\(\\mathbf{I}- \\mathbf{P}_{\\mathbf{X}^T}\\) is an orthogonal projection onto \\(C(\\mathbf{X}^T)^\\perp\\)\n\\((\\mathbf{I}- \\mathbf{P}_{\\mathbf{X}^T})\\boldsymbol{\\lambda}= (\\mathbf{I}- \\mathbf{P}_{\\mathbf{X}^T})\\boldsymbol{\\lambda}_* + (\\mathbf{I}- \\mathbf{P}_{\\mathbf{X}^T})\\mathbf{u}= \\mathbf{0}_p + \\mathbf{u}\\)\nso if \\(\\boldsymbol{\\lambda}\\in C(\\mathbf{X}^T)\\) we will have \\((\\mathbf{I}- \\mathbf{P}_{\\mathbf{X}^T})\\boldsymbol{\\lambda}= \\mathbf{0}_p\\)! (or \\(\\mathbf{P}_{\\mathbf{X}^T} \\boldsymbol{\\lambda}= \\boldsymbol{\\lambda}\\))\nNote this is really just a generalization of Proposition 2.1.6 in Christensen that \\(\\beta_j\\) is not identifiable iff there exist scalars such that \\(\\mathbf{X}_j = \\sum_{i \\neq j} \\mathbf{X}_i \\alpha_i\\)"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#prediction-example-again",
    "href": "resources/slides/05-BLUE-MVUE.html#prediction-example-again",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Prediction Example Again",
    "text": "Prediction Example Again\nFor prediction at a new \\(\\mathbf{x}_*\\), this is implemented in the R package estimability\n\nrequire(\"estimability\" )\ncbind(epredict(lm1234, test.set), epredict(lm3412, test.set))\n\n  [,1] [,2]\n1   14   14\n2   NA   NA\n3   23   23\n4    5    5\n5   NA   NA\n6   NA   NA\n\n\nRows 2, 5, and 6 do not have a unique best linear unbiased estimator, \\(\\mathbf{x}_*^T \\boldsymbol{\\beta}\\)"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#mvue-minimum-variance-unbiased-estimators",
    "href": "resources/slides/05-BLUE-MVUE.html#mvue-minimum-variance-unbiased-estimators",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "MVUE: Minimum Variance Unbiased Estimators",
    "text": "MVUE: Minimum Variance Unbiased Estimators\n\nGauss-Markov Theorem says that OLS has minimum variance in the class of all Linear Unbiased estimators for \\(\\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\) and \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\sigma^2 \\mathbf{I}_n\\)\nRequires just first and second moments\nAdditional assumption of normality and full rank, OLS of \\(\\boldsymbol{\\beta}\\) is the same as MLEs and have minimum variance out of ALL unbiased estimators (MVUE); not just linear estimators (section 2.5 in Christensen)\nrequires Complete Sufficient Statististics and Rao-Blackwell Theorem - next semester in STA732)\nso Best Unbiased Estimators (BUE) not just BLUE!"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#what-about",
    "href": "resources/slides/05-BLUE-MVUE.html#what-about",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "What about ?",
    "text": "What about ?\n\nare there nonlinear estimators that are better than OLS under the assumptions ?\nAnderson (1962) showed OLS is not generally the MVUE with \\(\\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\) and \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\sigma^2 \\mathbf{I}_n\\)\npointed out that linear-plus-quadratic (LPQ) estimators can outperform the OLS estimator for certain error distributions.\nOther assumptions on \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\boldsymbol{\\Sigma}\\)?\n\nGeneralized Least Squares are BLUE (not necessarily equivalent to OLS)\n\nmore recently Hansen (2022) concludes that OLS is BUE over the broader class of linear models with \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}]\\) finite and \\(\\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\)\nlively ongoing debate! - see What Estimators are Unbiased for Linear Models (2023) and references within"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#next-up",
    "href": "resources/slides/05-BLUE-MVUE.html#next-up",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Next Up",
    "text": "Next Up\n\nGLS under assumptions \\(\\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\) and \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\boldsymbol{\\Sigma}\\)\nOblique projections and orthogonality with other inner products on \\(\\mathbb{R}^n\\)\nMLEs in Multivariate Normal setting\nGauss-Markov\n\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#outline",
    "href": "resources/slides/11-shrinkage.html#outline",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Outline",
    "text": "Outline\n\nLasso\nBayesian Lasso\n\n\n\nReadings (see reading link)\n\nSeber & Lee Chapter Chapter 12\nTibshirani (JRSS B 1996)\nPark & Casella (JASA 2008)\nHans (Biometrika 2010)\nCarvalho, Polson & Scott (Biometrika 2010)"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#lasso-estimator",
    "href": "resources/slides/11-shrinkage.html#lasso-estimator",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "LASSO Estimator",
    "text": "LASSO Estimator\n\n\nTibshirani (JRSS B 1996) proposed estimating coefficients through \\(L_1\\) constrained least squares via the Least Absolute Shrinkage and Selection Operator or lasso \\[\\hat{\\boldsymbol{\\beta}}_{L} = \\mathop{\\mathrm{argmin}}_{\\boldsymbol{\\beta}} \\left\\{ \\|\\mathbf{Y}_c - \\mathbf{X}_s \\boldsymbol{\\beta}\\|^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_1 \\right\\}\\]\n\n\\(\\mathbf{Y}_c\\) is the centered \\(\\mathbf{Y}\\), \\(\\mathbf{Y}_c =    \\mathbf{Y}- \\bar{\\mathbf{Y}} \\mathbf{1}\\)\n\\(\\mathbf{X}_s\\) is the centered and standardized \\(\\mathbf{X}\\) matrix so that the diagonal elements of \\(\\mathbf{X}_s^T\\mathbf{X}_s = c\\).\nuse the scale function but standardization usually handled within packages\n\n\n\n\nControl how large coefficients may grow \\[\\arg \\min_{\\boldsymbol{\\beta}} (\\mathbf{Y}_c - \\mathbf{X}_s \\boldsymbol{\\beta})^T (\\mathbf{Y}_c - \\mathbf{X}_s\\boldsymbol{\\beta})\\]\n\\[ \\text{ subject to } \\sum |\\beta_j| \\le t\\]\n\n\n\nimage from Machine Learning with R"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#lasso-solutions",
    "href": "resources/slides/11-shrinkage.html#lasso-solutions",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Lasso Solutions",
    "text": "Lasso Solutions\nThe entire path of solutions can be easily found using the ``Least Angle Regression’’ Algorithm of Efron et al (Annals of Statistics 2004)\n\nlibrary(lars); datasets::longley\nlongley.lars = lars(as.matrix(longley[,-7]), longley[,7], type=\"lasso\")"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#coefficients",
    "href": "resources/slides/11-shrinkage.html#coefficients",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Coefficients",
    "text": "Coefficients\n\nround(coef(longley.lars),4)\n\n      GNP.deflator     GNP Unemployed Armed.Forces Population   Year\n [1,]       0.0000  0.0000     0.0000       0.0000     0.0000 0.0000\n [2,]       0.0000  0.0327     0.0000       0.0000     0.0000 0.0000\n [3,]       0.0000  0.0362    -0.0037       0.0000     0.0000 0.0000\n [4,]       0.0000  0.0372    -0.0046      -0.0010     0.0000 0.0000\n [5,]       0.0000  0.0000    -0.0124      -0.0054     0.0000 0.9068\n [6,]       0.0000  0.0000    -0.0141      -0.0071     0.0000 0.9438\n [7,]       0.0000  0.0000    -0.0147      -0.0086    -0.1534 1.1843\n [8,]      -0.0077  0.0000    -0.0148      -0.0087    -0.1708 1.2289\n [9,]       0.0000 -0.0121    -0.0166      -0.0093    -0.1303 1.4319\n[10,]       0.0000 -0.0253    -0.0187      -0.0099    -0.0951 1.6865\n[11,]       0.0151 -0.0358    -0.0202      -0.0103    -0.0511 1.8292"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#selecting-a-solution-from-the-path",
    "href": "resources/slides/11-shrinkage.html#selecting-a-solution-from-the-path",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Selecting a Solution from the Path",
    "text": "Selecting a Solution from the Path\n\n\n\nsummary(longley.lars)\n\nLARS/LASSO\nCall: lars(x = as.matrix(longley[, -7]), y = longley[, 7], type = \"lasso\")\n   Df     Rss        Cp\n0   1 185.009 1976.7120\n1   2   6.642   59.4712\n2   3   3.883   31.7832\n3   4   3.468   29.3165\n4   5   1.563   10.8183\n5   4   1.339    6.4068\n6   5   1.024    5.0186\n7   6   0.998    6.7388\n8   7   0.907    7.7615\n9   6   0.847    5.1128\n10  7   0.836    7.0000\n\n\n\n\nFor \\(p\\) predictors, \\[C_p = \\frac{\\textsf{SSE}_p}{s^2} -n + 2p\\]\n\\(s^2\\) is the residual variance from the full model\n\\(\\textsf{SSE}_p\\) is the sum of squared errors for the model with \\(p\\) predictors (RSS)\nif the model includes all the predictors with non-zero coefficients, then \\(C_p \\approx p\\)\nchoose minimum \\(C_p \\approx p\\)\nin practice use Cross-validation or Generalized Cross Validation (GCV) to choose \\(\\lambda\\)"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#features-and-issues",
    "href": "resources/slides/11-shrinkage.html#features-and-issues",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Features and Issues",
    "text": "Features and Issues\n\nCombines shrinkage (like Ridge Regression) with Variable Selection to deal with collinearity\nCan be used for prediction or variable selection\nnot invariant under linear transformations of the predictors\ntypically no uncertainty estimates for the coefficients or predictions\nignores uncertainty in the choice of \\(\\lambda\\)\nmay overshrink large coefficients"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#bayesian-lasso",
    "href": "resources/slides/11-shrinkage.html#bayesian-lasso",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Bayesian LASSO",
    "text": "Bayesian LASSO\n\nEquivalent to finding posterior mode with a Double Laplace Prior \\[\n\\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\beta}} -\\frac{\\phi}{2} \\{ \\| \\mathbf{Y}_c - \\mathbf{X}_s \\boldsymbol{\\beta}\\|^2 + \\lambda^* \\|\\boldsymbol{\\beta}\\|_1 \\}\n\\]\nPark & Casella (JASA 2008) and Hans (Biometrika 2010) propose Bayesian versions of the Lasso \\[\\begin{eqnarray*}\n\\mathbf{Y}\\mid \\alpha, \\boldsymbol{\\beta}, \\phi & \\sim & \\textsf{N}(\\mathbf{1}_n \\alpha + \\mathbf{X}^s \\boldsymbol{\\beta}^s, \\mathbf{I}_n/\\phi) \\\\\n\\boldsymbol{\\beta}\\mid \\alpha, \\phi, \\boldsymbol{\\tau}& \\sim & \\textsf{N}(\\mathbf{0}, \\textsf{diag}(\\boldsymbol{\\tau}^2)/\\phi)  \\\\\n\\tau_1^2 \\ldots, \\tau_p^2 \\mid \\alpha, \\phi & \\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}& \\textsf{Exp}(\\lambda^2/2)  \\\\\n  p(\\alpha, \\phi) & \\propto& 1/\\phi  \\\\\n\\end{eqnarray*}\\]\nGeneralizes Ridge Priors to allow different prior variances for each coefficient"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#double-exponential-or-double-laplace-prior",
    "href": "resources/slides/11-shrinkage.html#double-exponential-or-double-laplace-prior",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Double Exponential or Double Laplace Prior",
    "text": "Double Exponential or Double Laplace Prior\n\nMarginal distribution of \\(\\beta_j\\) \\[\\begin{eqnarray*}\n\\boldsymbol{\\beta}\\mid \\alpha, \\phi, \\boldsymbol{\\tau}& \\sim & \\textsf{N}(\\mathbf{0}, \\textsf{diag}(\\boldsymbol{\\tau}^2)/\\phi)  \\\\\n\\tau_1^2 \\ldots, \\tau_p^2 \\mid \\alpha, \\phi & \\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}& \\textsf{Exp}(\\lambda^2/2)  \\\\\np(\\beta_j \\mid \\phi, \\lambda) & = & \\int_0^\\infty p(\\beta_i \\mid \\phi, \\tau^2_j) p(\\tau^2_j \\mid \\phi, \\lambda) \\, d\\tau^2 \\\\\n\\end{eqnarray*}\\]\nCan show that \\(\\beta_j \\mid \\phi, \\lambda \\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}DE(\\lambda \\sqrt{\\phi})\\) \\[\\int_0^\\infty \\frac{1}{\\sqrt{2 \\pi t}}\ne^{-\\frac{1}{2} \\phi \\frac{\\beta^2}{t }}\n\\, \\frac{\\lambda^2}{2} e^{- \\frac{\\lambda^2 t}{2}}\\, dt =\n\\frac{\\lambda \\phi^{1/2}}{2} e^{-\\lambda \\phi^{1/2} |\\beta|}\n\\]\nScale Mixture of Normals (Andrews and Mallows 1974)"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#gibbs-sampler",
    "href": "resources/slides/11-shrinkage.html#gibbs-sampler",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Gibbs Sampler",
    "text": "Gibbs Sampler\n\nIntegrate out \\(\\alpha\\): \\(\\alpha \\mid \\mathbf{Y}, \\phi \\sim \\textsf{N}(\\bar{y},\n  1/(n \\phi)\\)\n\n\\(\\boldsymbol{\\beta}\\mid \\boldsymbol{\\tau}, \\phi, \\lambda, \\mathbf{Y}_c \\sim \\textsf{N}(, )\\)\n\n\\(\\phi \\mid \\boldsymbol{\\tau}, \\boldsymbol{\\beta}, \\lambda, \\mathbf{Y}_c \\sim \\mathbf{G}( , )\\)\n\n\\(1/\\tau_j^2 \\mid \\boldsymbol{\\beta}, \\phi, \\lambda, \\mathbf{Y}\\sim \\textsf{InvGaussian}(\n, )\\)\n\\(X \\sim \\textsf{InvGaussian}(\\mu,  \\lambda)\\) has density \\[\nf(x) =  \\sqrt{\\frac{\\lambda^2}{2 \\pi}}  x^{-3/2} e^{- \\frac{1}{2} \\frac{\n  \\lambda^2( x - \\mu)^2} {\\mu^2 x}} \\qquad x &gt; 0\n\\]\nHomework: Derive the full conditionals for \\(\\boldsymbol{\\beta}^s\\), \\(\\phi\\), \\(1/\\tau^2\\)\nsee Casella & Park"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#horseshoe-priors",
    "href": "resources/slides/11-shrinkage.html#horseshoe-priors",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Horseshoe Priors",
    "text": "Horseshoe Priors\nCarvalho, Polson & Scott (2010) propose an alternative shrinkage prior \\[\\begin{align*}\n\\boldsymbol{\\beta}\\mid \\phi & \\sim \\textsf{N}(\\mathbf{0}_p, \\frac{\\textsf{diag}(\\tau^2)}{ \\phi\n    }) \\\\\n\\tau_j^2 \\mid \\lambda & \\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}C^+(0, \\lambda) \\\\\n\\lambda & \\sim \\textsf{C}^+(0, 1/\\phi) \\\\\np(\\alpha, \\phi) & \\propto 1/\\phi\n\\end{align*}\\]\n\n\\(C^+(0, \\lambda)\\) is the half-Cauchy distribution with scale \\(\\lambda\\) \\[\np(\\tau_j^2 \\mid \\lambda) = \\frac{2}{\\pi} \\frac{\\lambda}{\\lambda^2 + \\tau_j^2}\n\\]\n\\(\\textsf{C}^+(0, 1/\\phi)\\) is the half-Cauchy distribution with scale \\(1/\\phi\\)"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#special-case",
    "href": "resources/slides/11-shrinkage.html#special-case",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Special Case",
    "text": "Special Case\n\n\nIn the case \\(\\lambda = \\phi = 1\\) and with \\(\\mathbf{X}^t\\mathbf{X}= \\mathbf{I}\\), \\(\\mathbf{Y}^* =\n\\mathbf{X}^T\\mathbf{Y}\\) \\[\\begin{align*}\nE[\\beta_i \\mid \\mathbf{Y}] & = \\textsf{E}_{\\kappa_i \\mid \\mathbf{Y}}[ \\textsf{E}_{\\beta_i \\mid \\kappa_i, \\mathbf{Y}}[\\beta_i \\mid \\mathbf{Y}] \\\\\n& = \\int_0^1 (1 - \\kappa_i) y^*_i p(\\kappa_i \\mid \\mathbf{Y})\n\\ d\\kappa_i \\\\\n& = (1 - \\textsf{E}[\\kappa \\mid y^*_i]) y^*_i\n\\end{align*}\\] where \\(\\kappa_i = 1/(1 + \\tau_i^2)\\) is the shrinkage factor (like in James-Stein)\n\nHalf-Cauchy prior induces a Beta(1/2, 1/2) distribution on \\(\\kappa_i\\) a priori (change of variables)"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#features-and-issues-1",
    "href": "resources/slides/11-shrinkage.html#features-and-issues-1",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Features and Issues",
    "text": "Features and Issues\n\n\n\nthe posterior mode also induces shrinkage and variable selection if the mode is at zero\nthe posterior mean is a shrinkage estimator (no selection)\nthe tails of the distribution are heavier than the Laplace prior (like a Cauchy distribution) so that there is less shrinkage of large \\(|\\hat{\\boldsymbol{\\beta}}|\\).\nDesirable in the orthogonal case, where lasso is more like ridge regression (related to bounded influence)\nMCMC is slow to mix using programs like stan but specialized R packages like horseshoe and monomvm::bhsare available"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#bounded-influence-and-posterior-mean",
    "href": "resources/slides/11-shrinkage.html#bounded-influence-and-posterior-mean",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Bounded Influence and Posterior Mean",
    "text": "Bounded Influence and Posterior Mean\n\n\nPosterior mean of \\(\\beta\\) may also be written as \\[E[\\beta_i \\mid y^*_i] = y^*_i + \\frac{d} {d y} \\log m(y^*_i)\\] where \\(m(y)\\) is the predictive density \\(y^*_i\\) under the prior (known \\(\\lambda\\))\n\nHS has Bounded Influence: \\[\\lim_{|y| \\to \\infty} \\frac{d}{dy} \\log m(y) = 0\\]\n\\(\\lim_{|y_i^*| \\to \\infty} E[\\beta_i \\mid y^*_i) \\to y^*_i\\) (the MLE)\nsince the MLE \\(\\to \\beta_i^*\\) as \\(n \\to \\infty\\), the HS is asymptotically consistent\n\n\n\n\nthe DE also has bounded influence, but the bound does not decay to zero in tails so that the posterior mean does not shrink to the MLE"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#comparison",
    "href": "resources/slides/11-shrinkage.html#comparison",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Comparison",
    "text": "Comparison\n\n\n\nDiabetes data (from the lars package)\n64 predictors: 10 main effects, 2-way interactions and quadratic terms\n\n\nsample size of 442\nsplit into training and test sets\ncompare MSE for out-of-sample prediction using OLS, lasso and horseshoe priors\nRoot MSE for prediction for left out data based on 25 different random splits with 100 test cases"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#summary",
    "href": "resources/slides/11-shrinkage.html#summary",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Summary",
    "text": "Summary\nThe literature on shrinkage estimators (with or without selection) is vast\n\nElastic Net (Zou & Hastie 2005)\nSCAD (Fan & Li 2001)\nGeneralized Double Pareto Prior (Armagan, Dunson & Lee 2013)\nSpike-and-Slab Lasso (Rockova & George 2018)\n\n\nFor Bayes, choice of estimator\n\nposterior mean (easy via MCMC)\nposterior mode (optimization)\nposterior median (via MCMC)\n\n\n\nProperties?\n\nFan & Li (JASA 2001) discuss variable selection via non-concave penalties and oracle properties (next time …)\n\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/10-james-stein.html#outline",
    "href": "resources/slides/10-james-stein.html#outline",
    "title": "James-Stein Estimation and Shrinkage",
    "section": "Outline",
    "text": "Outline\n\nFrequentist Risk in Orthogonal Regression\nJames-Stein Estimation\n\n\nReadings:\n\nSeber & Lee Chapter Chapter 12"
  },
  {
    "objectID": "resources/slides/10-james-stein.html#orthogonal-regression",
    "href": "resources/slides/10-james-stein.html#orthogonal-regression",
    "title": "James-Stein Estimation and Shrinkage",
    "section": "Orthogonal Regression",
    "text": "Orthogonal Regression\n\nConsider the model \\(\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{e}\\) where \\(\\mathbf{X}\\) is \\(n \\times p\\) with \\(n &gt; p\\) and \\(\\mathbf{X}^T\\mathbf{X}= \\mathbf{I}_p\\).\nIf \\(\\mathbf{X}\\) has orthogonal columns, then \\(\\hat{\\boldsymbol{\\beta}}= \\mathbf{X}^T\\mathbf{Y}\\) is the OLS estimator of \\(\\boldsymbol{\\beta}\\).\nThe OLS estimator is unbiased and has minimum variance among all\nThe MSE for estimating \\(\\boldsymbol{\\beta}\\) is \\(\\textsf{E}_\\mathbf{Y}[( \\boldsymbol{\\beta}- \\hat{\\boldsymbol{\\beta}})^T(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})] = \\sigma^2\n  \\textsf{tr}[(\\mathbf{X}^T\\mathbf{X})^{-1}] = p\\sigma^2\\)\nCan always take a general regression problem and transform design so that the model matrix has orthogonal columns \\[\\mathbf{X}\\boldsymbol{\\beta}= \\mathbf{U}\\boldsymbol{\\Delta}\\mathbf{V}^T \\boldsymbol{\\beta}= \\mathbf{U}\\boldsymbol{\\alpha}\\] where new parameters are \\(\\boldsymbol{\\alpha}= \\boldsymbol{\\Delta}\\mathbf{V}^T \\boldsymbol{\\beta}\\) and \\(\\mathbf{U}^T\\mathbf{U}= \\mathbf{I}_p\\).\nOrthogonal polynomials, Fourier bases and wavelet regression are other examples.\n\\(\\hat{\\boldsymbol{\\alpha}} = \\mathbf{U}^T\\mathbf{Y}\\) and MSE of \\(\\hat{\\boldsymbol{\\alpha}}\\) is \\(p\\sigma^2\\)\nso WLOG we will assume that \\(\\mathbf{X}\\) has orthogonal columns"
  },
  {
    "objectID": "resources/slides/10-james-stein.html#shrinkage-estimators",
    "href": "resources/slides/10-james-stein.html#shrinkage-estimators",
    "title": "James-Stein Estimation and Shrinkage",
    "section": "Shrinkage Estimators",
    "text": "Shrinkage Estimators\n\nthe \\(g\\)-prior and Ridge prior are equivalent in the orthogonal case \\[\\boldsymbol{\\beta}\\sim \\textsf{N}(\\mathbf{0}_p, \\sigma^2 \\mathbf{I}_p/\\kappa)\\] using the ridge parameterization of the prior \\(\\kappa = 1/g\\)\nBayes estimator in this case is \\[\\hat{\\boldsymbol{\\beta}}_\\kappa = \\frac{1}{1+\\kappa} \\hat{\\boldsymbol{\\beta}}\\]\nMSE of \\(\\hat{\\boldsymbol{\\beta}}_\\kappa\\) is \\[\\textsf{MSE}(\\hat{\\boldsymbol{\\beta}}_\\kappa) = \\frac{1}{(1+\\kappa)^2} \\sigma^2 p + \\frac{\\kappa^2}{(1 + \\kappa)^2} \\sum_{j=1}^{p} \\beta_j^2\\]\nsquared bias term grows with \\(\\kappa\\) and variance term decreases with \\(\\kappa\\)"
  },
  {
    "objectID": "resources/slides/10-james-stein.html#shrinkage",
    "href": "resources/slides/10-james-stein.html#shrinkage",
    "title": "James-Stein Estimation and Shrinkage",
    "section": "Shrinkage",
    "text": "Shrinkage\n\n\n\nin principle, with the right choice of \\(\\kappa\\) we can get a better estimator and reduce the MSE\nwhile not unbiased, what we pay for bias we can make up for with a reduction in variance\nthe variance-bias decomposition of MSE based on the plot suggests there is an optimal value of \\(\\kappa\\) the improves over OLS in terms of MSE\n“optimal” \\(\\kappa\\) \\[\\kappa = \\frac{p \\sigma^2}{\\|\\boldsymbol{\\beta}^*\\|^2}\\] where \\(\\boldsymbol{\\beta}^*\\) is the true value of \\(\\boldsymbol{\\beta}\\)\nbut never know that in practice!"
  },
  {
    "objectID": "resources/slides/10-james-stein.html#estimating-kappa",
    "href": "resources/slides/10-james-stein.html#estimating-kappa",
    "title": "James-Stein Estimation and Shrinkage",
    "section": "Estimating \\(\\kappa\\)",
    "text": "Estimating \\(\\kappa\\)\n\nif we use the optimal \\(\\kappa\\), the shrinkage estimator can be written as \\[\\tilde{\\boldsymbol{\\beta}}= \\frac{\\|\\boldsymbol{\\beta}\\|^2}{p \\sigma^2 + \\|\\boldsymbol{\\beta}\\|^2} \\hat{\\boldsymbol{\\beta}}\\] or \\[\\tilde{\\boldsymbol{\\beta}}= \\left(1 - \\frac{p\\sigma^2}{p\\sigma^2 + \\|\\boldsymbol{\\beta}\\|^2} \\right) \\hat{\\boldsymbol{\\beta}}\\]\nbut note that \\(\\textsf{E}\\|\\hat{\\boldsymbol{\\beta}}\\|^2 = p \\sigma^2 + \\|\\boldsymbol{\\beta}\\|^2\\) (the denominator)\nplugging in \\(\\|\\hat{\\boldsymbol{\\beta}}\\|^2\\) for the denominator leads to an estimator that we can compute! \\[\\tilde{\\boldsymbol{\\beta}}= \\left(1 - \\frac{p\\sigma^2}{\\|\\hat{\\boldsymbol{\\beta}}\\|^2} \\right) \\hat{\\boldsymbol{\\beta}}\\]"
  },
  {
    "objectID": "resources/slides/10-james-stein.html#james-stein-estimators",
    "href": "resources/slides/10-james-stein.html#james-stein-estimators",
    "title": "James-Stein Estimation and Shrinkage",
    "section": "James-Stein Estimators",
    "text": "James-Stein Estimators\nin James and Stein (1961) proposed a shrinkage estimator that dominated the MLE for the mean of a multivariate normal distribution \\[\\tilde{\\boldsymbol{\\beta}}_{JS} = \\left(1 - \\frac{(p-2)\\sigma^2}{\\|\\hat{\\boldsymbol{\\beta}}\\|^2} \\right) \\hat{\\boldsymbol{\\beta}}\\] (equivalent to our orthogonal regression case; just multiply everything by \\(\\mathbf{X}^T\\) to show)\n\nthey showed that this is the best (in terms of smallest MSE) of all estimators of the form \\(\\left(1- \\frac{b}{\\|\\hat{\\boldsymbol{\\beta}}\\|^2} \\right) \\hat{\\boldsymbol{\\beta}}\\)\nit is possible to show that the MSE of the James-Stein estimator is \\[\\textsf{MSE}(\\tilde{\\boldsymbol{\\beta}}_{JS}) = 2\\sigma^2\\] which is less than the MSE of the OLS estimator if \\(p &gt; 2\\)! (more on this in STA732)"
  },
  {
    "objectID": "resources/slides/10-james-stein.html#negative-shrinkage",
    "href": "resources/slides/10-james-stein.html#negative-shrinkage",
    "title": "James-Stein Estimation and Shrinkage",
    "section": "Negative Shrinkage?",
    "text": "Negative Shrinkage?\n\n\n\none potential problem with the James-Stein estimator \\[\\tilde{\\boldsymbol{\\beta}}_{JS} = \\left(1 - \\frac{(p-2)\\sigma^2}{\\|\\hat{\\boldsymbol{\\beta}}\\|^2} \\right) \\hat{\\boldsymbol{\\beta}}\\] is that the term in the parentheses can be negative if \\(\\|\\hat{\\boldsymbol{\\beta}}\\|^2 &lt; (p-2)\\sigma^2\\)\nHow likely is this to happen?\nSuppose that each of the parameters \\(\\beta_j\\) are actually zero, then \\(\\hat{\\boldsymbol{\\beta}}\\sim \\textsf{N}(\\mathbf{0}_p, \\sigma^2 \\mathbf{I}_p)\\) then \\(\\|\\hat{\\boldsymbol{\\beta}}\\|^2 /\\sigma^2 \\sim  \\chi^2_p\\)\ncompute the probability that \\(\\chi^2_p &lt; (p-2)\\)\nso if the model is full of small effects, the James-Stein can lead to negative shrinkage!"
  },
  {
    "objectID": "resources/slides/10-james-stein.html#positive-part-james-stein-estimator",
    "href": "resources/slides/10-james-stein.html#positive-part-james-stein-estimator",
    "title": "James-Stein Estimation and Shrinkage",
    "section": "Positive Part James-Stein Estimator",
    "text": "Positive Part James-Stein Estimator\n\nany shrinkage estimator of the form \\(\\tilde{\\boldsymbol{\\beta}}= (1 - b) \\hat{\\boldsymbol{\\beta}}\\) is inadmissible if the shrinkage factor is negative or greater than one (there is a better estimator)\nBaranchik (1964) proposed the positive part James-Stein estimator \\[\\tilde{\\boldsymbol{\\beta}}_{PPJS} = \\left(1 - \\frac{(p-2)\\sigma^2}{\\|\\hat{\\boldsymbol{\\beta}}\\|^2} \\right)^+ \\hat{\\boldsymbol{\\beta}}\\] where \\((x)^+ = \\max(x, 0)\\)\nThis is the same as the James-Stein estimator if the shrinkage factor is positive and zero otherwise. (related to testing the null hypothesis that all the \\(\\beta_j\\) are zero)\nit turns out this is also inadmissible! ie there is another estimator that has smaller MSE!\nif we care about admissibility, we may want to stick to Bayes rules that do not depend on the data!\nmore on admissibility & minimaxity in STA732!"
  },
  {
    "objectID": "resources/slides/10-james-stein.html#bayes-and-admissibilty",
    "href": "resources/slides/10-james-stein.html#bayes-and-admissibilty",
    "title": "James-Stein Estimation and Shrinkage",
    "section": "Bayes and Admissibilty",
    "text": "Bayes and Admissibilty\n\nBayes rules based on proper priors are generally always admissible (see Christian Robert (2007) The Bayesian Choice for more details)\nunique Bayes rules are admissible\nGeneralized Bayes rules based on improper priors may not be inadmissible, but this will depend on the loss function and the prior\nunder regularity conditions, limits of Bayes rules will be admissible\nthe Positive-Part James-Stein estimator fails to be admissible under squared error loss as Bayes risk is not continuous"
  },
  {
    "objectID": "resources/slides/10-james-stein.html#positive-part-james-stein-estimator-and-testimators",
    "href": "resources/slides/10-james-stein.html#positive-part-james-stein-estimator-and-testimators",
    "title": "James-Stein Estimation and Shrinkage",
    "section": "Positive Part James-Stein Estimator and Testimators",
    "text": "Positive Part James-Stein Estimator and Testimators\n\nthe positive part James-Stein estimator can be shown to be related to testimators where if we fail to reject the hypothesis that all the \\(\\beta_j\\) are zero at some level, we set all coefficients to zero, and otherwise we shrink the coefficients by an amount that depends on how large the test statistic (\\(\\|\\hat{\\boldsymbol{\\beta}}\\|^2\\)) is.\nnote this can shrink all the coefficients to zero if the majority are small so increased bias for large coefficients that are not zero!\nthis is a form of model selection where we are selecting the model that has all the coefficients zero!"
  },
  {
    "objectID": "resources/slides/10-james-stein.html#lasso-estimator",
    "href": "resources/slides/10-james-stein.html#lasso-estimator",
    "title": "James-Stein Estimation and Shrinkage",
    "section": "LASSO Estimator",
    "text": "LASSO Estimator\n\n\n\nan alternative estimator that allows for shrinkage and selection is the LASSO (Least Absolute Shrinkage and Selection Operator).\nthe LASSO replaces the penalty term in the ridge regression with an \\(L_1\\) penalty term \\[\\hat{\\boldsymbol{\\beta}}_{LASSO} = \\mathop{\\mathrm{argmin}}_{\\boldsymbol{\\beta}} \\left\\{ \\|\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}\\|^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_1 \\right\\}\\]\nthe LASSO can also be shown to be the posterior mode of a Bayesian model with independent Laplace or double exponential prior distributions on the coefficients.\nas the double exponential prior is a “scale” mixture of normals, this provides a generalization of the ridge regression.\n\n\n\nfrom Machine Learning with R\n\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": " Schedule",
    "section": "",
    "text": "Please refresh often in case links/content has been updated\n\n\n\n\n\n\n\n\nWeek\nDate\nLesson\nReading\nSlides\nLabs\nHomework\n\n\n\n\nWEEK 1\nTues, Aug 26\nLecture 1: Introduction to Linear Models\n\n\n\n\n\n\n\n\n\n\nThur, Aug 28\nLecture 2: MLEs & Projections\n\n\n\n\n hw-01\n\n\n\n\nFri, Aug 29\n\n\n\n\n\n\n\n\n\n\n\nWEEK 2\nTues, Sept 3\nLecture 3: Rank Deficient Models\n\n\n\n\n\n\n\n\n\n\nThur, Sept 5\nLecture 4: Best Linear Unbiased Estimation and Gauss-Markov Theorem\n\n\n\n\n hw-02\n\n\n\n\nFri, Sept 6\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 3\nTues, Sept 10\nLecture 5: BLUES for Prediction and MVUE\n\n\n\n\n\n\n\n\n\n\nThur, Sept 12\nLecture 6: Generalized Linear Squares\n\n\n\n\n hw-03\n\n\n\n\nFri, Sept 13\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 4\nTues, Sept 17\nLecture 7: Sampling Distributions & Distribution Theory\n\n\n\n\n\n\n\n\n\n\nThur, Sept 19\nLecture 8: Bayesian Estimation\n\n\n\n\n hw-04\n\n\n\n\nFri, Sept 20\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 5\nTues, Sept 24\nLecture 9: Bayes and Frequentist Risk\n\n\n\n\n\n\n\n\n\n\nThur, Sept 26\nLecture 10: James-Stein\n\n\n\n\n hw-05\n\n\n\n\nFri, Sept 28\nLecture 11: Shrinkage Estimators and Hierarchical Bayes\n\n\n\n\n\n\n\n\nWEEK 6\nTues, Oct 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThu, Oct 3\n\n\n\n\n\n\n\n\n hw-06\n\n\n\n\nFri, Oct 4\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 7\nTue, Oct 8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThu, Oct 10\n\n\n\n\n\n\n\n\n hw-07\n\n\n\n\nFri, Oct 11\nReview for Midterm I\n\n\n\n\n\n\n\n\n\n\nWEEK 8\nTue, Oct 15\nNO CLASS FALL BREAK\n\n\n\n\n\n\n\n\n\n\n\n\nThur, Oct 17\nMidterm 1\n\n\n\n\n\n\n\n\n\n\n\n\nFri, Oct 18\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 9\nTue, Oct 22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThur, Oct 24\n\n\n\n\n\n\n\n\n hw-08\n\n\n\n\nFri, Oct 25\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 10\nTues, Oct 29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThur, Oct 31\n\n\n\n\n\n\n\n\n hw-09\n\n\n\n\nFri, Nov 1\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 11\nTues, Nov 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThurs, Nov 7\n\n\n\n\n\n\n\n\n hw-10\n\n\n\n\nFri, Nov 8\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 12\nTues, Nov 12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThurs, Nov 14\nMidterm2\n\n\n\n\n\n\n hw-11\n\n\n\n\nFri, Nov 15\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 12\nTues, Nov 19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThurs, Nov 21\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 13\nTues, Nov 26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThur, Nov 30\nNO CLASS THANKSGIVING\n\n\n\n\n\n\n\n\n\n\nWeek 14\n\n\nGraduate Reading Period\n\n\n\n\n\n\n\n\n\n\nFinals Period\nSunt, Dec 15 9am-12pm (in classroom)"
  },
  {
    "objectID": "labs/lab-01.html",
    "href": "labs/lab-01.html",
    "title": "lab-01",
    "section": "",
    "text": "Please see the instructions for HW-01 and be preapred to ask questions in lab (using R, theory, etc)"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": " Syllabus",
    "section": "",
    "text": "When in doubt about anything at all, ask questions!!!\n\nPrerequisites\nALL students are expected to be familiar with all the topics covered within the required prerequisites to be in this course. That is - mathematical statistics and probability, linear algebra, and multivariate calculus. Students are also expected to be familiar with R and are encouraged to learn LaTeX during the course.\n\n\nWorkload\nWork hours will include time spent going through the preassigned readings, attending lectures and lab sessions, and doing all graded work.\n\n\nGraded Work\nGraded work for the course will consist of homework assignments, lab exercises, two midterms and a final exam. Regrade requests for problem sets and lab exercises must be done via Gradescope AT MOST 24 hours after grades are released! Regrade requests for quizzes, midterm, and final exams must be done via Gradescope AT MOST 12 hours after grades are released! Always write in complete sentences and show your steps.\nStudents’ final grades will be determined as shown below:\n\nComponent Percentage\n\n\nComponent\nPercentage\n\n\n\n\nHomework\n20%\n\n\nMidterm\n25%\n\n\nMidterm II\n25%\n\n\nParticipation\n5%\n\n\nFinal Exam\n25%\n\n\n\nThere are no make-ups for any of the graded work except for medical or familial emergencies or for reasons approved by the instructor BEFORE the due date. See the instructor in advance of relevant due dates to discuss possible alternatives.\nGrades may be curved at the end of the semester. Cumulative averages of 90% – 100% are guaranteed at least an A-, 80% – 89% at least a B-, and 70% – 79% at least a C-, however the exact ranges for letter grades will be determined at the end of the course.\n\n\nDescriptions of graded work\n\nProblem sets\nHomework will be handed out on a weekly basis. They will be based on both the lectures and labs and will be announced every Thursday or Friday – be sure to check the website regularly! Also, please note that any work that is not legible by the instructor or TAs will not be graded (given a score of 0). Every write-up must be clearly written in full sentences and clear English. Any assignment that is completely unclear to the instructors and/or TAs, may result in a grade of a 0. For programming exercises, we will be using R/knitr with \\(\\LaTeX\\) for preparing assignments using github classroom for data analysis.\nEach student MUST write up and turn in her or his own answers. You are encouraged to talk to each other regarding homework problems or to the instructor/TA. However, the write-up, solution, and code must be entirely your own work. No sharing of solutions or code! The assignments must be submitted on Gradescope under Assignments. Note that you will not be able to make online submissions after the due date, so be sure to submit before or by the Gradescope-specified deadline. You may resubmit, so when in doubt submit work early. In certain situations if there are issues with submissions, the TA may review your GitHub repository prior to the due date.\nSolutions will be curated from student solutions with proper attribution. Every week the TAs will select a representative correct solution for the assigned problems and put them together into one solutions set with each answer being attributed to the student who wrote it. If you would like to OPT OUT of having your homework solutions used for the class solutions, please let the Instructor and TAs know in advance.\nFinally, your lowest homework score will be dropped!\n\n\nLab exercises\nThe objective of the lab assignments is to give you more hands-on experience with Bayesian data analysis. Attend the lab session and learn a concept or two and some R from the TA, and then work on the computational part of the problem sets. Each lab assignment should be submitted in timely fashion. You are REQUIRED to use R/knitr (or R/Rmarkdown in some cases).\n\n\nMidterm Exams\nThere will be two inclass midterm exams. Detailed instructions on the midterm will be made available later but please check dates on the calendar well in advance!\n\n\nFinal Exam\nThere will be a final exam after the reading week. If you miss any quiz or the midterm, your grade will depend more on the final exam score since there are no make-up exams. You cannot miss the final exam! Please check the important dates on the homepage for the date and time of the final before making plans to return home at the end of the semester. Detailed instructions on the final will be made available later.\n\n\n\nLate Submission Policy\n\nno late submission of homework or lab assignments, however we will drop the lowest score in each.\n\n\n\nCourse Topics\nFor a detailed day-by-day list of topics, please refer to the Course Schedule\n\n\nAcademic integrity\nDuke University is a community dedicated to scholarship, leadership, and service and to the principles of honesty, fairness, respect, and accountability. Citizens of this community commit to reflect upon and uphold these principles in all academic and nonacademic endeavors, and to protect and promote a culture of integrity.\nRemember the Duke Community Standard that you have agreed to abide by:\n\nTo uphold the Duke Community Standard:\n\n\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\nCheating or plagiarism on any graded assessments, lying about an illness or absence and other forms of academic dishonesty are a breach of trust with classmates and faculty, violate the Duke Community Standard, and will not be tolerated. Such incidences will result in a 0 grade for all parties involved. Additionally, there may be penalties to your final class grade along with being reported to the Office of Student Conduct. Review the academic dishonesty policies at https://studentaffairs.duke.edu/conduct/z-policies/academic-dishonesty.\n\n\nDiversity & Inclusiveness\nThis course is designed so that students from all backgrounds and perspectives all feel welcome both in and out of class. Please feel free to talk to me (in person or via email) if you do not feel well-served by any aspect of this class, or if some aspect of class is not welcoming or accessible to you. My goal is for you to succeed in this course, therefore, let me know immediately if you feel you are struggling with any part of the course more than you know how to manage. Doing so will not affect your grades, but it will allow me to provide the resources to help you succeed in the course.\n\n\nDisability Statement\nStudents with disabilities who believe that they may need accommodations in the class are encouraged to contact the Student Disabilities Access Office at 919-668-1267 or disabilities@aas.duke.edu as soon as possible to better ensure that such accommodations are implemented in a timely fashion.\n\n\nOther Information\nIt can be a lot more pleasant oftentimes to get one-on-one answers and help. Make use of the teaching team’s office hours, we’re here to help! Do not hesitate to talk to me during office hours or by appointment to discuss a problem set or any aspect of the course. Questions related to course assignments and honesty policy should be directed to me. When the teaching team has announcements for you we will send an email to your Duke email address. Be sure to check your email daily.\nMost of the course components will be held in person, but occasionally may need to be held online using Zoom meetings. If you have any concerns, issues or challenges, let the instructor know as soon as possible. Also, all students are strongly encouraged to rely on the forums in Sakai, for interacting among yourself and asking other students questions. You can also ask the instructor or the TAs questions on there and we will try to respond as soon as possible. If you experience any technical issues with joining or using the forums, let the instructor know.\n\n\nProfessionalism\nTry as much as possible to refrain from texting or using your computer for anything other than coursework during class and labs. Again, the more engaged you are, the quicker you will be able to get through the materials. You are responsible for everything covered in the lecture videos, lecture notes/slides, and in the assigned readings."
  },
  {
    "objectID": "reading/08-bayes.html",
    "href": "reading/08-bayes.html",
    "title": "Bayesian Estimation and Shrinkage",
    "section": "",
    "text": "We study Bayes estimators of the regression coefficients, and show how these can be viewed as shrunken versions of the OLS estimator. We discuss two commonly used priors, the \\(g\\)-prior of Zellner (1986) and an independent prior. In the latter case, the posterior mean is equivalent to the classic ridge regression estimator of Hoerl and Kennedy. The posterior mean in both cases may offer improvements over OLS. Rigde regression in particular can reduce variance and estimation error in cases where the predictors are highly collinear.\nReadings:\n\nChristensen Chapter 2.9 and Chapter 15\nSeber & Lee Chapter 3.12 and Chapter 12\nHoerl, A.E. and Kennard, R.W. (1970) Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1):55–67\nZellner, A. (1986) On assessing prior distributions and Bayesian regression analysis with \\(g\\)-prior distributions. In Bayesian Inference and Decision Techniques: Essays in Honor of Bruno de Finetti, eds. P. K. Goel and A. Zellner, 233–243. Amsterdam: North-Holland"
  },
  {
    "objectID": "reading/05-BLUE-MVUE.html",
    "href": "reading/05-BLUE-MVUE.html",
    "title": "Best Linear Unbiased Estimation in Prediction, MVUEs and BUEs",
    "section": "",
    "text": "We will continue our discussion of OLS/MLE estimators with exploring conditions for when the Best Linear Unbiased Estimators exist, with a special focus on out of sample prediction in the non-full rank case. We will outline the proof for why MLEs are also Minimum Variance Unbiased Estimators or “BUE” out of all unbiased estimators linear or non-linear under the additional assumption of normality of the errors. This opens up the question of what estimators are unbiased for linear models and if there are other nonlinear unbiased estimators that are better than OLS, a topic that has received recent attention.\nReadings:\n\nChristensen Chapter 2, Appendix B\nSeber & Lee Chapter 3\n\nFor the curious\n\nWhat Estimators are Unbiased for Linear Models (2023) and references within\nAnderson, T.W. (1962). Least squares and best unbiased estimates. The Annals of Mathematical Statistics, 33(1): 266–272\nHansen, B.E. (2022) A modern gauss-markov theorem. Econometrica"
  },
  {
    "objectID": "reading/03-non-full-rank.html",
    "href": "reading/03-non-full-rank.html",
    "title": "Rank Deficient Models",
    "section": "",
    "text": "We will explore MLEs/OLS using rank deficient models using generalized inverses. We will characterize orthogonal projections using Spectral Decompositions and review how that is related to the Singular Value Decomposition of the model matrix.\nReadings:\n\nChristensen Chapter 1-2, Appendix B\nSeber & Lee Chapter 3, Appendix A & Appendix B"
  },
  {
    "objectID": "reading/02-mles.html",
    "href": "reading/02-mles.html",
    "title": "MLEs & Projections",
    "section": "",
    "text": "Readings:\n\nChristensen Chapter 1-2, Appendix A, and Appendix B\nSeber & Lee Chapter 3, Appendix B"
  },
  {
    "objectID": "reading/04-BLUE.html",
    "href": "reading/04-BLUE.html",
    "title": "Best Linear Unbiased Estimation",
    "section": "",
    "text": "We will explore properties of MLEs/OLS and linear functionals of them. In particular characterizing the class of linear unbiased estimates and minimimum variances, cumulating with the Guass Markov Theorem for both the full rank case and rank deficient models and establing conditions under which OLS/MLE estimators are the Best Linear Unbiased Estimators under the assumption that the covariance of the errors is spherically symmetric.\nReadings:\n\nChristensen Chapter 1-2, Appendix B\nSeber & Lee Chapter 3, Appendix A & Appendix B"
  }
]