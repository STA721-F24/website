[
  {
    "objectID": "reading/15-CI.html",
    "href": "reading/15-CI.html",
    "title": "Confidence and Credible Regions",
    "section": "",
    "text": "We present confidence and credible regions for parameters in the linear model\nReadings:\n\nChristensen Chapter 3, Appendix C"
  },
  {
    "objectID": "reading/12-oracle.html",
    "href": "reading/12-oracle.html",
    "title": "Optimal Shrinkage/Selection and Oracle Properties",
    "section": "",
    "text": "We discuss properties of the shrinkage estimators that provide both shrinkage and selection from frequentist and Bayesian paradigms.\nReadings:\n\nSeber & Lee Chapter 12\nFan & Li (2001) “Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properties”\nTibshirani (1996) “Regression Shrinkage and Selection via the Lasso”\nCarvalho, Polson & Scott (2009) “Handling Sparsity via the Horseshoe”\nArmagan, Dunson & Lee (2013) “Generalized Double Pareto Shrinkage”"
  },
  {
    "objectID": "reading/04-BLUE.html",
    "href": "reading/04-BLUE.html",
    "title": "Best Linear Unbiased Estimation",
    "section": "",
    "text": "We will explore properties of MLEs/OLS and linear functionals of them. In particular characterizing the class of linear unbiased estimates and minimimum variances, cumulating with the Guass Markov Theorem for both the full rank case and rank deficient models and establing conditions under which OLS/MLE estimators are the Best Linear Unbiased Estimators under the assumption that the covariance of the errors is spherically symmetric.\nReadings:\n\nChristensen Chapter 1-2, Appendix B\nSeber & Lee Chapter 3, Appendix A & Appendix B"
  },
  {
    "objectID": "reading/10-james-stein.html",
    "href": "reading/10-james-stein.html",
    "title": "James-Stein Estimation",
    "section": "",
    "text": "We discuss properties of the James-Stein shrinkage estimator and its relationship to Empirical Bayes in the case of ridge regression and the \\(g\\)-prior in regression with orthonormal predictors. While the James-Stein and positive-part James-Stein shrinkage estimators dominate the MLE/OLS estimator, they are not admissible.\nReadings:\n\nChristensen Chapter 15\nSeber & Lee Chapter 12\nJames and Stein (1961) “Estimation with Quadratic Loss”\nEfron & Morris (1973) “Stein’s Estimation Rule and its Competitors”\nRobert, C. (2007) “The Bayesian Choice” Chapter 5"
  },
  {
    "objectID": "reading/16-Bayes-tests.html",
    "href": "reading/16-Bayes-tests.html",
    "title": "Lecture 16: Basics of Bayesian Hypothesis Testing",
    "section": "",
    "text": "Bayesian Data Analysis (Third Edition) by Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin.:\n\nSection 7.4: Model comparison using Bayes factors\n\nThe Bayesian Choice (Second Edition) by Christian Robert\n\n5: Tests and Confidence Regions\n5.1: Introduction\n5.2: A first approach to testing theory\n5.3: Comparison with the classical approach\n5.4: A second decision-theoretic approach\n\nIntrinsic Bayes Factors for Model Selection and Comparison by J. O. Berger and L. R. Pericchi\n\nJournal of the Royal Statistical Society: Series B (Statistical Methodology) Volume 67, Issue 3, pages 725–748, June 2005"
  },
  {
    "objectID": "reading/16-Bayes-tests.html#readings",
    "href": "reading/16-Bayes-tests.html#readings",
    "title": "Lecture 16: Basics of Bayesian Hypothesis Testing",
    "section": "",
    "text": "Bayesian Data Analysis (Third Edition) by Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin.:\n\nSection 7.4: Model comparison using Bayes factors\n\nThe Bayesian Choice (Second Edition) by Christian Robert\n\n5: Tests and Confidence Regions\n5.1: Introduction\n5.2: A first approach to testing theory\n5.3: Comparison with the classical approach\n5.4: A second decision-theoretic approach\n\nIntrinsic Bayes Factors for Model Selection and Comparison by J. O. Berger and L. R. Pericchi\n\nJournal of the Royal Statistical Society: Series B (Statistical Methodology) Volume 67, Issue 3, pages 725–748, June 2005"
  },
  {
    "objectID": "reading/08-bayes.html",
    "href": "reading/08-bayes.html",
    "title": "Bayesian Estimation and Shrinkage",
    "section": "",
    "text": "We study Bayes estimators of the regression coefficients, and show how these can be viewed as shrunken versions of the OLS estimator. We discuss two commonly used priors, the \\(g\\)-prior of Zellner (1986) and an independent prior. In the latter case, the posterior mean is equivalent to the classic ridge regression estimator of Hoerl and Kennedy. The posterior mean in both cases may offer improvements over OLS. Rigde regression in particular can reduce variance and estimation error in cases where the predictors are highly collinear.\nReadings:\n\nChristensen Chapter 2.9 and Chapter 15\nSeber & Lee Chapter 3.12 and Chapter 12\nHoerl, A.E. and Kennard, R.W. (1970) Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1):55–67\nZellner, A. (1986) On assessing prior distributions and Bayesian regression analysis with \\(g\\)-prior distributions. In Bayesian Inference and Decision Techniques: Essays in Honor of Bruno de Finetti, eds. P. K. Goel and A. Zellner, 233–243. Amsterdam: North-Holland"
  },
  {
    "objectID": "reading/06-GLS.html",
    "href": "reading/06-GLS.html",
    "title": "Generalize Least Squares, MLEs, MVUEs and BUEs",
    "section": "",
    "text": "We will develop GLS/MLE estimators when the covariance of the errors is is not a scale multiple of the identity matrix, and establish conditions for when the OLS and GLS are equivalent. Inthe case that the errors have a multivatiate normal distribution, the MLEs are equivalent to GLS and are not only the BLUE but are also Minimum Variance Unbiased Estimators or “BUE” out of all unbiased estimators linear or non-linear.\nReadings:\n\nChristensen Chapter 2 and 10 Appendix B\nSeber & Lee Chapter 3"
  },
  {
    "objectID": "reading/13-testing.html",
    "href": "reading/13-testing.html",
    "title": "Testing Hypotheses",
    "section": "",
    "text": "We develop an F-test for testing hypotheses about the coefficients in a linear model.\nReadings:\n\nChristensen Chapter 3, Appendix C"
  },
  {
    "objectID": "reading/03-non-full-rank.html",
    "href": "reading/03-non-full-rank.html",
    "title": "Rank Deficient Models",
    "section": "",
    "text": "We will explore MLEs/OLS using rank deficient models using generalized inverses. We will characterize orthogonal projections using Spectral Decompositions and review how that is related to the Singular Value Decomposition of the model matrix.\nReadings:\n\nChristensen Chapter 1-2, Appendix B\nSeber & Lee Chapter 3, Appendix A & Appendix B"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": " Syllabus",
    "section": "",
    "text": "When in doubt about anything at all, ask questions!!!\n\nPrerequisites\nALL students are expected to be familiar with all the topics covered within the required prerequisites to be in this course. That is - mathematical statistics and probability, linear algebra, and multivariate calculus. Students are also expected to be familiar with R and are encouraged to learn LaTeX during the course.\n\n\nWorkload\nWork hours will include time spent going through the preassigned readings, attending lectures and lab sessions, and doing all graded work.\n\n\nGraded Work\nGraded work for the course will consist of homework assignments, lab exercises, two midterms and a final exam. Regrade requests for problem sets and lab exercises must be done via Gradescope AT MOST 24 hours after grades are released! Regrade requests for quizzes, midterm, and final exams must be done via Gradescope AT MOST 12 hours after grades are released! Always write in complete sentences and show your steps.\nStudents’ final grades will be determined as shown below:\n\nComponent Percentage\n\n\nComponent\nPercentage\n\n\n\n\nHomework\n20%\n\n\nMidterm\n25%\n\n\nMidterm II\n25%\n\n\nParticipation\n5%\n\n\nFinal Exam\n25%\n\n\n\nThere are no make-ups for any of the graded work except for medical or familial emergencies or for reasons approved by the instructor BEFORE the due date. See the instructor in advance of relevant due dates to discuss possible alternatives.\nGrades may be curved at the end of the semester. Cumulative averages of 90% – 100% are guaranteed at least an A-, 80% – 89% at least a B-, and 70% – 79% at least a C-, however the exact ranges for letter grades will be determined at the end of the course.\n\n\nDescriptions of graded work\n\nProblem sets\nHomework will be handed out on a weekly basis. They will be based on both the lectures and labs and will be announced every Thursday or Friday – be sure to check the website regularly! Also, please note that any work that is not legible by the instructor or TAs will not be graded (given a score of 0). Every write-up must be clearly written in full sentences and clear English. Any assignment that is completely unclear to the instructors and/or TAs, may result in a grade of a 0. For programming exercises, we will be using R/knitr with \\(\\LaTeX\\) for preparing assignments using github classroom for data analysis.\nEach student MUST write up and turn in her or his own answers. You are encouraged to talk to each other regarding homework problems or to the instructor/TA. However, the write-up, solution, and code must be entirely your own work. No sharing of solutions or code! The assignments must be submitted on Gradescope under Assignments. Note that you will not be able to make online submissions after the due date, so be sure to submit before or by the Gradescope-specified deadline. You may resubmit, so when in doubt submit work early. In certain situations if there are issues with submissions, the TA may review your GitHub repository prior to the due date.\nSolutions will be curated from student solutions with proper attribution. Every week the TAs will select a representative correct solution for the assigned problems and put them together into one solutions set with each answer being attributed to the student who wrote it. If you would like to OPT OUT of having your homework solutions used for the class solutions, please let the Instructor and TAs know in advance.\nFinally, your lowest homework score will be dropped!\n\n\nLab exercises\nThe objective of the lab assignments is to give you more hands-on experience with Bayesian data analysis. Attend the lab session and learn a concept or two and some R from the TA, and then work on the computational part of the problem sets. Each lab assignment should be submitted in timely fashion. You are REQUIRED to use R/knitr (or R/Rmarkdown in some cases).\n\n\nMidterm Exams\nThere will be two inclass midterm exams. Detailed instructions on the midterm will be made available later but please check dates on the calendar well in advance!\n\n\nFinal Exam\nThere will be a final exam after the reading week. If you miss any quiz or the midterm, your grade will depend more on the final exam score since there are no make-up exams. You cannot miss the final exam! Please check the important dates on the homepage for the date and time of the final before making plans to return home at the end of the semester. Detailed instructions on the final will be made available later.\n\n\n\nLate Submission Policy\n\nno late submission of homework or lab assignments, however we will drop the lowest score in each.\n\n\n\nCourse Topics\nFor a detailed day-by-day list of topics, please refer to the Course Schedule\n\n\nAcademic integrity\nDuke University is a community dedicated to scholarship, leadership, and service and to the principles of honesty, fairness, respect, and accountability. Citizens of this community commit to reflect upon and uphold these principles in all academic and nonacademic endeavors, and to protect and promote a culture of integrity.\nRemember the Duke Community Standard that you have agreed to abide by:\n\nTo uphold the Duke Community Standard:\n\n\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\nCheating or plagiarism on any graded assessments, lying about an illness or absence and other forms of academic dishonesty are a breach of trust with classmates and faculty, violate the Duke Community Standard, and will not be tolerated. Such incidences will result in a 0 grade for all parties involved. Additionally, there may be penalties to your final class grade along with being reported to the Office of Student Conduct. Review the academic dishonesty policies at https://studentaffairs.duke.edu/conduct/z-policies/academic-dishonesty.\n\n\nDiversity & Inclusiveness\nThis course is designed so that students from all backgrounds and perspectives all feel welcome both in and out of class. Please feel free to talk to me (in person or via email) if you do not feel well-served by any aspect of this class, or if some aspect of class is not welcoming or accessible to you. My goal is for you to succeed in this course, therefore, let me know immediately if you feel you are struggling with any part of the course more than you know how to manage. Doing so will not affect your grades, but it will allow me to provide the resources to help you succeed in the course.\n\n\nDisability Statement\nStudents with disabilities who believe that they may need accommodations in the class are encouraged to contact the Student Disabilities Access Office at 919-668-1267 or disabilities@aas.duke.edu as soon as possible to better ensure that such accommodations are implemented in a timely fashion.\n\n\nOther Information\nIt can be a lot more pleasant oftentimes to get one-on-one answers and help. Make use of the teaching team’s office hours, we’re here to help! Do not hesitate to talk to me during office hours or by appointment to discuss a problem set or any aspect of the course. Questions related to course assignments and honesty policy should be directed to me. When the teaching team has announcements for you we will send an email to your Duke email address. Be sure to check your email daily.\nMost of the course components will be held in person, but occasionally may need to be held online using Zoom meetings. If you have any concerns, issues or challenges, let the instructor know as soon as possible. Also, all students are strongly encouraged to rely on the forums in Sakai, for interacting among yourself and asking other students questions. You can also ask the instructor or the TAs questions on there and we will try to respond as soon as possible. If you experience any technical issues with joining or using the forums, let the instructor know.\n\n\nProfessionalism\nTry as much as possible to refrain from texting or using your computer for anything other than coursework during class and labs. Again, the more engaged you are, the quicker you will be able to get through the materials. You are responsible for everything covered in the lecture videos, lecture notes/slides, and in the assigned readings."
  },
  {
    "objectID": "HW/hw-05.html",
    "href": "HW/hw-05.html",
    "title": "Homework 5",
    "section": "",
    "text": "See Gradescope for any updates on due dates.\nCreate your repo for the HW from GitHub Classroom"
  },
  {
    "objectID": "HW/hw-05.html#due-1000pm-fri-oct-4",
    "href": "HW/hw-05.html#due-1000pm-fri-oct-4",
    "title": "Homework 5",
    "section": "",
    "text": "See Gradescope for any updates on due dates.\nCreate your repo for the HW from GitHub Classroom"
  },
  {
    "objectID": "HW/hw-05.html#rstudio",
    "href": "HW/hw-05.html#rstudio",
    "title": "Homework 5",
    "section": "RStudio",
    "text": "RStudio\nYou all should have R and RStudio installed on your computers by now or access to the Department RStudio Server https://rstudio.stat.duke.edu. If you need to setup your local version, first install the latest version of R here: https://cran.rstudio.com - remember to select the right installer for your operating system). Next, install the latest version of RStudio here: https://www.rstudio.com/products/rstudio/download/. Scroll down to the “Installers for Supported Platforms” section and find the right installer for your operating system."
  },
  {
    "objectID": "HW/hw-05.html#getting-started-with-github-classroom",
    "href": "HW/hw-05.html#getting-started-with-github-classroom",
    "title": "Homework 5",
    "section": "Getting started with Github Classroom",
    "text": "Getting started with Github Classroom\nGo to Github classroom to accept the invitation to create a repository for this assignment.\nThis will create a private repo in the STA721-F24 organization on Github with your github user name. Note: You may receive an email invitation to join STA702-F24 on your behalf.\nHit refresh, to see the link for the repo, and then click on it to go to the STA721-F24 organization in Github.\nFollow the instructions in the README in your repo and in the hw#.Rnw file to complete the assignment and push to GitHub. If you encounter issues with Gradescope submission, it is critical that you have pushed your final assignment to GitHub by the due date to validate timestamps."
  },
  {
    "objectID": "HW/hw-05.html#r-rnw",
    "href": "HW/hw-05.html#r-rnw",
    "title": "Homework 5",
    "section": "R & RNW",
    "text": "R & RNW\nYou are required to use the .Rnw format to type up this report report if there are any R problems. To get started see Knitr with LaTeX article by Karl Broman. Feel free to include images of technical derivations if you are short of time to typeset in LaTeX. The macros.tex file should help with shortcuts. You may need to change some of your Project or Global options to get your document to compile with KnitR. If so please post under Ed in the General category or ask in Lab."
  },
  {
    "objectID": "HW/hw-05.html#gradescope-submission",
    "href": "HW/hw-05.html#gradescope-submission",
    "title": "Homework 5",
    "section": "Gradescope Submission",
    "text": "Gradescope Submission\nYou MUST submit both your final .Rnw .pdf files to the repo on Github and upload the pdf to Gradescope here: https://www.gradescope.com/courses/843802/assignments\nMake sure to to pdf; ask the TA about knitting to pdf if you cannot figure it out. Be sure to submit under the right assignment entry by the due date!"
  },
  {
    "objectID": "HW/hw-05.html#grading",
    "href": "HW/hw-05.html#grading",
    "title": "Homework 5",
    "section": "Grading",
    "text": "Grading\nTotal: 20 points."
  },
  {
    "objectID": "HW/hw-07.html",
    "href": "HW/hw-07.html",
    "title": "Homework 7",
    "section": "",
    "text": "See Gradescope for any updates on due dates.\nCreate your repo for the HW from GitHub Classroom"
  },
  {
    "objectID": "HW/hw-07.html#due-1000pm-fri-oct-25",
    "href": "HW/hw-07.html#due-1000pm-fri-oct-25",
    "title": "Homework 7",
    "section": "",
    "text": "See Gradescope for any updates on due dates.\nCreate your repo for the HW from GitHub Classroom"
  },
  {
    "objectID": "HW/hw-07.html#rstudio",
    "href": "HW/hw-07.html#rstudio",
    "title": "Homework 7",
    "section": "RStudio",
    "text": "RStudio\nYou all should have R and RStudio installed on your computers by now or access to the Department RStudio Server https://rstudio.stat.duke.edu. If you need to setup your local version, first install the latest version of R here: https://cran.rstudio.com - remember to select the right installer for your operating system). Next, install the latest version of RStudio here: https://www.rstudio.com/products/rstudio/download/. Scroll down to the “Installers for Supported Platforms” section and find the right installer for your operating system."
  },
  {
    "objectID": "HW/hw-07.html#getting-started-with-github-classroom",
    "href": "HW/hw-07.html#getting-started-with-github-classroom",
    "title": "Homework 7",
    "section": "Getting started with Github Classroom",
    "text": "Getting started with Github Classroom\nGo to Github classroom to accept the invitation to create a repository for this assignment.\nThis will create a private repo in the STA721-F24 organization on Github with your github user name. Note: You may receive an email invitation to join STA702-F24 on your behalf.\nHit refresh, to see the link for the repo, and then click on it to go to the STA721-F24 organization in Github.\nFollow the instructions in the README in your repo and in the hw#.Rnw file to complete the assignment and push to GitHub. If you encounter issues with Gradescope submission, it is critical that you have pushed your final assignment to GitHub by the due date to validate timestamps."
  },
  {
    "objectID": "HW/hw-07.html#r-rnw",
    "href": "HW/hw-07.html#r-rnw",
    "title": "Homework 7",
    "section": "R & RNW",
    "text": "R & RNW\nYou are required to use the .Rnw format to type up this report report if there are any R problems. To get started see Knitr with LaTeX article by Karl Broman. Feel free to include images of technical derivations if you are short of time to typeset in LaTeX. The macros.tex file should help with shortcuts. You may need to change some of your Project or Global options to get your document to compile with KnitR. If so please post under Ed in the General category or ask in Lab."
  },
  {
    "objectID": "HW/hw-07.html#gradescope-submission",
    "href": "HW/hw-07.html#gradescope-submission",
    "title": "Homework 7",
    "section": "Gradescope Submission",
    "text": "Gradescope Submission\nYou MUST submit both your final .Rnw .pdf files to the repo on Github and upload the pdf to Gradescope here: https://www.gradescope.com/courses/843802/assignments\nMake sure to to pdf; ask the TA about knitting to pdf if you cannot figure it out. Be sure to submit under the right assignment entry by the due date!"
  },
  {
    "objectID": "HW/hw-07.html#grading",
    "href": "HW/hw-07.html#grading",
    "title": "Homework 7",
    "section": "Grading",
    "text": "Grading\nTotal: 20 points."
  },
  {
    "objectID": "HW/hw-03.html",
    "href": "HW/hw-03.html",
    "title": "Homework 3",
    "section": "",
    "text": "See Gradescope for any updates on due dates."
  },
  {
    "objectID": "HW/hw-03.html#due-1100pm-thurs-sept-19",
    "href": "HW/hw-03.html#due-1100pm-thurs-sept-19",
    "title": "Homework 3",
    "section": "",
    "text": "See Gradescope for any updates on due dates."
  },
  {
    "objectID": "HW/hw-03.html#rstudio",
    "href": "HW/hw-03.html#rstudio",
    "title": "Homework 3",
    "section": "RStudio",
    "text": "RStudio\nYou all should have R and RStudio installed on your computers by now or access to the Department RStudio Server https://rstudio.stat.duke.edu. If you need to setup your local version, first install the latest version of R here: https://cran.rstudio.com - remember to select the right installer for your operating system). Next, install the latest version of RStudio here: https://www.rstudio.com/products/rstudio/download/. Scroll down to the “Installers for Supported Platforms” section and find the right installer for your operating system."
  },
  {
    "objectID": "HW/hw-03.html#getting-started-with-github-classroom",
    "href": "HW/hw-03.html#getting-started-with-github-classroom",
    "title": "Homework 3",
    "section": "Getting started with Github Classroom",
    "text": "Getting started with Github Classroom\nGo to Github classroom to accept the invitation to create a repository for this assignment at HW3\nThis will create a private repo in the STA721-F24 organization on Github with your github user name. Note: You may receive an email invitation to join STA702-F24 on your behalf.\nHit refresh, to see the link for the repo, and then click on it to go to the STA721-F24 organization in Github.\nFollow the instructions in the README in your repo and in the hw#.Rnw file to complete the assignment and push to GitHub. If you encounter issues with Gradescope submission, it is critical that you have pushed your final assignment to GitHub by the due date to validate timestamps."
  },
  {
    "objectID": "HW/hw-03.html#r-rnw",
    "href": "HW/hw-03.html#r-rnw",
    "title": "Homework 3",
    "section": "R & RNW",
    "text": "R & RNW\nYou are required to use the .Rnw format to type up this report report if there are any R problems. To get started see Knitr with LaTeX article by Karl Broman. Feel free to include images of technical derivations if you are short of time to typeset in LaTeX. The macros.tex file should help with shortcuts. You may need to change some of your Project or Global options to get your document to compile with KnitR. If so please post under Ed in the General category or ask in Lab."
  },
  {
    "objectID": "HW/hw-03.html#gradescope-submission",
    "href": "HW/hw-03.html#gradescope-submission",
    "title": "Homework 3",
    "section": "Gradescope Submission",
    "text": "Gradescope Submission\nYou MUST submit both your final .Rnw .pdf files to the repo on Github and upload the pdf to Gradescope here: https://www.gradescope.com/courses/843802/assignments\nMake sure to to pdf; ask the TA about knitting to pdf if you cannot figure it out. Be sure to submit under the right assignment entry by the due date!"
  },
  {
    "objectID": "HW/hw-03.html#grading",
    "href": "HW/hw-03.html#grading",
    "title": "Homework 3",
    "section": "Grading",
    "text": "Grading\nTotal: 20 points."
  },
  {
    "objectID": "HW/hw-01.html",
    "href": "HW/hw-01.html",
    "title": "Homework 1",
    "section": "",
    "text": "See Gradescope for any updates on due dates."
  },
  {
    "objectID": "HW/hw-01.html#due-1100pm-thurs-sept-5",
    "href": "HW/hw-01.html#due-1100pm-thurs-sept-5",
    "title": "Homework 1",
    "section": "",
    "text": "See Gradescope for any updates on due dates."
  },
  {
    "objectID": "HW/hw-01.html#rstudio",
    "href": "HW/hw-01.html#rstudio",
    "title": "Homework 1",
    "section": "RStudio",
    "text": "RStudio\nYou all should have R and RStudio installed on your computers by now or access to the Department RStudio Server https://rstudio.stat.duke.edu. If you need to setup your local version, first install the latest version of R here: https://cran.rstudio.com - remember to select the right installer for your operating system). Next, install the latest version of RStudio here: https://www.rstudio.com/products/rstudio/download/. Scroll down to the “Installers for Supported Platforms” section and find the right installer for your operating system."
  },
  {
    "objectID": "HW/hw-01.html#r-quarto",
    "href": "HW/hw-01.html#r-quarto",
    "title": "Homework 1",
    "section": "R & Quarto",
    "text": "R & Quarto\nYou are required to use the .qmd format to type up this report report. To get started see technical writing with Quarto. Feel free to include images of technical derivations if you are short of time to typeset in LaTeX. The macros.tex file should help with shortcuts."
  },
  {
    "objectID": "HW/hw-01.html#getting-started-with-github-classroom",
    "href": "HW/hw-01.html#getting-started-with-github-classroom",
    "title": "Homework 1",
    "section": "Getting started with Github Classroom",
    "text": "Getting started with Github Classroom\nGo to Github classroom to accept the invitation to create a repository for this assignment at HW1\nThis will create a private repo in the STA721-F24 organization on Github with your github user name. Note: You may receive an email invitation to join STA702-F24 on your behalf.\nHit refresh, to see the link for the repo, and then click on it to go to the STA721-F24 organization in Github.\nFollow the instructions in the README in your repo and in the hw1.qmd file to complete the assignment and push to GitHub. If you encounter issues with Gradescope submission, it is critical that you have pushed your final assignment to GitHub by the due date to validate timestamps."
  },
  {
    "objectID": "HW/hw-01.html#gradescope-submission",
    "href": "HW/hw-01.html#gradescope-submission",
    "title": "Homework 1",
    "section": "Gradescope Submission",
    "text": "Gradescope Submission\nYou MUST submit both your final .qmd .pdf files to the repo on Github and upload the pdf to Gradescope here: https://www.gradescope.com/courses/843802/assignments\nMake sure to Render to pdf; ask the TA about knitting to pdf if you cannot figure it out. Be sure to submit under the right assignment entry by the due date!"
  },
  {
    "objectID": "HW/hw-01.html#grading",
    "href": "HW/hw-01.html#grading",
    "title": "Homework 1",
    "section": "Grading",
    "text": "Grading\nTotal: 20 points."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": " Resources",
    "section": "",
    "text": "Primary Textbooks\nThese textbooks are great resources for some of the topics we will cover.\n\nPlane Answers to Complex Questions, Ronald Christensen. eBook in Duke Library\nLinear Regression Analysis, George A.F Seber and Alan J. Lee eBook in Duke Library. Duke Library is aware the link is broken\nThe Linear Model and Hypothesis, George A. F. Seber eBook in Duke Library\n\n\n\nSupplementary Textbooks on Linear/Matrix Algebra\n\nGilbert Strang’s Online Course at MIT\nVideo Lectures\n\nIntroduction to Linear Algebra. Strang, Gilbert. 4th ed. Wellesley, MA: Wellesley-Cambridge Press, 2009. ISBN: 9780980232714. Buy @ Amazon\n\nMatrix Algebra from a Statistician’s Perspective. Harville, David A. eBook in Duke Library\n\n\n\nR and R Markdown Resources\nQuarto/R Markdown/LaTeX can be used to create high quality reports and presentations with embedded chunks of R code and LaTeX equations! You are required to use Quarto in RStudio to type up your homework assignments that involve Data Analysis/Simulation for this course, but you are welcome to use any word processor of your choice for those. To learn more about Quarto/R Markdown and for other resources for programming in R, see the links below.\n\nUsing R in Quarto for Documents\nR for Data Science (by Hadley Wickham & Garrett Grolemund)\nIntroduction to R Markdown (Article by Garrett Grolemund)\nIntroduction to R Markdown (Slides by Andrew Cho)\nR Markdown Cheat Sheet\nData Visualization with ggplot2 Cheat Sheet\nOther Useful Cheat Sheets\nA very (very!) basic R Markdown template\n\n\n\nLaTeX\nYou may also use LaTeX to type up your assignments. You may find it easier to create your TeX and LaTeX documents using online editors such as Overleaf (simply create a free account and you are good to go!). However, that need not be the case. If you prefer to create them locally/offline on your personal computers, you will need to download a TeX distribution (the most popular choices are MiKTeX for Windows and MacTeX for macOS) plus an editor (I personally prefer TeXstudio but feel free to download any editor of your choice). Follow the links below for some options, and to also learn how to use LaTeX.\n\nLearn LaTeX in 30 minutes\nChoosing a LaTeX Compiler.\n\n\n\nInteresting Articles\nI will add articles I find interesting below. These are articles I find useful as supplementary readings for topics covered in class, or as good sources that cover concepts I think you should know, but which we may not have time to cover. I strongly suggest you find time to (at the very least) take a “quick peek” at each article.\n\nEfron, B., 1986. Why isn’t everyone a Bayesian?. The American Statistician, 40(1), pp. 1-5.\nGelman, A., 2008. Objections to Bayesian statistics. Bayesian Analysis, 3(3), pp. 445-449.\nDiaconis, P., 1977. Finite forms of de Finetti’s theorem on exchangeability. Synthese, 36(2), pp. 271-281.\nGelman, A., Meng, X. L. and Stern, H., 1996. Posterior predictive assessment of model fitness via realized discrepancies. Statistica sinica, pp. 733-760.\nDunson, D. B., 2018. Statistics in the big data era: Failures of the machine. Statistics & Probability Letters, 136, pp. 4-9."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "resources/slides/14-testing-submodels.html#outline",
    "href": "resources/slides/14-testing-submodels.html#outline",
    "title": "Hypothesis Testing Related to SubModels",
    "section": "Outline",
    "text": "Outline\nHypothesis Testing:\n\nTesting submodels\n\nExtra sum of squares\nF-tests\nNull distribution\nDecision procedure\nP-values\n\nTesting individual coefficients\n\nt-tests\n\nLikelihood Ratio Tests\n\n\nReadings:\n\nChristensen Appendix C, Chapter 3"
  },
  {
    "objectID": "resources/slides/14-testing-submodels.html#testing-recap",
    "href": "resources/slides/14-testing-submodels.html#testing-recap",
    "title": "Hypothesis Testing Related to SubModels",
    "section": "Testing Recap",
    "text": "Testing Recap\n\nWe assume the Gaussian Linear Model \\[\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\text{M1} \\quad \\mathbf{Y}∼ \\textsf{N}(\\mathbf{W}\\boldsymbol{\\alpha}+ \\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2\\mathbf{I}) \\equiv \\textsf{N}(\\mathbf{Z}\\boldsymbol{\\theta}, \\sigma^2\\mathbf{I})\\] where \\(\\mathbf{W}\\) is \\(n \\times q\\), \\(\\mathbf{X}\\) is \\(n \\times p\\), \\(\\mathbf{Z}= [\\mathbf{W}\\mathbf{X}]\\),\nWe wish to evaluate the hypothesis \\(\\boldsymbol{\\beta}= \\mathbf{0}\\)\nequivalent to comparing M1 to M0: \\[\\text{M0} \\quad \\mathbf{Y}∼ \\textsf{N}(\\mathbf{W}\\boldsymbol{\\alpha}, \\sigma^2\\mathbf{I})\\]\n\\(\\textsf{SSE}_{M0}/(n-q)\\) and \\(\\textsf{SSE}_{M1}/(n- q - p)\\) are unbiased estimates of \\(\\sigma^2\\) under null model M0\nbut the ratio \\(\\frac{\\textsf{SSE}_{M0}/(n-q)}{\\textsf{SSE}_{M1}/(n- q - p)}\\) does not have a F distribution"
  },
  {
    "objectID": "resources/slides/14-testing-submodels.html#extra-sum-of-squares",
    "href": "resources/slides/14-testing-submodels.html#extra-sum-of-squares",
    "title": "Hypothesis Testing Related to SubModels",
    "section": "Extra Sum of Squares",
    "text": "Extra Sum of Squares\nRewrite \\(\\textsf{SSE}_{M0}\\): \\[\\begin{align*}\n\\textsf{SSE}_{M0} & = \\mathbf{Y}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}\\\\\n          & = \\mathbf{Y}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{Z}} + \\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}\\\\\n          & = \\mathbf{Y}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{Z}})\\mathbf{Y}+ \\mathbf{Y}^T(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}\\\\\n          & = \\textsf{SSE}_{M1} + \\mathbf{Y}^T(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}\n\\end{align*}\\]\n\nExtra Sum of Squares: \\[\\textsf{SSE}_{M0} - \\textsf{SSE}_{M1}  = \\mathbf{Y}^T(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}\\]"
  },
  {
    "objectID": "resources/slides/14-testing-submodels.html#expectation-of-extra-sum-of-squares",
    "href": "resources/slides/14-testing-submodels.html#expectation-of-extra-sum-of-squares",
    "title": "Hypothesis Testing Related to SubModels",
    "section": "Expectation of Extra Sum of Squares",
    "text": "Expectation of Extra Sum of Squares\n\\(\\textsf{E}[\\textsf{SSE}_{M0} - \\textsf{SSE}_{M1}] = \\textsf{E}[\\mathbf{Y}^T(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}]\\)\n\nunder M0: \\(\\boldsymbol{\\mu}= \\mathbf{W}\\boldsymbol{\\alpha}\\) \\[\\begin{align*}\n\\textsf{E}[(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}] & = \\mathbf{P}_{\\mathbf{Z}}\\mathbf{W}\\boldsymbol{\\alpha}- \\mathbf{P}_{\\mathbf{W}}\\mathbf{W}\\boldsymbol{\\alpha}\\\\\n& = \\mathbf{W}\\boldsymbol{\\alpha}\\mathbf{W}_\\boldsymbol{\\alpha}\\\\\n& = \\mathbf{0}\\\\\n\\textsf{E}[\\mathbf{Y}^T(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}] & = \\sigma^2(\\textsf{tr}\\mathbf{P}_\\mathbf{Z}+ \\textsf{tr}\\mathbf{P}_\\mathbf{W}) \\\\\n& = \\sigma^2 (q + p - q) = p \\sigma^2\n\\end{align*}\\]\nunder M1: \\(\\boldsymbol{\\mu}= \\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{W}\\boldsymbol{\\alpha}\\) \\[\\begin{align*}\n\\textsf{E}[(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}] & = \\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{W}\\boldsymbol{\\alpha}- \\mathbf{P}_{\\mathbf{W}}\\mathbf{X}\\boldsymbol{\\beta}- \\mathbf{W}\\boldsymbol{\\alpha}\\\\\n& = (\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\mathbf{X}\\boldsymbol{\\beta}\\\\\n\\textsf{E}[\\mathbf{Y}^T(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}] & = p \\sigma^2 + \\boldsymbol{\\beta}^T\\mathbf{X}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\mathbf{X}\\boldsymbol{\\beta}\n\\end{align*}\\]"
  },
  {
    "objectID": "resources/slides/14-testing-submodels.html#test-statistic",
    "href": "resources/slides/14-testing-submodels.html#test-statistic",
    "title": "Hypothesis Testing Related to SubModels",
    "section": "Test Statistic",
    "text": "Test Statistic\nPropose ratio: \\[F = \\frac{(\\textsf{SSE}_{M0} - \\textsf{SSE}_{M1})/p} {\\textsf{SSE}_{M1}/(n - q - p)} = \\frac{\\mathbf{Y}^T(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}/p}{\\textsf{SSE}_{M1}/(n - q - p)}\\] as a test statistic.\n\nDoes \\(F\\) have an F distribution under M0?\n\ndenominator \\(\\textsf{SSE}_{M1}/\\sigma^2\\) does have a \\(\\chi^2\\) distribution?\ndoes numerator \\(\\textsf{SSE}_{M0}/\\sigma^2\\) have a \\(\\chi^2\\) distribution?\nare they independent?"
  },
  {
    "objectID": "resources/slides/14-testing-submodels.html#properties-of-mathbfp_mathbfz---mathbfp_mathbfw",
    "href": "resources/slides/14-testing-submodels.html#properties-of-mathbfp_mathbfz---mathbfp_mathbfw",
    "title": "Hypothesis Testing Related to SubModels",
    "section": "Properties of \\(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}}\\)",
    "text": "Properties of \\(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}}\\)\nTo show that \\(\\mathbf{Y}^T(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}\\) has a \\(\\chi^2\\) distribution under M0 or M1, we need to show that \\(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}}\\) is a projection matrix.\n\nsymmetric?\nidempotent?\n\n\n\\[\\begin{align*}\n(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}})^2 & = \\mathbf{P}_{\\mathbf{Z}}^2 - \\mathbf{P}_{\\mathbf{Z}}\\mathbf{P}_{\\mathbf{W}} - \\mathbf{P}_{\\mathbf{W}}\\mathbf{P}_{\\mathbf{Z}} + \\mathbf{P}_{\\mathbf{W}}^2 \\\\\n& = \\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{Z}}\\mathbf{P}_{\\mathbf{W}} - \\mathbf{P}_{\\mathbf{W}}\\mathbf{P}_{\\mathbf{Z}} + \\mathbf{P}_{\\mathbf{W}} \\\\\n& = \\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{Z}}\\mathbf{P}_{\\mathbf{W}} - (\\mathbf{P}_{\\mathbf{Z}}\\mathbf{P}_{\\mathbf{W}})^T + \\mathbf{P}_{\\mathbf{W}} \\\\\n& = \\mathbf{P}_{\\mathbf{Z}} - 2\\mathbf{P}_{\\mathbf{W}}  + \\mathbf{P}_{\\mathbf{W}} \\\\\n& = \\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}}\n\\end{align*}\\]\n\nNote: we are using \\(\\mathbf{P}_{\\mathbf{Z}}\\mathbf{P}_{\\mathbf{W}} = \\mathbf{P}_{\\mathbf{W}}\\) as each column of \\(\\mathbf{P}_{\\mathbf{W}}\\) is in \\(C(\\mathbf{W})\\) and hence also in \\(C(\\mathbf{Z})\\)\n\n\n\nSo \\(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}}\\) is a projection matrix"
  },
  {
    "objectID": "resources/slides/14-testing-submodels.html#projection-matrix-mathbfp_mathbfz---mathbfp_mathbfw",
    "href": "resources/slides/14-testing-submodels.html#projection-matrix-mathbfp_mathbfz---mathbfp_mathbfw",
    "title": "Hypothesis Testing Related to SubModels",
    "section": "Projection Matrix \\(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}}\\)",
    "text": "Projection Matrix \\(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}}\\)\nOnto what space is it projecting?\n\nIntuitively, it is projecting onto the part of \\(\\mathbf{X}\\) that is not in \\(\\mathbf{W}\\), \\(\\tilde{\\mathbf{X}}= (\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\mathbf{X}\\) (the part of \\(\\mathbf{X}\\) that is orthogonal to \\(\\mathbf{W}\\))\n\\(C(\\tilde{\\mathbf{X}})\\) and \\(C(\\mathbf{W})\\) are complementary orthogonal subspaces of \\(C(\\mathbf{Z})\\)\n\\(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}}\\) is a projection matrix onto \\(C(\\tilde{\\mathbf{X}})\\) along \\(C(\\mathbf{W})\\)\nwe are decomposing \\(C(\\mathbf{Z})\\) into two orthogonal subspaces \\(C(\\mathbf{W})\\) and \\(C(\\tilde{\\mathbf{X}})\\)\nWe can write \\(\\mathbf{P}_{\\mathbf{Z}} = \\mathbf{P}_{\\tilde{\\mathbf{X}}} + \\mathbf{P}_{\\mathbf{W}}\\) where \\(\\mathbf{P}_{\\tilde{\\mathbf{X}}} \\mathbf{P}_{\\mathbf{W}} = \\mathbf{P}_{\\mathbf{W}} \\mathbf{P}_{\\tilde{\\mathbf{X}}} = \\mathbf{0}\\)\n\n\nNote: we can always write \\[\\begin{align*}\n\\boldsymbol{\\mu}& = \\mathbf{W}\\boldsymbol{\\alpha}+ \\mathbf{X}\\boldsymbol{\\beta}\\\\\n     & = \\mathbf{W}\\boldsymbol{\\alpha}+ (\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{P}_{\\mathbf{W}}\\mathbf{X}\\boldsymbol{\\beta}\\\\\n     & = \\mathbf{W}\\tilde{\\boldsymbol{\\alpha}} + \\tilde{\\mathbf{X}}\\boldsymbol{\\beta}\n\\end{align*}\\]"
  },
  {
    "objectID": "resources/slides/14-testing-submodels.html#distribution-of-extra-sum-of-squares",
    "href": "resources/slides/14-testing-submodels.html#distribution-of-extra-sum-of-squares",
    "title": "Hypothesis Testing Related to SubModels",
    "section": "Distribution of Extra Sum of Squares",
    "text": "Distribution of Extra Sum of Squares\n\nSince \\(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}}\\) is a projection matrix\n\\(\\mathbf{Y}^T(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}/\\sigma^2\\) has a \\(\\chi^2_p\\) distribution under M0\n\n\n\\[\\begin{align*}\n\\mathbf{Y}^T(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}& = \\|(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}\\|^2 \\\\\n& = \\|(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}})(\\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{W}\\alpha + \\boldsymbol{\\epsilon}\\|^2 \\\\\n& = \\|(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}})(\\mathbf{X}\\boldsymbol{\\beta}\\boldsymbol{\\epsilon}\\|^2 \\\\\n& = \\|(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}})\\boldsymbol{\\epsilon}\\|^2 \\quad \\text{ if } \\boldsymbol{\\beta}= \\mathbf{0}\\\\\n& = \\boldsymbol{\\epsilon}^T(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}})\\boldsymbol{\\epsilon}\\\\\n& \\sim \\sigma^2 \\chi^2_p \\quad \\text{ if } \\boldsymbol{\\beta}= \\mathbf{0}\n\\end{align*}\\]\n\nshow that \\(\\mathbf{Y}^T(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}\\) and \\(\\mathbf{Y}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{Z}})\\mathbf{Y}\\) are independent"
  },
  {
    "objectID": "resources/slides/14-testing-submodels.html#f-statistic",
    "href": "resources/slides/14-testing-submodels.html#f-statistic",
    "title": "Hypothesis Testing Related to SubModels",
    "section": "F-Statistic",
    "text": "F-Statistic\nUnder M1: \\(\\boldsymbol{\\beta}= \\mathbf{0}\\)\n\\[\\begin{align*}\nF(\\mathbf{Y}) & = \\frac{(\\textsf{SSE}_{M0} - \\textsf{SSE}_{M1})/p}{\\textsf{SSE}_{M1}/(n - q - p)} \\\\\n      & = \\frac{(\\textsf{SSE}_{M0} - \\textsf{SSE}_{M1})/\\sigma^2p}{\\textsf{SSE}_{M1}/\\sigma^2(n - q - p)} \\\\\n      & \\mathrel{\\mathop{=}\\limits^{\\rm D}}\\frac{\\chi^2_p/p}{\\chi^2_{n-q-p}/(n-q-p)} \\\\\n      & \\mathrel{\\mathop{=}\\limits^{\\rm D}}F_{p, n-q-p}\n\\end{align*}\\]\n\nUnder M1, \\(\\mathbf{Y}^T(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}/\\sigma^2\\) has a non-central \\(\\chi^2_{p, \\eta}\\) where the non-centrality parameter \\(\\eta = \\boldsymbol{\\beta}^T\\mathbf{X}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\mathbf{X}\\boldsymbol{\\beta}/2\\sigma^2\\).\n\\(F\\) has a non-central F distribution with \\(p\\) and \\(n-q-p\\) degrees of freedom and non-centrality parameter \\(\\eta = \\boldsymbol{\\beta}^T\\mathbf{X}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\mathbf{X}\\boldsymbol{\\beta}/2\\sigma^2\\) (See Christensen Theorem 3.2.1 and Appendix C)"
  },
  {
    "objectID": "resources/slides/14-testing-submodels.html#testing-individual-coefficients",
    "href": "resources/slides/14-testing-submodels.html#testing-individual-coefficients",
    "title": "Hypothesis Testing Related to SubModels",
    "section": "Testing Individual Coefficients",
    "text": "Testing Individual Coefficients\nConsider the model with \\(p = 1\\), \\(\\mathbf{Y}= \\mathbf{W}\\boldsymbol{\\alpha}+ \\mathbf{x}\\beta + \\boldsymbol{\\epsilon}\\) and we want to test that \\(\\beta = 0\\) (M0)\n\nfit the full model and compute \\(\\textsf{SSE}_{M1}\\)\nfit the reduced model and compute \\(\\textsf{SSE}_{M0}\\)\ncalculate the \\(F\\) statistic and \\(p\\)-value\n\n\nIt turns out that we can obtain this \\(F\\) statistic by fitting the full model and the test reduces to a familiar \\(t\\)-test\n\n\n\\[\\begin{align*}\n\\hspace{-1in}{\\text{Note: }} \\hspace{1in} \\textsf{SSE}_{M0} - \\textsf{SSE}_{M1} & =  \\mathbf{Y}^T(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}\\\\\n& = \\|(\\mathbf{P}_{\\tilde{\\mathbf{X}}} + \\mathbf{P}_{\\mathbf{W}} - \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}\\|^2 \\\\\n& = \\|\\mathbf{P}_{\\tilde{\\mathbf{X}}}\\mathbf{Y}\\|^2 \\\\\n& = \\|(\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\|^2 \\\\\n& = \\hat{\\boldsymbol{\\beta}}^T\\mathbf{X}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\end{align*}\\]"
  },
  {
    "objectID": "resources/slides/14-testing-submodels.html#testing-individual-coefficients-1",
    "href": "resources/slides/14-testing-submodels.html#testing-individual-coefficients-1",
    "title": "Hypothesis Testing Related to SubModels",
    "section": "Testing Individual Coefficients",
    "text": "Testing Individual Coefficients\nFor \\(p = 1\\), the \\(F\\) statistic\n\\[\\begin{align*}\nF(\\mathbf{Y}) & = \\frac{(\\textsf{SSE}_{M0} - \\textsf{SSE}_{M1})/1}{\\textsf{SSE}_{M1}/(n - q - 1)} \\\\\n  & = \\frac{\\hat{\\beta}^T\\mathbf{x}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\mathbf{x}\\hat{\\beta}}{s^2} \\\\\n  & = \\frac{\\hat{\\beta}^2}{s^2/\\mathbf{x}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\mathbf{x}} \\\\\nF(\\mathbf{Y})   & \\sim F_{1, n - q - 1}  \\quad \\text{ under } \\beta = 0\n\\end{align*}\\]\n\n\nvariance of \\(\\hat{\\beta}\\): \\[\\begin{align*}\n\\textsf{var}[\\hat{\\beta}] & = \\sigma^2/\\mathbf{x}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\mathbf{x}= \\sigma^2 v\\\\\nv & = 1/\\mathbf{x}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\mathbf{x}\n\\end{align*}\\]"
  },
  {
    "objectID": "resources/slides/14-testing-submodels.html#t-statistic",
    "href": "resources/slides/14-testing-submodels.html#t-statistic",
    "title": "Hypothesis Testing Related to SubModels",
    "section": "\\(t\\)-statistic",
    "text": "\\(t\\)-statistic\n\\[\\begin{align*}\nF(\\mathbf{Y}) & = \\frac{\\hat{\\beta}^2}{s^2/\\mathbf{x}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\mathbf{x}}\n= \\left(\\frac{\\hat{\\beta}}{s \\sqrt{v}}\\right)^2\n=  t(\\mathbf{Y})^2\n\\end{align*}\\]\n\nSince \\(F(\\mathbf{Y}) \\sim F(1, n - q - 1)\\) under M0: \\(\\beta = 0\\), \\(t(\\mathbf{Y})^2 \\sim F(1,n - q - 1)\\) under M0: \\(\\beta = 0\\)\nwhat is distribution of \\(t(\\mathbf{Y})\\) under M0: \\(\\beta \\ne 0\\)?\n\n\nRecall that under M0: \\(\\beta = 0\\),\n\n\\(\\hat{\\beta}/\\sqrt{v\\sigma^2} \\sim \\textsf{N}(0, 1)\\)\n\\((n-q-1)s^2/\\sigma^2 \\sim \\chi^2_{n-q-1}\\)\n\\(\\hat{\\beta}\\) and \\(s^2\\) are independent"
  },
  {
    "objectID": "resources/slides/14-testing-submodels.html#student-t-distribution",
    "href": "resources/slides/14-testing-submodels.html#student-t-distribution",
    "title": "Hypothesis Testing Related to SubModels",
    "section": "Student \\(t\\) Distribution",
    "text": "Student \\(t\\) Distribution\n\nTheorem: Student \\(t\\) DistributionA random variable \\(T\\) has a Student \\(t\\) distribution with \\(\\nu\\) degrees of freedom if \\[T \\mathrel{\\mathop{=}\\limits^{\\rm D}}\\frac{Z}{X/\\nu}\\]\nwhere \\[\\begin{align*}\nZ & \\sim \\textsf{N}(0,1) \\\\\nX & \\sim \\chi^2_\\nu \\\\\nZ &\\text{ and }  X \\text{ are independent }\n\\end{align*}\\]\n\n\n\n\\(\\therefore \\, t(\\mathbf{Y}) = \\hat{\\beta}/\\sqrt{v\\sigma^2}\\) has a Student \\(t\\) distribution with \\(n - q - 1\\) degrees of freedom under M0: \\(\\beta = 0\\)"
  },
  {
    "objectID": "resources/slides/14-testing-submodels.html#decision-rules-and-p-values",
    "href": "resources/slides/14-testing-submodels.html#decision-rules-and-p-values",
    "title": "Hypothesis Testing Related to SubModels",
    "section": "Decision rules and \\(p\\)-values",
    "text": "Decision rules and \\(p\\)-values\n\n\n\nan \\(F_{1, \\nu}\\) is equal in distribution to the square of Student \\(t_{\\nu}\\) distribution under the null model (also equal in distribution under the full model, but have a non-centrality parameter)\nDecision rule was to reject M0 if \\(F(\\mathbf{Y}) &gt; F_{1, n - q - 1, \\alpha}\\)\n\\(p\\)-value is \\(\\Pr(F_{1, n - q - 1} &gt; F(\\mathbf{Y})\\); the probability of observing a value of \\(F\\) as extreme as the observed value under the null model\nusing a t-distribution, the equivalent decision rule is to reject M0 if \\(|t(\\mathbf{Y})| &gt; t_{n - q - 1, \\alpha/2}\\)\n\\(p\\)-value is \\(\\Pr(|T_{n - q - 1}| &gt; |t(\\mathbf{Y})|)\\)\nequal-tailed \\(t\\)-test"
  },
  {
    "objectID": "resources/slides/14-testing-submodels.html#likelihood-ratio-tests",
    "href": "resources/slides/14-testing-submodels.html#likelihood-ratio-tests",
    "title": "Hypothesis Testing Related to SubModels",
    "section": "Likelihood Ratio Tests",
    "text": "Likelihood Ratio Tests\n\nwe derived the \\(F\\)-test heurestically, but the formally this test may be derived as a likelihood ratio test.\nconsider a statistical model \\(\\mathbf{Y}\\sim P,  P \\in \\{P_\\boldsymbol{\\theta}: \\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}\\}\\)\n\\(P\\) is the true unknown distribution for \\(\\mathbf{Y}\\)\n\\(\\{P_\\boldsymbol{\\theta}: \\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}\\}\\) is the model, the set of possible distributions for \\(\\mathbf{Y}\\) with \\(\\boldsymbol{\\Theta}\\) the parameter space\nwe might hypothesize that \\(\\boldsymbol{\\theta}\\subset \\boldsymbol{\\Theta}_0 \\subset \\boldsymbol{\\Theta}\\)\nfor our linear model this translates as \\(\\boldsymbol{\\theta}= (\\boldsymbol{\\alpha}, \\boldsymbol{\\beta}, \\sigma^2) \\subset \\mathbb{R}^q \\times \\{\\mathbf{0}\\} \\times \\mathbb{R}^+ \\subset \\mathbb{R}^g \\times \\mathbb{R}^p \\times \\mathbb{R}^+\\)\ncompute the likelihood ratio statistic \\[R(\\mathbf{Y}) = \\frac{\\sup_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}_0} p_\\boldsymbol{\\theta}(\\mathbf{Y}))}{\\sup_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}} p_\\boldsymbol{\\theta}(\\mathbf{Y}))}\\]"
  },
  {
    "objectID": "resources/slides/14-testing-submodels.html#likelihood-ratio-tests-1",
    "href": "resources/slides/14-testing-submodels.html#likelihood-ratio-tests-1",
    "title": "Hypothesis Testing Related to SubModels",
    "section": "Likelihood Ratio Tests",
    "text": "Likelihood Ratio Tests\nEquivalently, we can look at -2 times the log likelihood ratio statistic \\[\\lambda(\\mathbf{Y}) = -2\\log(R(\\mathbf{Y})) = -2 [\\sup_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}_0} \\cal{l}(\\boldsymbol{\\theta})- \\sup_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}} \\cal{l}(\\boldsymbol{\\theta})]\\] where \\(\\cal{l}(\\boldsymbol{\\theta}) \\propto \\log p_\\boldsymbol{\\theta}(\\mathbf{Y})\\) (the log likelihood)\n\nSteps:\n\nFind the MLEs of \\(\\boldsymbol{\\theta}\\) in the reduced model \\(\\boldsymbol{\\Theta}_0\\), \\(\\hat{\\boldsymbol{\\theta}}_0\\)\nFind the MLEs of $the full model \\(\\boldsymbol{\\Theta}\\), \\(\\hat{\\boldsymbol{\\theta}}\\)\nCompute \\(\\lambda(\\mathbf{Y}) = -2 [\\cal{l}(\\hat{\\boldsymbol{\\theta}}_0)- \\cal{l}(\\hat{\\boldsymbol{\\theta}})]\\)\nFind the distribution of \\(\\lambda(\\mathbf{Y})\\) under the reduced model\n\n\n\nwith some rearranging and 1-to-1 transformations, can show that this is equivalent to the \\(F\\)-test! (HW)\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/12-oracle.html#outline",
    "href": "resources/slides/12-oracle.html#outline",
    "title": "Optimal Shrinkage/Selection and Oracle Properties",
    "section": "Outline",
    "text": "Outline\n\nBounded Influence and Posterior Mean\nShrinkage properties and nonconcave penalties\nconditions for optimal shrinkage and selection . . .\nReadings (see reading link)\n\nTibshirani (JRSS B 1996)\nCarvalho, Polson & Scott (Biometrika 2010)\nArmagan, Dunson & Lee (Statistica Sinica 2013)\nFan & Li (JASA 2001)"
  },
  {
    "objectID": "resources/slides/12-oracle.html#horseshoe-priors",
    "href": "resources/slides/12-oracle.html#horseshoe-priors",
    "title": "Optimal Shrinkage/Selection and Oracle Properties",
    "section": "Horseshoe Priors",
    "text": "Horseshoe Priors\nCarvalho, Polson & Scott (2010) propose an alternative shrinkage prior \\[\\begin{align*}\n\\boldsymbol{\\beta}\\mid \\phi & \\sim \\textsf{N}(\\mathbf{0}_p, \\frac{\\textsf{diag}(\\tau^2)}{ \\phi\n    }) \\\\\n\\tau \\mid \\lambda & \\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}C^+(0, \\lambda) \\\\\n\\lambda & \\sim \\textsf{C}^+(0, 1/\\phi) \\\\\np(\\alpha, \\phi) & \\propto 1/\\phi\n\\end{align*}\\]\n\n\\(C^+(0, \\lambda)\\) is the half-Cauchy distribution with scale \\(\\lambda\\) \\[\np(\\tau \\mid \\lambda) = \\frac{2}{\\pi} \\frac{\\lambda}{\\lambda^2 + \\tau_j^2}\n\\]\n\\(\\textsf{C}^+(0, 1/\\phi)\\) is the half-Cauchy distribution with scale \\(1/\\phi\\)"
  },
  {
    "objectID": "resources/slides/12-oracle.html#special-case-orthonormal-regression",
    "href": "resources/slides/12-oracle.html#special-case-orthonormal-regression",
    "title": "Optimal Shrinkage/Selection and Oracle Properties",
    "section": "Special Case: Orthonormal Regression",
    "text": "Special Case: Orthonormal Regression\n\n\nIn the case \\(\\lambda = \\phi = 1\\) and with \\(\\mathbf{X}^t\\mathbf{X}= \\mathbf{I}\\), \\(\\mathbf{Y}^* =\n\\mathbf{X}^T\\mathbf{Y}\\) \\[\\begin{align*}\nE[\\beta_i \\mid \\mathbf{Y}] & = \\textsf{E}_{\\kappa_i \\mid \\mathbf{Y}}[ \\textsf{E}_{\\beta_i \\mid \\kappa_i, \\mathbf{Y}}[\\beta_i \\mid \\mathbf{Y}] \\\\\n& = \\int_0^1 (1 - \\kappa_i) y^*_i p(\\kappa_i \\mid \\mathbf{Y})\n\\ d\\kappa_i \\\\\n& = (1 - \\textsf{E}[\\kappa \\mid y^*_i]) y^*_i\n\\end{align*}\\] where \\(\\kappa_i = 1/(1 + \\tau_i^2)\\) is the shrinkage factor (like in James-Stein)\n\nHalf-Cauchy prior induces a Beta(1/2, 1/2) distribution on \\(\\kappa_i\\) a priori (change of variables)\nmarginal prior (after integrating out )"
  },
  {
    "objectID": "resources/slides/12-oracle.html#bounded-influence-mathbfxtmathbfx-mathbfi",
    "href": "resources/slides/12-oracle.html#bounded-influence-mathbfxtmathbfx-mathbfi",
    "title": "Optimal Shrinkage/Selection and Oracle Properties",
    "section": "Bounded Influence (\\(\\mathbf{X}^T\\mathbf{X}= \\mathbf{I}\\))",
    "text": "Bounded Influence (\\(\\mathbf{X}^T\\mathbf{X}= \\mathbf{I}\\))\n\n\n\nPosterior mean of \\(\\beta_i\\) may be written as \\[E[\\beta_i \\mid y^*_i] = y^*_i + \\frac{d} {d y} \\log m(y^*_i)\\] where \\(m(y)\\) is the predictive density \\(y^*_i\\) under the prior (known \\(\\lambda\\))\nBounded Influence of the prior (in this setting) means that \\[\\lim_{|y| \\to \\infty} \\frac{d}{dy} \\log m(y) = c\\]\nFor HS \\(\\lim_{|y| \\to \\infty} \\frac{d}{dy} \\log m(y) = 0\\)\n\\(\\lim_{|y_i^*| \\to \\infty} E[\\beta_i \\mid y^*_i) \\to y^*_i\\) (the MLE)\nunbiasedness for large \\(|y_i^*|\\)\n\n\n\n\nDE has bounded influence, but bound does not decay to zero in tails so the posterior mean does not shrink to the MLE (bounded away)"
  },
  {
    "objectID": "resources/slides/12-oracle.html#comparison",
    "href": "resources/slides/12-oracle.html#comparison",
    "title": "Optimal Shrinkage/Selection and Oracle Properties",
    "section": "Comparison",
    "text": "Comparison\n\n\n\nDiabetes data (from the lars package)\n64 predictors: 10 main effects, 2-way interactions and quadratic terms\n\n\nsample size of 442\nsplit into training and test sets\ncompare MSE for out-of-sample prediction using OLS, lasso and horseshoe priors\nRoot MSE for prediction for left out data based on 25 different random splits with 100 test cases\nboth Lasso and Horseshoe much better than OLS"
  },
  {
    "objectID": "resources/slides/12-oracle.html#duality-for-modal-estimators",
    "href": "resources/slides/12-oracle.html#duality-for-modal-estimators",
    "title": "Optimal Shrinkage/Selection and Oracle Properties",
    "section": "Duality for Modal Estimators",
    "text": "Duality for Modal Estimators\n\nModel \\(Y = \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\) with \\(\\mathbf{X}^T\\mathbf{X}= \\mathbf{I}_p\\) and \\(\\hat{\\boldsymbol{\\beta}}= \\mathbf{X}^T \\mathbf{Y}\\equiv \\mathbf{Y}^*\\) (take \\(\\sigma^2 = 1\\))\nPenalized Least Squares \\[\n\\hat{\\boldsymbol{\\beta}}_j^\\lambda = \\mathop{\\mathrm{argmin}}_{\\boldsymbol{\\beta}} \\ \\frac{1}{2}\\| \\mathbf{Y}- \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\|^2 + \\frac{1}{2} \\sum_j(\\beta_j -\\hat{\\beta}_j)^2 + \\sum_j \\textsf{pen}_\\lambda(\\beta_j)\n\\]\nBayes posterior mode (conditional) with prior \\(p(\\boldsymbol{\\beta}\\mid \\lambda) = \\prod_j p(\\beta_j \\mid \\lambda)\\) \\[\\begin{align*}\n\\hat{\\boldsymbol{\\beta}}_j^\\lambda & =\\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\beta}} -\\frac{1}{2}  \\| \\mathbf{Y}- \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\|^2 - \\frac{1}{2} \\sum_j(\\beta_j -\\hat{\\beta}_j)^2 + \\sum_j \\log(p(\\beta_j \\mid \\lambda)) \\\\\n& = \\mathop{\\mathrm{argmin}}_{\\boldsymbol{\\beta}}  \\frac{1}{2}\\| \\mathbf{Y}- \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\|^2 + \\frac{1}{2} \\sum_j(\\beta_j -\\hat{\\beta}_j)^2  -  \\sum_j\\log(p(\\beta_j \\mid \\lambda)) \\\\\n& = \\mathop{\\mathrm{argmin}}_{\\boldsymbol{\\beta}}  \\frac{1}{2}\\| \\mathbf{Y}- \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\|^2 + \\frac{1}{2} \\sum_j(\\beta_j -\\hat{\\beta}_j)^2  + \\sum_j\\textsf{pen}_\\lambda(\\beta_j) \\\\\n\\end{align*}\\]"
  },
  {
    "objectID": "resources/slides/12-oracle.html#properties-for-modal-estimates",
    "href": "resources/slides/12-oracle.html#properties-for-modal-estimates",
    "title": "Optimal Shrinkage/Selection and Oracle Properties",
    "section": "Properties for Modal Estimates",
    "text": "Properties for Modal Estimates\nFan & Li (JASA 2001) discuss variable selection via nonconcave penalties and oracle properties in the context of penalized likelihoods in this setting\n\nwith duality of the negative log prior as their penalty we can extend to Bayesian modal estimates where the prior is a function of \\(|\\beta_j|\\) \\[\\frac 1 2 \\sum(\\beta_i - y_i^*)^2 + \\frac 1 2 \\sum_j(\\beta_j - \\hat{\\beta}_j)^2 +  \\sum_j \\textsf{pen}_\\lambda(|\\beta_j|)\\]\nRequirements on penality\n\nUnbiasedness: The resulting estimator is nearly unbiased when the true unknown parameter is large (avoid unnecessary modeling bias).\nSparsity: thresholding rule sets small coefficients to 0 (avoid model complexity)\nContinuity: continuous in the data \\(\\hat{\\beta}_j = y_i^*\\) (avoid instability in model prediction)"
  },
  {
    "objectID": "resources/slides/12-oracle.html#conditions-for-unbiasedness",
    "href": "resources/slides/12-oracle.html#conditions-for-unbiasedness",
    "title": "Optimal Shrinkage/Selection and Oracle Properties",
    "section": "Conditions for Unbiasedness",
    "text": "Conditions for Unbiasedness\nTo find the optimal estimator take derivative of \\(\\frac 1 2 \\sum_j(\\beta_j - \\hat{\\beta}_j)^2 +  \\sum_j \\textsf{pen}_\\lambda(|\\beta_j|)\\) componentwise and set to zero\n\nDerivative is \\[\\begin{align*}\n\\frac{d}{d\\,\\beta_j} \\left\\{\\frac 1 2 (\\beta_j - \\hat{\\beta}_j)^2 +   \\textsf{pen}_\\lambda(|\\beta_j|)\\right\\}\n& = (\\beta_j - \\hat{\\beta}_j) + \\mathop{\\mathrm{sgn}}(\\beta_j)\\textsf{pen}^\\prime_\\lambda(|\\beta_j|) \\\\\n& = \\mathop{\\mathrm{sgn}}(\\beta_j)\\left\\{|\\beta_j| + \\textsf{pen}^\\prime_\\lambda(|\\beta_j|) \\right\\} - \\hat{\\beta}_j\n\\end{align*}\\]\nsetting derivative to zero gives \\(\\hat{\\beta}_j = \\mathop{\\mathrm{sgn}}(\\beta_j)\\left\\{|\\beta_j| + \\textsf{pen}^\\prime_\\lambda(|\\beta_j|) \\right\\}\\)\nif \\(\\lim_{|\\beta_j| \\to \\infty} \\textsf{pen}^\\prime_\\lambda(|\\beta|) = 0\\) then \\(\\hat{\\beta}_j = \\mathop{\\mathrm{sgn}}(\\beta_j) |\\beta_j| = \\beta_j\\)\nfor large \\(|\\beta_j|\\), \\(|\\hat{\\beta}_j|\\) is large with high probability\nas MLE is unbiased, the optimal estimator is approximately unbiased for large \\(|\\beta_j|\\)"
  },
  {
    "objectID": "resources/slides/12-oracle.html#conditions-for-thresholding-continuity",
    "href": "resources/slides/12-oracle.html#conditions-for-thresholding-continuity",
    "title": "Optimal Shrinkage/Selection and Oracle Properties",
    "section": "Conditions for Thresholding & Continuity",
    "text": "Conditions for Thresholding & Continuity\n\n\nAs sufficient condition for a thresholding rule \\(\\hat{\\boldsymbol{\\beta}}_j^\\lambda = 0\\) is if \\[0 &lt; \\min \\left\\{ |\\beta_j| + \\textsf{pen}^\\prime_\\lambda(|\\beta_j|)\\right\\}\\]\n\nif \\(|\\hat{\\beta}_j| &lt; \\min \\left\\{ |\\beta_j| + \\textsf{pen}^\\prime_\\lambda(|\\beta_j|) \\right\\}\\) then the derivative is positive for all positive \\(\\beta_j\\) and negative for all negative \\(\\beta_j\\) so \\(\\hat{\\beta}_j^\\lambda = 0\\) is a local minimum\nif \\(|\\hat{\\beta}_j| &gt; \\min \\left\\{ |\\beta_j| + \\textsf{pen}^\\prime_\\lambda(|\\beta_j|) \\right\\}\\) multiple crossings (local roots)\na sufficient and necessary condition for continuity is that the minimum of \\(|\\beta_j| + \\textsf{pen}^\\prime_\\lambda(|\\beta_j|)\\) is obtained at zero"
  },
  {
    "objectID": "resources/slides/12-oracle.html#example-gaussian-prior",
    "href": "resources/slides/12-oracle.html#example-gaussian-prior",
    "title": "Optimal Shrinkage/Selection and Oracle Properties",
    "section": "Example: Gaussian Prior",
    "text": "Example: Gaussian Prior\n\n\n\nPrior \\(\\textsf{N}(0, 1/\\lambda^2)\\)\nPenalty: \\(\\textsf{pen}_\\lambda(|\\beta_j|) = \\frac{1}{2} \\lambda |\\beta_j|^2\\)\nUnbiasedness: for large \\(|\\beta_j|\\)?\n\nDerivative of \\(\\textsf{pen}_\\lambda(|\\beta_j|) = \\lambda \\beta_j = \\mathop{\\mathrm{sgn}}(\\beta_j) \\lambda |\\beta_j|\\)\ndoes not go to zero as \\(|\\beta_j| \\to \\infty\\)\nNo! (bias towards zero)\n\nnot a thresholding rule as \\[\\min \\left\\{ |\\beta_j| + \\textsf{pen}^\\prime_\\lambda(|\\beta_j|)\\right\\}  = (1 + \\lambda)|\\beta_j|\\] is zero\nis continuous as minimum is at zero"
  },
  {
    "objectID": "resources/slides/12-oracle.html#example-lasso-prior",
    "href": "resources/slides/12-oracle.html#example-lasso-prior",
    "title": "Optimal Shrinkage/Selection and Oracle Properties",
    "section": "Example: Lasso Prior",
    "text": "Example: Lasso Prior\n\n\n\nPenalty: \\(\\textsf{pen}_\\lambda(|\\beta_j|) =  \\lambda |\\beta_j|\\)\nUnbiasedness: for large \\(|\\beta_j|\\)?\n\nDerivative of \\(\\textsf{pen}_\\lambda(|\\beta_j|) = \\lambda \\mathop{\\mathrm{sgn}}(\\beta_j)\\)\ndoes not go to zero as \\(|\\beta_j| \\to \\infty\\)\nNo! (bias towards zero)\n\nIs a thresholding rule as \\[\\min \\left\\{ |\\beta_j| + \\textsf{pen}^\\prime_\\lambda(|\\beta_j|)\\right\\}  = (|\\beta_j| +  \\lambda) &gt; 0 \\]\nis continuous as minimum is at \\(\\beta_j = 0\\)"
  },
  {
    "objectID": "resources/slides/12-oracle.html#generalized-double-pareto-prior",
    "href": "resources/slides/12-oracle.html#generalized-double-pareto-prior",
    "title": "Optimal Shrinkage/Selection and Oracle Properties",
    "section": "Generalized Double Pareto Prior",
    "text": "Generalized Double Pareto Prior\nThe Generalized Double Pareto of Armagan, Dunson & Lee (2013)\nhas a prior density for \\(\\beta_j\\) of the form \\[\np(\\beta_j \\mid \\xi, \\alpha) = \\frac{1}{2 \\xi} \\left(1 + \\frac{\\beta_j}{\\alpha \\xi}\\right)^{-(1 + \\alpha)}  \n\\]\n\nexpress as \\(\\beta_j \\mid \\xi, \\alpha \\sim \\textsf{GDP}(\\xi, \\alpha)\\)\nScale mixtures of Normals representation \\[\\begin{align*}\n\\beta \\mid \\tau_j & \\sim \\textsf{N}(0, \\tau_j) \\\\\n\\tau_j \\mid \\lambda_j & \\sim \\textsf{Exp}(\\lambda_j^2/2) \\\\\n\\lambda_j & \\sim \\textsf{Gamma}(\\alpha, \\eta) \\\\\n\\beta_j & \\sim \\textsf{GDP}(\\xi = \\eta/\\alpha, \\alpha)\n\\end{align*}\\]\nis this a thresholding rule? unbiasedness? continuity?\nfor all parameters or are there restrictions?"
  },
  {
    "objectID": "resources/slides/12-oracle.html#choice-of-penaltyprior-and-conditions",
    "href": "resources/slides/12-oracle.html#choice-of-penaltyprior-and-conditions",
    "title": "Optimal Shrinkage/Selection and Oracle Properties",
    "section": "Choice of Penalty/Prior and Conditions",
    "text": "Choice of Penalty/Prior and Conditions\n\nRidge: none\nLasso: does not satisfy conditions for unbiasedness\nGDP: Can show that Generalized Double Pareto does for some choices of hyperparameters\nHorseshoe: need marginal distribution of \\(\\beta_j\\) for penalty\n\nmarginal generally not available in closed form\ncan show for a special case where there is an analytic expression for the marginal density (\\(\\lambda = \\phi = 1\\)) \\[p(\\beta) = k \\exp(\\beta^2/2) E_1(\\beta^2/2)\\]\n\nwhere \\(E_n(x) = \\int_1^\\infty \\frac{e^{-xt}}{t^n} dt\\) for \\(n = 1, 2, \\ldots\\)\n\\(E_n^\\prime(x) = -E_{n-1}(x)\\) for \\(n = 1, 2, \\ldots\\)"
  },
  {
    "objectID": "resources/slides/12-oracle.html#shrinkage-estimators",
    "href": "resources/slides/12-oracle.html#shrinkage-estimators",
    "title": "Optimal Shrinkage/Selection and Oracle Properties",
    "section": "Shrinkage Estimators",
    "text": "Shrinkage Estimators\nThe literature on shrinkage estimators (with or without selection) is extensive\n\nRidge\nLasso\nElastic Net (Zou & Hastie 2005)\nSCAD (Fan & Li 2001)\nGeneralized Double Pareto Prior (Armagan, Dunson & Lee 2013)\nSpike-and-Slab Lasso (Rockova & George 2018)\n\n\nFor Bayes, choice of estimator\n\nposterior mean (easy via MCMC)\nposterior mode (optimization)\nposterior median (via MCMC)"
  },
  {
    "objectID": "resources/slides/12-oracle.html#selection-and-uncertainty",
    "href": "resources/slides/12-oracle.html#selection-and-uncertainty",
    "title": "Optimal Shrinkage/Selection and Oracle Properties",
    "section": "Selection and Uncertainty",
    "text": "Selection and Uncertainty\n\nPrior/Posterior do not put any probability on the event \\(\\beta_j = 0\\)\nUncertainty that the coefficient is zero?\nSelection solved as a post-analysis decision problem\nSelection part of model uncertainty\n\nadd prior probability that \\(\\beta_j = 0\\)\ncombine with decision problem\n\n\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#outline",
    "href": "resources/slides/04-BLUE.html#outline",
    "title": "Best Linear Unbiased Estimators",
    "section": "Outline",
    "text": "Outline\n\nCharacterizing Linear Unbiased Estimators\nGauss-Markov Theorem\nBest Linear Unbiased Estimators\n\n\nReadings: - Christensen Chapter 1-2 and Appendix B - Seber & Lee Chapter 3"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#full-rank-case",
    "href": "resources/slides/04-BLUE.html#full-rank-case",
    "title": "Best Linear Unbiased Estimators",
    "section": "Full Rank Case",
    "text": "Full Rank Case\n\nModel: \\(\\mathbf{Y}= \\boldsymbol{\\mu}+ \\boldsymbol{\\epsilon}\\)\nMinimal Assumptions:\n\nMean \\(\\boldsymbol{\\mu}\\in C(\\mathbf{X})\\) for \\(\\mathbf{X}\\in \\mathbb{R}^{n \\times p}\\)\nErrors \\(\\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\)\n\n\n\n\nDefinition: Linear Unbiased Estimators (LUEs)\nAn estimator \\(\\tilde{\\boldsymbol{\\beta}}\\) is a Linear Unbiased Estimator (LUE) of \\(\\boldsymbol{\\beta}\\) if\n\nlinearity: \\(\\tilde{\\boldsymbol{\\beta}}= \\mathbf{A}\\mathbf{Y}\\) for \\(\\mathbf{A}\\in \\mathbb{R}^{p \\times n}\\)\nunbiasedness: \\(\\textsf{E}[\\tilde{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\\) for all \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\)\n\n\n\n\n\nThe class of linear unbiased estimators is the same for every model with parameter space \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\) and \\(P \\in \\cal{P}\\), for any collection \\(\\cal{P}\\) of mean-zero distributions over \\(\\mathbb{R}^n\\)."
  },
  {
    "objectID": "resources/slides/04-BLUE.html#linear-unbiased-estimators-lues-1",
    "href": "resources/slides/04-BLUE.html#linear-unbiased-estimators-lues-1",
    "title": "Best Linear Unbiased Estimators",
    "section": "Linear Unbiased Estimators (LUEs)",
    "text": "Linear Unbiased Estimators (LUEs)\n\nLet \\(\\textsf{N}\\) be an ONB for \\(\\boldsymbol{{\\cal N}}= \\boldsymbol{{\\cal M}}^\\perp = N(\\mathbf{X}^T)\\):\n\n\\(\\textsf{N}^T\\mathbf{m}=  \\textsf{N}^T\\mathbf{X}\\mathbf{b}= \\mathbf{0}\\quad \\forall \\mathbf{m}=\\mathbf{X}\\mathbf{b}\\in \\boldsymbol{{\\cal M}}\\)\n\\(\\textsf{N}^T\\textsf{N}= \\mathbf{I}_{n-p}\\)\n\n\n\nConsider another linear estimator \\(\\tilde{\\boldsymbol{\\beta}}= \\mathbf{A}\\mathbf{Y}\\)\n\nDifference between \\(\\tilde{\\boldsymbol{\\beta}}\\) and \\(\\hat{\\boldsymbol{\\beta}}\\) (OLS/MLE): \\[\\begin{align*}\n  \\mathbf{\\delta}= \\tilde{\\boldsymbol{\\beta}}- \\hat{\\boldsymbol{\\beta}}& = \\left(\\mathbf{A}- (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\right)\\mathbf{Y}\\\\\n                  & \\equiv \\mathbf{H}^T \\mathbf{Y}\n\\end{align*}\\]\nSince both \\(\\tilde{\\boldsymbol{\\beta}}\\) and \\(\\hat{\\boldsymbol{\\beta}}\\) are unbiased, \\(\\textsf{E}[\\mathbf{\\delta}] = \\mathbf{0}_p \\quad \\forall \\boldsymbol{\\beta}\\in \\mathbb{R}^p\\) \\[\\mathbf{0}_p = \\textsf{E}[\\mathbf{H}^T \\mathbf{Y}] = \\mathbf{H}^T \\mathbf{X}\\boldsymbol{\\beta}\\quad \\forall \\boldsymbol{\\beta}\\in \\mathbb{R}^p\\]\n\\(\\mathbf{X}^T \\mathbf{H}= \\mathbf{0}\\) so each column of \\(\\mathbf{H}\\) is in \\(\\boldsymbol{{\\cal M}}^\\perp \\equiv \\boldsymbol{{\\cal N}}\\)"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#lues-continued",
    "href": "resources/slides/04-BLUE.html#lues-continued",
    "title": "Best Linear Unbiased Estimators",
    "section": "LUEs continued",
    "text": "LUEs continued\nSince each column of \\(\\mathbf{H}\\) is in \\(\\boldsymbol{{\\cal N}}\\) there exists a \\(\\mathbf{G}\\in \\mathbb{R}^{p \\times (n-p)} \\ni \\mathbf{H}= \\textsf{N}\\mathbf{G}^T\\)\n\nRewriting \\(\\mathbf{\\delta}= \\tilde{\\boldsymbol{\\beta}}- \\hat{\\boldsymbol{\\beta}}\\): \\[\\begin{align*}\n\\tilde{\\boldsymbol{\\beta}}& = \\hat{\\boldsymbol{\\beta}}+ \\mathbf{\\delta}\\\\\n         & = \\hat{\\boldsymbol{\\beta}}+ \\mathbf{H}^T\\mathbf{Y}\\\\\n         & = \\hat{\\boldsymbol{\\beta}}+ \\mathbf{G}\\textsf{N}^T\\mathbf{Y}\n\\end{align*}\\]\n\ntherefore \\(\\tilde{\\boldsymbol{\\beta}}\\) is linear and unbiased: \\[\\begin{align*}\n\\textsf{E}[\\tilde{\\boldsymbol{\\beta}}] & = \\textsf{E}[\\hat{\\boldsymbol{\\beta}}+ \\mathbf{G}\\textsf{N}^T\\mathbf{Y}] \\\\\n           & =  \\textsf{E}[\\hat{\\boldsymbol{\\beta}}] + \\textsf{E}[\\mathbf{G}\\textsf{N}^T\\mathbf{Y}] \\\\\n           & =  \\boldsymbol{\\beta}+ \\mathbf{G}\\textsf{N}^T\\mathbf{X}\\boldsymbol{\\beta}\\\\\n           & = \\boldsymbol{\\beta}\n\\end{align*}\\]"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#characterization-of-lues",
    "href": "resources/slides/04-BLUE.html#characterization-of-lues",
    "title": "Best Linear Unbiased Estimators",
    "section": "Characterization of LUEs",
    "text": "Characterization of LUEs\nSummary of previous results:\n\nTheoremAn estimator \\(\\tilde{\\boldsymbol{\\beta}}\\) is a linear unbiased estimator of \\(\\boldsymbol{\\beta}\\) in a linear statistical model if and only if \\[\\tilde{\\boldsymbol{\\beta}}= \\hat{\\boldsymbol{\\beta}}+ \\mathbf{H}^T\\mathbf{Y}\\] for some \\(\\mathbf{H}\\in \\mathbb{R}^{n \\times p}\\) such that \\(\\mathbf{X}^T \\mathbf{H}= \\mathbf{0}\\) or equivalently for some \\(\\mathbf{G}\\in \\mathbb{R}^{p \\times (n-p)}\\) \\[\\tilde{\\boldsymbol{\\beta}}= \\hat{\\boldsymbol{\\beta}}+ \\mathbf{G}\\textsf{N}^T\\mathbf{Y}\\]"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#numerical",
    "href": "resources/slides/04-BLUE.html#numerical",
    "title": "Best Linear Unbiased Estimators",
    "section": "Numerical",
    "text": "Numerical\n\n# X is model matrix; Y is response\n  p = ncol(X)\n  n = nrow(X)\n  G = matrix(rnorm(p*(n-p)), nrow=p, ncol=n-p)\n  H = MASS::Null(X) %*% t(G)\n  btilde = bhat + t(H) %*% Y\n\ninfinite number of LUEs!"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#lues-via-generalized-inverses",
    "href": "resources/slides/04-BLUE.html#lues-via-generalized-inverses",
    "title": "Best Linear Unbiased Estimators",
    "section": "LUEs via Generalized Inverses",
    "text": "LUEs via Generalized Inverses\nLet \\(\\tilde{\\boldsymbol{\\beta}}= \\mathbf{A}\\mathbf{Y}\\) be a LUE in the statistical linear model \\(\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\) with \\(\\mathbf{X}\\) full column rank \\(p\\) \\[\\begin{align*}\n\\textsf{E}[\\tilde{\\boldsymbol{\\beta}}] & = \\textsf{E}[\\mathbf{A}\\mathbf{Y}] \\\\\n            & = \\mathbf{A}\\textsf{E}[\\mathbf{Y}] \\\\\n            & = \\mathbf{A}\\mathbf{X}\\boldsymbol{\\beta}\\quad \\forall \\boldsymbol{\\beta}\\in \\mathbb{R}^p\n\\end{align*}\\]\n\nMust have \\(\\mathbf{A}\\mathbf{X}= \\mathbf{I}_p\\) (\\(\\mathbf{A}\\) is a generalized inverse of \\(\\mathbf{X}\\))\n\\(\\mathbf{X}\\mathbf{X}^- \\mathbf{X}= \\mathbf{X}\\)\none generalized inverse is \\(\\mathbf{X}_{MP}^- = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\)\n\\(\\mathbf{X}_{MP}^- = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T = \\mathbf{V}\\boldsymbol{\\Delta}^{-1} \\mathbf{U}^T\\) (using SVD of \\(\\mathbf{X}= \\mathbf{U}\\boldsymbol{\\Delta}\\mathbf{V}^T\\))\n\\(\\mathbf{A}\\) is a generalized inverse of \\(\\mathbf{X}\\) iff \\(\\mathbf{A}= \\mathbf{X}_{MP}^- + \\mathbf{H}^T\\) for \\(\\mathbf{H}\\in \\mathbb{R}^{n \\times p} \\ni \\mathbf{H}^T \\mathbf{U}= \\mathbf{0}\\)\n\\(\\mathbf{A}\\mathbf{Y}= (\\mathbf{X}_{MP}^- + \\mathbf{H}^T)\\mathbf{Y}= \\hat{\\boldsymbol{\\beta}}+  \\mathbf{H}^T \\mathbf{Y}\\)"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#best-linear-unbiased-estimators",
    "href": "resources/slides/04-BLUE.html#best-linear-unbiased-estimators",
    "title": "Best Linear Unbiased Estimators",
    "section": "Best Linear Unbiased Estimators",
    "text": "Best Linear Unbiased Estimators\n\nthe distribution of values of any unbiased estimator is centered around \\(\\boldsymbol{\\beta}\\)\nout of the infinite number of LUEs is there one that is more concentrated around \\(\\boldsymbol{\\beta}\\)?\nis there an unbiased estimator that has a lower variance than all other unbiased estimators?\nRecall variance-covariance matrix of a random vector \\(\\mathbf{Z}\\) with mean \\(\\boldsymbol{\\theta}\\) \\[\\begin{align*}\n\\textsf{Cov}[\\mathbf{Z}]      & \\equiv \\textsf{E}[(\\mathbf{Z}- \\boldsymbol{\\theta})(\\mathbf{Z}- \\boldsymbol{\\theta})^T] \\\\\n\\textsf{Cov}[\\mathbf{Z}]_{ij} &  =     \\textsf{E}[(z_i - \\theta_i)(z_j - \\theta_j)]\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\nLemma\n\n\nLet \\(\\mathbf{A}\\in \\mathbb{R}^{q \\times p}\\) and \\(\\mathbf{b}\\in \\mathbb{R}^q\\) with \\(\\mathbf{Z}\\) a random vector in \\(\\mathbb{R}^p\\) then \\[\\textsf{Cov}[\\mathbf{A}\\mathbf{Z}+ \\mathbf{b}] = \\mathbf{A}\\textsf{Cov}[\\mathbf{Z}] \\mathbf{A}^T \\ge 0\\]"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#variance-of-linear-unbiased-estimators",
    "href": "resources/slides/04-BLUE.html#variance-of-linear-unbiased-estimators",
    "title": "Best Linear Unbiased Estimators",
    "section": "Variance of Linear Unbiased Estimators",
    "text": "Variance of Linear Unbiased Estimators\nLet’s look at the variance of any LUE under assumption \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\sigma^2 \\mathbf{I}_n\\)\n\nfor \\(\\hat{\\boldsymbol{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{Y}= \\boldsymbol{\\beta}+ (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\boldsymbol{\\epsilon}\\) \\[\\begin{align*}\n\\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}] & = \\textsf{Cov}[\\boldsymbol{\\beta}+ (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\boldsymbol{\\epsilon}] \\\\\n          & =  (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\textsf{Cov}[\\boldsymbol{\\epsilon}] \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n          & = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n          & = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1}\n\\end{align*}\\]\nCovariance is increasing in \\(\\sigma^2\\) and generally decreasing in \\(n\\)\nRewrite \\(\\mathbf{X}^T\\mathbf{X}\\) as \\(\\mathbf{X}^T\\mathbf{X}= \\sum_{i=1}^n \\mathbf{x}_i \\mathbf{x}_i^T\\) (a sum of \\(n\\) outer-products)"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#variance-of-arbitrary-lue",
    "href": "resources/slides/04-BLUE.html#variance-of-arbitrary-lue",
    "title": "Best Linear Unbiased Estimators",
    "section": "Variance of Arbitrary LUE",
    "text": "Variance of Arbitrary LUE\n\nfor \\(\\tilde{\\boldsymbol{\\beta}}= \\left((\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T + \\mathbf{H}^T \\right)\\mathbf{Y}= \\boldsymbol{\\beta}+ \\left((\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T + \\mathbf{H}^T \\right)\\boldsymbol{\\epsilon}\\)\nrecall \\(\\mathbf{X}_{MP}^- \\equiv  (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\) \\[\\begin{align*}\n\\textsf{Cov}[\\tilde{\\boldsymbol{\\beta}}] & = \\textsf{Cov}[\\left(\\mathbf{X}_{MP}^- + \\mathbf{H}^T \\right)\\boldsymbol{\\epsilon}]  \\\\\n            & = \\sigma^2 \\left(\\mathbf{X}_{MP}^- + \\mathbf{H}^T \\right)\\left(\\mathbf{X}_{MP}^- + \\mathbf{H}^T \\right)^T \\\\\n            & = \\sigma^2\\left( \\mathbf{X}_{MP}^-(\\mathbf{X}_{MP}^-)^T + \\mathbf{X}_{MP}^-\\mathbf{H}+\n                \\mathbf{H}^T (\\mathbf{X}_{MP}^-)^T + \\mathbf{H}^T \\mathbf{H}\\right) \\\\\n            & =   \\sigma^2\\left( (\\mathbf{X}^T\\mathbf{X})^{-1} +  \\mathbf{H}^T \\mathbf{H}\\right)\n\\end{align*}\\]\nCross-product term \\(\\mathbf{H}^T(\\mathbf{X}_{MP}^-)^T = \\mathbf{H}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} =  \\mathbf{0}\\)\nTherefor the \\(\\textsf{Cov}[\\tilde{\\boldsymbol{\\beta}}] = \\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}] + \\mathbf{H}^T\\mathbf{H}\\)\nthe sum of a positive definite matrix plus a positive semi-definite matrix"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#gauss-markov-theorem",
    "href": "resources/slides/04-BLUE.html#gauss-markov-theorem",
    "title": "Best Linear Unbiased Estimators",
    "section": "Gauss-Markov Theorem",
    "text": "Gauss-Markov Theorem\nIs \\(\\textsf{Cov}[\\tilde{\\boldsymbol{\\beta}}] \\ge \\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}]\\) in some sense?\n\n\nDefinition: Loewner OrderingFor two positive semi-definite matrices \\(\\boldsymbol{\\Sigma}_1\\) and \\(\\boldsymbol{\\Sigma}_2\\), we say that \\(\\boldsymbol{\\Sigma}_1 &gt; \\boldsymbol{\\Sigma}_2\\) if \\(\\boldsymbol{\\Sigma}_1 - \\boldsymbol{\\Sigma}_2\\) is positive definite, \\(\\mathbf{x}^T(\\boldsymbol{\\Sigma}_1 - \\boldsymbol{\\Sigma}_2)\\mathbf{x}) &gt; 0\\), and \\(\\boldsymbol{\\Sigma}_1 \\ge \\boldsymbol{\\Sigma}_2\\) if \\(\\boldsymbol{\\Sigma}_1 - \\boldsymbol{\\Sigma}_2\\) is positive semi-definite, \\(\\mathbf{x}^T(\\boldsymbol{\\Sigma}_1 - \\boldsymbol{\\Sigma}_2)\\mathbf{x}) \\ge 0\\)\n\n\n\nSince \\(\\textsf{Cov}[\\tilde{\\boldsymbol{\\beta}}] - \\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}] = \\mathbf{H}^T\\mathbf{H}\\), we have that \\(\\textsf{Cov}[\\tilde{\\boldsymbol{\\beta}}] \\ge \\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}]\\)\n\n\n\n\nTheorem: Gauss-MarkovLet \\(\\tilde{\\boldsymbol{\\beta}}\\) be a linear unbiased estimator of \\(\\boldsymbol{\\beta}\\) in a linear model where \\(\\textsf{E}[\\mathbf{Y}] = \\mathbf{X}\\boldsymbol{\\beta}, \\boldsymbol{\\beta}\\in \\mathbb{R}^p\\), \\(\\mathbf{X}\\) rank \\(p\\), and \\(\\textsf{Cov}[\\mathbf{Y}] = \\sigma^2\\mathbf{I}_n, \\sigma^2 &gt; 0\\). Then \\(\\textsf{Cov}[\\tilde{\\boldsymbol{\\beta}}] \\ge \\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}]\\) where \\(\\hat{\\boldsymbol{\\beta}}\\) is the OLS estimator and is the Best Linear Unbiased Estimator (BLUE) of \\(\\boldsymbol{\\beta}\\)."
  },
  {
    "objectID": "resources/slides/04-BLUE.html#slide13-id",
    "href": "resources/slides/04-BLUE.html#slide13-id",
    "title": "Best Linear Unbiased Estimators",
    "section": "",
    "text": "Theorem: Gauss-Markov Theorem (Classic)For \\(\\mathbf{Y}= \\boldsymbol{\\mu}+ \\boldsymbol{\\epsilon}\\), with \\(\\boldsymbol{\\mu}\\in \\boldsymbol{{\\cal M}}\\), \\(\\textsf{E}[\\boldsymbol{\\epsilon}]= \\mathbf{0}_n\\) and \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] =\\sigma^2 \\mathbf{I}_n\\) and \\(\\mathbf{P}\\) the orthogonal projection onto \\(\\boldsymbol{{\\cal M}}\\), \\(\\mathbf{P}\\mathbf{Y}= \\hat{\\boldsymbol{\\mu}}\\) is the BLUE of \\(\\boldsymbol{\\mu}\\) out of the class of LUEs \\(\\mathbf{A}\\mathbf{Y}\\) where \\(\\textsf{E}[\\mathbf{A}\\mathbf{Y}] = \\boldsymbol{\\mu}\\), \\(\\mathbf{A}\\in \\mathbb{R}^{n \\times n}\\) equality iff \\(\\mathbf{A}= \\mathbf{P}\\)\n\n\n\nProof\n\nwrite \\(\\mathbf{A}= \\mathbf{P}+ \\mathbf{H}^T\\) so \\(\\mathbf{H}^T = \\mathbf{A}- \\mathbf{P}\\)\nsince \\(\\mathbf{A}\\boldsymbol{\\mu}= \\boldsymbol{\\mu}\\), \\(\\mathbf{H}^T\\mu = \\mathbf{0}_n\\) for \\(\\mu \\in \\boldsymbol{{\\cal M}}\\) and \\(\\mathbf{H}^T \\mathbf{P}= \\mathbf{P}\\mathbf{H}= \\mathbf{0}\\) (columns of \\(\\mathbf{H}\\in \\boldsymbol{{\\cal M}}^\\perp\\)) \\[\\begin{align*}\n\\textsf{E}[\\|\\mathbf{A}\\mathbf{Y}- \\boldsymbol{\\mu}\\|^2]  & =  \\textsf{E}[\\|\\mathbf{P}(\\mathbf{Y}- \\boldsymbol{\\mu}) + \\mathbf{H}^T(\\mathbf{Y}- \\boldsymbol{\\mu})\\|^2]  \\\\\n& = \\textsf{E}[\\|\\mathbf{P}(\\mathbf{Y}- \\boldsymbol{\\mu})\\|^2] + \\underbrace{\\textsf{E}[\\|\\mathbf{H}^T(\\mathbf{Y}- \\boldsymbol{\\mu})\\|^2]} + \\underbrace{{\\text{cross-product}}} \\\\\n& \\hspace{4.35in} \\ge 0 \\quad  + \\hspace{1.25in} 0\\\\\n& \\ge \\textsf{E}[\\|\\mathbf{P}(\\mathbf{Y}- \\boldsymbol{\\mu})\\|^2]\n\\end{align*}\\]\nCross-product is \\(2\\textsf{E}[(\\mathbf{H}^T(\\mathbf{Y}- \\mu))^T\\mathbf{P}(\\mathbf{Y}- \\boldsymbol{\\mu})] = 0\\) (see last slide)"
  },
  {
    "objectID": "resources/slides/04-BLUE.html#estimation-of-linear-functionals-of-boldsymbolmu",
    "href": "resources/slides/04-BLUE.html#estimation-of-linear-functionals-of-boldsymbolmu",
    "title": "Best Linear Unbiased Estimators",
    "section": "Estimation of Linear Functionals of \\(\\boldsymbol{\\mu}\\)",
    "text": "Estimation of Linear Functionals of \\(\\boldsymbol{\\mu}\\)\nIf \\(\\mathbf{P}\\mathbf{Y}= \\hat{\\boldsymbol{\\mu}}\\) is the BLUE of \\(\\boldsymbol{\\mu}\\), is \\(\\mathbf{B}\\mathbf{P}\\mathbf{Y}= \\mathbf{B}\\hat{\\boldsymbol{\\mu}}\\) the BLUE of \\(\\mathbf{B}\\boldsymbol{\\mu}\\)?\n\nYes! Similar proof as above to show that out of the class of LUEs \\(\\mathbf{A}\\mathbf{Y}\\) of \\(\\mathbf{B}\\boldsymbol{\\mu}\\) where \\(\\mathbf{A}\\in \\mathbb{R}^{d \\times n}\\) that \\[\\textsf{E}[\\|\\mathbf{A}\\mathbf{Y}- \\mathbf{B}\\boldsymbol{\\mu}\\|^2] \\ge \\textsf{E}[\\|\\mathbf{B}\\mathbf{P}\\mathbf{Y}- \\mathbf{B}\\boldsymbol{\\mu}\\|^2]\\] with equality iff \\(\\mathbf{A}= \\mathbf{B}\\mathbf{P}\\).\n\n\nWhat about linear functionals of \\(\\boldsymbol{\\beta}\\), \\(\\boldsymbol{\\Lambda}^T \\boldsymbol{\\beta}\\), for \\(\\mathbf{X}\\) rank \\(r \\le p\\)?\n\n\\(\\hat{\\boldsymbol{\\beta}}\\) is not unique if \\(r &lt; p\\) even though \\(\\hat{\\boldsymbol{\\mu}}\\) is unique (\\(\\hat{\\boldsymbol{\\beta}}\\) is not BLUE)\nSince \\(\\mathbf{B}\\boldsymbol{\\mu}= \\mathbf{B}\\mathbf{X}\\boldsymbol{\\beta}\\) is always identifiable, the only linear functions of \\(\\boldsymbol{\\beta}\\) that are identifiable and can be estimated uniquely are functions of \\(\\mathbf{X}\\boldsymbol{\\beta}\\), i.e. estimates in the form \\(\\boldsymbol{\\Lambda}^T \\boldsymbol{\\beta}= \\mathbf{B}\\mathbf{X}\\boldsymbol{\\beta}\\) or \\(\\boldsymbol{\\Lambda}= \\mathbf{X}^T \\mathbf{B}^T\\).\ncolumns of \\(\\boldsymbol{\\Lambda}\\) must be in the \\(C(\\mathbf{X}^T)\\)\ndetailed discussion and proof in Christensen Ch. 2 for scalar functionals \\(\\lambda^T\\beta\\)."
  },
  {
    "objectID": "resources/slides/04-BLUE.html#blue-of-boldsymbollambdat-boldsymbolbeta",
    "href": "resources/slides/04-BLUE.html#blue-of-boldsymbollambdat-boldsymbolbeta",
    "title": "Best Linear Unbiased Estimators",
    "section": "BLUE of \\(\\boldsymbol{\\Lambda}^T \\boldsymbol{\\beta}\\)",
    "text": "BLUE of \\(\\boldsymbol{\\Lambda}^T \\boldsymbol{\\beta}\\)\nIf \\(\\boldsymbol{\\Lambda}^T= \\mathbf{B}\\mathbf{X}\\) for some matrix \\(\\mathbf{B}\\) then\n\n\\(\\textsf{E}[\\mathbf{B}\\mathbf{P}\\mathbf{Y}] = \\textsf{E}[\\boldsymbol{\\Lambda}^T \\hat{\\boldsymbol{\\beta}}] = \\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\)\nThe unique OLS estimate of \\(\\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\) is \\(\\boldsymbol{\\Lambda}^T\\hat{\\boldsymbol{\\beta}}\\)\n\\(\\mathbf{B}\\mathbf{P}\\mathbf{Y}= \\boldsymbol{\\Lambda}^T\\hat{\\boldsymbol{\\beta}}\\) is the BLUE of \\(\\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\) \\[\\begin{align*}\n& \\textsf{E}[\\|\\mathbf{B}\\mathbf{P}\\mathbf{Y}- \\mathbf{B}\\boldsymbol{\\mu}\\|^2]  \\le \\textsf{E}[\\|\\mathbf{A}\\mathbf{Y}- \\mathbf{B}\\boldsymbol{\\mu}\\|^2] \\\\\n\\Leftrightarrow & \\\\\n& \\textsf{E}[\\|\\boldsymbol{\\Lambda}^T\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta})\\|^2]  \\le \\textsf{E}[\\|\\mathbf{L}^T\\tilde{\\boldsymbol{\\beta}}- \\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\|^2]\n\\end{align*}\\] for LUE \\(\\mathbf{A}\\mathbf{Y}= \\mathbf{L}^T\\tilde{\\boldsymbol{\\beta}}\\) of \\(\\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\)\nProof proceeds as the classic case."
  },
  {
    "objectID": "resources/slides/04-BLUE.html#proof-of-cross-product",
    "href": "resources/slides/04-BLUE.html#proof-of-cross-product",
    "title": "Best Linear Unbiased Estimators",
    "section": "Proof of Cross-Product",
    "text": "Proof of Cross-Product\nLet \\(\\mathbf{D}= \\mathbf{H}\\mathbf{P}\\) and write \\[\\begin{align*}\n\\textsf{E}[(\\mathbf{H}^T(\\mathbf{Y}- \\mu))^T\\mathbf{P}(\\mathbf{Y}- \\boldsymbol{\\mu})] & = \\textsf{E}[(\\mathbf{Y}- \\mu))^T\\mathbf{H}\\mathbf{P}(\\mathbf{Y}- \\boldsymbol{\\mu})] \\\\\n& = \\textsf{E}[(\\mathbf{Y}- \\mu))^T\\mathbf{D}(\\mathbf{Y}- \\boldsymbol{\\mu})]\n\\end{align*}\\]\n\n\\[\\begin{align*}\n\\textsf{E}[(\\mathbf{Y}- \\mu))^T\\mathbf{D}(\\mathbf{Y}- \\boldsymbol{\\mu})] = & \\textsf{E}[\\textsf{tr}(\\mathbf{Y}- \\mu))^T\\mathbf{D}(\\mathbf{Y}- \\boldsymbol{\\mu}))]  \\\\\n= & \\textsf{E}[\\textsf{tr}(\\mathbf{D}(\\mathbf{Y}- \\boldsymbol{\\mu})(\\mathbf{Y}- \\mu)^T)] \\\\\n= & \\textsf{tr}(\\textsf{E}[\\mathbf{D}(\\mathbf{Y}- \\boldsymbol{\\mu})(\\mathbf{Y}- \\mu)^T]) \\\\\n= & \\textsf{tr}(\\mathbf{D}\\textsf{E}[(\\mathbf{Y}- \\boldsymbol{\\mu})(\\mathbf{Y}- \\mu)^T]) \\\\\n  = & \\sigma^2 \\textsf{tr}(\\mathbf{D}\\mathbf{I}_n)\\\\\n\\end{align*}\\]\n\n\nSince \\(\\textsf{tr}(\\mathbf{D}) = \\textsf{tr}(\\mathbf{H}\\mathbf{P}) = \\textsf{tr}(\\mathbf{P}\\mathbf{H})\\) we can conclude that the cross-product term is zero.\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/07-sampling.html#outline",
    "href": "resources/slides/07-sampling.html#outline",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Outline",
    "text": "Outline\n\ndistributions of \\(\\hat{\\boldsymbol{\\beta}}\\), \\(\\hat{\\mathbf{Y}}\\), \\(\\hat{\\boldsymbol{\\epsilon}}\\) under normality\nUnbiased Estimation of \\(\\sigma^2\\)\nsampling distribution of \\(\\hat{\\sigma^2}\\)\nindependence\n\n\nReadings:\n\nChristensen Chapter 1, 2.91 and Appendix C\nSeber & Lee Chapter 3.3 - 3.5"
  },
  {
    "objectID": "resources/slides/07-sampling.html#multivariate-normal",
    "href": "resources/slides/07-sampling.html#multivariate-normal",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Multivariate Normal",
    "text": "Multivariate Normal\nUnder the linear model \\(\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\), \\(\\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\) and \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\sigma^2 \\mathbf{I}_n\\), we had\n\n\\(\\textsf{E}[\\hat{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\\)\n\\(\\textsf{E}[\\hat{\\mathbf{Y}}] = \\mathbf{P}_\\mathbf{X}\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}\\)\n\\(\\textsf{E}[\\hat{\\boldsymbol{\\epsilon}}] = (\\mathbf{I}_n - \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}= \\mathbf{0}_n\\)\ndistributions if \\(\\epsilon_i \\sim \\textsf{N}(0, \\sigma^2)\\)?\n\n\nFor a \\(d\\) dimensional multivariate normal random vector, we write \\(\\mathbf{Y}\\sim \\textsf{N}_d(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\)\n\n\\(\\textsf{E}[\\mathbf{Y}] = \\boldsymbol{\\mu}\\): \\(d\\) dimensional vector with means \\(E[Y_j]\\)\n\\(\\textsf{Cov}[\\mathbf{Y}] = \\boldsymbol{\\Sigma}\\): \\(d \\times d\\) matrix with diagonal elements that are the variances of \\(Y_j\\) and off diagonal elements that are the covariances \\(\\textsf{E}[(Y_j - \\mu_j)(Y_k - \\mu_k)]\\)\nIf \\(\\boldsymbol{\\Sigma}\\) is positive definite (\\(\\mathbf{x}'\\boldsymbol{\\Sigma}\\mathbf{x}&gt; 0\\) for any \\(\\mathbf{x}\\ne\n  0\\) in \\(\\mathbb{R}^d\\)) then \\(\\mathbf{Y}\\) has a density\\(^\\dagger\\) \\[p(\\mathbf{Y}) = (2 \\pi)^{-d/2} |\\boldsymbol{\\Sigma}|^{-1/2} \\exp(-\\frac{1}{2}(\\mathbf{Y}- \\boldsymbol{\\mu})^T\n\\boldsymbol{\\Sigma}^{-1} (\\mathbf{Y}- \\boldsymbol{\\mu}))\\]\n\n\n\\(\\dagger\\) density with respect to Lebesgue measure on \\(\\mathbb{R}^d\\)"
  },
  {
    "objectID": "resources/slides/07-sampling.html#transformations-of-normal-random-variables",
    "href": "resources/slides/07-sampling.html#transformations-of-normal-random-variables",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Transformations of Normal Random Variables",
    "text": "Transformations of Normal Random Variables\nIf \\(\\mathbf{Y}\\sim \\textsf{N}_n(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) then for \\(\\mathbf{A}\\) \\(m \\times n\\) \\[\\mathbf{A}\\mathbf{Y}\\sim \\textsf{N}_m(\\mathbf{A}\\boldsymbol{\\mu}, \\mathbf{A}\\boldsymbol{\\Sigma}\\mathbf{A}^T)\\]\n\n\n\\(\\hat{\\boldsymbol{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\sim \\textsf{N}(\\boldsymbol{\\beta}, \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1})\\)\n\\(\\hat{\\mathbf{Y}}= \\mathbf{P}_\\mathbf{X}\\mathbf{Y}\\sim \\textsf{N}(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2 \\mathbf{P}_\\mathbf{X})\\)\n\\(\\hat{\\boldsymbol{\\epsilon}}= (\\mathbf{I}_n - \\mathbf{P}_\\mathbf{X})\\mathbf{Y}\\sim \\textsf{N}(\\mathbf{0}, \\sigma^2 (\\mathbf{I}_n - \\mathbf{P}_\\mathbf{X}))\\)\n\n\n\n\\(\\mathbf{A}\\boldsymbol{\\Sigma}\\mathbf{A}^T\\) does not have to be positive definite!"
  },
  {
    "objectID": "resources/slides/07-sampling.html#singular-case",
    "href": "resources/slides/07-sampling.html#singular-case",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Singular Case",
    "text": "Singular Case\nIf the covariance is singular then there is no density (on \\(\\mathbb{R}^n\\)), but claim that \\(\\mathbf{Y}\\) still has a multivariate normal distribution!\n\n\nDefinition: Multivariate Normal\\(\\mathbf{Y}\\in \\mathbb{R}^n\\) has a multivariate normal distribution \\(\\textsf{N}(\\boldsymbol{\\mu},\n  \\boldsymbol{\\Sigma})\\) if for any \\(\\mathbf{v}\\in \\mathbb{R}^n\\) \\(\\mathbf{v}^T\\mathbf{Y}\\) has a univariate normal distribution with mean \\(\\mathbf{v}^T\\boldsymbol{\\mu}\\) and variance \\(\\mathbf{v}^T\\boldsymbol{\\Sigma}\\mathbf{v}\\)\n\n\n\n\n\nProofUse moment generating or characteristic functions which uniquely characterize distribution to show that \\(\\mathbf{v}^T\\mathbf{Y}\\) has a univariate normal distribution.\n\n\n\nboth \\(\\hat{\\mathbf{Y}}\\) and \\(\\hat{\\boldsymbol{\\epsilon}}\\) have multivariate normal distributions even though they do not have densities! (singular distributions)"
  },
  {
    "objectID": "resources/slides/07-sampling.html#distribution-of-mle-of-sigma2",
    "href": "resources/slides/07-sampling.html#distribution-of-mle-of-sigma2",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Distribution of MLE of \\(\\sigma^2\\)",
    "text": "Distribution of MLE of \\(\\sigma^2\\)\nRecall we found the MLE of \\(\\sigma^2\\) \\[{\\hat{\\sigma}}^2= \\frac{\\hat{\\boldsymbol{\\epsilon}}^T\\hat{\\boldsymbol{\\epsilon}}} {n}\\]\n\nlet \\(\\textsf{RSS}= \\| \\hat{\\boldsymbol{\\epsilon}}\\|^2 = \\hat{\\boldsymbol{\\epsilon}}^T\\hat{\\boldsymbol{\\epsilon}}\\)\nthen \\[\\begin{align*}\n\\| \\hat{\\boldsymbol{\\epsilon}}\\|^2  & = \\hat{\\boldsymbol{\\epsilon}}^T\\hat{\\boldsymbol{\\epsilon}}\\\\\n             & = \\boldsymbol{\\epsilon}^T(\\mathbf{I}_n - \\mathbf{P}_\\mathbf{X})^T (\\mathbf{I}_n - \\mathbf{P}_\\mathbf{X}) \\boldsymbol{\\epsilon}\\\\\n             & = \\boldsymbol{\\epsilon}^T(\\mathbf{I}_n - \\mathbf{P}_\\mathbf{X}) \\boldsymbol{\\epsilon}\\\\\n             & = \\boldsymbol{\\epsilon}^N \\textsf{N}\\textsf{N}^T \\boldsymbol{\\epsilon}\\\\\n             & = \\mathbf{e}^T\\mathbf{e}\n\\end{align*}\\]\n\\(\\textsf{N}\\) is the matrix of the \\((n - p)\\) eigen vectors from the spectral decomposition of \\((\\mathbf{I}_n - \\mathbf{P}_\\mathbf{X})\\) associated with the non-zero eigen-values."
  },
  {
    "objectID": "resources/slides/07-sampling.html#distribution-of-textsfrss",
    "href": "resources/slides/07-sampling.html#distribution-of-textsfrss",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Distribution of \\(\\textsf{RSS}\\)",
    "text": "Distribution of \\(\\textsf{RSS}\\)\nSince \\(\\boldsymbol{\\epsilon}\\sim \\textsf{N}(\\mathbf{0}_n, \\sigma^2 \\mathbf{I}_n)\\) and \\(\\textsf{N}\\in \\mathbb{R}^{n \\times (n - p)}\\), \\[\\textsf{N}^T \\boldsymbol{\\epsilon}= \\mathbf{e}\\sim \\textsf{N}(\\mathbf{0}_{n - p}, \\sigma^2\\textsf{N}^T\\textsf{N}) = \\textsf{N}(\\mathbf{0}_{n - p}, \\sigma^2\\mathbf{I}_{n - p} )\\]\n\n\\[\\begin{align*}\n\\textsf{RSS}& =  \\sum_{i = 1}^{n-p} e_i^2 \\\\\n     & \\mathrel{\\mathop{=}\\limits^{\\rm D}}\\sum_{i = 1}^{n-p} (\\sigma z_i)^2  \\quad \\text{ where } \\mathbf{Z}\\sim \\textsf{N}(\\mathbf{0}_{n-p}, \\mathbf{I}_{n-p}) \\\\\n     & = \\sigma^2 \\sum_{i = 1}^{n-p} z_i^2 \\\\\n     &\\mathrel{\\mathop{=}\\limits^{\\rm D}}\\sigma^2 \\chi^2_{n-p}\n     \n\\end{align*}\\]\n\n\nBackground Theory: If \\(\\mathbf{Z}\\sim \\textsf{N}_d(\\mathbf{0}_d, \\mathbf{I}_d)\\), then \\(\\mathbf{Z}^T\\mathbf{Z}\\sim \\chi^2_{d}\\)"
  },
  {
    "objectID": "resources/slides/07-sampling.html#unbiased-estimate-of-sigma2",
    "href": "resources/slides/07-sampling.html#unbiased-estimate-of-sigma2",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Unbiased Estimate of \\(\\sigma^2\\)",
    "text": "Unbiased Estimate of \\(\\sigma^2\\)\n\nExpected value of a \\(\\chi^2_d\\) random variable is \\(d\\) (the degrees of freedom)\n\\(\\textsf{E}[\\textsf{RSS}] = \\textsf{E}[\\sigma^2 \\chi^2_{n-p}] = \\sigma^2 (n-p)\\)\nthe expected value of the MLE is \\[{\\hat{\\sigma}}^2= \\textsf{E}[\\textsf{RSS}]/n = \\sigma^2 \\frac{(n-p)}{n}\\] so is biased\nan unbiased estimator of \\(\\sigma^2\\), is \\(s^2 = \\textsf{RSS}/(n-p)\\)\nnote: we can find the expectation of \\({\\hat{\\sigma}}^2\\) or \\(s^2\\) based on the covariance of \\(\\boldsymbol{\\epsilon}\\) without assuming normality by exploiting properties of the trace."
  },
  {
    "objectID": "resources/slides/07-sampling.html#distribution-of-hatboldsymbolbeta",
    "href": "resources/slides/07-sampling.html#distribution-of-hatboldsymbolbeta",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Distribution of \\(\\hat{\\boldsymbol{\\beta}}\\)",
    "text": "Distribution of \\(\\hat{\\boldsymbol{\\beta}}\\)\n\\(\\hat{\\boldsymbol{\\beta}}\\sim \\textsf{N}\\left(\\boldsymbol{\\beta}, \\sigma^2( \\mathbf{X}^T\\mathbf{X})^{-1}\\right)\\)\n\ndo not know \\(\\sigma^2\\)\nNeed a distribution that does not depend on unknown parameters for deriving confidence intervals and hypothesis tests for \\(\\boldsymbol{\\beta}\\).\nwhat if we plug in \\(s^2\\) or \\({\\hat{\\sigma}}^2\\) for \\(\\sigma^2\\)?\nwon’t be multivariate normal\nneed to reflect uncertainty in estimating \\(\\sigma^2\\)\nfirst show that \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(s^2\\) are independent"
  },
  {
    "objectID": "resources/slides/07-sampling.html#independence-of-hatboldsymbolbeta-and-s2",
    "href": "resources/slides/07-sampling.html#independence-of-hatboldsymbolbeta-and-s2",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Independence of \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(s^2\\)",
    "text": "Independence of \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(s^2\\)\nIf the distribution of \\(\\mathbf{Y}\\) is normal, then \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(s^2\\) are statistically independent.\n\nThe derivation of this result basically has three steps:\n\n\\(\\hat{\\boldsymbol{\\beta}}\\) and \\(\\hat{\\boldsymbol{\\epsilon}}\\) or \\(\\mathbf{e}\\) have zero covariance\n\\(\\hat{\\boldsymbol{\\beta}}\\) and \\(\\hat{\\boldsymbol{\\epsilon}}\\) or \\(\\mathbf{e}\\) are independent\nConclude \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(\\textsf{RSS}\\) (or \\(s^2\\)) are independent\n\n\n\nStep 1:\n\n\n\\[\\begin{align*}\n\\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}, \\hat{\\boldsymbol{\\epsilon}}] & = \\textsf{E}[(\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}) \\hat{\\boldsymbol{\\epsilon}}^T] \\\\\n                   & = \\textsf{E}[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T (\\mathbf{I}- \\mathbf{P}_\\mathbf{X})] \\\\\n                   & = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\\\\n                   & = \\mathbf{0}\n\\end{align*}\\]"
  },
  {
    "objectID": "resources/slides/07-sampling.html#zero-covariance-leftrightarrow-independence-in-multivariate-normals",
    "href": "resources/slides/07-sampling.html#zero-covariance-leftrightarrow-independence-in-multivariate-normals",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Zero Covariance \\(\\Leftrightarrow\\) Independence in Multivariate Normals",
    "text": "Zero Covariance \\(\\Leftrightarrow\\) Independence in Multivariate Normals\nStep 2: \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(\\hat{\\boldsymbol{\\epsilon}}\\) are independent\n\nTheorem: Zero Correlation and IndependenceFor a random vector \\(\\mathbf{W}\\sim \\textsf{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) partitioned as \\[\n\\mathbf{W}= \\left[\n  \\begin{array}{c}\n\\mathbf{W}_1  \\\\ \\mathbf{W}_2 \\end{array} \\right]  \\sim \\textsf{N}\\left( \\left[\n  \\begin{array}{c} \\boldsymbol{\\mu}_1  \\\\ \\boldsymbol{\\mu}_2 \\end{array} \\right],\n  \\left[ \\begin{array}{cc}\n\\boldsymbol{\\Sigma}_{11} &  \\boldsymbol{\\Sigma}_{12}  \\\\\n\\boldsymbol{\\Sigma}_{21} & \\boldsymbol{\\Sigma}_{22} \\end{array} \\right]\n\\right)\n\\]\nthen \\(\\textsf{Cov}(\\mathbf{W}_1, \\mathbf{W}_2) = \\boldsymbol{\\Sigma}_{12} = \\boldsymbol{\\Sigma}_{21}^T = \\mathbf{0}\\) if and only if \\(\\mathbf{W}_1\\) and \\(\\mathbf{W}_2\\) are independent."
  },
  {
    "objectID": "resources/slides/07-sampling.html#proof-independence-implies-zero-covariance",
    "href": "resources/slides/07-sampling.html#proof-independence-implies-zero-covariance",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Proof: Independence implies Zero Covariance",
    "text": "Proof: Independence implies Zero Covariance\nEasy direction\n\n\\(\\textsf{Cov}[\\mathbf{W}_1, \\mathbf{W}_2] = \\textsf{E}[(\\mathbf{W}_1 - \\boldsymbol{\\mu}_1)(\\mathbf{W}_2 - \\boldsymbol{\\mu}_2)^T]\\)\nsince they are independent \\[\\begin{align*}\n\\textsf{Cov}[\\mathbf{W}_1, \\mathbf{W}_2] & = \\textsf{E}[(\\mathbf{W}_1 - \\boldsymbol{\\mu}_1)] \\textsf{E}[(\\mathbf{W}_2 - \\boldsymbol{\\mu}_2)^T] \\\\\n& = \\mathbf{0}\\mathbf{0}^T \\\\\n& = \\mathbf{0}\n\\end{align*}\\]\n\n\nso \\(\\mathbf{W}_1\\) and \\(\\mathbf{W}_2\\) are uncorrelated"
  },
  {
    "objectID": "resources/slides/07-sampling.html#zero-covariance-implies-independence",
    "href": "resources/slides/07-sampling.html#zero-covariance-implies-independence",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Zero Covariance Implies Independence",
    "text": "Zero Covariance Implies Independence\n\nProof\nAssume \\(\\boldsymbol{\\Sigma}_{12} = \\mathbf{0}\\):\n\nChoose an \\[\\mathbf{A}= \\left[\n\\begin{array}{ll}\n  \\mathbf{A}_1 & \\mathbf{0}\\\\\n  \\mathbf{0}& \\mathbf{A}_2\n\\end{array}\n\\right]\\] such that \\(\\mathbf{A}_1 \\mathbf{A}_1^T = \\boldsymbol{\\Sigma}_{11}\\), \\(\\mathbf{A}_2 \\mathbf{A}_2^T = \\boldsymbol{\\Sigma}_{22}\\)\nPartition\n\\[ \\mathbf{Z}= \\left[\n\\begin{array}{c}\n  \\mathbf{Z}_1 \\\\ \\mathbf{Z}_2\n\\end{array}\n\\right] \\sim \\textsf{N}\\left(\n\\left[\n\\begin{array}{c}\n  \\mathbf{0}_1 \\\\ \\mathbf{0}_2\n\\end{array}\n\\right],\n\\left[\n\\begin{array}{ll}\n  \\mathbf{I}_1 &\\mathbf{0}\\\\\n\\mathbf{0}& \\mathbf{I}_2\n\\end{array}\n\\right]\n\\right)  \\text{ and } \\boldsymbol{\\mu}= \\left[\n\\begin{array}{c}\n  \\boldsymbol{\\mu}_1 \\\\ \\boldsymbol{\\mu}_2\n\\end{array}\n\\right]\\]\nthen \\(\\mathbf{W}\\mathrel{\\mathop{=}\\limits^{\\rm D}}\\mathbf{A}\\mathbf{Z}+ \\boldsymbol{\\mu}\\sim  \\textsf{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\)"
  },
  {
    "objectID": "resources/slides/07-sampling.html#step-3",
    "href": "resources/slides/07-sampling.html#step-3",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Step 3:",
    "text": "Step 3:\nShow \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(\\textsf{RSS}\\) are independent\n\n\\(\\hat{\\boldsymbol{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{Y}\\) and \\(\\hat{\\boldsymbol{\\epsilon}}= (\\mathbf{I}- \\mathbf{P}_\\mathbf{X})\\mathbf{Y}\\) are independent\nfunctions of independent random variables are independent so \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(\\textsf{RSS}= \\hat{\\boldsymbol{\\epsilon}}^T\\hat{\\boldsymbol{\\epsilon}}\\) are independent\nso \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(s^2 = \\textsf{RSS}/(n-p)\\) are independent\n\n\nThis result will be critical for creating confidence regions and intervals for \\(\\boldsymbol{\\beta}\\) and linear combinations of \\(\\boldsymbol{\\beta}\\), \\(\\lambda^T \\boldsymbol{\\beta}\\) as well as testing hypotheses"
  },
  {
    "objectID": "resources/slides/07-sampling.html#next-class",
    "href": "resources/slides/07-sampling.html#next-class",
    "title": "Sampling Distributions and Distribution Theory",
    "section": "Next Class",
    "text": "Next Class\n\nshrinkage estimators\nBayes and penalized loss functions\n\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/16-Bayes-tests.html#outline",
    "href": "resources/slides/16-Bayes-tests.html#outline",
    "title": "Basics of Bayesian Hypothesis Testing",
    "section": "Outline",
    "text": "Outline\n\nConfidence Interverals from Test Statistics\nPivotal Quantities\nConfidence intervals for parameters\nPrediction Intervals\nBayesian Credible Regions and Intervals\n\n\nReadings:\n\nChristensen Appendix C, Chapter 3"
  },
  {
    "objectID": "resources/slides/16-Bayes-tests.html#feature-selection-via-shrinkage",
    "href": "resources/slides/16-Bayes-tests.html#feature-selection-via-shrinkage",
    "title": "Basics of Bayesian Hypothesis Testing",
    "section": "Feature Selection via Shrinkage",
    "text": "Feature Selection via Shrinkage\n\nmodal estimates in regression models under certain shrinkage priors will set a subset of coefficients to zero\nnot true with posterior mean\nmulti-modal posterior\nno prior probability that coefficient is zero\nhow should we approach selection/hypothesis testing?\nBayesian Hypothesis Testing"
  },
  {
    "objectID": "resources/slides/16-Bayes-tests.html#basics-of-bayesian-hypothesis-testing",
    "href": "resources/slides/16-Bayes-tests.html#basics-of-bayesian-hypothesis-testing",
    "title": "Basics of Bayesian Hypothesis Testing",
    "section": "Basics of Bayesian Hypothesis Testing",
    "text": "Basics of Bayesian Hypothesis Testing\nSuppose we have univariate data \\(Y_i \\overset{iid}{\\sim} \\textsf{N}(\\theta, 1)\\), \\(\\mathbf{Y}= (y_i, \\ldots, y_n)^T\\)\n\ngoal is to test \\(\\mathcal{H}_0: \\theta = 0; \\ \\ \\text{vs } \\mathcal{H}_1: \\theta \\neq 0\\)\nAdditional unknowns are \\(\\mathcal{H}_0\\) and \\(\\mathcal{H}_1\\)\nPut a prior on the actual hypotheses/models, that is, on \\(\\pi(\\mathcal{H}_0) = \\Pr(\\mathcal{H}_0 = \\text{True})\\) and \\(\\pi(\\mathcal{H}_1) = \\Pr(\\mathcal{H}_1 = \\text{True})\\).\n(Marginal) Likelihood of the hypotheses: \\(\\cal{L}(\\mathcal{H}_i) \\propto p( \\mathbf{y}\\mid \\mathcal{H}_i)\\)\n\n\n\\[p( \\mathbf{y}\\mid \\mathcal{H}_0) = \\prod_{i = 1}^n (2 \\pi)^{-1/2} \\exp{- \\frac{1}{2} (y_i - 0)^2}\\]\n\n\n\\[p( \\mathbf{y}\\mid \\mathcal{H}_1)  = \\int_\\Theta p( \\mathbf{y}\\mid \\mathcal{H}_1, \\theta) p(\\theta \\mid \\mathcal{H}_1) \\, d\\theta\\]"
  },
  {
    "objectID": "resources/slides/16-Bayes-tests.html#bayesian-approach",
    "href": "resources/slides/16-Bayes-tests.html#bayesian-approach",
    "title": "Basics of Bayesian Hypothesis Testing",
    "section": "Bayesian Approach",
    "text": "Bayesian Approach\n\nNeed priors distributions on parameters under each hypothesis\n\nin our simple normal model, the only additional unknown parameter is \\(\\theta\\)\nunder \\(\\mathcal{H}_0\\), \\(\\theta = 0\\) with probability 1\nunder \\(\\mathcal{H}_0\\), \\(\\theta \\in \\mathbb{R}\\) we could take \\(\\pi(\\theta) = \\mathcal{N}(\\theta_0, 1/\\tau_0^2)\\).\n\nCompute marginal likelihoods for each hypothesis, that is, \\(\\cal{L}(\\mathcal{H}_0)\\) and \\(\\cal{L}(\\mathcal{H}_1)\\).\nObtain posterior probabilities of \\(\\cal{H}_0\\) and \\(\\cal{H}_1\\) via Bayes Theorem. \\[\n\\begin{split}\n\\pi(\\mathcal{H}_1 \\mid \\mathbf{y}) = \\frac{ p( \\mathbf{y}\\mid \\mathcal{H}_1) \\pi(\\mathcal{H}_1) }{ p( \\mathbf{y}\\mid \\mathcal{H}_0) \\pi(\\mathcal{H}_0) + p( \\mathbf{y}\\mid \\mathcal{H}_1) \\pi(\\mathcal{H}_1)}\n\\end{split}\n\\]\nProvides a joint posterior distribution for \\(\\theta\\) and \\(\\mathcal{H}_i\\): \\(p(\\theta \\mid   \\mathcal{H}_i,  \\mathbf{y})\\) and \\(\\pi(\\mathcal{H}_i \\mid \\mathbf{y})\\)"
  },
  {
    "objectID": "resources/slides/16-Bayes-tests.html#hypothesis-tests-via-decision-theory",
    "href": "resources/slides/16-Bayes-tests.html#hypothesis-tests-via-decision-theory",
    "title": "Basics of Bayesian Hypothesis Testing",
    "section": "Hypothesis Tests via Decision Theory",
    "text": "Hypothesis Tests via Decision Theory\n\nLoss function for hypothesis testing\n\n\\(\\hat{\\cal{H}}\\) is the chosen hypothesis\n\\(\\cal{H}_{\\text{true}}\\) is the true hypothesis, \\(\\cal{H}\\) for short\n\nTwo types of errors:\n\nType I error: \\(\\hat{\\cal{H}} = 1\\) and \\(\\cal{H} = 0\\)\nType II error: \\(\\hat{\\cal{H}} = 0\\) and \\(\\cal{H} = 1\\)\n\nLoss function: \\[L(\\hat{\\cal{H}}, \\cal{H}) =  w_1  \\, 1(\\hat{\\cal{H}} = 1, \\cal{H} = 0) + w_2 \\, 1(\\hat{\\cal{H}} = 0, \\cal{H} = 1)\\]\n\n\\(w_1\\) weights how bad it is to make a Type I error\n\\(w_2\\) weights how bad it is to make a Type II error"
  },
  {
    "objectID": "resources/slides/16-Bayes-tests.html#loss-function-functions-and-decisions",
    "href": "resources/slides/16-Bayes-tests.html#loss-function-functions-and-decisions",
    "title": "Basics of Bayesian Hypothesis Testing",
    "section": "Loss Function Functions and Decisions",
    "text": "Loss Function Functions and Decisions\n\nRelative weights \\(w = w_2/w_1\\) \\[L(\\hat{\\cal{H}}, \\cal{H}) =   \\, 1(\\hat{\\cal{H}} = 1, \\cal{H} = 0) + w \\, 1(\\hat{\\cal{H}} = 0, \\cal{H} = 1)\\]\nSpecial case \\(w=1\\) \\[L(\\hat{\\cal{H}}, \\cal{H}) =    1(\\hat{\\cal{H}} \\neq \\cal{H})\\]\nknown as 0-1 loss (most common)\nBayes Risk (Posterior Expected Loss) \\[\\textsf{E}_{\\cal{H} \\mid  \\mathbf{y}}[L(\\hat{\\cal{H}}, \\cal{H}) ] =\n1(\\hat{\\cal{H}} = 1)\\pi(\\cal{H}_0 \\mid  \\mathbf{y}) +  1(\\hat{\\cal{H}} = 0) \\pi(\\cal{H}_1 \\mid  \\mathbf{y})\\]\nMinimize loss by picking hypothesis with the highest posterior probability"
  },
  {
    "objectID": "resources/slides/16-Bayes-tests.html#bayesian-hypothesis-testing",
    "href": "resources/slides/16-Bayes-tests.html#bayesian-hypothesis-testing",
    "title": "Basics of Bayesian Hypothesis Testing",
    "section": "Bayesian hypothesis testing",
    "text": "Bayesian hypothesis testing\n\nUsing Bayes theorem, \\[\n\\begin{split}\n\\pi(\\mathcal{H}_1 \\mid \\mathbf{y}) = \\frac{ p( \\mathbf{y}\\mid \\mathcal{H}_1) \\pi(\\mathcal{H}_1) }{ p( \\mathbf{y}\\mid \\mathcal{H}_0) \\pi(\\mathcal{H}_0) + p( \\mathbf{y}\\mid \\mathcal{H}_1) \\pi(\\mathcal{H}_1)},\n\\end{split}\n\\]\nIf \\(\\pi(\\mathcal{H}_0) = 0.5\\) and \\(\\pi(\\mathcal{H}_1) = 0.5\\) a priori, then \\[\n\\begin{split}\n\\pi(\\mathcal{H}_1 \\mid \\mathbf{y}) & = \\frac{ 0.5 p( \\mathbf{y}\\mid \\mathcal{H}_1) }{ 0.5 p( \\mathbf{y}\\mid \\mathcal{H}_0) + 0.5 p( \\mathbf{y}\\mid \\mathcal{H}_1) } \\\\\n\\\\\n& = \\frac{ p( \\mathbf{y}\\mid \\mathcal{H}_1) }{ p( \\mathbf{y}\\mid \\mathcal{H}_0) + p( \\mathbf{y}\\mid \\mathcal{H}_1) }= \\frac{ 1 }{ \\frac{p( \\mathbf{y}\\mid \\mathcal{H}_0)}{p( \\mathbf{y}\\mid \\mathcal{H}_1)} + 1 }\\\\\n\\end{split}\n\\]"
  },
  {
    "objectID": "resources/slides/16-Bayes-tests.html#bayes-factors",
    "href": "resources/slides/16-Bayes-tests.html#bayes-factors",
    "title": "Basics of Bayesian Hypothesis Testing",
    "section": "Bayes factors",
    "text": "Bayes factors\n\nThe ratio \\(\\frac{p( \\mathbf{y}\\mid \\mathcal{H}_0)}{p( \\mathbf{y}\\mid \\mathcal{H}_1)}\\) is a ratio of marginal likelihoods and is known as the Bayes factor in favor of \\(\\mathcal{H}_0\\), written as \\(\\mathcal{BF}_{01}\\). Similarly, we can compute \\(\\mathcal{BF}_{10}\\) via the inverse ratio.\nBayes factors provide a weight of evidence in the data in favor of one model over another. and are used as an alternative to the frequentist p-value.\nRule of Thumb: \\(\\mathcal{BF}_{01} &gt; 10\\) is strong evidence for \\(\\mathcal{H}_0\\); \\(\\mathcal{BF}_{01} &gt; 100\\) is decisive evidence for \\(\\mathcal{H}_0\\). (Kass & Raftery 1995 JASA)\nIn the example (with equal prior probabilities), \\[\n\\begin{split}\n\\pi(\\mathcal{H}_1 \\mid \\mathbf{y}) = \\frac{ 1 }{ \\frac{p( \\mathbf{y}\\mid \\mathcal{H}_0)}{p( \\mathbf{y}\\mid \\mathcal{H}_1)} + 1 } = \\frac{ 1 }{ \\mathcal{BF}_{01} + 1 } \\\\\n\\end{split}\n\\]\nthe higher the value of \\(\\mathcal{BF}_{01}\\), that is, the weight of evidence in the data in favor of \\(\\mathcal{H}_0\\), the lower the marginal posterior probability that \\(\\mathcal{H}_1\\) is true.\n\\(\\mathcal{BF}_{01} \\uparrow\\), \\(\\pi(\\mathcal{H}_1 \\mid \\mathbf{y}) \\downarrow\\)."
  },
  {
    "objectID": "resources/slides/16-Bayes-tests.html#posterior-odds-and-bayes-factors",
    "href": "resources/slides/16-Bayes-tests.html#posterior-odds-and-bayes-factors",
    "title": "Basics of Bayesian Hypothesis Testing",
    "section": "Posterior Odds and Bayes Factors",
    "text": "Posterior Odds and Bayes Factors\n\nPosterior odds \\(\\frac{\\pi(\\mathcal{H}_0 \\mid \\mathbf{y})}{\\pi(\\mathcal{H}_1 \\mid \\mathbf{y})}\\) \\[\n\\begin{split}\n\\frac{\\pi(\\mathcal{H}_0 | \\mathbf{y})}{\\pi(\\mathcal{H}_1 | \\mathbf{y})} & = \\frac{ p( \\mathbf{y}|\\mathcal{H}_0) \\pi(\\mathcal{H}_0) }{ p( \\mathbf{y}| \\mathcal{H}_0) \\pi(\\mathcal{H}_0) + p( \\mathbf{y}| \\mathcal{H}_1) \\pi(\\mathcal{H}_1)} \\div \\frac{ p( \\mathbf{y}| \\mathcal{H}_1) \\pi(\\mathcal{H}_1) }{ p( \\mathbf{y}\\mathcal{H}_0) \\pi(\\mathcal{H}_0) + p( \\mathbf{y}| \\mathcal{H}_1) \\pi(\\mathcal{H}_1)}\\\\\n\\\\\n& = \\frac{ p( \\mathbf{y}| \\mathcal{H}_0) \\pi(\\mathcal{H}_0) }{ p( \\mathbf{y}| \\mathcal{H}_0) \\pi(\\mathcal{H}_0) + p( \\mathbf{y}| \\mathcal{H}_1) \\pi(\\mathcal{H}_1)} \\times \\frac{ p( \\mathbf{y}| \\mathcal{H}_0) \\pi(\\mathcal{H}_0) + p( \\mathbf{y}| \\mathcal{H}_1) \\pi(\\mathcal{H}_1)}{ p( \\mathbf{y}| \\mathcal{H}_1) \\pi(\\mathcal{H}_1) }\\\\\n\\\\\n\\therefore \\underbrace{\\frac{\\pi(\\mathcal{H}_0 \\mid \\mathbf{y})}{\\pi(\\mathcal{H}_1 \\mid \\mathbf{y})}}_{\\text{posterior odds}} & = \\underbrace{\\frac{ \\pi(\\mathcal{H}_0) }{ \\pi(\\mathcal{H}_1) }}_{\\text{prior odds}} \\times \\underbrace{\\frac{ p( \\mathbf{y}\\mid \\mathcal{H}_0) }{ p( \\mathbf{y}\\mid \\mathcal{H}_1) }}_{\\text{Bayes factor } \\mathcal{BF}_{01}} \\\\\n\\end{split}\n\\]\nThe Bayes factor can be thought of as the factor by which our prior odds change (towards the posterior odds) in the light of the data."
  },
  {
    "objectID": "resources/slides/16-Bayes-tests.html#likelihoods-evidence",
    "href": "resources/slides/16-Bayes-tests.html#likelihoods-evidence",
    "title": "Basics of Bayesian Hypothesis Testing",
    "section": "Likelihoods & Evidence",
    "text": "Likelihoods & Evidence\nMaximized Likelihood. \\(n = 10\\)\n\np-value = 0.05"
  },
  {
    "objectID": "resources/slides/16-Bayes-tests.html#marginal-likelihoods-evidence",
    "href": "resources/slides/16-Bayes-tests.html#marginal-likelihoods-evidence",
    "title": "Basics of Bayesian Hypothesis Testing",
    "section": "Marginal Likelihoods & Evidence",
    "text": "Marginal Likelihoods & Evidence\nMaximized & Marginal Likelihoods\n\n\\(\\cal{BF}_{10}\\) = 1.73 or \\(\\cal{BF}_{01}\\) = 0.58\nPosterior Probability of \\(\\cal{H}_0\\) = 0.3665"
  },
  {
    "objectID": "resources/slides/16-Bayes-tests.html#candidates-formula-besag-1989",
    "href": "resources/slides/16-Bayes-tests.html#candidates-formula-besag-1989",
    "title": "Basics of Bayesian Hypothesis Testing",
    "section": "Candidate’s Formula (Besag 1989)",
    "text": "Candidate’s Formula (Besag 1989)\nAlternative expression for BF based on Candidate’s Formula or Savage-Dickey ratio \\[\\cal{BF}_{01} = \\frac{p( \\mathbf{y}\\mid \\cal{H}_0)}\n       {p( \\mathbf{y}\\mid \\cal{H}_1)} =\n  \\frac{\\pi_\\theta(0 \\mid \\cal{H}_1, \\mathbf{y})}\n       {\\pi_\\theta(0 \\mid \\cal{H}_1)}\\]\n\n\\[\\pi_\\theta(\\theta \\mid \\cal{H}_i, \\mathbf{y})  =  \\frac{p(\\mathbf{y}\\mid \\theta, \\cal{H}_i) \\pi(\\theta \\mid \\cal{H}_i)} {p(\\mathbf{y}\\mid \\cal{H}_i)}  \\Rightarrow  \np(\\mathbf{y}\\mid \\cal{H}_i)   = \\frac{p(\\mathbf{y}\\mid \\theta, \\cal{H}_i) \\pi(\\theta \\mid \\cal{H}_i)} {\\pi_\\theta(\\theta \\mid \\cal{H}_i, \\mathbf{y})}\\]\n\n\n\\[\\cal{BF}_{01}  = \\frac{\\frac{p(\\mathbf{y}\\mid \\theta, \\cal{H}_0) \\pi(\\theta \\mid \\cal{H}_0)} {\\pi_\\theta(\\theta \\mid \\cal{H}_0, \\mathbf{y})} } { \\frac{p(\\mathbf{y}\\mid \\theta, \\cal{H}_1) \\pi(\\theta \\mid \\cal{H}_1)} {\\pi_\\theta(\\theta \\mid \\cal{H}_1, \\mathbf{y})}}  =   \\frac{\\frac{p(\\mathbf{y}\\mid \\theta = 0) \\delta_0(\\theta)} {\\delta_0(\\theta)} } { \\frac{p(\\mathbf{y}\\mid \\theta, \\cal{H}_1) \\pi(\\theta \\mid \\cal{H}_1)} {\\pi_\\theta(\\theta \\mid \\cal{H}_1, \\mathbf{y})}}\n=   \\frac{p(\\mathbf{y}\\mid \\theta = 0)}{p(\\mathbf{y}\\mid \\theta, \\cal{H}_1)}\n   \\frac{\\delta_0(\\theta)} {\\delta_0(\\theta)}  \\frac{\\pi_\\theta(\\theta \\mid \\cal{H}_1, \\mathbf{y})}{\\pi(\\theta \\mid \\cal{H}_1)} \\]\n\nSimplifies to the ratio of the posterior to prior densities when evaluated \\(\\theta\\) at zero"
  },
  {
    "objectID": "resources/slides/16-Bayes-tests.html#prior",
    "href": "resources/slides/16-Bayes-tests.html#prior",
    "title": "Basics of Bayesian Hypothesis Testing",
    "section": "Prior",
    "text": "Prior\nPlots were based on a \\(\\theta \\mid \\cal{H}_1 \\sim \\textsf{N}(0, 1)\\)\n\ncentered at value for \\(\\theta\\) under \\(\\cal{H}_0\\) (goes back to Jeffreys)\n“unit information prior” equivalent to a prior sample size is 1\nis this a “reasonable prior”?\n\nWhat happens if \\(n \\to \\infty\\)?\nWhat happens of \\(\\tau_0 \\to 0\\) ? (less informative)"
  },
  {
    "objectID": "resources/slides/16-Bayes-tests.html#choice-of-precision",
    "href": "resources/slides/16-Bayes-tests.html#choice-of-precision",
    "title": "Basics of Bayesian Hypothesis Testing",
    "section": "Choice of Precision",
    "text": "Choice of Precision\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\tau_0 = 1/10\\)\nBayes Factor for \\(\\cal{H}_0\\) to \\(\\cal{H}_1\\) is \\(1.5\\)\nPosterior Probability of \\(\\cal{H}_0\\) = 0.6001\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\tau_0 = 1/1000\\)\nBayes Factor for \\(\\cal{H}_0\\) to \\(\\cal{H}_1\\) is \\(14.65\\)\nPosterior Probability of \\(\\cal{H}_0\\) = 0.9361"
  },
  {
    "objectID": "resources/slides/16-Bayes-tests.html#vague-priors-hypothesis-testing",
    "href": "resources/slides/16-Bayes-tests.html#vague-priors-hypothesis-testing",
    "title": "Basics of Bayesian Hypothesis Testing",
    "section": "Vague Priors & Hypothesis Testing",
    "text": "Vague Priors & Hypothesis Testing\n\nAs \\(\\tau_0 \\to 0\\) the \\(\\cal{BF}_{01} \\to \\infty\\) and \\(\\Pr(\\cal{H}_0 \\mid \\mathbf{y}\\to 1\\)!\nAs we use a less & less informative prior for \\(\\theta\\) under \\(\\cal{H}_1\\) we obtain more & more evidence for \\(\\cal{H}_0\\) over \\(\\cal{H}_1\\)!\nKnown as Bartlett’s Paradox - the paradox is that a seemingly non-informative prior for \\(\\theta\\) is very informative about \\(\\cal{H}\\)!\nGeneral problem with nested sequence of models. If we choose vague priors on the additional parameter in the larger model we will be favoring the smaller models under consideration!\nSimilar phenomenon with increasing sample size (Lindley’s Paradox)\n\n\n\n\n\n\n\n\nBottom Line Don’t use vague priors!\n\n\n\n\n\nWhat should we use then?"
  },
  {
    "objectID": "resources/slides/16-Bayes-tests.html#other-options",
    "href": "resources/slides/16-Bayes-tests.html#other-options",
    "title": "Basics of Bayesian Hypothesis Testing",
    "section": "Other Options",
    "text": "Other Options\n\nPlace a prior on \\(\\tau_0\\) \\[\\tau_0 \\sim \\textsf{Gamma}(1/2, 1/2)\\]\nIf \\(\\theta \\mid \\tau_0, \\cal{H}_1 \\sim \\textsf{N}(0, 1/\\tau_0)\\), then \\(\\theta_0  \\mid \\cal{H}_1\\) has a \\(\\textsf{Cauchy}(0,1)\\) distribution! Recommended by Jeffreys (1961)\nno closed form expressions for marginal likelihood!\ncan use Numerical Integration (a one dimensional integral) to estimate the marginal likelihood under \\(\\cal{H}_1\\)"
  },
  {
    "objectID": "resources/slides/16-Bayes-tests.html#intrinsic-bayes-factors-priors-berger-pericchi",
    "href": "resources/slides/16-Bayes-tests.html#intrinsic-bayes-factors-priors-berger-pericchi",
    "title": "Basics of Bayesian Hypothesis Testing",
    "section": "Intrinsic Bayes Factors & Priors (Berger & Pericchi)",
    "text": "Intrinsic Bayes Factors & Priors (Berger & Pericchi)\n\nCan’t use improper priors under \\(\\cal{H}_1\\)\nuse part of the data \\(y(l)\\) to update an improper prior on \\(\\theta\\) to get a proper posterior \\(\\pi(\\theta \\mid \\cal{H}_i, y(l))\\)\nuse \\(\\pi(\\theta \\mid y(l), \\cal{H}_i)\\) to obtain the posterior for \\(\\theta\\) based on the rest of the training data\nCalculate a Bayes Factor (avoids arbitrary normalizing constants!)\nChoice of training sample \\(y(l)\\)?\nBerger & Pericchi (1996) propose “averaging” over training samples intrinsic Bayes Factors\nintrinsic prior on \\(\\theta\\) given \\(\\cal{H}_1\\) in model \\(\\mathbf{Y}\\mid \\theta \\sim \\textsf{N}(\\theta, \\sigma^2)\\) \\[\n\\pi(\\theta \\mid \\sigma^2) = \\frac{1 - \\exp[-\\theta^2/\\sigma^2]}{2 \\sqrt{\\pi} \\theta^2\\sigma}\n\\] leads to the Intrisic Bayes Factor\n\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/08-bayes.html#outline",
    "href": "resources/slides/08-bayes.html#outline",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Outline",
    "text": "Outline\n\nReadings:\n\nChristensen Chapter 2.9 and Chapter 15\nSeber & Lee Chapter 3.12"
  },
  {
    "objectID": "resources/slides/08-bayes.html#bayes-estimation",
    "href": "resources/slides/08-bayes.html#bayes-estimation",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Bayes Estimation",
    "text": "Bayes Estimation\nModel \\(\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\) with \\(\\boldsymbol{\\epsilon}\\sim \\textsf{N}(\\mathbf{0}_n , \\sigma^2\n  \\mathbf{I}_n)\\) is equivalent to \\[\n\\mathbf{Y}\\sim \\textsf{N}(\\mathbf{X}\\boldsymbol{\\beta}, \\mathbf{I}_n/\\phi)\n\\]\n\n\\(\\phi = 1/\\sigma^2\\) is the precision of the data.\nwe might expect \\(\\boldsymbol{\\beta}\\) to be close to some vector \\(\\mathbf{b}_0\\)\nrepresent this a priori with a Prior Distribution for \\(\\boldsymbol{\\beta}\\), e.g. \\[\\boldsymbol{\\beta}\\sim \\textsf{N}(\\mathbf{b}_0, \\boldsymbol{\\Phi}_0^{-1})\\]\n\\(\\mathbf{b}_0\\) is the prior mean and \\(\\boldsymbol{\\Phi}_0\\) is the prior precision of \\(\\boldsymbol{\\beta}\\) that captures how close \\(\\boldsymbol{\\beta}\\) is to \\(\\mathbf{b}_0\\)\nSimilarly, we could represent prior uncertainty about \\(\\sigma\\), \\(\\sigma^2\\) or equivalently \\(\\phi\\) with a probability distribution\nfor now treat \\(\\phi\\) as fixed"
  },
  {
    "objectID": "resources/slides/08-bayes.html#bayesian-inference",
    "href": "resources/slides/08-bayes.html#bayesian-inference",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Bayesian Inference",
    "text": "Bayesian Inference\n\nonce we see data \\(\\mathbf{Y}\\), Bayesian inference proceeds by updating prior beliefs\nrepresented by the posterior distribution of \\(\\boldsymbol{\\beta}\\) which is the conditional distribution of \\(\\boldsymbol{\\beta}\\) given the data \\(\\mathbf{Y}\\) (and \\(\\phi\\) for now)\nPosterior \\(p(\\boldsymbol{\\beta}\\mid \\mathbf{Y}, \\phi)\\) \\[p(\\boldsymbol{\\beta}\\mid \\mathbf{Y}) = \\frac{p(\\mathbf{Y}\\mid \\boldsymbol{\\beta}, \\phi) p(\\boldsymbol{\\beta}\\mid \\phi)}{c}\\]\n\\(c\\) is a constant so that the posterior density integrates to \\(1\\) \\[c = \\int_{\\mathbb{R}^p} p(\\mathbf{Y}\\mid \\boldsymbol{\\beta}, \\phi) p(\\boldsymbol{\\beta}\\mid \\phi) d\\, \\boldsymbol{\\beta}\\equiv p(\\mathbf{Y})\\]\nsince \\(c\\) is a constant that doesn’t depend on \\(\\boldsymbol{\\beta}\\) just ignore\nwork with density up to constant of proportionality"
  },
  {
    "objectID": "resources/slides/08-bayes.html#posterior-density",
    "href": "resources/slides/08-bayes.html#posterior-density",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Posterior Density",
    "text": "Posterior Density\nPosterior for \\(\\boldsymbol{\\beta}\\) is \\(p(\\boldsymbol{\\beta}\\mid \\mathbf{Y}) \\propto p(\\mathbf{Y}\\mid \\boldsymbol{\\beta}, \\phi) p(\\boldsymbol{\\beta}\\mid \\phi)\\)\n\nLikelihood for \\(\\boldsymbol{\\beta}\\) is proportional to \\(p(\\mathbf{Y}\\mid \\boldsymbol{\\beta}, \\phi\\))\n\n\n\\[\\begin{align*} p(\\mathbf{Y}\\mid \\boldsymbol{\\beta}, \\phi) & = (2 \\pi)^{-n/2} |\\mathbf{I}_n / \\phi |^{-1/2}\n\\exp\\left\\{-\\frac{1}{2} \\left( (\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta})^T \\phi \\mathbf{I}_n (\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}) \\right) \\right\\} \\\\\n& \\propto \\exp\\left\\{-\\frac{1}{2} \\left( \\phi \\mathbf{Y}^T\\mathbf{Y}- 2 \\boldsymbol{\\beta}^T \\phi \\mathbf{X}^T\\mathbf{Y}+ \\phi \\boldsymbol{\\beta}\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\\right) \\right\\}\n\\end{align*}\\]\n\nsimilarly expand prior \\[\\begin{align*}\np(\\boldsymbol{\\beta}\\mid \\phi) & = (2 \\pi)^{-p/2} |\\boldsymbol{\\Phi}_0^{-1}|^{-1/2}\n\\exp\\left\\{-\\frac{1}{2} \\left( (\\boldsymbol{\\beta}- \\mathbf{b}_0)^T \\boldsymbol{\\Phi}_0 (\\boldsymbol{\\beta}- \\mathbf{b}_0) \\right) \\right\\} \\\\\n& \\propto \\exp\\left\\{-\\frac{1}{2} \\left(  \\mathbf{b}_0^T \\boldsymbol{\\Phi}_0\\mathbf{b}_0 - 2 \\boldsymbol{\\beta}^T\\boldsymbol{\\Phi}_0 \\mathbf{b}_0 + \\boldsymbol{\\beta}\\boldsymbol{\\Phi}_0 \\boldsymbol{\\beta}\\right) \\right\\}\n\\end{align*}\\]"
  },
  {
    "objectID": "resources/slides/08-bayes.html#posterior-steps",
    "href": "resources/slides/08-bayes.html#posterior-steps",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Posterior Steps",
    "text": "Posterior Steps\n\nExpand quadratics and regroup terms \\[\\begin{align*}\np(\\boldsymbol{\\beta}\\mid \\mathbf{Y}, \\phi)\n& \\propto e^{\\left\\{-\\frac{1}{2} \\left( \\phi \\boldsymbol{\\beta}\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\beta}\\boldsymbol{\\Phi}_0 \\boldsymbol{\\beta}- 2(\\phi \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{Y}+ \\boldsymbol{\\beta}^T\\boldsymbol{\\Phi}_0 \\mathbf{b}_0)  + \\phi \\mathbf{Y}^T\\mathbf{Y}+ \\mathbf{b}_0^T \\boldsymbol{\\Phi}_0\\mathbf{b}_0 \\right) \\right\\} } \\\\\n&  \\propto \\exp\\left\\{-\\frac{1}{2} \\left( \\boldsymbol{\\beta}( \\phi\\mathbf{X}^T\\mathbf{X}+ \\boldsymbol{\\Phi}_0) \\boldsymbol{\\beta}- 2 \\boldsymbol{\\beta}^T(\\phi \\mathbf{X}^T\\mathbf{Y}+ \\boldsymbol{\\Phi}_0 \\mathbf{b}_0)  \\right) \\right\\}  \n\\end{align*}\\]\n\n\nKernel of a Multivariate Normal\n\nRead off posterior precision from Quadratic in \\(\\boldsymbol{\\beta}\\)\nRead off posterior precision \\(\\times\\) posterior mean from Linear term in \\(\\boldsymbol{\\beta}\\)\n\nwill need to complete the quadratic in the posterior mean\\(^{\\dagger}\\)\n\n\n\\(\\dagger\\) necessary to keep track of all terms for \\(\\phi\\) when we do not condition on \\(\\phi\\)"
  },
  {
    "objectID": "resources/slides/08-bayes.html#posterior-precision-and-covariance",
    "href": "resources/slides/08-bayes.html#posterior-precision-and-covariance",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Posterior Precision and Covariance",
    "text": "Posterior Precision and Covariance\n\\[ p(\\boldsymbol{\\beta}\\mid \\mathbf{Y}, \\phi)  \\propto \\exp\\left\\{-\\frac{1}{2} \\left( \\boldsymbol{\\beta}( \\phi\\mathbf{X}^T\\mathbf{X}+ \\boldsymbol{\\Phi}_0) \\boldsymbol{\\beta}- 2 \\boldsymbol{\\beta}^T(\\phi \\mathbf{X}^T\\mathbf{Y}+ \\boldsymbol{\\Phi}_0 \\mathbf{b}_0)  \\right) \\right\\}\n\\]\n\nPosterior Precision \\[\\boldsymbol{\\Phi}_n \\equiv \\phi\\mathbf{X}^T\\mathbf{X}+ \\boldsymbol{\\Phi}_0\\]\nsum of data precision and prior precision\nposterior Covariance \\[\\textsf{Cov}[\\boldsymbol{\\beta}\\mid \\mathbf{Y}, \\phi] = \\boldsymbol{\\Phi}_n^{-1} = (\\phi\\mathbf{X}^T\\mathbf{X}+ \\boldsymbol{\\Phi}_0)^{-1}\\]\nif \\(\\boldsymbol{\\Phi}_0\\) is full rank, then \\(\\textsf{Cov}[\\boldsymbol{\\beta}\\mid \\mathbf{Y}, \\phi]\\) is full rank even if \\(\\mathbf{X}^T\\mathbf{X}\\) is not"
  },
  {
    "objectID": "resources/slides/08-bayes.html#posterior-mean-updating",
    "href": "resources/slides/08-bayes.html#posterior-mean-updating",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Posterior Mean Updating",
    "text": "Posterior Mean Updating\n\\[\\begin{align*} p(\\boldsymbol{\\beta}\\mid \\mathbf{Y}, \\phi) & \\propto \\exp\\left\\{\\frac{1}{2} \\left( \\boldsymbol{\\beta}( \\phi\\mathbf{X}^T\\mathbf{X}+ \\boldsymbol{\\Phi}_0) \\boldsymbol{\\beta}- 2 \\boldsymbol{\\beta}^T(\\phi \\mathbf{X}^T\\mathbf{Y}+ \\boldsymbol{\\Phi}_0 \\mathbf{b}_0)  \\right) \\right\\} \\\\\n& \\propto \\exp\\left\\{\\frac{1}{2} \\left( \\boldsymbol{\\beta}( \\phi\\mathbf{X}^T\\mathbf{X}+ \\boldsymbol{\\Phi}_0) \\boldsymbol{\\beta}- 2 \\boldsymbol{\\beta}^T\\boldsymbol{\\Phi}_n \\boldsymbol{\\Phi}_n^{-1}(\\phi \\mathbf{X}^T\\mathbf{Y}+ \\boldsymbol{\\Phi}_0 \\mathbf{b}_0)  \\right) \\right\\} \\\\\n\\end{align*}\\]\n\nposterior mean \\(\\mathbf{b}_n\\) \\[\\begin{align*}\n\\mathbf{b}_n & \\equiv \\boldsymbol{\\Phi}_n^{-1} (\\phi \\mathbf{X}^T\\mathbf{Y}+ \\boldsymbol{\\Phi}_0 \\mathbf{b}_0 ) \\\\\n    & = (\\phi\\mathbf{X}^T\\mathbf{X}+ \\boldsymbol{\\Phi}_0)^{-1}\\left(\\phi (\\mathbf{X}^T\\mathbf{X}) (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{Y}+ \\boldsymbol{\\Phi}_0 \\mathbf{b}_0 \\right) \\\\\n    & = (\\phi\\mathbf{X}^T\\mathbf{X}+ \\boldsymbol{\\Phi}_0)^{-1} \\left( \\phi (\\mathbf{X}^T\\mathbf{X}) \\hat{\\boldsymbol{\\beta}}+ \\boldsymbol{\\Phi}_0 \\mathbf{b}_0 \\right)\n\\end{align*}\\]\na precision weighted linear combination of MLE and prior mean\nfirst expression useful if \\(\\mathbf{X}\\) is not full rank!"
  },
  {
    "objectID": "resources/slides/08-bayes.html#notes",
    "href": "resources/slides/08-bayes.html#notes",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Notes",
    "text": "Notes\nPosterior is a Multivariate Normal \\(p(\\boldsymbol{\\beta}\\mid \\mathbf{Y}, \\phi) \\sim \\textsf{N}(\\mathbf{b}_n, \\boldsymbol{\\Phi}_n^{-1})\\)\n\nposterior mean: \\(\\mathbf{b}_n  =  \\boldsymbol{\\Phi}_n^{-1} (\\phi \\mathbf{X}^T\\mathbf{Y}+ \\boldsymbol{\\Phi}_0 \\mathbf{b}_0 )\\)\nposterior precision: \\(\\boldsymbol{\\Phi}_n = \\phi\\mathbf{X}^T\\mathbf{X}+ \\boldsymbol{\\Phi}_0\\)\nthe posterior precision (inverse posterior variance) is the sum of the prior precision and the data precision.\nthe posterior mean is a linear combination of MLE/OLS and prior mean\nif the prior precision \\(\\boldsymbol{\\Phi}_n\\) is very large compared to the data precision \\(\\phi \\mathbf{X}^T\\mathbf{X}\\), the posterior mean will be close to the prior mean \\(\\mathbf{b}_0\\).\nif the prior precision \\(\\boldsymbol{\\Phi}_n\\) is very small compared to the data precision \\(\\phi \\mathbf{X}^T\\mathbf{X}\\), the posterior mean will be close to the MLE/OLS estimator.\ndata precision will generally be increasing with sample size"
  },
  {
    "objectID": "resources/slides/08-bayes.html#bayes-estimators",
    "href": "resources/slides/08-bayes.html#bayes-estimators",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Bayes Estimators",
    "text": "Bayes Estimators\nA Bayes estimator is a potential value of \\(\\boldsymbol{\\beta}\\) that is obtained from the posterior distribution in some principled way.\n\nStandard estimators include\n\nthe posterior mean estimator, which is the minimizer of the Bayes risk under squared error loss\nthe maximum a posteriori (MAP) estimator, the value \\(\\boldsymbol{\\beta}\\) that maximizes the posterior density (or log posterior density)\n\nThe first estimator is based on principles from classical decision theory, whereas the second can be related to penalized likelihood estimation.\nin the case of linear regression they turn out to be the same estimator!"
  },
  {
    "objectID": "resources/slides/08-bayes.html#bayes-estimator-under-squared-error-loss",
    "href": "resources/slides/08-bayes.html#bayes-estimator-under-squared-error-loss",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Bayes Estimator under Squared Error Loss",
    "text": "Bayes Estimator under Squared Error Loss\n\nthe Frequentist Risk \\(R(\\beta, \\delta) \\equiv \\textsf{E}_{\\mathbf{Y}\\mid \\boldsymbol{\\beta}}[\\| \\delta(\\mathbf{Y})− \\boldsymbol{\\beta}\\|^2]\\) is the expected loss of decision \\(\\delta\\) for a given \\(\\boldsymbol{\\beta}\\)\n\n\n\nDefinition: Bayes Rule and Bayes RiskThe Bayes rule under squared error loss is the function of \\(\\mathbf{Y}\\), \\(\\delta^*(\\mathbf{Y})\\), that minimizes the Bayes risk \\(B(p_\\boldsymbol{\\beta}, \\delta)\\) \\[\\delta^*(\\mathbf{Y}) =  \\arg \\min_{\\delta \\in \\cal{D}} B(p_\\boldsymbol{\\beta}, \\delta)\\]\n\\[B(p_\\boldsymbol{\\beta}, \\delta) = \\textsf{E}_\\boldsymbol{\\beta}R(\\boldsymbol{\\beta}, \\delta) = \\textsf{E}_{\\boldsymbol{\\beta}} \\textsf{E}_{\\mathbf{Y}\\mid \\boldsymbol{\\beta}}[\\| \\delta(\\mathbf{Y})− \\boldsymbol{\\beta}\\|^2]\\] where the expectation is with respect to the prior distribution, \\(p_\\boldsymbol{\\beta}\\), over \\(\\boldsymbol{\\beta}\\) and the conditional distribution of \\(\\mathbf{Y}\\) given \\(\\boldsymbol{\\beta}\\)"
  },
  {
    "objectID": "resources/slides/08-bayes.html#bayes-estimators-1",
    "href": "resources/slides/08-bayes.html#bayes-estimators-1",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Bayes Estimators",
    "text": "Bayes Estimators\n\nDefinition: Bayes ActionThe Bayes Action is the action \\(a \\in {\\cal{A}}\\) that minimizes the posterior expected loss: \\[ \\delta_B^*(\\mathbf{Y}) = \\arg \\min_{\\delta \\in \\cal{D}} E_{\\boldsymbol{\\beta}\\mid \\mathbf{Y}} [\\| \\delta − \\boldsymbol{\\beta}\\|^2]\n\\]\n\n\n\ncan show that the Bayes action that minimizes the posterior expected loss is the posterior mean \\(\\boldsymbol{\\beta}_n = (\\phi \\mathbf{X}^T\\mathbf{X}+ \\boldsymbol{\\Phi}_0)^{-1}(\\phi \\mathbf{X}^T\\mathbf{Y}+ \\boldsymbol{\\Phi}_0 \\mathbf{b}_0\\) and is also the Bayes rule.\ndifferent values of \\(\\mathbf{b}_0\\) and \\(\\boldsymbol{\\Phi}_0\\) will lead to different Bayes estimators as will different prior distributions besides the Normal\ntake \\(\\mathbf{b}_0 = \\mathbf{0}\\); Bayes estimators are often referred to as shrinkage estimators \\[\\boldsymbol{\\beta}_n = \\left( \\mathbf{X}^T\\mathbf{X}+ \\boldsymbol{\\Phi}_0/\\phi  \\right)^{-1}  \\mathbf{X}^T\\mathbf{Y}\\] as they shrink the MLE/OLS estimator towards \\(\\mathbf{0}\\)"
  },
  {
    "objectID": "resources/slides/08-bayes.html#prior-choice",
    "href": "resources/slides/08-bayes.html#prior-choice",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Prior Choice",
    "text": "Prior Choice\nOne of the most common priors for the normal linear model is the g-prior of Zellner (1986) where \\(\\boldsymbol{\\Phi}_0 = \\frac{\\phi}{g} \\mathbf{X}^T\\mathbf{X}\\) \\[\\boldsymbol{\\beta}\\mid \\phi, g \\sim \\textsf{N}(\\mathbf{0}, g/\\phi (\\mathbf{X}^T\\mathbf{X})^{-1})\\]\n\n\\[\\begin{align*}\n\\mathbf{b}_n & = \\left( \\mathbf{X}^T\\mathbf{X}+ \\frac{\\phi}{g} \\frac{\\mathbf{X}^T\\mathbf{X}}{\\phi} \\right)^{-1} \\mathbf{X}^T\\mathbf{Y}\\\\\n  & = \\left( \\mathbf{X}^T\\mathbf{X}+ \\frac{1}{g} \\mathbf{X}^T\\mathbf{X}\\right)^{-1} \\mathbf{X}^T\\mathbf{Y}\\\\\n  & = \\left( \\frac{1 +g}{g} \\mathbf{X}^T\\mathbf{X}\\right)^{-1} \\mathbf{X}^T\\mathbf{Y}\\\\\n  & = \\frac{g}{1+g} \\hat{\\boldsymbol{\\beta}}\n\\end{align*}\\]\n\n\\(g\\) controls the amount of shrinkage where all of the MLEs are shrunk to zero by the same fraction \\(g/(1+g)\\)"
  },
  {
    "objectID": "resources/slides/08-bayes.html#another-common-choice",
    "href": "resources/slides/08-bayes.html#another-common-choice",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Another Common Choice",
    "text": "Another Common Choice\n\nanother common choice is the independent prior \\[\\boldsymbol{\\beta}\\mid \\phi \\sim \\textsf{N}(\\mathbf{0}, \\boldsymbol{\\Phi}_0^{-1})\\] where \\(\\boldsymbol{\\Phi}_0 = \\phi \\kappa \\mathbf{I}_b\\) for some \\(\\kappa&gt; 0\\)\nthe posterior mean is \\[\\begin{align*}\n\\boldsymbol{\\beta}_n & = (\\mathbf{X}^T\\mathbf{X}+ \\kappa \\mathbf{I})^{-1} \\mathbf{X}^T\\mathbf{Y}\\\\\n   & =  (\\mathbf{X}^T\\mathbf{X}+ \\kappa \\mathbf{I})^{-1} \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\end{align*}\\]\nthis is also a shrinkage estimator but the amount of shrinkage is different for the different components of \\(\\mathbf{b}_n\\) depending on the eigenvalues of \\(\\mathbf{X}^T\\mathbf{X}\\)\neasiest to see this via an orthogonal rotation of the model"
  },
  {
    "objectID": "resources/slides/08-bayes.html#rotated-regression",
    "href": "resources/slides/08-bayes.html#rotated-regression",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Rotated Regression",
    "text": "Rotated Regression\n\nUse the singular value decomposition of \\(\\mathbf{X}= \\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{V}^T\\) and multiply thru by \\(\\mathbf{U}^T\\) to get the rotated model \\[\\begin{align*}\n\\mathbf{U}^T \\mathbf{Y}& =  \\boldsymbol{\\Lambda}\\mathbf{V}^T\\boldsymbol{\\beta}+ \\mathbf{U}^T\\boldsymbol{\\epsilon}\\\\\n\\tilde{\\mathbf{Y}}& = \\boldsymbol{\\Lambda}\\boldsymbol{\\alpha}+ \\tilde{\\boldsymbol{\\epsilon}}\n\\end{align*}\\] where \\(\\boldsymbol{\\alpha}= \\mathbf{V}^T\\boldsymbol{\\beta}\\) and \\(\\tilde{\\boldsymbol{\\epsilon}} = \\mathbf{U}^T\\boldsymbol{\\epsilon}\\)\nthe induced prior is still \\(\\boldsymbol{\\alpha}\\mid \\phi \\sim \\textsf{N}(\\mathbf{0}, (\\phi \\kappa)^{-1} \\mathbf{I})\\)\nthe posterior mean of \\(\\boldsymbol{\\alpha}\\) is \\[\\begin{align*}\n\\mathbf{a}& =  (\\boldsymbol{\\Lambda}^2 + \\kappa \\mathbf{I})^{-1} \\boldsymbol{\\Lambda}^2 \\hat{\\boldsymbol{\\alpha}}\\\\\na_j & = \\frac{\\lambda_j^2}{\\lambda_j^2 + \\kappa} \\hat{\\alpha}_j\n\\end{align*}\\]\nsets to zero the components of the OLS solution where eigenvalues are zero!"
  },
  {
    "objectID": "resources/slides/08-bayes.html#connections-to-frequentist-estimators",
    "href": "resources/slides/08-bayes.html#connections-to-frequentist-estimators",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Connections to Frequentist Estimators",
    "text": "Connections to Frequentist Estimators\n\nThe posterior mean under this independent prior is the same as the classic ridge regression estimator of Hoerl and\nthe variance of \\(\\hat{\\alpha}_j\\) is \\(\\sigma^2/\\lambda_j^2\\) while the variance of \\(a_j\\) is \\(\\sigma^2/(\\lambda_j^2 + \\kappa)\\)\nclearly components of \\(\\boldsymbol{\\alpha}\\) with small eigenvalues will have large variances\nridge regression keeps those components from “blowing up” by shrinking them towards zero and having a finite variance\nrotate back to get the ridge estimator for \\(\\boldsymbol{\\beta}\\), \\(\\hat{\\boldsymbol{\\beta}}_R = \\mathbf{V}\\mathbf{a}\\)\nridge regression applies a high degree of shrinkage to the “parts” (linear combinations) of \\(\\boldsymbol{\\beta}\\) that have high variability, and a low degree of shrinkage to the parts that are well-estimated.\nturns out there always exists a value of \\(\\kappa\\) that will improve over OLS!\n\nUnfortunately no closed form solution except in orthogonal regression and then it depends on the unknown \\(\\|\\boldsymbol{\\beta}\\|^2\\)!"
  },
  {
    "objectID": "resources/slides/08-bayes.html#next-class",
    "href": "resources/slides/08-bayes.html#next-class",
    "title": "Bayesian Estimation in Linear Models",
    "section": "Next Class",
    "text": "Next Class\n\nFrequentist risk of Bayes estimators\nBayes and penalized loss functions\n\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/template.html#outline",
    "href": "resources/slides/template.html#outline",
    "title": "template",
    "section": "Outline",
    "text": "Outline\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/01-introduction.html#introduction-to-sta721",
    "href": "resources/slides/01-introduction.html#introduction-to-sta721",
    "title": "Introduction to STA721",
    "section": "Introduction to STA721",
    "text": "Introduction to STA721\n\nCourse: Theory and Application of linear models from both a frequentist (classical) and Bayesian perspective\nPrerequisites: linear algebra and a mathematical statistics course covering likelihoods and distribution theory (normal, t, F, chi-square, gamma distributions)\nIntroduce R programming as needed in the lab\nIntroduce Bayesian methods, but assume that you are co-registered in 702 or have taken it previously\nmore info on Course website https://sta721-F24.github.io/website/\n\nschedule and slides, HW, etc\ncritical dates (Midterms and Finals)\noffice hours\n\nCanvas for grades, email, announcements\n\n\nPlease let me know if there are broken links for slides, etc!"
  },
  {
    "objectID": "resources/slides/01-introduction.html#notation",
    "href": "resources/slides/01-introduction.html#notation",
    "title": "Introduction to STA721",
    "section": "Notation",
    "text": "Notation\n\nscalors are \\(a\\) (italics or math italics)\nvectors are in bold lower case, \\(\\mathbf{a}\\), with the exception of random variables\nall vectors are column vectors \\[\\mathbf{a}= \\left[\\begin{array}{c}\n    a_1 \\\\\n    a_2 \\\\\n    \\vdots \\\\\n    a_n\n     \\end{array} \\right]\n\\]\n\n\\(\\mathbf{1}_n\\) is a \\(n \\times 1\\) vector of all ones\n\ninner product \\(\\langle    \\mathbf{a}, \\mathbf{a}\\rangle = \\mathbf{a}^T\\mathbf{a}= \\|\\mathbf{a}\\|^2 = \\sum_{i=1}^n a_i^2\\); \\(\\langle    \\mathbf{a}, \\mathbf{b}\\rangle = \\mathbf{a}^T\\mathbf{b}\\)\nlength or norm of \\(\\mathbf{a}\\) is \\(\\|\\mathbf{a}\\|\\)"
  },
  {
    "objectID": "resources/slides/01-introduction.html#matrices",
    "href": "resources/slides/01-introduction.html#matrices",
    "title": "Introduction to STA721",
    "section": "Matrices",
    "text": "Matrices\n\nMatrices are represented in bold \\(\\mathbf{A}= (a_{ij})\\) \\[\\mathbf{A}= \\left[\\begin{array}{cccc}\n    a_{11} & a_{12} & \\cdots & a_{1m}  \\\\\n    a_{21} & a_{22} & \\cdots & a_{2m} \\\\\n    \\vdots & \\vdots & \\vdots & \\vdots\\\\\n    a_{n1} & a_{n2} & \\cdots & a_{nm}\n     \\end{array} \\right]\n\\]\n\nidentity matrix \\(\\mathbf{I}_n\\) square matrix with diagonal elements 1 and off diagonal 0\ntrace: if \\(\\mathbf{A}\\) is \\(n \\times m\\) \\(\\textsf{tr}(\\mathbf{A}) = \\sum_i^{\\max n,m } a_{ii}\\)\ndeterminant: for \\(\\mathbf{A}\\) is \\(n \\times n\\) then the determinant is \\(\\det(A)\\)\ninverse: if \\(\\mathbf{A}\\) is nonsingular \\(\\mathbf{A}&gt; 0\\), then its inverse is \\(\\mathbf{A}^{-1}\\)"
  },
  {
    "objectID": "resources/slides/01-introduction.html#statistical-models",
    "href": "resources/slides/01-introduction.html#statistical-models",
    "title": "Introduction to STA721",
    "section": "Statistical Models",
    "text": "Statistical Models\nOhm’s Law: \\(Y\\) is voltage across a resistor of \\(r\\) ohms and \\(X\\) is the amperes of the current through the resistor (in theory) \\[Y = rX\\]\n\nSimple linear regression for observational data \\[Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\text{  for  } i = 1,\n\\ldots, n\\]\nRewrite in vectors: \\[\\begin{eqnarray*}\n\\left[\n\\begin{array}{c}  y_1 \\\\ \\vdots \\\\  y_n \\end{array}\n\\right]   =  &\n\\left[ \\begin{array}{c}  1 \\\\ \\vdots \\\\ 1 \\end{array}  \\right]   \\beta_0 +\n\\left[ \\begin{array}{c}  x_1 \\\\ \\vdots \\\\  x_n \\end{array}\n\\right] \\beta_1 +\n\\left[ \\begin{array}{c}  \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n  \\end{array}\n\\right]\n=  &\n\\left[ \\begin{array}{cc}  1 &  x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_n\\end{array}  \\right]\n\\left[ \\begin{array}{c}  \\beta_0  \\\\  \\beta_1 \\end{array}\n\\right] +\n\\left[ \\begin{array}{c}  \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n  \\end{array}\n\\right] \\\\\n\\\\\n\\mathbf{Y}= & \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "resources/slides/01-introduction.html#nonlinear-models",
    "href": "resources/slides/01-introduction.html#nonlinear-models",
    "title": "Introduction to STA721",
    "section": "Nonlinear Models",
    "text": "Nonlinear Models\nGravitational Law: \\(F = \\alpha/d^\\beta\\) where \\(d\\) is distance between 2 objects and \\(F\\) is the force of gravity between them\n\nlog transformations \\[\\log(F) = \\log(\\alpha) - \\beta \\log(d)\\]\ncompare to noisy experimental data \\(Y_i =\\log(F_i)\\) observed at \\(x_i = \\log(d_i)\\)\nwrite \\(\\mathbf{X}= [\\mathbf{1}_n \\, \\mathbf{x}]\\)\n\\(\\boldsymbol{\\beta}= (\\log(\\alpha), -\\beta)^T\\)\nmodel with additive error on log scale \\(\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{e}\\)\ntest if \\(\\beta = 2\\)\nerror assumptions?"
  },
  {
    "objectID": "resources/slides/01-introduction.html#intrinsically-nonlinear-models",
    "href": "resources/slides/01-introduction.html#intrinsically-nonlinear-models",
    "title": "Introduction to STA721",
    "section": "Intrinsically Nonlinear Models",
    "text": "Intrinsically Nonlinear Models\nRegression function may be an intrinsically nonlinear function of \\(t_i\\) (time) and parameters \\(\\boldsymbol{\\theta}\\) \\[Y_i = f(t_i, \\boldsymbol{\\theta}) + \\epsilon_i\\]"
  },
  {
    "objectID": "resources/slides/01-introduction.html#quadratic-linear-regression",
    "href": "resources/slides/01-introduction.html#quadratic-linear-regression",
    "title": "Introduction to STA721",
    "section": "Quadratic Linear Regression",
    "text": "Quadratic Linear Regression\nTaylor’s Theorem: \\[f(t_i, \\boldsymbol{\\theta}) = f(t_0, \\boldsymbol{\\theta}) + (t_i - t_0) f'(t_0, \\boldsymbol{\\theta}) + (t_i - t_0)^2\n\\frac{f^{''}(t_0, \\boldsymbol{\\theta})}{2}  + R(t_i, \\boldsymbol{\\theta})\\]\n\n\\[Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i \\text{  for  } i = 1, \\ldots, n\\]\n\n\nRewrite in vectors: \\[\\begin{eqnarray*}\n\\left[\n\\begin{array}{c}  y_1 \\\\ \\vdots \\\\  y_n \\end{array}\n  \\right]   =  &\n\\left[ \\begin{array}{ccc}  1 &  x_1 & x_1^2 \\\\ \\vdots & \\vdots \\\\ 1 &\n     x_n &  x_n^2\\end{array}  \\right]\n\\left[ \\begin{array}{c}  \\beta_0  \\\\  \\beta_1 \\\\ \\beta_2 \\end{array}\n\\right] +\n\\left[ \\begin{array}{c}  \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n  \\end{array}\n\\right] \\\\\n& \\\\\n\\mathbf{Y}= & \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\n\\end{eqnarray*}\\]\n\n\nQuadratic in \\(x\\), but linear in \\(\\beta\\)’s - how do we know this model is adequate?"
  },
  {
    "objectID": "resources/slides/01-introduction.html#kernel-regression-nonparametric",
    "href": "resources/slides/01-introduction.html#kernel-regression-nonparametric",
    "title": "Introduction to STA721",
    "section": "Kernel Regression (NonParametric)",
    "text": "Kernel Regression (NonParametric)\n\\[y_i =  \\beta_0 + \\sum_{j = 1}^J \\beta_j e^{-\\lambda (x_i - k_j)^d} + \\epsilon_i \\text{  for  } i = 1, \\ldots, n\\] where \\(k_j\\) are kernel locations and \\(\\lambda\\) is a smoothing parameter \\[\\begin{eqnarray*}\n\\left[\n\\begin{array}{c}  y_1 \\\\ \\vdots \\\\  y_n \\end{array}\n  \\right]   =  &\n\\left[ \\begin{array}{cccc}  1 &  e^{-\\lambda (x_1 - k_1)^d} &\n     \\ldots &  e^{-\\lambda (x_1 - k_J)^d}  \\\\\n     \\vdots & \\vdots & & \\vdots \\\\ 1 & e^{-\\lambda (x_n - k_1)^d} &  \\ldots & e^{-\\lambda (x_n - k_J)^d} \\end{array}  \\right]\n\\left[ \\begin{array}{c}  \\beta_0  \\\\  \\beta_1 \\\\\\vdots \\\\ \\beta_J \\end{array}\n\\right] +\n\\left[ \\begin{array}{c}  \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n  \\end{array}\n\\right] \\\\\n& \\\\\n\\mathbf{Y}= & \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\n\\end{eqnarray*}\\]\n\nLinear in \\(\\beta\\) given \\(\\lambda\\) and \\(k_1, \\ldots k_J\\)\nLearn \\(\\lambda\\), \\(k_1, \\ldots k_J\\) and \\(J\\)"
  },
  {
    "objectID": "resources/slides/01-introduction.html#hierarchical-models",
    "href": "resources/slides/01-introduction.html#hierarchical-models",
    "title": "Introduction to STA721",
    "section": "Hierarchical Models",
    "text": "Hierarchical Models\n\n\n\n\n\n\n\n\n\n\n\n\n\neach line represent individual sample trajectories\ncorrelation between an individual’s measurements\nsimilarities within groups\ndifferences among groups?\nallow individual regressions for each individual ?\nadd more structure?"
  },
  {
    "objectID": "resources/slides/01-introduction.html#linear-regression-models",
    "href": "resources/slides/01-introduction.html#linear-regression-models",
    "title": "Introduction to STA721",
    "section": "Linear Regression Models",
    "text": "Linear Regression Models\nResponse \\(Y_i\\) and \\(p\\) predictors \\(x_{i1}, x_{i2}, \\dots x_ip\\) \\[Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\ldots \\beta_{p}\n  x_{ip} + \\epsilon_i\\]\n\nDesign matrix \\[\\mathbf{X}=\n\\left[\\begin{array}{cccc}\n1 & x_{11} & \\ldots & x_{1p} \\\\\n1 & x_{21}  & \\ldots & x_{2p} \\\\\n\\vdots & \\vdots  & \\vdots & \\vdots \\\\\n1 & x_{n1} & \\ldots & x_{np} \\\\\n\\end{array} \\right] = \\left[ \\begin{array}{cc}\n1 & \\mathbf{x}_1^T  \\\\\n\\vdots & \\vdots \\\\\n1 & \\mathbf{x}_n^T\n\\end{array} \\right] =\n\\left[\\begin{array}{cccc}\n\\mathbf{1}_n & \\mathbf{X}_1 & \\mathbf{X}_2 \\cdots \\mathbf{X}_p\n\\end{array} \\right]\n\\]\nmatrix version \\[\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\epsilon\\] what should go into \\(\\mathbf{X}\\) and do we need all columns of \\(\\mathbf{X}\\) for inference about \\(\\mathbf{Y}\\)?"
  },
  {
    "objectID": "resources/slides/01-introduction.html#linear-model",
    "href": "resources/slides/01-introduction.html#linear-model",
    "title": "Introduction to STA721",
    "section": "Linear Model",
    "text": "Linear Model\n\n\\(\\mathbf{Y}= \\mathbf{X}\\, \\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\)\n\\(\\mathbf{Y}\\) (\\(n \\times 1\\)) vector of random response (observe \\(\\mathbf{y}\\)); \\(\\mathbf{Y}, \\mathbf{y}\\in \\mathbb{R}^n\\)\n\\(\\mathbf{X}\\) (\\(n \\times p\\)) design matrix (observe)\n\\(\\boldsymbol{\\beta}\\) (\\(p \\times 1\\)) vector of coefficients (unknown)\n\\(\\boldsymbol{\\epsilon}\\) (\\(n \\times 1\\)) vector of “errors” (unobservable)\n\n\nGoals:\n\nWhat goes into \\(\\mathbf{X}\\)? (model building, model selection - post-selection inference?)\nWhat if multiple models are “good”? (model averaging or ensembles) \nWhat about the future? (Prediction)\nUncertainty Quantification - assumptions about \\(\\boldsymbol{\\epsilon}\\)\n\n\n\nAll models are wrong, but some may be useful (George Box)"
  },
  {
    "objectID": "resources/slides/01-introduction.html#ordinary-least-squares",
    "href": "resources/slides/01-introduction.html#ordinary-least-squares",
    "title": "Introduction to STA721",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\nGoal: Find the best fitting “line” or “hyper-plane” that minimizes \\[\\sum_i  (Y_i - \\mathbf{x}_i^T \\boldsymbol{\\beta})^2 = (\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}) = \\| \\mathbf{Y}-\n\\mathbf{X}\\boldsymbol{\\beta}\\|^2 \\]\n\nOptimization problem - seek \\(\\boldsymbol{\\beta}\\ni \\mathbf{X}\\boldsymbol{\\beta}\\) is close to \\(\\mathbf{Y}\\) in squared error\nMay over-fit \\(\\Rightarrow\\) add other criteria that provide a penalty Penalized Least Squares\nRobustness to extreme points \\(\\Rightarrow\\) replace quadratic loss with other functions\n\nno notion of uncertainty of estimates\n\nno structure of problem (repeated measures on individual, randomization restrictions, etc)\n\n\nNeed Distribution Assumptions of \\(\\mathbf{Y}\\) (or \\(\\boldsymbol{\\epsilon}\\)) for testing and uncertainty measures \\(\\Rightarrow\\) Likelihood and Bayesian inference"
  },
  {
    "objectID": "resources/slides/01-introduction.html#random-vectors",
    "href": "resources/slides/01-introduction.html#random-vectors",
    "title": "Introduction to STA721",
    "section": "Random Vectors",
    "text": "Random Vectors\n\nLet \\(Y_1, \\ldots Y_n\\) be random variables in \\(\\mathbb{R}\\) Then \\[\\mathbf{Y}\\equiv\n\\left[ \\begin{array}{c}\nY_1 \\\\\n\\vdots \\\\\nY_n\n\\end{array} \\right]\\] is a random vector in \\(\\mathbb{R}^n\\)\nExpectations of random vectors are defined element-wise: \\[\\textsf{E}[\\mathbf{Y}] \\equiv\n\\textsf{E}\\left[ \\begin{array}{c}\nY_1 \\\\\n\\vdots \\\\\nY_n\n\\end{array} \\right] \\equiv\n\\left[ \\begin{array}{c}\n\\textsf{E}[Y_1] \\\\\n\\vdots \\\\\n\\textsf{E}[Y_n]\n\\end{array} \\right] =\n\\left[ \\begin{array}{c}\n\\mu_1 \\\\\n\\vdots \\\\\n\\mu_n\n\\end{array} \\right]\n\\equiv \\boldsymbol{\\mu}\\in \\mathbb{R}^n\n\\] where mean or expected value \\(\\textsf{E}[Y_i] = \\mu_i\\)"
  },
  {
    "objectID": "resources/slides/01-introduction.html#model-space",
    "href": "resources/slides/01-introduction.html#model-space",
    "title": "Introduction to STA721",
    "section": "Model Space",
    "text": "Model Space\nWe will work with inner product spaces: a vector spaces, say \\(\\mathbb{R}^n\\) equipped with an inner product \\(\\langle \\mathbf{x},\\mathbf{y}\\rangle \\equiv \\mathbf{x}^T\\mathbf{y}, \\quad \\mathbf{x}, \\mathbf{y}\\in \\mathbb{R}^n\\)\n\n\nDefinition: SubspaceA set \\(\\boldsymbol{{\\cal M}}\\) is a subspace of \\(\\mathbb{R}^n\\) if is a subset of \\(\\mathbb{R}^n\\) and also a vector space.\nThat is, if \\(\\mathbf{x}_1 \\in \\boldsymbol{{\\cal M}}\\) and \\(\\mathbf{x}_2 \\in \\boldsymbol{{\\cal M}}\\), then \\(b_1\\mathbf{x}_1 + b_2 \\mathbf{x}_2 \\in \\boldsymbol{{\\cal M}}\\) for all \\(b_1, b_2 \\in \\mathbb{R}\\)\n\n\n\n\n\nDefinition: Column SpaceThe column space of \\(\\mathbf{X}\\) is \\(C(\\mathbf{X}) = \\mathbf{X}\\boldsymbol{\\beta}\\) for \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\)\n\n\n\n\nIf \\(\\mathbf{X}\\) is full column rank, then the columns of \\(\\mathbf{X}\\) form a basis for \\(C(\\mathbf{X})\\) and \\(C(\\mathbf{X})\\) is a p-dimensional subspace of \\(\\mathbb{R}^n\\)\n\n\nIf we have just a single model matrix \\(\\mathbf{X}\\), then the subspace \\(\\boldsymbol{{\\cal M}}\\) is the model space."
  },
  {
    "objectID": "resources/slides/01-introduction.html#philosophy",
    "href": "resources/slides/01-introduction.html#philosophy",
    "title": "Introduction to STA721",
    "section": "Philosophy",
    "text": "Philosophy\n\nfor many problems frequentist and Bayesian methods will give similar answers (more a matter of taste in interpretation)\n\nFor small problems, Bayesian methods allow us to incorporate prior information which provides better calibrated answers\n\nfor problems with complex designs and/or missing data Bayesian methods are often easier to implement (do not need to rely on asymptotics) \n\nFor problems involving hypothesis testing or model selection frequentist and Bayesian methods can be strikingly different.\n\nFrequentist methods often faster (particularly with “big data”) so great for exploratory analysis and for building a “data-sense”\n\nBayesian methods sit on top of Frequentist Likelihood\n\nGoemetric perspective important in both!\n\n\nImportant to understand advantages and problems of each perspective!\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#outline",
    "href": "resources/slides/05-BLUE-MVUE.html#outline",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Outline",
    "text": "Outline\n\nGauss-Markov Theorem for non-full rank \\(\\mathbf{X}\\) (recap)\nBest Linear Unbiased Estimators for Prediction\nMVUE\nDiscussion of recent papers on Best Unbiased Estimators beyond linearity\n\n\nReadings:\n\nChristensen Chapter 2 (Appendix B as needed)\nSeber & Lee Chapter 3\nFor the curious:\n\nAndersen (1962) Least squares and best unbiased estimates\nHansen (2022) A modern gauss-markov theorem\nWhat Estimators are Unbiased for Linear Models (2023) and references within"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#identifiability",
    "href": "resources/slides/05-BLUE-MVUE.html#identifiability",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Identifiability",
    "text": "Identifiability\n\nDefinition: Identifiable\\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) are identifiable if the distribution of \\(\\mathbf{Y}\\), \\(f_\\mathbf{Y}(\\mathbf{y};\n\\boldsymbol{\\beta}_1, \\sigma^2_1) = f_\\mathbf{Y}(\\mathbf{y};\n\\boldsymbol{\\beta}_2, \\sigma^2_2)\\) implies that \\((\\boldsymbol{\\beta}_1, \\sigma^2_1)^T =  (\\boldsymbol{\\beta}_2, \\sigma^2_2)^T\\)\n\n\n\nFor linear models, equivalent definition is that \\(\\boldsymbol{\\beta}\\) is identifiable if for any \\(\\boldsymbol{\\beta}_1\\) and \\(\\boldsymbol{\\beta}_2\\), \\(\\mu(\\boldsymbol{\\beta}_1)  = \\mu(\\boldsymbol{\\beta}_2)\\) or \\(\\mathbf{X}\\boldsymbol{\\beta}_1 =\\mathbf{X}\\boldsymbol{\\beta}_2\\) implies that \\(\\boldsymbol{\\beta}_1 = \\boldsymbol{\\beta}_2\\).\nIf \\(r(\\mathbf{X}) = p\\) then \\(\\boldsymbol{\\beta}\\) is identifiable\nIf \\(\\mathbf{X}\\) is not full rank, there exists \\(\\boldsymbol{\\beta}_1 \\neq \\boldsymbol{\\beta}_2\\), but \\(\\mathbf{X}\\boldsymbol{\\beta}_1 =\n\\mathbf{X}\\boldsymbol{\\beta}_2\\) and hence \\(\\boldsymbol{\\beta}\\) is not identifiable!\nidentifiable linear functions of \\(\\boldsymbol{\\beta}\\), \\(\\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\) that have an unbiased estimator are historically referred to as estimable in linear models."
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#blue-of-boldsymbollambdat-boldsymbolbeta",
    "href": "resources/slides/05-BLUE-MVUE.html#blue-of-boldsymbollambdat-boldsymbolbeta",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "BLUE of \\(\\boldsymbol{\\Lambda}^T \\boldsymbol{\\beta}\\)",
    "text": "BLUE of \\(\\boldsymbol{\\Lambda}^T \\boldsymbol{\\beta}\\)\nIf \\(\\boldsymbol{\\Lambda}^T= \\mathbf{B}\\mathbf{X}\\) for some matrix \\(\\mathbf{B}\\) (or \\(\\boldsymbol{\\Lambda}= \\mathbf{X}^T\\mathbf{B}\\) then\n\n\\(\\textsf{E}[\\mathbf{B}\\mathbf{P}\\mathbf{Y}] = \\textsf{E}[\\mathbf{B}\\mathbf{X}\\hat{\\boldsymbol{\\beta}}] = \\textsf{E}[\\boldsymbol{\\Lambda}^T \\hat{\\boldsymbol{\\beta}}] = \\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\)\nidentifiable as it is a function of \\(\\boldsymbol{\\mu}\\), linear and unbiased\nThe unique OLS estimate of \\(\\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\) is \\(\\boldsymbol{\\Lambda}^T\\hat{\\boldsymbol{\\beta}}\\)\n\\(\\mathbf{B}\\mathbf{P}\\mathbf{Y}= \\boldsymbol{\\Lambda}^T\\hat{\\boldsymbol{\\beta}}\\) is the BLUE of \\(\\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\) \\[\\begin{align*}\n& \\textsf{E}[\\|\\mathbf{B}\\mathbf{P}\\mathbf{Y}- \\mathbf{B}\\boldsymbol{\\mu}\\|^2]  \\le \\textsf{E}[\\|\\mathbf{A}\\mathbf{Y}- \\mathbf{B}\\boldsymbol{\\mu}\\|^2] \\\\\n\\Leftrightarrow & \\\\\n& \\textsf{E}[\\|\\boldsymbol{\\Lambda}^T\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta})\\|^2]  \\le \\textsf{E}[\\|\\mathbf{L}^T\\tilde{\\boldsymbol{\\beta}}- \\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\|^2]\n\\end{align*}\\] for LUE \\(\\mathbf{A}\\mathbf{Y}= \\mathbf{L}^T\\tilde{\\boldsymbol{\\beta}}\\) of \\(\\boldsymbol{\\Lambda}^T\\boldsymbol{\\beta}\\)"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#non-identifiable-example",
    "href": "resources/slides/05-BLUE-MVUE.html#non-identifiable-example",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Non-Identifiable Example",
    "text": "Non-Identifiable Example\nOne-way ANOVA model \\[\\mu_{ij} = \\mu + \\tau_j \\qquad \\boldsymbol{\\mu}= (\n    \\mu_{11}, \\ldots,\\mu_{n_1 1},\\mu_{12},\\ldots, \\mu_{n_2,2},\\ldots, \\mu_{1J},\n\\ldots,\n\\mu_{n_J J})^T \\]\n\nLet \\(\\boldsymbol{\\beta}_{1} = (\\mu, \\tau_1, \\ldots, \\tau_J)^T\\)\nLet \\(\\boldsymbol{\\beta}_{2} = (\\mu - 42, \\tau_1 + 42, \\ldots, \\tau_J + 42)^T\\)\nThen \\(\\boldsymbol{\\mu}_{1} = \\boldsymbol{\\mu}_{2}\\) even though \\(\\boldsymbol{\\beta}_1 \\neq \\boldsymbol{\\beta}_2\\)\n\\(\\boldsymbol{\\beta}\\) is not identifiable\nyet \\(\\boldsymbol{\\mu}\\) is identifiable, where \\(\\boldsymbol{\\mu}= \\mathbf{X}\\boldsymbol{\\beta}\\) (a linear combination of \\(\\boldsymbol{\\beta}\\))"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#lues-of-individual-beta_j",
    "href": "resources/slides/05-BLUE-MVUE.html#lues-of-individual-beta_j",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "LUEs of Individual \\(\\beta_j\\)",
    "text": "LUEs of Individual \\(\\beta_j\\)\n\nProposition: Christensen 2.1.6For \\(\\boldsymbol{\\mu}= \\mathbf{X}\\boldsymbol{\\beta}= \\sum_j \\mathbf{X}_j \\beta_j\\) \\(\\beta_j\\) is not identifiable if and only if there exists \\(\\alpha_j\\) such that \\(\\mathbf{X}_j = \\sum_{i \\neq j} \\mathbf{X}_i \\alpha_i\\)\n\n\n\nOne-way Anova Model: \\(Y_{ij} = \\mu + \\tau_j + \\epsilon_{ij}\\) \\[\\boldsymbol{\\mu}=  \\left[\n    \\begin{array}{lllll}\n\\mathbf{1}_{n_1} & \\mathbf{1}_{n_1} & \\mathbf{0}_{n_1} &  \\ldots & \\mathbf{0}_{n_1} \\\\\n\\mathbf{1}_{n_2} & \\mathbf{0}_{n_2} & \\mathbf{1}_{n_2} &  \\ldots & \\mathbf{0}_{n_2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbf{1}_{n_J} & \\mathbf{0}_{n_J} & \\mathbf{0}_{n_J} &  \\ldots & \\mathbf{1}_{n_J} \\\\\n    \\end{array} \\right]\n\\left(   \\begin{array}{l}\n      \\mu \\\\\n      \\tau_1 \\\\\n   \\tau_2 \\\\\n\\vdots \\\\\n\\tau_J\n    \\end{array} \\right)\n\\]\n\nAre any parameters \\(\\mu\\) or \\(\\tau_j\\) identifiable?"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#examples-of-boldsymbollambda-of-interest",
    "href": "resources/slides/05-BLUE-MVUE.html#examples-of-boldsymbollambda-of-interest",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Examples of \\(\\boldsymbol{\\lambda}\\) of Interest:",
    "text": "Examples of \\(\\boldsymbol{\\lambda}\\) of Interest:\n\nA \\(j\\)th element of \\(\\boldsymbol{\\beta}\\): \\(\\boldsymbol{\\lambda}= (0, 0, \\ldots,1, 0, \\ldots, 0)^T\\), \\[\\boldsymbol{\\lambda}^T\\boldsymbol{\\beta}= \\beta_j\\]\nDifference between two treatements: \\(\\tau_1 - \\tau_2\\): \\(\\boldsymbol{\\lambda}= (0, 1, -1, \\ldots, 0, \\ldots, 0)^T\\), \\[\\boldsymbol{\\lambda}^T\\boldsymbol{\\beta}= \\tau_1 - \\tau_2\\]\nEstimation at observed \\(\\mathbf{x}_i\\): \\(\\boldsymbol{\\lambda}= \\mathbf{x}_i\\) \\[\\mu_i = \\mathbf{x}_i^T \\boldsymbol{\\beta}\\]\nEstimation or prediction at a new point \\(\\mathbf{x}_*\\): \\(\\boldsymbol{\\lambda}= \\mathbf{x}_*\\), \\[\\mu_* = \\mathbf{x}_*^T \\boldsymbol{\\beta}\\]"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#another-non-full-rank-example",
    "href": "resources/slides/05-BLUE-MVUE.html#another-non-full-rank-example",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Another Non-Full Rank Example",
    "text": "Another Non-Full Rank Example\n\nx1 = -4:4\nx2 = c(-2, 1, -1, 2, 0, 2, -1, 1, -2)\nx3 = 3*x1  -2*x2\nx4 = x2 - x1 + 4\nY = 1+x1+x2+x3+x4 + c(-.5,.5,.5,-.5,0,.5,-.5,-.5,.5)\ndev.set = data.frame(Y, x1, x2, x3, x4)\n\n# Order 1\nlm1234 = lm(Y ~ x1 + x2 + x3 + x4, data=dev.set)\nround(coefficients(lm1234), 4)\n\n(Intercept)          x1          x2          x3          x4 \n          5           3           0          NA          NA \n\n# Order 2\nlm3412 = lm(Y ~ x3 + x4 + x1 + x2, data = dev.set)\nround(coefficients(lm3412), 4)\n\n(Intercept)          x3          x4          x1          x2 \n        -19           3           6          NA          NA"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#in-sample-predictions",
    "href": "resources/slides/05-BLUE-MVUE.html#in-sample-predictions",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "In Sample Predictions",
    "text": "In Sample Predictions\n\ncbind(dev.set, predict(lm1234), predict(lm3412))\n\n     Y x1 x2  x3 x4 predict(lm1234) predict(lm3412)\n1 -7.5 -4 -2  -8  6              -7              -7\n2 -3.5 -3  1 -11  8              -4              -4\n3 -0.5 -2 -1  -4  5              -1              -1\n4  1.5 -1  2  -7  7               2               2\n5  5.0  0  0   0  4               5               5\n6  8.5  1  2  -1  5               8               8\n7 10.5  2 -1   8  1              11              11\n8 13.5  3  1   7  2              14              14\n9 17.5  4 -2  16 -2              17              17\n\n\n\nBoth models agree for estimating the mean at the observed \\(\\mathbf{X}\\) points!"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#out-of-sample",
    "href": "resources/slides/05-BLUE-MVUE.html#out-of-sample",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Out of Sample",
    "text": "Out of Sample\n\nout = data.frame(test.set,\n      Y1234=predict(lm1234, new=test.set),\n      Y3412=predict(lm3412, new=test.set))\nout\n\n  x1 x2 x3 x4 Y1234 Y3412\n1  3  1  7  2    14    14\n2  6  2 14  4    23    47\n3  6  2 14  0    23    23\n4  0  0  0  4     5     5\n5  0  0  0  0     5   -19\n6  1  2  3  4     8    14\n\n\n\nAgreement for cases 1, 3, and 4 only!\nCan we determine that without finding the predictions and comparing?\nConditions for general \\(\\boldsymbol{\\Lambda}\\) or \\(\\boldsymbol{\\lambda}\\) without findingn \\(\\mathbf{B}\\) (\\(\\boldsymbol{\\beta}^T\\))?"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#conditions-for-lue-of-boldsymbollambda",
    "href": "resources/slides/05-BLUE-MVUE.html#conditions-for-lue-of-boldsymbollambda",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Conditions for LUE of \\(\\boldsymbol{\\lambda}\\)",
    "text": "Conditions for LUE of \\(\\boldsymbol{\\lambda}\\)\n\nGM requires that \\(\\boldsymbol{\\lambda}^T = \\mathbf{b}^T\\mathbf{X}\\Leftrightarrow \\boldsymbol{\\lambda}= \\mathbf{X}^T \\mathbf{b}\\) therefore \\(\\boldsymbol{\\lambda}\\in C(\\mathbf{X}^T)\\)\nSuppose we have an arbitrary \\(\\boldsymbol{\\lambda}= \\boldsymbol{\\lambda}_* + \\mathbf{u}\\), where \\(\\boldsymbol{\\lambda}_*  \\in C(\\mathbf{X}^T)\\) and \\(\\mathbf{u}\\in C(\\mathbf{X}^T)^\\perp\\) (orthogonal complement)\nLet \\(\\mathbf{P}_{\\mathbf{X}^T}\\) denote an orthogonal projection onto \\(C(\\mathbf{X}^T)\\) then \\(\\mathbf{I}- \\mathbf{P}_{\\mathbf{X}^T}\\) is an orthogonal projection onto \\(C(\\mathbf{X}^T)^\\perp\\)\n\\((\\mathbf{I}- \\mathbf{P}_{\\mathbf{X}^T})\\boldsymbol{\\lambda}= (\\mathbf{I}- \\mathbf{P}_{\\mathbf{X}^T})\\boldsymbol{\\lambda}_* + (\\mathbf{I}- \\mathbf{P}_{\\mathbf{X}^T})\\mathbf{u}= \\mathbf{0}_p + \\mathbf{u}\\)\nso if \\(\\boldsymbol{\\lambda}\\in C(\\mathbf{X}^T)\\) we will have \\((\\mathbf{I}- \\mathbf{P}_{\\mathbf{X}^T})\\boldsymbol{\\lambda}= \\mathbf{0}_p\\)! (or \\(\\mathbf{P}_{\\mathbf{X}^T} \\boldsymbol{\\lambda}= \\boldsymbol{\\lambda}\\))\nNote this is really just a generalization of Proposition 2.1.6 in Christensen that \\(\\beta_j\\) is not identifiable iff there exist scalars such that \\(\\mathbf{X}_j = \\sum_{i \\neq j} \\mathbf{X}_i \\alpha_i\\)"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#prediction-example-again",
    "href": "resources/slides/05-BLUE-MVUE.html#prediction-example-again",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Prediction Example Again",
    "text": "Prediction Example Again\nFor prediction at a new \\(\\mathbf{x}_*\\), this is implemented in the R package estimability\n\nrequire(\"estimability\" )\ncbind(epredict(lm1234, test.set), epredict(lm3412, test.set))\n\n  [,1] [,2]\n1   14   14\n2   NA   NA\n3   23   23\n4    5    5\n5   NA   NA\n6   NA   NA\n\n\nRows 2, 5, and 6 do not have a unique best linear unbiased estimator, \\(\\mathbf{x}_*^T \\boldsymbol{\\beta}\\)"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#mvue-minimum-variance-unbiased-estimators",
    "href": "resources/slides/05-BLUE-MVUE.html#mvue-minimum-variance-unbiased-estimators",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "MVUE: Minimum Variance Unbiased Estimators",
    "text": "MVUE: Minimum Variance Unbiased Estimators\n\nGauss-Markov Theorem says that OLS has minimum variance in the class of all Linear Unbiased estimators for \\(\\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\) and \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\sigma^2 \\mathbf{I}_n\\)\nRequires just first and second moments\nAdditional assumption of normality and full rank, OLS of \\(\\boldsymbol{\\beta}\\) is the same as MLEs and have minimum variance out of ALL unbiased estimators (MVUE); not just linear estimators (section 2.5 in Christensen)\nrequires Complete Sufficient Statististics and Rao-Blackwell Theorem - next semester in STA732)\nso Best Unbiased Estimators (BUE) not just BLUE!"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#what-about",
    "href": "resources/slides/05-BLUE-MVUE.html#what-about",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "What about ?",
    "text": "What about ?\n\nare there nonlinear estimators that are better than OLS under the assumptions ?\nAnderson (1962) showed OLS is not generally the MVUE with \\(\\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\) and \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\sigma^2 \\mathbf{I}_n\\)\npointed out that linear-plus-quadratic (LPQ) estimators can outperform the OLS estimator for certain error distributions.\nOther assumptions on \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\boldsymbol{\\Sigma}\\)?\n\nGeneralized Least Squares are BLUE (not necessarily equivalent to OLS)\n\nmore recently Hansen (2022) concludes that OLS is BUE over the broader class of linear models with \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}]\\) finite and \\(\\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\)\nlively ongoing debate! - see What Estimators are Unbiased for Linear Models (2023) and references within"
  },
  {
    "objectID": "resources/slides/05-BLUE-MVUE.html#next-up",
    "href": "resources/slides/05-BLUE-MVUE.html#next-up",
    "title": "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs",
    "section": "Next Up",
    "text": "Next Up\n\nGLS under assumptions \\(\\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\) and \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\boldsymbol{\\Sigma}\\)\nOblique projections and orthogonality with other inner products on \\(\\mathbb{R}^n\\)\nMLEs in Multivariate Normal setting\nGauss-Markov\n\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#outline",
    "href": "resources/slides/03-non-full-rank.html#outline",
    "title": "Rank Deficient Models",
    "section": "Outline",
    "text": "Outline\n\nRank Deficient Models\nGeneralized Inverses, Projections and MLEs/OLS\nClass of Unbiased Estimators\n\n\nReadings: - Christensen Chapter 2 and Appendix B - Seber & Lee Chapter 3"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#geometric-view",
    "href": "resources/slides/03-non-full-rank.html#geometric-view",
    "title": "Rank Deficient Models",
    "section": "Geometric View",
    "text": "Geometric View"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#non-full-rank-case",
    "href": "resources/slides/03-non-full-rank.html#non-full-rank-case",
    "title": "Rank Deficient Models",
    "section": "Non-Full Rank Case",
    "text": "Non-Full Rank Case\n\nModel: \\(\\mathbf{Y}= \\boldsymbol{\\mu}+ \\boldsymbol{\\epsilon}\\)\nAssumption: \\(\\boldsymbol{\\mu}\\in C(\\mathbf{X})\\) for \\(\\mathbf{X}\\in \\mathbb{R}^{n \\times p}\\)\nWhat if the rank of \\(\\mathbf{X}\\), \\(r(\\mathbf{X}) \\equiv r \\ne p\\)?\nStill have result that the OLS/MLE solution satisfies \\[\\mathbf{P}_\\mathbf{X}\\mathbf{Y}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\]\nHow can we characterize \\(\\mathbf{P}\\) and \\(\\hat{\\boldsymbol{\\beta}}\\) in this case? 2 cases\n\n\n\n\\(p \\le n\\), \\(r(\\mathbf{X}) \\ne p\\) \\(\\Rightarrow r(\\mathbf{X}) &lt; p\\)\n\\(p \\gt n\\), \\(r(\\mathbf{X}) \\ne p\\)\n\n\n\nFocus on the first case for OLS/MLE for now…"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#model-space",
    "href": "resources/slides/03-non-full-rank.html#model-space",
    "title": "Rank Deficient Models",
    "section": "Model Space",
    "text": "Model Space\n\n\\(\\boldsymbol{{\\cal M}}= C(\\mathbf{X})\\) is an \\(r\\)-dimensional subspace of \\(\\mathbb{R}^n\\)\n\\(\\boldsymbol{{\\cal M}}\\) has an \\((n - r)\\)-dimensional orthogonal complement \\(\\boldsymbol{{\\cal N}}\\)\neach \\(\\mathbf{y}\\in \\mathbb{R}^n\\) has a unique representation as \\[ \\mathbf{y}= \\hat{\\mathbf{y}}+ \\mathbf{e}\\] for \\(\\hat{\\mathbf{y}}\\in \\boldsymbol{{\\cal M}}\\) and \\(\\mathbf{e}\\in \\boldsymbol{{\\cal N}}\\)\n\\(\\hat{\\mathbf{y}}\\) is the orthogonal projection of \\(\\mathbf{y}\\) onto \\(\\boldsymbol{{\\cal M}}\\) and is the OLS/MLE estimate of \\(\\boldsymbol{\\mu}\\) that satisfies \\[\\mathbf{P}_\\mathbf{X}\\mathbf{y}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}= \\hat{\\mathbf{y}}\\]\n\\(\\mathbf{X}^T\\mathbf{X}\\) is not invertible so need another way to represent \\(\\mathbf{P}_\\mathbf{X}\\) and \\(\\hat{\\boldsymbol{\\beta}}\\)"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#spectral-decomposition-sd",
    "href": "resources/slides/03-non-full-rank.html#spectral-decomposition-sd",
    "title": "Rank Deficient Models",
    "section": "Spectral Decomposition (SD)",
    "text": "Spectral Decomposition (SD)\nEvery symmetric \\(n \\times n\\) matrix, \\({\\mathbf{S}}\\), has an eigen decomposition \\({\\mathbf{S}}= \\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{U}^T\\)\n\n\\(\\boldsymbol{\\Lambda}\\) is a diagonal matrix with eigenvalues \\((\\lambda_1, \\ldots, \\lambda_n)\\) of \\({\\mathbf{S}}\\)\n\\(\\mathbf{U}\\) is a \\(n \\times n\\) orthogonal matrix \\(\\mathbf{U}^T\\mathbf{U}= \\mathbf{U}\\mathbf{U}^T = \\mathbf{I}_n\\) ( \\(\\mathbf{U}^{-1} = \\mathbf{U}^T\\))\nthe columns of \\(\\mathbf{U}\\) from an Orthonormal Basis (ONB) for \\(\\mathbb{R}^n\\)\nthe columns of \\(\\mathbf{U}\\) associated with non-zero eigenvalues form an ONB for \\(C({\\mathbf{S}})\\)\nthe number of non-zero eigenvalues is the rank of \\({\\mathbf{S}}\\)\nthe columns of \\(\\mathbf{U}\\) associated with zero eigenvalues form an ONB for \\(C({\\mathbf{S}})^\\perp\\)\n\\({\\mathbf{S}}^d = \\mathbf{U}\\boldsymbol{\\Lambda}^d \\mathbf{U}^T\\) (matrix powers)"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#positive-definite-and-non-negative-definite-matrices",
    "href": "resources/slides/03-non-full-rank.html#positive-definite-and-non-negative-definite-matrices",
    "title": "Rank Deficient Models",
    "section": "Positive Definite and Non-Negative Definite Matrices",
    "text": "Positive Definite and Non-Negative Definite Matrices\n\nDefinition: B.21 Positive Definite and Non-Negative DefiniteA symmetric matrix \\({\\mathbf{S}}\\) is positive definite (\\({\\mathbf{S}}\\gt 0\\)) if and only if \\(\\mathbf{x}^T{\\mathbf{S}}\\mathbf{x}&gt; 0\\) for \\(\\mathbf{x}\\in \\mathbb{R}^n\\), \\(\\mathbf{x}\\ne \\mathbf{0}_n\\), and positive semi-definite or non-negative definite (\\({\\mathbf{S}}\\ge 0\\)) if and only if \\(\\mathbf{x}^T{\\mathbf{S}}\\mathbf{x}\\ge 0\\) for \\(\\mathbf{x}\\in \\mathbb{R}^n\\), \\(\\mathbf{x}\\ne \\mathbf{0}_n\\)\n\n\n\n\n\n\n\n\n\nExercise\n\n\nShow that a symmetric matrix \\({\\mathbf{S}}\\) is positive definite if and only if its eigenvalues are all strictly greater than zero, and positive semi-definite if all the eigenvalues are non-negative."
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#projections",
    "href": "resources/slides/03-non-full-rank.html#projections",
    "title": "Rank Deficient Models",
    "section": "Projections",
    "text": "Projections\nLet \\(\\mathbf{P}\\) be an orthogonal projection matrix onto \\(\\boldsymbol{{\\cal M}}\\), then\n\nthe eigenvalues of \\(\\mathbf{P}\\), \\(\\lambda_i\\), are either zero or one\nthe trace of \\(\\mathbf{P}\\) is the rank of \\(\\mathbf{P}\\)\nthe dimension of the subspace that \\(\\mathbf{P}\\) projects onto is the rank of \\(\\mathbf{P}\\)\nthe columns of \\(\\mathbf{U}_r = [u_1, u_2, \\ldots u_r]\\) form an ONB for the \\(C(\\mathbf{P})\\)\nthe projection \\(\\mathbf{P}\\) has the representation \\(\\mathbf{P}= \\mathbf{U}_r \\mathbf{U}_r^T = \\sum_{i = 1}^r u_i u_i^T\\) (the sum of \\(r\\) rank \\(1\\) projections)\nthe projection \\(\\mathbf{I}_n - \\mathbf{P}= \\mathbf{I}- \\mathbf{U}_r \\mathbf{U}_r^T = \\mathbf{U}_\\perp \\mathbf{U}_\\perp^T\\) where \\(\\mathbf{U}_\\perp = [u_{r+1}, \\ldots u_n]\\) is an orthogonal projection onto \\(\\boldsymbol{{\\cal N}}\\)\n\n\nMLE/OLS:\n\n\\(\\mathbf{P}_X \\mathbf{y}= \\mathbf{U}_r \\mathbf{U}_r^T \\mathbf{y}= \\mathbf{U}_r \\tilde{\\boldsymbol{\\beta}}\\)\nClaim \\(\\tilde{\\boldsymbol{\\beta}}\\) is a MLE/OLS estimate of \\(\\boldsymbol{\\beta}\\) where \\(\\tilde{\\mathbf{X}}= \\mathbf{U}_r\\)."
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#singular-value-decomposition-connections-to-spectral-decompositions",
    "href": "resources/slides/03-non-full-rank.html#singular-value-decomposition-connections-to-spectral-decompositions",
    "title": "Rank Deficient Models",
    "section": "Singular Value Decomposition & Connections to Spectral Decompositions",
    "text": "Singular Value Decomposition & Connections to Spectral Decompositions\nA matrix \\(\\mathbf{X}\\in \\mathbb{R}^{n \\times p}\\), \\(p \\le n\\) has a singular value decomposition \\[\\mathbf{X}= \\mathbf{U}_p \\mathbf{D}\\mathbf{V}^T\\]\n\n\\(\\mathbf{U}_p\\) is a \\(n \\times p\\) matrix with the first \\(p\\) eigenvectors in \\(\\mathbf{U}\\) associated with the \\(p\\) largest eigenvectors of \\(\\mathbf{X}\\mathbf{X}^T = \\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{U}^T\\) with \\(\\mathbf{U}_p^T\\mathbf{U}_p = I_p\\) but \\(\\mathbf{U}_p \\mathbf{U}_p^T \\ne \\mathbf{I}_n\\) (or \\(\\mathbf{P}_p\\))\n\\(\\mathbf{V}\\) is a \\(p \\times p\\) orthogonal matrix associated with the \\(p\\) eigenvectors of \\(\\mathbf{X}^T\\mathbf{X}= \\mathbf{V}\\boldsymbol{\\Lambda}_p \\mathbf{V}^T\\) where \\(\\boldsymbol{\\Lambda}_p\\) is the diagonal matrix of eigenvalues associated with the \\(p\\) largest eigenvalues of \\(\\boldsymbol{\\Lambda}\\)\n\\(\\mathbf{D}\\) = \\(\\boldsymbol{\\Lambda}_p^{1/2}\\) are the singular values\nif \\(\\mathbf{X}\\) has rank \\(r &lt; p\\), then \\(C(\\mathbf{X}) = C(\\mathbf{U}_p) = C(\\mathbf{U}_r)\\), where \\(\\mathbf{U}_r\\) are the eigenvectors of \\(\\mathbf{U}\\) or \\(\\mathbf{U}_p\\) associated with the non-zero eigenvalues.\n\\(\\mathbf{U}_r\\) is an ONB for \\(C(X)\\)"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#mleols-for-non-full-rank-case",
    "href": "resources/slides/03-non-full-rank.html#mleols-for-non-full-rank-case",
    "title": "Rank Deficient Models",
    "section": "MLE/OLS for non-full rank case",
    "text": "MLE/OLS for non-full rank case\n\nif \\(\\mathbf{X}^T\\mathbf{X}\\) is invertible, \\(\\mathbf{P}_X = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\) and \\(\\hat{\\boldsymbol{\\beta}}\\) is the unique estimator that satisfies \\(\\mathbf{P}_\\mathbf{X}\\mathbf{y}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\) or \\(\\hat{\\boldsymbol{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{y}\\)\nif \\(\\mathbf{X}^T\\mathbf{X}\\) is not invertible, replace \\(\\mathbf{X}\\) by \\(\\tilde{\\mathbf{X}}\\) that is rank \\(r\\)\nor represent \\(\\mathbf{P}_\\mathbf{X}= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-} \\mathbf{X}^T\\) where \\((\\mathbf{X}^T\\mathbf{X})^{-}\\) is a generalized inverse of \\(\\mathbf{X}^T\\mathbf{X}\\) and \\(\\hat{\\boldsymbol{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-}\\mathbf{X}^T \\mathbf{y}\\)"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#generalized-inverses",
    "href": "resources/slides/03-non-full-rank.html#generalized-inverses",
    "title": "Rank Deficient Models",
    "section": "Generalized Inverses",
    "text": "Generalized Inverses\n\nDefinition: Generalized-Inverse (B.36)A generalized inverse of any matrix \\(\\mathbf{A}\\): \\(\\mathbf{A}^{-}\\) satisfies \\(\\mathbf{A}\\mathbf{A}^- \\mathbf{A}= \\mathbf{A}\\)\n\n\n\nA generalized inverse of \\(\\mathbf{A}\\) symmetric always exists!\n\n\n\nTheorem: Christensen B.39If \\(\\mathbf{G}_1\\) and \\(\\mathbf{G}_2\\) are generalized inverses of \\(\\mathbf{A}\\) then \\(\\mathbf{G}_1 \\mathbf{A}\\mathbf{G}_2\\) is also a generalized inverse of \\(\\mathbf{A}\\)\n\n\n\nif \\(\\mathbf{A}\\) is symmetric, then \\(\\mathbf{A}^-\\) need not be!"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#orthogonal-projections-in-general",
    "href": "resources/slides/03-non-full-rank.html#orthogonal-projections-in-general",
    "title": "Rank Deficient Models",
    "section": "Orthogonal Projections in General",
    "text": "Orthogonal Projections in General\n\n\n\n\n\n\n\nLemma B.43\n\n\nIf \\(\\mathbf{G}\\) and \\(\\mathbf{H}\\) are generalized inverses of \\(\\mathbf{X}^T\\mathbf{X}\\) then \\[\\begin{align*}\n\\mathbf{X}\\mathbf{G}\\mathbf{X}^T \\mathbf{X}& = \\mathbf{X}\\mathbf{H}\\mathbf{X}^T \\mathbf{X}= \\mathbf{X}\\\\\n\\mathbf{X}\\mathbf{G}\\mathbf{X}^T & = \\mathbf{X}\\mathbf{H}\\mathbf{X}^T\n\\end{align*}\\]\n\n\n\n\n\n\nTheorem: B.44\\(\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^-\\mathbf{X}^T\\) is an orthogonal projection onto \\(C(\\mathbf{X})\\).\n\n\n\n\n\nWe need to show that (i) \\(\\mathbf{P}\\mathbf{m}= \\mathbf{m}\\) for \\(\\mathbf{m}\\in C(\\mathbf{X})\\) and (ii) \\(\\mathbf{P}\\mathbf{n}= 0\\) for \\(\\mathbf{n}\\in C(\\mathbf{X})^\\perp\\).\n\nFor \\(\\mathbf{m}\\in C(\\mathbf{X})\\), write \\(\\mathbf{m}= \\mathbf{X}\\mathbf{b}\\). Then \\(\\mathbf{P}\\mathbf{m}= \\mathbf{P}\\mathbf{X}\\mathbf{b}= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^-\\mathbf{X}^T \\mathbf{X}\\mathbf{b}\\) and by Lemma B43, we have that \\(\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^-\\mathbf{X}^T \\mathbf{X}\\mathbf{b}= \\mathbf{X}\\mathbf{b}= \\mathbf{m}\\)\nFor \\(\\mathbf{n}\\perp C(\\mathbf{X})\\), \\(\\mathbf{P}\\mathbf{n}= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^-\\mathbf{X}^T \\mathbf{n}= \\mathbf{0}_n\\) as \\(C(\\mathbf{X})^\\perp = N(\\mathbf{X}^T)\\)."
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#mles-ols",
    "href": "resources/slides/03-non-full-rank.html#mles-ols",
    "title": "Rank Deficient Models",
    "section": "MLEs & OLS",
    "text": "MLEs & OLS\nMLE/OLS satisfies\n\n\\(\\mathbf{P}\\mathbf{y}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\)\n\\(\\mathbf{P}\\mathbf{y}= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^-\\mathbf{X}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\) (does not depend on choice of generalized inverse)\n\\(\\hat{\\boldsymbol{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^-\\mathbf{X}^T \\mathbf{y}\\)\n\\(\\hat{\\boldsymbol{\\beta}}\\) is not unique - does depend on choice of generalized inverse unless \\(\\mathbf{X}\\) is full rank"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#moore-penrose-generalized-inverse",
    "href": "resources/slides/03-non-full-rank.html#moore-penrose-generalized-inverse",
    "title": "Rank Deficient Models",
    "section": "Moore-Penrose Generalized Inverse:",
    "text": "Moore-Penrose Generalized Inverse:\n\nDecompose symmetric \\(\\mathbf{A}= \\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{U}^T\\) (i.e \\(\\mathbf{X}^T\\mathbf{X}\\))\n\\(\\mathbf{A}^-_{MP} = \\mathbf{U}\\boldsymbol{\\Lambda}^- \\mathbf{U}^T\\)\n\n\\(\\boldsymbol{\\Lambda}^-\\) is diagonal with \\[ \\lambda_i^- = \\left\\{\n\\begin{array}{l}\n1/\\lambda_i \\text{ if } \\lambda_i \\neq 0 \\\\\n0 \\quad \\, \\text{  if } \\lambda_i = 0\n\\end{array}\n\\right.\\]\n\nSymmetric \\(\\mathbf{A}^-_{MP} = (\\mathbf{A}^-_{MP})^T\\)\n\nReflexive \\(\\mathbf{A}^-_{MP}\\mathbf{A}\\mathbf{A}^-_{MP} = \\mathbf{A}^-_{MP}\\)\n\n\n\nShow that \\(\\mathbf{A}_{MP}^-\\) is a generalized inverse of \\(\\mathbf{A}\\)\nCan you construct another generalized inverse of \\(\\mathbf{X}^T\\mathbf{X}\\) ?\nCan you find the Moore-Penrose generalized inverse of \\(\\mathbf{X}\\in \\mathbb{R}^{n \\times p}\\)?"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#properties-of-ols-full-rank-case",
    "href": "resources/slides/03-non-full-rank.html#properties-of-ols-full-rank-case",
    "title": "Rank Deficient Models",
    "section": "Properties of OLS (full rank case)",
    "text": "Properties of OLS (full rank case)\nHow good is \\(\\hat{\\boldsymbol{\\beta}}\\) as an estimator of \\(\\beta\\)\n\n\\(\\hat{\\boldsymbol{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{Y}=  (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{X}\\boldsymbol{\\beta}+ (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}= \\boldsymbol{\\beta}+  (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}\\)\ndon’t know \\(\\boldsymbol{\\epsilon}\\), but can talk about behavior on average over\n\ndifferent runs of an experiment\ndifferent samples from a population\ndifferent values of \\(\\boldsymbol{\\epsilon}\\)\n\nwith minimal assumption \\(\\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\), \\[\\begin{align*}\n\\textsf{E}[\\hat{\\boldsymbol{\\beta}}] & = \\textsf{E}[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}+ (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}]\\\\\n& = \\boldsymbol{\\beta}+ (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\textsf{E}[\\boldsymbol{\\epsilon}] \\\\\n& = \\boldsymbol{\\beta}\n\\end{align*}\\]\nBias of \\(\\hat{\\boldsymbol{\\beta}}\\), \\(\\text{Bias}[\\hat{\\boldsymbol{\\beta}}] =  \\textsf{E}[\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}] = \\mathbf{0}_p\\)\n\\(\\hat{\\boldsymbol{\\beta}}\\) is an unbiased estimator of \\(\\boldsymbol{\\beta}\\) if \\(\\boldsymbol{\\mu}\\in C(\\mathbf{X})\\)"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#class-of-unbiased-estimators",
    "href": "resources/slides/03-non-full-rank.html#class-of-unbiased-estimators",
    "title": "Rank Deficient Models",
    "section": "Class of Unbiased Estimators",
    "text": "Class of Unbiased Estimators\nClass of linear statistical models: \\[\\begin{align*}\n\\mathbf{Y}& = \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\\\\n\\boldsymbol{\\epsilon}& \\sim P \\\\\nP & \\in \\cal{P}\n\\end{align*}\\]\n\nAn estimator \\(\\tilde{\\boldsymbol{\\beta}}\\) is unbiased for \\(\\boldsymbol{\\beta}\\) if \\(\\textsf{E}_P[\\tilde{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\\quad \\forall \\boldsymbol{\\beta}\\in \\mathbb{R}^p\\) and \\(P \\in \\cal{P}\\)\n\n\nExamples:\n\n\n\\(\\cal{P}_1= \\{P = \\textsf{N}(\\mathbf{0}_n ,\\mathbf{I}_n)\\}\\)\n\n\n\\(\\cal{P}_2 = \\{P = \\textsf{N}(\\mathbf{0}_n ,\\sigma^2 \\mathbf{I}_n), \\sigma^2 &gt;0\\}\\)\n\n\n\\(\\cal{P}_3 = \\{P = \\textsf{N}(\\mathbf{0}_n ,\\boldsymbol{\\Sigma}), \\boldsymbol{\\Sigma}\\in \\cal{{\\cal{S}}}^+ \\}\\) (\\(\\cal{{\\cal{S}}}^+\\) is the set of all \\(n \\times n\\) symmetric positive definite matrices.)\n\n\n\\(\\cal{P}_4\\) is the set of distributions with \\(\\textsf{E}_P[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\) and \\(\\textsf{E}_P[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] \\gt 0\\)\n\n\n\\(\\cal{P}_5\\) is the set of distributions with \\(\\textsf{E}_P[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\) and \\(\\textsf{E}_P[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] \\ge 0\\)"
  },
  {
    "objectID": "resources/slides/03-non-full-rank.html#linear-unbiased-estimation",
    "href": "resources/slides/03-non-full-rank.html#linear-unbiased-estimation",
    "title": "Rank Deficient Models",
    "section": "Linear Unbiased Estimation",
    "text": "Linear Unbiased Estimation\n\n\n\n\n\n\n\nExercise\n\n\n\nExplain why an estimator that is unbiased for the model with parameter space \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\) and \\(P \\in  \\cal{P}_{k+1}\\) is unbiased for the model with parameter space \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\) and \\(P \\in  \\cal{P}_{k}\\) .\nFind an estimator that is unbiased for \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\) and \\(P \\in  \\cal{P}_{1}\\) that but is biased for \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\) and \\(P \\in  \\cal{P}_{2}\\).\n\n\n\n\n\n\nRestrict attention to linear unbiased estimators\n\n\n\nDefinition: Linear Unbiased Estimators (LUEs)\nAn estimator \\(\\tilde{\\boldsymbol{\\beta}}\\) is a Linear Unbiased Estimator (LUE) of \\(\\boldsymbol{\\beta}\\) if\n\nlinearity: \\(\\tilde{\\boldsymbol{\\beta}}= \\mathbf{A}\\mathbf{Y}\\) for \\(\\mathbf{A}\\in \\mathbb{R}^{p \\times n}\\)\nunbiasedness: \\(\\textsf{E}[\\tilde{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\\) for all \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\)\n\n\n\n\n\n\nAre there other LUEs besides the OLS/MLE estimator?\nWhich is “best”? (and in what sense?)\n\n\n\n\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "labs/lab-01.html",
    "href": "labs/lab-01.html",
    "title": "lab-01",
    "section": "",
    "text": "Please see the instructions for HW-01 and be preapred to ask questions in lab (using R, theory, etc)"
  },
  {
    "objectID": "resources/slides/17-bma.html#normal-regression-model",
    "href": "resources/slides/17-bma.html#normal-regression-model",
    "title": "Bayesian Model Averaging",
    "section": "Normal Regression Model",
    "text": "Normal Regression Model\nCentered regression model where \\(\\mathbf{X}^c\\) is the \\(n \\times p\\) centered design matrix where all variables have had their means subtracted (may or may not need to be standardized)\n\\[\\mathbf{Y}= \\mathbf{1}_n \\alpha + \\mathbf{X}^c \\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\]\n\n“Redundant” variables lead to unstable estimates\n\nSome variables may not be relevant at all (\\(\\beta_j = 0\\))\nWe want to reduce the dimension of the predictor space\nHow can we infer a “good” model that uses a subset of predictors from the data?\nExpand model hierarchically to introduce another latent variable \\(\\boldsymbol{\\gamma}\\) that encodes models \\({\\cal M}_\\gamma\\) \\(\\boldsymbol{\\gamma}= (\\gamma_1, \\gamma_2, \\ldots \\gamma_p)^T\\) where \\[\\begin{align*}\n  \\gamma_j = 0 & \\Leftrightarrow \\beta_j = 0 \\\\\n  \\gamma_j = 1 &  \\Leftrightarrow \\beta_j \\neq 0\n  \\end{align*}\\]\nFind Bayes factors and posterior probabilities of models \\({\\cal M}_\\gamma\\)"
  },
  {
    "objectID": "resources/slides/17-bma.html#priors",
    "href": "resources/slides/17-bma.html#priors",
    "title": "Bayesian Model Averaging",
    "section": "Priors",
    "text": "Priors\nWith \\(2^p\\) models, subjective priors for \\(\\boldsymbol{\\beta}\\) are out of the question for moderate \\(p\\) and improper priors lead to arbitrary Bayes factors leading to conventional priors on model specific parameters\n\nZellner’s g-prior and related have attractive properties as a starting point \\[\\boldsymbol{\\beta}_\\gamma \\mid \\alpha, \\phi, \\boldsymbol{\\gamma}\\sim \\textsf{N}(0, g \\phi^{-1}\n({\\mathbf{X}_{\\boldsymbol{\\gamma}}^c}^\\prime \\mathbf{X}_{\\boldsymbol{\\gamma}}^c)^{-1})\\]\nIndependent Jeffrey’s prior on common parameters \\((\\alpha, \\phi)\\)\n\\(p(\\alpha, \\phi) \\propto 1/\\phi\\)\nmarginal likelihood of \\(\\boldsymbol{\\gamma}\\) that is proportional to \\[ p(\\mathbf{Y}\\mid \\boldsymbol{\\gamma}) = C (1 + g)^{\\frac{n-p_{\\boldsymbol{\\gamma}}-1}{2}} ( 1 + g (1 -\nR^2_\\gamma))^{- \\frac{(n-1)}{2}}\\]\n\\(R^2_\\gamma\\) is the usual coefficient of determination for model \\({\\cal M}_\\gamma\\).\n\\(C\\) is a constant common to all models (proportional to the marginal likelihood of the null model where \\(\\boldsymbol{\\beta_\\gamma}= \\mathbf{0}_p\\)"
  },
  {
    "objectID": "resources/slides/17-bma.html#sketch-for-marginal",
    "href": "resources/slides/17-bma.html#sketch-for-marginal",
    "title": "Bayesian Model Averaging",
    "section": "Sketch for Marginal",
    "text": "Sketch for Marginal\n\nIntegrate out \\(\\boldsymbol{\\beta_\\gamma}\\) using sums of normals\n\nFind inverse of \\(\\mathbf{I}_n + g \\mathbf{P}_{\\mathbf{X}_{\\boldsymbol{\\gamma}}}\\) (properties of projections or Sherman-Woodbury-Morrison Theorem)\n\nFind determinant of \\(\\phi (\\mathbf{I}_n + g \\mathbf{P}_{\\mathbf{X}_{\\boldsymbol{\\gamma}}})\\)\n\nIntegrate out intercept (normal)\n\nIntegrate out \\(\\phi\\) (gamma)\n\nalgebra to simplify quadratic forms to \\(R^2_{\\boldsymbol{\\gamma}}\\)\nin the following slides we will assume that \\(\\mathbf{X}_{\\boldsymbol{\\gamma}}\\) is centered so that \\(\\mathbf{1}_n^T\\mathbf{X}_{\\boldsymbol{\\gamma}}= \\mathbf{0}_p\\) for all models \\({\\cal M}_\\gamma\\)\n\n\nOr integrate \\(\\alpha\\), \\(\\boldsymbol{\\beta_\\gamma}\\) and \\(\\phi\\) (complete the square!)"
  },
  {
    "objectID": "resources/slides/17-bma.html#posterior-distributions-on-parameters",
    "href": "resources/slides/17-bma.html#posterior-distributions-on-parameters",
    "title": "Bayesian Model Averaging",
    "section": "Posterior Distributions on Parameters",
    "text": "Posterior Distributions on Parameters\n\\[\\begin{align*}\\alpha \\mid \\boldsymbol{\\gamma}, \\phi, y & \\sim \\textsf{N}\\left(\\bar{y}, \\frac{1}{n \\phi}\\right)\\\\\n\\boldsymbol{\\beta}_{\\boldsymbol{\\gamma}} \\mid \\boldsymbol{\\gamma}, \\phi, g, y &\\sim \\textsf{N}\\left( \\frac{g}{1 + g} \\hat{\\boldsymbol{\\beta}}_{\\gamma}, \\frac{g}{1 + g} \\frac{1}{\\phi} \\left[{\\boldsymbol{X}_{\\gamma}}^T \\boldsymbol{X}_{\\gamma} \\right]^{-1}  \\right) \\\\\n\\phi \\mid \\gamma, y & \\sim \\textsf{Gamma}\\left(\\frac{n-1}{2}, \\frac{\\textsf{TotalSS} - \\frac{g}{1+g} \\textsf{RegSS}}{2}\\right) \\\\\n\\textsf{TotalSS} & \\equiv \\sum_i (y_i - \\bar{y})^2 \\\\\n\\textsf{RegSS} & \\equiv \\hat{\\boldsymbol{\\beta}}_\\gamma^T \\boldsymbol{X}_\\gamma^T \\boldsymbol{X}_\\gamma \\hat{\\beta}\\gamma \\\\\nR^2_\\gamma & = \\frac{\\textsf{RegSS}}{\\textsf{TotalSS}}  = 1 - \\frac{\\textsf{ErrorSS}}{\\textsf{TotalSS}}\n\\end{align*}\\]"
  },
  {
    "objectID": "resources/slides/17-bma.html#priors-on-model-space",
    "href": "resources/slides/17-bma.html#priors-on-model-space",
    "title": "Bayesian Model Averaging",
    "section": "Priors on Model Space",
    "text": "Priors on Model Space\n\\(p({\\cal M}_\\gamma) \\Leftrightarrow p(\\boldsymbol{\\gamma})\\)\n\nFixed prior probability \\(\\gamma_j\\) \\(p(\\gamma_j = 1) = .5 \\Rightarrow P({\\cal M}_\\gamma) = .5^p\\)\nUniform on space of models \\(p_{\\boldsymbol{\\gamma}}\\sim \\textsf{Bin}(p, .5)\\)\nHierarchical prior \\[\\begin{align}\n\\gamma_j \\mid \\pi & \\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}\\textsf{Ber}(\\pi) \\\\\n\\pi & \\sim \\textsf{Beta}(a,b) \\\\\n\\text{then  }  p_{\\boldsymbol{\\gamma}}& \\sim \\textsf{BB}_p(a, b)\n\\end{align}\\]\n\n\n\\[\np(p_{\\boldsymbol{\\gamma}}\\mid p, a, b) = \\frac{ \\Gamma(p + 1) \\Gamma(p_{\\boldsymbol{\\gamma}}+ a) \\Gamma(p - p_{\\boldsymbol{\\gamma}}+ b) \\Gamma (a + b) }{\\Gamma(p_{\\boldsymbol{\\gamma}}+1) \\Gamma(p - p_{\\boldsymbol{\\gamma}}+ 1) \\Gamma(p + a + b) \\Gamma(a) \\Gamma(b)}\n\\] - Uniform on Model Size \\(\\Rightarrow p_{\\boldsymbol{\\gamma}}\\sim \\textsf{BB}_p(1, 1) \\sim \\textsf{Unif}(0, p)\\)"
  },
  {
    "objectID": "resources/slides/17-bma.html#posterior-probabilities-of-models",
    "href": "resources/slides/17-bma.html#posterior-probabilities-of-models",
    "title": "Bayesian Model Averaging",
    "section": "Posterior Probabilities of Models",
    "text": "Posterior Probabilities of Models\n\nCalculate posterior distribution analytically under enumeration. \\[p({\\cal M}_\\gamma\\mid \\mathbf{Y})= \\frac{p(\\mathbf{Y}\\mid \\boldsymbol{\\gamma}) p(\\boldsymbol{\\gamma})} {\\sum_{\\boldsymbol{\\gamma}^\\prime \\in \\Gamma} p(\\mathbf{Y}\\mid \\boldsymbol{\\gamma}^\\prime) p(\\boldsymbol{\\gamma}^\\prime)}\\]\nExpress as a function of Bayes factors and prior odds!\nUse MCMC over \\(\\Gamma\\) - Gibbs, Metropolis Hastings if \\(p\\) is large (depends on Bayes factors and prior odds)\nslow convergence/poor mixing with high correlations\n\nMetropolis Hastings algorithms more flexibility\n(swap pairs of variables or use adaptive Independent Metropolis!)\n\n\n\n\n\n\n\n\nNo need to run MCMC over \\(\\boldsymbol{\\gamma}\\), \\(\\boldsymbol{\\beta_\\gamma}\\), \\(\\alpha\\), and \\(\\phi\\)!"
  },
  {
    "objectID": "resources/slides/17-bma.html#choice-of-g-bartletts-paradox",
    "href": "resources/slides/17-bma.html#choice-of-g-bartletts-paradox",
    "title": "Bayesian Model Averaging",
    "section": "Choice of \\(g\\): Bartlett’s Paradox",
    "text": "Choice of \\(g\\): Bartlett’s Paradox\nThe Bayes factor for comparing \\(\\boldsymbol{\\gamma}\\) to the null model: \\[\nBF(\\boldsymbol{\\gamma}: \\boldsymbol{\\gamma}_0) =    (1 + g)^{(n - 1 - p_{\\boldsymbol{\\gamma}})/2} (1 + g(1 - R_{\\boldsymbol{\\gamma}}^2))^{-(n-1)/2}\n\\]\n\nFor fixed sample size \\(n\\) and \\(R_{\\boldsymbol{\\gamma}}^2\\), consider taking values of \\(g\\) that go to infinity\n\nIncreasing vagueness in prior\nWhat happens to BF as \\(g \\to \\infty\\)?\n\n\n\n\n\n\n\n\nBartlett Paradox\n\n\nWhy is this a paradox?"
  },
  {
    "objectID": "resources/slides/17-bma.html#information-paradox",
    "href": "resources/slides/17-bma.html#information-paradox",
    "title": "Bayesian Model Averaging",
    "section": "Information Paradox",
    "text": "Information Paradox\nThe Bayes factor for comparing \\(\\boldsymbol{\\gamma}\\) to the null model: \\[\nBF(\\boldsymbol{\\gamma}: \\boldsymbol{\\gamma}_0) =    (1 + g)^{(n - 1 - p_{\\boldsymbol{\\gamma}})/2} (1 + g(1 - R_{\\boldsymbol{\\gamma}}^2))^{-(n-1)/2}\n\\]\n\nLet \\(g\\) be a fixed constant and take \\(n\\) fixed.\n\nUsual F statistic for testing \\(\\boldsymbol{\\gamma}\\) versus \\(\\boldsymbol{\\gamma}_0\\) is \\(F = \\frac{R_{\\boldsymbol{\\gamma}}^2/p_{\\boldsymbol{\\gamma}}}{(1 - R_{\\boldsymbol{\\gamma}}^2)/(n - 1 - p_{\\boldsymbol{\\gamma}})}\\)\n\nAs \\(R^2_{\\boldsymbol{\\gamma}} \\to 1\\), \\(F \\to \\infty\\) Likelihood Rqtio test (F-test) would reject \\(\\boldsymbol{\\gamma}_0\\) where \\(F\\) is the usual \\(F\\) statistic for comparing model \\(\\boldsymbol{\\gamma}\\) to \\(\\boldsymbol{\\gamma}_0\\)\n\nBF converges to a fixed constant \\((1+g)^{n - 1 -p_{\\boldsymbol{\\gamma}}/2}\\) (does not go to infinity !\n\n\nInformation Inconsistency of Liang et al JASA 2008"
  },
  {
    "objectID": "resources/slides/17-bma.html#mixtures-of-g-priors-information-consistency",
    "href": "resources/slides/17-bma.html#mixtures-of-g-priors-information-consistency",
    "title": "Bayesian Model Averaging",
    "section": "Mixtures of \\(g\\)-priors & Information consistency",
    "text": "Mixtures of \\(g\\)-priors & Information consistency\n\nWant \\(\\textsf{BF}\\to \\infty\\) if \\(\\textsf{R}_{\\boldsymbol{\\gamma}}^2 \\to 1\\) if model is full rank\nPut a prior on \\(g\\) \\[BF(\\boldsymbol{\\gamma}: \\boldsymbol{\\gamma}_0) =  \\frac{ C \\int (1 + g)^{(n - 1 - p_{\\boldsymbol{\\gamma}})/2} (1 + g(1 - R_{\\boldsymbol{\\gamma}}^2))^{-(n-1)/2} \\pi(g) dg}{C}\\]\ninterchange limit and integration as \\(R^2 \\to 1\\) want \\[ \\textsf{E}_g[(1 +\ng)^{(n-1-p_{\\boldsymbol{\\gamma}})/2}]\\] to diverge under the prior"
  },
  {
    "objectID": "resources/slides/17-bma.html#one-solution",
    "href": "resources/slides/17-bma.html#one-solution",
    "title": "Bayesian Model Averaging",
    "section": "One Solution",
    "text": "One Solution\n\nhyper-g prior (Liang et al JASA 2008) \\[p(g) = \\frac{a-2}{2}(1 + g)^{-a/2}\\] or \\(g/(1+g) \\sim Beta(1, (a-2)/2)\\) for \\(a &gt; 2\\)\nprior expectation converges if \\(a &gt; n + 1 - p_{\\boldsymbol{\\gamma}}\\) (properties of \\(_2F_1\\) function)\nConsider minimal model \\(p_{\\boldsymbol{\\gamma}}= 1\\) and \\(n = 3\\) (can estimate intercept, one coefficient, and \\(\\sigma^2\\), then for \\(a &gt; 3\\) integral exists\nFor \\(2 &lt; a \\le 3\\) integral diverges and resolves the information paradox! (see proof in Liang et al JASA 2008 )"
  },
  {
    "objectID": "resources/slides/17-bma.html#examples-of-priors-on-g",
    "href": "resources/slides/17-bma.html#examples-of-priors-on-g",
    "title": "Bayesian Model Averaging",
    "section": "Examples of Priors on \\(g\\)",
    "text": "Examples of Priors on \\(g\\)\n\nhyper-g prior (Liang et al JASA 2008)\n\nSpecial case is Jeffreys prior for \\(g\\) which corresponds to \\(a = 2\\) (improper)\n\n\nZellner-Siow Cauchy prior \\(1/g \\sim \\textsf{Gamma}(1/2, n/2)\\)\n\nHyper-g/n \\((g/n)(1 + g/n) \\sim \\textsf{Beta}(1, (a-2)/2)\\) (generalized Beta distribution)\nrobust prior (Bayarri et al Annals of Statistics 2012 )\nIntrinsic prior (Womack et al JASA 2015)\n\n\nAll have prior tails for \\(\\boldsymbol{\\beta}\\) that behave like a Cauchy distribution and all except the Gamma prior have marginal likelihoods that can be computed using special hypergeometric functions (\\(_2F_1\\), Appell \\(F_1\\))\n\n\nNo fixed value of \\(g\\) (i.e a point mass prior) will resolve this!"
  },
  {
    "objectID": "resources/slides/17-bma.html#us-air-example",
    "href": "resources/slides/17-bma.html#us-air-example",
    "title": "Bayesian Model Averaging",
    "section": "US Air Example",
    "text": "US Air Example\n\nlibrary(BAS)\ndata(usair, package=\"HH\")\npoll.bma = bas.lm(log(SO2) ~ temp + log(mfgfirms) +\n                             log(popn) + wind +\n                             precip + raindays,\n                  data=usair,\n                  prior=\"JZS\",  #Jeffrey-Zellner-Siow\n                  alpha=nrow(usair), # n\n                  n.models=2^6,\n                  modelprior = uniform(),\n                  method=\"deterministic\")"
  },
  {
    "objectID": "resources/slides/17-bma.html#summary",
    "href": "resources/slides/17-bma.html#summary",
    "title": "Bayesian Model Averaging",
    "section": "Summary",
    "text": "Summary\n\nsummary(poll.bma, n.models=4)\n\n              P(B != 0 | Y) model 1   model 2   model 3   model 4\nIntercept        1.00000000 1.00000 1.0000000 1.0000000 1.0000000\ntemp             0.91158530 1.00000 1.0000000 1.0000000 1.0000000\nlog(mfgfirms)    0.31718916 0.00000 0.0000000 0.0000000 1.0000000\nlog(popn)        0.09223957 0.00000 0.0000000 0.0000000 0.0000000\nwind             0.29394451 0.00000 0.0000000 0.0000000 1.0000000\nprecip           0.28384942 0.00000 1.0000000 0.0000000 1.0000000\nraindays         0.22903262 0.00000 0.0000000 1.0000000 0.0000000\nBF                       NA 1.00000 0.3286643 0.2697945 0.2655873\nPostProbs                NA 0.29410 0.0967000 0.0794000 0.0781000\nR2                       NA 0.29860 0.3775000 0.3714000 0.5427000\ndim                      NA 2.00000 3.0000000 3.0000000 5.0000000\nlogmarg                  NA 3.14406 2.0313422 1.8339656 1.8182487"
  },
  {
    "objectID": "resources/slides/17-bma.html#plots-of-coefficients",
    "href": "resources/slides/17-bma.html#plots-of-coefficients",
    "title": "Bayesian Model Averaging",
    "section": "Plots of Coefficients",
    "text": "Plots of Coefficients\n\n beta = coef(poll.bma)\n par(mfrow=c(2,3));  plot(beta, subset=2:7,ask=F)"
  },
  {
    "objectID": "resources/slides/17-bma.html#posterior-distribution-with-uniform-prior-on-model-space",
    "href": "resources/slides/17-bma.html#posterior-distribution-with-uniform-prior-on-model-space",
    "title": "Bayesian Model Averaging",
    "section": "Posterior Distribution with Uniform Prior on Model Space",
    "text": "Posterior Distribution with Uniform Prior on Model Space\n\nimage(poll.bma, rotate=FALSE)"
  },
  {
    "objectID": "resources/slides/17-bma.html#posterior-distribution-with-bb11-prior-on-model-space",
    "href": "resources/slides/17-bma.html#posterior-distribution-with-bb11-prior-on-model-space",
    "title": "Bayesian Model Averaging",
    "section": "Posterior Distribution with BB(1,1) Prior on Model Space",
    "text": "Posterior Distribution with BB(1,1) Prior on Model Space\n\npoll.bb.bma = bas.lm(log(SO2) ~ temp + log(mfgfirms) +\n                                log(popn) + wind +\n                                precip + raindays,\n                     data=usair,\n                     prior=\"JZS\",\n                     alpha=nrow(usair),\n                     n.models=2^6,  #enumerate\n                     modelprior=beta.binomial(1,1))"
  },
  {
    "objectID": "resources/slides/17-bma.html#posterior-distribution-with-bb11-prior-on-model-space-1",
    "href": "resources/slides/17-bma.html#posterior-distribution-with-bb11-prior-on-model-space-1",
    "title": "Bayesian Model Averaging",
    "section": "Posterior Distribution with BB(1,1) Prior on Model Space",
    "text": "Posterior Distribution with BB(1,1) Prior on Model Space\n\nimage(poll.bb.bma, rotate=FALSE)"
  },
  {
    "objectID": "resources/slides/17-bma.html#summary-1",
    "href": "resources/slides/17-bma.html#summary-1",
    "title": "Bayesian Model Averaging",
    "section": "Summary",
    "text": "Summary\n\nChoice of prior on \\(\\boldsymbol{\\beta_\\gamma}\\)\ng-priors or mixtures of \\(g\\) (sensitivity)\npriors on the models (sensitivity)\nposterior summaries - select a model or “average” over all models\n\n\n\n\n\nhttps://sta702-F23.github.io/website/"
  },
  {
    "objectID": "resources/slides/13-testing.html#outline",
    "href": "resources/slides/13-testing.html#outline",
    "title": "Hypothesis Testing",
    "section": "Outline",
    "text": "Outline\nHypothesis Testing:\n\nThe hypothesis of no effects\n\nF-tests\nNull distribution\nDecision procedure\n\nTesting submodels\n\nExtra sum of squares\n\n\n\nReadings:\n\nChristensen Appendix C, Chapter 3"
  },
  {
    "objectID": "resources/slides/13-testing.html#the-hypothesis-of-no-effects",
    "href": "resources/slides/13-testing.html#the-hypothesis-of-no-effects",
    "title": "Hypothesis Testing",
    "section": "The Hypothesis of No Effects",
    "text": "The Hypothesis of No Effects\nSuppose we believe the model \\[\n{\\text{M1}} \\quad \\quad\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\quad \\quad \\boldsymbol{\\epsilon}\\sim \\textsf{N}(0, \\sigma^2 \\mathbf{I}_n)  \n\\] but hypothesize that there is no effect of the \\(\\mathbf{X}\\) variables on \\(\\mathbf{Y}\\)\n\nIf this were true, then the distribution for \\(\\mathbf{Y}\\) would be \\[\n\\quad \\quad  \\quad \\quad \\quad {\\text{  M0}} \\quad \\quad \\mathbf{Y}= \\boldsymbol{\\epsilon}\\quad \\quad \\boldsymbol{\\epsilon}\\sim \\textsf{N}(0, \\sigma^2 \\mathbf{I}_n)\n\\]\nFor M1, the distribution of \\(\\mathbf{Y}\\) is a collection of normal distributions with \\(\\boldsymbol{\\mu}\\in C(\\mathbf{X})\\) and Covariance a scalar multiple of the \\(\\mathbf{I}\\)\nthe distributions for the data \\(\\mathbf{Y}\\) under M0 is a subset of the distributions under M1 or submodel of M1 with \\(\\boldsymbol{\\mu}= \\mathbf{0}\\)\nObservations \\(\\mathbf{Y}\\) may give us evidence that supports or rejects our hypothesis that the null model, M0, is true"
  },
  {
    "objectID": "resources/slides/13-testing.html#goal",
    "href": "resources/slides/13-testing.html#goal",
    "title": "Hypothesis Testing",
    "section": "Goal",
    "text": "Goal\nOur goals are to\n\nobtain a numerical summary of the evidence\ncome up with a decision-making procedure that decides between M1 and M0,\n(frequentist) control the probability of making a certain type of incorrect decision\n\n\nProcedure based on the following steps:\n\nTest statistic: compute a statistic \\(t(\\mathbf{Y},\\mathbf{X})\\), a function of observable data;\nNull distribution: compare \\(t(\\mathbf{Y},\\mathbf{X})\\) to the types of values we would expect if M0 is true\nDecision rule: accept M0 if \\(t(\\mathbf{Y},\\mathbf{X})\\) is in accord with its distribution under M0, otherwise reject the submodel M0"
  },
  {
    "objectID": "resources/slides/13-testing.html#intuition",
    "href": "resources/slides/13-testing.html#intuition",
    "title": "Hypothesis Testing",
    "section": "Intuition",
    "text": "Intuition\nIf \\(\\hat{\\boldsymbol{\\beta}}\\approx \\boldsymbol{\\beta}\\) then\n\nif \\(\\boldsymbol{\\beta}= \\mathbf{0}\\), then \\(\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\approx \\mathbf{0}\\)\nif \\(\\boldsymbol{\\beta}\\ne \\mathbf{0}\\), then \\(\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\not \\approx \\mathbf{0}\\)\nIf the null model M0 is correct, then \\(\\|\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\|^2\\) should be small\nIf incorrect, \\(\\|\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\|^2\\) should be big\nWe need to quantify this intuition"
  },
  {
    "objectID": "resources/slides/13-testing.html#decomposition",
    "href": "resources/slides/13-testing.html#decomposition",
    "title": "Hypothesis Testing",
    "section": "Decomposition",
    "text": "Decomposition\n\\[\\begin{align*}\n\\mathbf{X}\\hat{\\boldsymbol{\\beta}}& = \\mathbf{P}_{\\mathbf{X}} \\mathbf{Y}\\\\\n& =  \\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{P}\\boldsymbol{\\epsilon}\n\\end{align*}\\]\n\n\\[\\begin{align*}\n\\|\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\|^2 & = (\\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{P}\\boldsymbol{\\epsilon})^T(\\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{P}\\boldsymbol{\\epsilon}) \\\\\n              & = \\boldsymbol{\\beta}^T \\mathbf{X}^T  \\mathbf{X}\\boldsymbol{\\beta}+ 2 \\boldsymbol{\\beta}^T \\mathbf{X}^T \\mathbf{P}\\boldsymbol{\\epsilon}+ \\boldsymbol{\\epsilon}^T \\mathbf{P}\\boldsymbol{\\epsilon}\\\\\n              & = \\| \\mathbf{X}\\boldsymbol{\\beta}\\|^2 + 2 \\boldsymbol{\\beta}^T \\mathbf{X}^T \\boldsymbol{\\epsilon}+ \\boldsymbol{\\epsilon}^T \\mathbf{P}\\boldsymbol{\\epsilon}\n\\end{align*}\\]\n\n\nHow big is \\(\\|\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\|^2\\) on average? How big do we expect it to be under our two models?\n\n\nTake expectations:\n\\[\\begin{align*}\n\\textsf{E}[\\|\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\|^2] & = \\|\\mathbf{X}\\boldsymbol{\\beta}\\|^2 + \\textsf{E}[2 \\boldsymbol{\\beta}^T \\mathbf{X}^T  \\boldsymbol{\\epsilon}] + \\textsf{E}[\\boldsymbol{\\epsilon}^T \\mathbf{P}\\boldsymbol{\\epsilon}] \\\\\n& = \\|\\mathbf{X}\\boldsymbol{\\beta}\\|^2 + 0 + \\sigma^2 \\textsf{tr}(\\mathbf{P}) = \\|\\mathbf{X}\\boldsymbol{\\beta}\\|^2 + \\sigma^2 p\n\\end{align*}\\]\n\nif \\(\\boldsymbol{\\beta}= \\mathbf{0}\\), then \\(\\textsf{E}[\\|\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\|^2] = \\sigma^2 p\\)"
  },
  {
    "objectID": "resources/slides/13-testing.html#comparison",
    "href": "resources/slides/13-testing.html#comparison",
    "title": "Hypothesis Testing",
    "section": "Comparison",
    "text": "Comparison\nIf we knew \\(\\sigma^2\\), then\n\nif \\(\\|\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\|^2/p \\approx \\sigma^2\\), we might decide M0 would be reasonable\nif \\(\\|\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\|^2/p \\gg \\sigma^2\\), then we might decide M0 is unreasonable\n\n\nBut we do not know \\(\\sigma^2\\)\n\nif we estimate \\(\\sigma^2\\) by \\(s^2 = \\frac{\\mathbf{Y}^T(\\mathbf{I}- \\mathbf{P})\\mathbf{Y}}{n - p}\\), then\n\nif \\(\\|\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\|^2/p \\approx s^2\\), we might decide M0 would be reasonable\nif \\(\\|\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\|^2/p \\gg s^2\\), then we might decide M0 is unreasonable"
  },
  {
    "objectID": "resources/slides/13-testing.html#test-statistic",
    "href": "resources/slides/13-testing.html#test-statistic",
    "title": "Hypothesis Testing",
    "section": "Test Statistic",
    "text": "Test Statistic\nNote: if the null model M0 is correct (\\(\\boldsymbol{\\beta}= \\mathbf{0}\\)), then both\n\n\\(\\|\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\|/p\\)\n\\(\\textsf{SSE}/(n-p) = \\frac{\\mathbf{Y}^T(\\mathbf{I}- \\mathbf{P})\\mathbf{Y}}{n - p}\\)\n\n\nare unbiased estimates of \\(\\sigma^2\\)\n\n\nIf the null model is not correct, but the linear model M1 is correct, then\n\n\\(s^2\\) is still an unbiased estimate of \\(\\sigma^2\\)\n\\(\\|\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\|^2/p\\) is expected to be bigger than \\(\\sigma^2\\)\nWe can use the ratio of the two quantities to form a test statistic \\[t(\\mathbf{Y}, \\mathbf{X}) = \\frac{\\|\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\|^2/p}{\\textsf{SSE}/(n-p)} = \\frac{\\textsf{RSS}/p}{\\textsf{SSE}/(n-p)}\\]\n\\(\\textsf{RSS}\\) is the regression or model sum of squares"
  },
  {
    "objectID": "resources/slides/13-testing.html#distributions-under-the-null-model-m0",
    "href": "resources/slides/13-testing.html#distributions-under-the-null-model-m0",
    "title": "Hypothesis Testing",
    "section": "Distributions under the Null Model M0",
    "text": "Distributions under the Null Model M0\n\n\\(\\textsf{SSE}\\sim \\sigma^2 \\chi^2_{n-p}\\)\n\\(\\|\\mathbf{X}(\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta})\\|^2 \\sim \\sigma^2 \\chi^2_p\\)\n\n\nso under the null model M0 (\\(\\boldsymbol{\\beta}= \\mathbf{0}\\)), we have\n\n\\(\\textsf{SSE}\\sim \\sigma^2 \\chi^2_{n-p}\\)\n\\(\\|\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\|^2 \\sim \\sigma^2 \\chi^2_p\\)\nthey are statistically independent (why?)\nso the ratio \\[\\begin{align*}\nt(\\mathbf{Y}, \\mathbf{X}) & = \\frac{\\textsf{RSS}/p}{\\textsf{SSE}/(n-p)} = \\frac{(\\textsf{RSS}/\\sigma^2)/p}{(\\textsf{SSE}/\\sigma^2)/(n-p)} \\\\\n& \\mathrel{\\mathop{=}\\limits^{\\rm D}}\\frac{\\chi^2_p/p}{\\chi^2_{n-p}/(n-p)} \\quad \\text{ is independent of } \\sigma^2\n\\end{align*}\\] is independent of \\(\\sigma^2\\)"
  },
  {
    "objectID": "resources/slides/13-testing.html#f-distribution",
    "href": "resources/slides/13-testing.html#f-distribution",
    "title": "Hypothesis Testing",
    "section": "F Distribution",
    "text": "F Distribution\n\nDefinition: F distributionIf \\(X_1 \\sim \\chi^2_{d1}\\) and \\(X_2 \\sim \\chi^2_{d2}\\) and are independent, then the ratio \\[F = \\frac{X_1/d1}{X_2/d2}\\] has an \\(F_{d1, d2}\\) distribution with \\(d1\\) and \\(d2\\) degrees of freedom.\n\n\n\n\n\\(F(\\mathbf{Y}) \\equiv t(\\mathbf{Y}, \\mathbf{X}) = \\frac{\\textsf{RSS}/p}{\\textsf{SSE}/(n-p)}\\) has an \\(F_{p, n-p}\\) distribution under the null model M0"
  },
  {
    "objectID": "resources/slides/13-testing.html#decision-procedure",
    "href": "resources/slides/13-testing.html#decision-procedure",
    "title": "Hypothesis Testing",
    "section": "Decision Procedure",
    "text": "Decision Procedure\n\n\nWe will accept M0 that \\(\\boldsymbol{\\beta}= \\mathbf{0}\\) unless \\(F(\\mathbf{Y})\\) is large compared to an \\(F_{p,n−p}\\) distribution.\n\naccept M0: \\(\\boldsymbol{\\beta}= \\mathbf{0}\\) if \\(F(\\mathbf{Y}) &lt; F_{p,n−p,1−\\alpha}\\)\n\\(F_{p,n−p,1−\\alpha}\\) is the \\(1 − \\alpha\\) quantile of a \\(F_{p,n−p}\\)\nreject M0: \\(\\boldsymbol{\\beta}= \\mathbf{0}\\) if \\(F(\\mathbf{Y}) &gt; F_{p,n−p,1−\\alpha}\\)\nthe probability that we reject M0 when it is true, is \\[\\begin{align*}\n\\Pr( \\text{ reject M0} & \\mid \\text{ M0 true})\\\\\n& = \\Pr(F(\\mathbf{Y}) &gt; F_{p,n−p,1−\\alpha} \\mid \\boldsymbol{\\beta}= \\mathbf{0}) \\\\\n& = \\alpha\n\\end{align*}\\]"
  },
  {
    "objectID": "resources/slides/13-testing.html#p-values",
    "href": "resources/slides/13-testing.html#p-values",
    "title": "Hypothesis Testing",
    "section": "P-values",
    "text": "P-values\n\n\nInstead of just declaring that M0 is true or false, statistical analyses report how extreme \\(F(\\mathbf{Y})\\) is compared to its null distribution.\n\nThis is usually reported in terms of the p-value:\n\nthe value \\(p \\in(0,1)\\) such that \\(F(\\mathbf{Y})\\) is the \\((1−p)\\) quantile of the \\(F_{p,n−p}\\) distribution\nthe probability that a random variable \\(F \\sim F_{p,n−p}\\) is larger than the observed value \\(F(\\mathbf{Y})\\), if the null model is true\n\nit is not the \\(\\Pr(\\text{M0 is true})\\) based on the observed data"
  },
  {
    "objectID": "resources/slides/13-testing.html#testing-submodels",
    "href": "resources/slides/13-testing.html#testing-submodels",
    "title": "Hypothesis Testing",
    "section": "Testing SubModels",
    "text": "Testing SubModels\nWe are usually not interested in testing that all of the coefficients are zero if there is an intercept in the model\n\nBut we can use the same idea to test submodels\nWe assume the Gaussian Linear Model \\[\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad M1 \\quad \\mathbf{Y}∼ \\textsf{N}(\\mathbf{W}\\boldsymbol{\\alpha}+ \\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2\\mathbf{I}) \\equiv \\textsf{N}(\\mathbf{Z}\\boldsymbol{\\theta}, \\sigma^2\\mathbf{I})\\] where \\(\\mathbf{W}\\) is \\(n \\times q\\), \\(\\mathbf{X}\\) is \\(n \\times p\\), \\(\\mathbf{Z}= [\\mathbf{W}\\mathbf{X}]\\),\nWe wish to evaluate the hypothesis \\(\\boldsymbol{\\beta}= \\mathbf{0}\\)\nequivalent to comparing M1 to M0: \\[M0 \\quad \\mathbf{Y}∼ \\textsf{N}(\\mathbf{W}\\boldsymbol{\\alpha}, \\sigma^2\\mathbf{I})\\]"
  },
  {
    "objectID": "resources/slides/13-testing.html#intuition-1",
    "href": "resources/slides/13-testing.html#intuition-1",
    "title": "Hypothesis Testing",
    "section": "Intuition",
    "text": "Intuition\nDevise a test statistic and procedure by\n\nfitting the full model M1 \\(\\mathbf{Y}\\sim \\textsf{N}(\\mathbf{W}\\boldsymbol{\\alpha}+ \\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2\\mathbf{I})\\)\nfitting the reduced/null model M0 \\(\\mathbf{Y}\\sim \\textsf{N}(\\mathbf{W}\\boldsymbol{\\alpha}, \\sigma^2\\mathbf{I})\\)\naccept M0 if the null model fits about as well as the full model\nreject M0 if the null model fits much worse than the full model\nmeasure fit through \\(\\textsf{SSE}_{M0}\\) and \\(\\textsf{SSE}_{M1}\\) \\[\\begin{align*}\n\\textsf{SSE}_{M1} & = \\min_{\\boldsymbol{\\alpha}, \\boldsymbol{\\beta}}\n\\|\\mathbf{Y}- (\\mathbf{W}\\boldsymbol{\\alpha}+ \\mathbf{X}\\boldsymbol{\\beta})\\|^2 \\\\\n        & = \\min_{\\boldsymbol{\\theta}} \\|\\mathbf{Y}- \\mathbf{Z}\\boldsymbol{\\theta}\\|^2 = \\mathbf{Y}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{Z}})\\mathbf{Y}\\\\\n\\textsf{SSE}_{M0} & = \\min_{\\boldsymbol{\\alpha}} \\|\\mathbf{Y}- \\mathbf{W}\\boldsymbol{\\alpha}\\|^2 = \\mathbf{Y}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}\\\\\n\\end{align*}\\]"
  },
  {
    "objectID": "resources/slides/13-testing.html#extra-sum-of-squares",
    "href": "resources/slides/13-testing.html#extra-sum-of-squares",
    "title": "Hypothesis Testing",
    "section": "Extra Sum of Squares",
    "text": "Extra Sum of Squares\nApproach 1: accept/choose the null model if \\(\\textsf{SSE}_{M0} &lt; \\textsf{SSE}_{M1}\\), and choose the full model if \\(\\textsf{SSE}_{M1} &lt; \\textsf{SSE}_{M0}\\).\n\nbut \\(\\textsf{SSE}_{M1}\\) is always less than \\(\\textsf{SSE}_{M0}\\)\n\n\nApproach 2: instead reject M1 \\(\\boldsymbol{\\beta}= \\mathbf{0}\\) if \\(\\textsf{SSE}_{M0}\\) is much bigger than \\(\\textsf{SSE}_{M1}\\).\n\nSpecifically, reject M1 \\(\\boldsymbol{\\beta}= \\mathbf{0}\\) if \\(\\textsf{SSE}_{M0} - \\textsf{SSE}_{M1}\\) is much bigger than what we would expect if the null hypothesis M0 were true.\n\n\n\nNeed:\n\nthe null distribution of \\(\\textsf{SSE}_{M0}\\)\nthe null distribution of \\(\\textsf{SSE}_{M1}\\)\nthe null distribution of their difference \\(\\textsf{SSE}_{M0} - \\textsf{SSE}_{M1}\\)"
  },
  {
    "objectID": "resources/slides/13-testing.html#distributions",
    "href": "resources/slides/13-testing.html#distributions",
    "title": "Hypothesis Testing",
    "section": "Distributions",
    "text": "Distributions\nDistribution under the full model M1 \\[ \\textsf{SSE}_{M1} = \\mathbf{Y}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{Z}})\\mathbf{Y}\\sim \\sigma^2 \\chi^2_{n - q - p}\\]\n\ntrue whether or not \\(\\boldsymbol{\\beta}= \\mathbf{0}\\)\n\\(\\textsf{E}[\\textsf{SSE}_{M1}] = E[\\mathbf{Y}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{Z}})\\mathbf{Y}] = \\sigma^2(n - q - p)\\)\n\n\nDistribution under the null model M0 \\[\\textsf{SSE}_{M0} = \\mathbf{Y}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}\\sim \\sigma^2 \\chi^2_{n - q}\\]\n\ntrue if \\(\\boldsymbol{\\beta}= \\mathbf{0}\\)\n\\(\\textsf{E}[\\textsf{SSE}_{M0}] = E[\\mathbf{Y}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}] = \\sigma^2(n - q)\\)\nif \\(\\boldsymbol{\\beta}\\neq \\mathbf{0}\\) then \\(\\textsf{SSE}_{M0}\\) has a non-central \\(\\chi^2_{n-q}\\) distribution"
  },
  {
    "objectID": "resources/slides/13-testing.html#expected-value-of-textsfsse_m0-under-m1",
    "href": "resources/slides/13-testing.html#expected-value-of-textsfsse_m0-under-m1",
    "title": "Hypothesis Testing",
    "section": "Expected Value of \\(\\textsf{SSE}_{M0}\\) under M1",
    "text": "Expected Value of \\(\\textsf{SSE}_{M0}\\) under M1\n\nRewrite \\((\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}\\) under M1: \\[\\begin{align*}\n(\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}& = (\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})(\\mathbf{W}\\boldsymbol{\\alpha}+ \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}) \\\\\n& = (\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\mathbf{X}\\boldsymbol{\\beta}+ (\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\boldsymbol{\\epsilon}\n\\end{align*}\\]\ncompute \\(\\textsf{E}[\\textsf{SSE}_{M0}]\\) under M1: \\[\\begin{align*}\n\\textsf{E}[\\mathbf{Y}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}] & = \\boldsymbol{\\beta}^T\\mathbf{X}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\mathbf{X}\\boldsymbol{\\beta}+ \\textsf{E}[\\boldsymbol{\\epsilon}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\boldsymbol{\\epsilon}] \\\\\n& = \\boldsymbol{\\beta}^T\\mathbf{X}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\mathbf{X}\\boldsymbol{\\beta}+ \\sigma^2 \\textsf{tr}(\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}}) \\\\\n&  = \\boldsymbol{\\beta}^T\\mathbf{X}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\mathbf{X}\\boldsymbol{\\beta}+ \\sigma^2(n - q)\n\\end{align*}\\]\nunder M0, both \\(\\textsf{SSE}_{M0}/(n-q)\\) and \\(\\textsf{SSE}_{M1}/(n- q - p)\\) are unbiased estimates of \\(\\sigma^2\\)\nbut does the ratio \\(\\frac{\\textsf{SSE}_{M0}/(n-q)}{\\textsf{SSE}_{M1}/(n- q - p)}\\) have a F distribution?"
  },
  {
    "objectID": "resources/slides/13-testing.html#extra-sum-of-squares-1",
    "href": "resources/slides/13-testing.html#extra-sum-of-squares-1",
    "title": "Hypothesis Testing",
    "section": "Extra Sum of Squares",
    "text": "Extra Sum of Squares\nRewrite \\(\\textsf{SSE}_{M0}\\): \\[\\begin{align*}\n\\textsf{SSE}_{M0} & = \\mathbf{Y}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}\\\\\n          & = \\mathbf{Y}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{Z}} + \\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}\\\\\n          & = \\mathbf{Y}^T(\\mathbf{I}- \\mathbf{P}_{\\mathbf{Z}})\\mathbf{Y}+ \\mathbf{Y}^T(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}\\\\\n          & = \\textsf{SSE}_{M1} + \\mathbf{Y}^T(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}\n\\end{align*}\\]\n\nExtra Sum of Squares: \\[\\textsf{SSE}_{M0} - \\textsf{SSE}_{M1}  = \\mathbf{Y}^T(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}})\\mathbf{Y}\\]\n\nis \\(\\mathbf{P}_{\\mathbf{Z}} - \\mathbf{P}_{\\mathbf{W}}\\) is a projection matrix?\nonto what space? along what space ?\nwhat is the distribution of \\(\\textsf{SSE}_{M0} - \\textsf{SSE}_{M1}\\) under the null model M0? under M1?\nis it independent of \\(\\textsf{SSE}_{M1}\\)?\n\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/06-GLS.html#outline",
    "href": "resources/slides/06-GLS.html#outline",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Outline",
    "text": "Outline\n\nGeneral Least Squares and MLEs\nGauss-Markov Theorem & BLUEs\nMVUE\n\n\nReadings:\n\nChristensen Chapter 2 and 10 (Appendix B as needed)\nSeber & Lee Chapter 3"
  },
  {
    "objectID": "resources/slides/06-GLS.html#other-error-distributions",
    "href": "resources/slides/06-GLS.html#other-error-distributions",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Other Error Distributions",
    "text": "Other Error Distributions\nModel:\n\\[\\begin{align} \\mathbf{Y}& = \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\quad\n              \\textsf{E}[\\boldsymbol{\\epsilon}]  = \\mathbf{0}_n \\\\\n              \\textsf{Cov}[\\boldsymbol{\\epsilon}] & = \\sigma^2 \\mathbf{V}\n\\end{align}\\] where \\(\\sigma^2\\) is a scalar and \\(\\mathbf{V}\\) is a \\(n \\times n\\) symmetric matrix\n\nExamples:\n\nHeteroscedasticity: \\(\\mathbf{V}\\) is a diagonal matrix with \\([\\mathbf{V}]_{ii} =  v_i\\)\n\n\\(v_{i} = 1/n_i\\) if \\(y_i\\) is the mean of \\(n_i\\) observations\nsurvey weights or propogation of measurement errors in physics models\n\nCorrelated data:\n\ntime series; first order auto-regressive model with equally spaced data \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\sigma^2 \\mathbf{V}\\), where \\(v_{ij} = \\rho^{|i−j|}\\).\n\nHierarchical models with random effects"
  },
  {
    "objectID": "resources/slides/06-GLS.html#ols-under-a-general-covariance",
    "href": "resources/slides/06-GLS.html#ols-under-a-general-covariance",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "OLS under a General Covariance",
    "text": "OLS under a General Covariance\n\nIs it still unbiased? What’s its variance? Is it still the BLUE?\nUnbiasedness of \\(\\hat{\\boldsymbol{\\beta}}\\) \\[\\begin{align}\n\\textsf{E}[\\hat{\\boldsymbol{\\beta}}] & = \\textsf{E}[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{Y}] \\\\\n        & = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\textsf{E}[\\mathbf{Y}] =  (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\textsf{E}[\\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}] \\\\\n        & = \\boldsymbol{\\beta}+ \\mathbf{0}_p = \\boldsymbol{\\beta}\n\\end{align}\\]\nCovariance of \\(\\hat{\\boldsymbol{\\beta}}\\) \\[\\begin{align}\n\\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}] & = \\textsf{Cov}[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{Y}] \\\\\n       & = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\textsf{Cov}[\\mathbf{Y}]  \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n       & = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T  \\mathbf{V}\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\n\\end{align}\\]\nNot necessarily \\(\\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1}\\) unless \\(\\mathbf{V}\\) has a special form"
  },
  {
    "objectID": "resources/slides/06-GLS.html#gls-via-whitening",
    "href": "resources/slides/06-GLS.html#gls-via-whitening",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "GLS via Whitening",
    "text": "GLS via Whitening\nTransform the data and reduce problem to one we have solved!\n\nFor \\(\\mathbf{V}&gt; 0\\) use the Spectral Decomposition \\[\\mathbf{V}= \\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{U}^T = \\mathbf{U}\\boldsymbol{\\Lambda}^{1/2} \\boldsymbol{\\Lambda}^{1/2} \\mathbf{U}^T\\]\ndefine the symmetric square root of \\(\\mathbf{V}\\) as \\[\\mathbf{V}^{1/2} \\equiv \\mathbf{U}\\boldsymbol{\\Lambda}^{1/2} \\mathbf{U}^T\\]\ntransform model: \\[\\begin{align*}\n\\mathbf{V}^{-1/2} \\mathbf{Y}& = \\mathbf{V}^{-1/2} \\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{V}^{-1/2}\\boldsymbol{\\epsilon}\\\\\n\\tilde{\\mathbf{Y}} & = \\tilde{\\mathbf{X}} \\boldsymbol{\\beta}+ \\tilde{\\boldsymbol{\\epsilon}}\n\\end{align*}\\]\nSince \\(\\textsf{Cov}[\\tilde{\\boldsymbol{\\epsilon}}] = \\sigma^2\\mathbf{V}^{-1/2} \\mathbf{V}\\mathbf{V}^{-1/2} = \\sigma^2 \\mathbf{I}_n\\), we know that \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\equiv (\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}})^{-1} \\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{Y}}\\) is the BLUE for \\(\\boldsymbol{\\beta}\\) based on \\(\\tilde{\\mathbf{Y}}\\) (\\(\\mathbf{X}\\) full rank)"
  },
  {
    "objectID": "resources/slides/06-GLS.html#gls",
    "href": "resources/slides/06-GLS.html#gls",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "GLS",
    "text": "GLS\n\nIf \\(\\mathbf{V}\\) is known, then \\(\\tilde{\\mathbf{Y}}\\) and \\(\\mathbf{Y}\\) are known linear transformations of each other\nany estimator of \\(\\boldsymbol{\\beta}\\) that is linear in \\(\\mathbf{Y}\\) is linear in \\(\\tilde{\\mathbf{Y}}\\) and vice versa from previous results\n\\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\) is the BLUE of \\(\\boldsymbol{\\beta}\\) based on either \\(\\tilde{\\mathbf{Y}}\\) or \\(\\mathbf{Y}\\)!\nSubstituting back, we have \\[\\begin{align}\n\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}& =  (\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}})^{-1} \\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{Y}}\\\\\n       & = (\\mathbf{X}^T \\mathbf{V}^{-1/2}\\mathbf{V}^{-1/2} \\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{V}^{-1/2}\\mathbf{V}^{-1/2}\\mathbf{Y}\\\\\n       & = (\\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{Y}\n\\end{align}\\] which is the Generalized Least Squares Estimator of \\(\\boldsymbol{\\beta}\\)\n\n\n\nExercise: Weighted RegressionConsider the model \\(\\mathbf{Y}= \\beta \\mathbf{x}+ \\boldsymbol{\\epsilon}\\) where \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}]\\) is a known diagonal matrix \\(\\mathbf{V}\\). Write out the GLS estimator in terms of sums and interpret."
  },
  {
    "objectID": "resources/slides/06-GLS.html#gls-of-boldsymbolmu-full-rank-casedagger",
    "href": "resources/slides/06-GLS.html#gls-of-boldsymbolmu-full-rank-casedagger",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "GLS of \\(\\boldsymbol{\\mu}\\) (Full Rank Case)\\(^{\\dagger}\\)",
    "text": "GLS of \\(\\boldsymbol{\\mu}\\) (Full Rank Case)\\(^{\\dagger}\\)\n\nthe OLS/MLE of \\(\\boldsymbol{\\mu}\\in C(\\mathbf{X})\\) with transformed variables is \\[\\begin{align*}\n\\mathbf{P}_{\\tilde{\\mathbf{X}}} \\tilde{\\mathbf{Y}}& = \\tilde{\\mathbf{X}}\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\\\\n\\tilde{\\mathbf{X}}\\left(\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}\\right)^{-1}\\tilde{\\mathbf{X}}^T \\tilde{\\mathbf{Y}}& = \\tilde{\\mathbf{X}}\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\\\\n\\mathbf{V}^{-1/2} \\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-1}\\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{Y}& = \\mathbf{V}^{-1/2} \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\end{align*}\\]\nsince \\(\\mathbf{V}\\) is positive definite, multiple thru by \\(\\mathbf{V}^{1/2}\\), to show that \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\) is a GLS/MLE estimator of \\(\\boldsymbol{\\beta}\\) iff \\[\\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-1}\\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{Y}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\]\nIs \\(\\mathbf{P}_\\mathbf{V}\\equiv \\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-1}\\mathbf{X}^T \\mathbf{V}^{-1}\\) a projection onto \\(C(\\mathbf{X})\\)? Is it an orthogonal projection onto \\(C(\\mathbf{X})\\)?\n\n\n\\(\\dagger\\) if \\(\\mathbf{X}\\) is not full rank replace \\(\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-1}\\) with \\(\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-}\\)"
  },
  {
    "objectID": "resources/slides/06-GLS.html#projections",
    "href": "resources/slides/06-GLS.html#projections",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Projections",
    "text": "Projections\nWe want to show that \\(\\mathbf{P}_\\mathbf{V}\\equiv \\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-1}\\mathbf{X}^T \\mathbf{V}^{-1}\\) is a projection onto \\(C(\\mathbf{X})\\)\n\nfrom the definition of \\(\\mathbf{P}_\\mathbf{V}\\) it follows that \\(\\mathbf{m}\\in C(\\mathbf{P}_\\mathbf{v})\\) implies that \\(\\mathbf{m}= \\mathbf{P}_\\mathbf{V}\\mathbf{m}= \\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{m}\\) so \\(C(\\mathbf{P}_\\mathbf{V}) \\subset C(\\mathbf{X})\\)\nsince \\(\\mathbf{P}_\\tilde{\\mathbf{X}}\\) is a projection onto \\(C(\\tilde{\\mathbf{X}})\\) we have \\[\\begin{align*}\n\\mathbf{P}_{\\tilde{\\mathbf{X}}} \\tilde{\\mathbf{X}}& = \\tilde{\\mathbf{X}}\\\\\n\\tilde{\\mathbf{X}}\\left(\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}\\right)^{-1}\\tilde{\\mathbf{X}}^T \\tilde{\\mathbf{X}}& = \\tilde{\\mathbf{X}}\\\\\n\\mathbf{V}^{-1/2} \\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-1}\\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{X}& = \\mathbf{V}^{-1/2} \\mathbf{X}\\\\\n\\mathbf{V}^{-1/2} \\mathbf{P}_\\mathbf{V}\\mathbf{X}& = \\mathbf{V}^{-1/2} \\mathbf{X}\n\\end{align*}\\]\nWe can multiply both sides by \\(\\mathbf{V}^{1/2} &gt; 0\\), so that \\(\\mathbf{P}_\\mathbf{V}\\mathbf{X}= \\mathbf{X}\\)\nfor \\(\\mathbf{m}\\in C(\\mathbf{X})\\), \\(\\mathbf{P}_\\mathbf{V}\\mathbf{m}= \\mathbf{m}\\) and \\(C(\\mathbf{X}) \\subset C(\\mathbf{P}_\\mathbf{V})\\)\n\\(\\quad \\quad \\therefore C(\\mathbf{P}_\\mathbf{V}) = C(\\mathbf{X})\\) so that \\(\\mathbf{P}_\\mathbf{V}\\) is a projection onto \\(C(\\mathbf{X})\\)"
  },
  {
    "objectID": "resources/slides/06-GLS.html#oblique-projections",
    "href": "resources/slides/06-GLS.html#oblique-projections",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Oblique Projections",
    "text": "Oblique Projections\n\nProposition: ProjectionThe \\(n \\times n\\) matrix \\(\\mathbf{P}_\\mathbf{V}\\equiv \\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{V}^{-1}\\mathbf{X}\\right)^{-1}\\mathbf{X}^T \\mathbf{V}^{-1}\\) is a projection onto the \\(C(\\mathbf{X})\\)\n\n\n\nShow that \\(\\mathbf{P}_\\mathbf{V}^2 = \\mathbf{P}_\\mathbf{V}\\) (idempotent)\nevery vector \\(\\mathbf{y}\\in \\mathbb{R}^n\\) may be written as \\(\\mathbf{y}= \\mathbf{m}+ \\mathbf{n}\\) where \\(\\mathbf{P}_\\mathbf{v}\\mathbf{y}= \\mathbf{m}\\) and \\((\\mathbf{I}_n - \\mathbf{P}_\\mathbf{v})\\mathbf{y}= \\mathbf{n}\\) where \\(\\mathbf{m}\\in C(\\mathbf{P}_\\mathbf{V})\\) and \\(\\mathbf{u}\\in N(\\mathbf{P}_\\mathbf{V})\\)\nIs \\(\\mathbf{P}_\\mathbf{V}\\) an orthogonal projection onto \\(C(\\mathbf{X})\\) for the inner product space \\((\\mathbb{R}^n, \\langle \\mathbf{v}, \\mathbf{u}\\rangle = \\mathbf{v}^T\\mathbf{u})\\)?\n\n\n\nDefinition: Oblique ProjectionFor the inner product space \\((\\mathbb{R}^n, \\langle \\mathbf{v}, \\mathbf{u}\\rangle = \\mathbf{v}^T\\mathbf{u})\\), a projection \\(\\mathbf{P}\\) that is not an orthogonal projection is called an oblique projection"
  },
  {
    "objectID": "resources/slides/06-GLS.html#loss-function",
    "href": "resources/slides/06-GLS.html#loss-function",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Loss Function",
    "text": "Loss Function\nThe GLS estimator minimizes the following generalized squared error loss: \\[\\begin{align}\n\\| \\tilde{\\mathbf{Y}}- \\tilde{\\mathbf{X}}\\boldsymbol{\\beta}\\|^2 & = (\\tilde{\\mathbf{Y}}- \\tilde{\\mathbf{X}}\\boldsymbol{\\beta})^T(\\tilde{\\mathbf{Y}}- \\tilde{\\mathbf{X}}\\boldsymbol{\\beta}) \\\\\n                    & = (\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta})^T \\mathbf{V}^{-1/2}\\mathbf{V}^{-1/2}(\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}) \\\\\n                    & = (\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta})^T \\mathbf{V}^{-1}(\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta})  \\\\\n                    & = \\| \\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}\\|^2_{\\mathbf{V}^{-1}}\n\\end{align}\\] where we can change the inner product to be \\[\\langle \\mathbf{u}, \\mathbf{v}\\rangle_{\\mathbf{V}^{-1}} \\equiv \\mathbf{u}^T\\mathbf{V}^{-1} \\mathbf{v}\\]"
  },
  {
    "objectID": "resources/slides/06-GLS.html#orthogonality-in-an-inner-product-space",
    "href": "resources/slides/06-GLS.html#orthogonality-in-an-inner-product-space",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Orthogonality in an Inner Product Space",
    "text": "Orthogonality in an Inner Product Space\n\nDefinition: Orthogonal ProjectonFor an inner product space, (\\(\\mathbb{R}^n, \\langle , \\rangle\\)). The projection \\(\\mathbf{P}\\) is an orthogonal projection if for every vector \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) in \\(\\mathbb{R}^n\\), \\[\n  \\langle \\mathbf{P}\\mathbf{x}, (\\mathbf{I}_n -\\mathbf{P})\\mathbf{y}\\rangle = \\langle (\\mathbf{I}_n - \\mathbf{P}) \\mathbf{x},\\mathbf{P}\\mathbf{y}\\rangle = 0\n\\] Equivalently: \\[\n  \\langle \\mathbf{x},\\mathbf{P}\\mathbf{y}\\rangle = \\langle \\mathbf{P}\\mathbf{x}, \\mathbf{P}\\mathbf{y}\\rangle =\\langle \\mathbf{P}\\mathbf{x},\\mathbf{y}\\rangle\n\\]\n\n\n\nExerciseShow that \\(\\mathbf{P}_\\mathbf{V}\\) is an orthogonal projection under the inner product \\(\\langle \\mathbf{x}, \\mathbf{y}\\rangle_{\\mathbf{V}^{-1}} \\equiv \\mathbf{x}^T\\mathbf{V}^{-1} \\mathbf{y}\\)"
  },
  {
    "objectID": "resources/slides/06-GLS.html#variance-of-gls",
    "href": "resources/slides/06-GLS.html#variance-of-gls",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Variance of GLS",
    "text": "Variance of GLS\n\nVariance of the GLS estimator \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}=  (\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})^{−1}\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{Y}\\) is much simpler \\[\\begin{align}\n\\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}] & = (\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})^{−1}\\mathbf{X}^T\\mathbf{V}^{−1}\\textsf{Cov}[\\mathbf{Y}]\\mathbf{V}^{−1}\\mathbf{X}(\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})^{−1} \\\\\n& = (\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})^{−1}\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{V}\\mathbf{V}^{−1}\\mathbf{X}(\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})^{−1} \\\\\n& = \\sigma^2(\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})^{−1}(\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})(\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})^{−1} \\\\\n& = \\sigma^2(\\mathbf{X}^T\\mathbf{V}^{−1}\\mathbf{X})^{−1}\n\\end{align}\\]\n\n\n\nTheorem: Gauss-Markov-AitkinLet \\(\\tilde{\\boldsymbol{\\beta}}\\) be a linear unbiased estimator of \\(\\boldsymbol{\\beta}\\) and \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\) be the GLS estimator of \\(\\boldsymbol{\\beta}\\) in the linear model \\(\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\) with \\(\\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\) and \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\sigma^2 \\mathbf{V}\\) with \\(\\mathbf{X}\\) and \\(\\mathbf{V}&gt;0\\) known. Then \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\) is the BLUE where \\[\\textsf{Cov}[\\tilde{\\boldsymbol{\\beta}}] \\ge  \\sigma^2 (\\mathbf{X}^T\\mathbf{V}^{-1} \\mathbf{X})^{-1} = \\textsf{Cov}[\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}] \\]"
  },
  {
    "objectID": "resources/slides/06-GLS.html#when-will-ols-and-gls-be-equal",
    "href": "resources/slides/06-GLS.html#when-will-ols-and-gls-be-equal",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "When will OLS and GLS be Equal?",
    "text": "When will OLS and GLS be Equal?\n\nFor what covariance matrices \\(\\mathbf{V}\\) will the OLS and GLS estimators be the same?\nFiguring this out can help us understand why the GLS estimator has a lower variance in general.\n\n\n\nTheoremThe estimators \\(\\hat{\\boldsymbol{\\beta}}\\) (OLS) and \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\) (GLS) are the same for all \\(\\mathbf{Y}\\in \\mathbb{R}^n\\) iff \\[\\mathbf{V}= \\mathbf{X}\\boldsymbol{\\Psi}\\mathbf{X}^T + \\mathbf{H}\\boldsymbol{\\Phi}\\mathbf{H}^T\\] for some positive definite matrices \\(\\boldsymbol{\\Psi}\\) and \\(\\boldsymbol{\\Phi}\\) and a matrix \\(\\mathbf{H}\\) such that \\(\\mathbf{H}^T \\mathbf{X}= \\mathbf{0}\\)."
  },
  {
    "objectID": "resources/slides/06-GLS.html#outline-of-proof",
    "href": "resources/slides/06-GLS.html#outline-of-proof",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Outline of Proof",
    "text": "Outline of Proof\nWe need to show that \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\) are the same for all \\(\\mathbf{Y}\\). Since both \\(\\mathbf{P}\\) and \\(\\mathbf{P}_\\mathbf{V}\\) are projections onto \\(C(\\mathbf{X})\\), \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\) will be the same iff \\(\\mathbf{P}_\\mathbf{V}\\) is an orthogonal projection onto \\(C(\\mathbf{X})\\) so that \\(\\mathbf{P}_\\mathbf{V}\\mathbf{n}= 0\\) for \\(\\mathbf{n}\\in C(\\mathbf{X})^\\perp\\) (they have the same null spaces)\n\nShow that \\(C(\\mathbf{X}) = C(\\mathbf{V}\\mathbf{X})\\) iff \\(\\mathbf{V}\\) can be written as \\[\\mathbf{V}= \\mathbf{X}\\boldsymbol{\\Psi}\\mathbf{X}^T + \\mathbf{H}\\boldsymbol{\\Phi}\\mathbf{H}^T\\] (Show \\(C(\\mathbf{V}\\mathbf{X}) \\subset C( \\mathbf{X})\\) iff \\(\\mathbf{V}\\) has the above form and since the two subspaces have the same rank \\(C(\\mathbf{X}) = C(\\mathbf{V}\\mathbf{X})\\)\nShow that \\(C(\\mathbf{X}) = C(\\mathbf{V}^{-1} \\mathbf{X})\\) iff \\(C(\\mathbf{X}) = C(\\mathbf{V}\\mathbf{X})\\)\nShow that \\(C(\\mathbf{X})^\\perp = C(\\mathbf{V}^{-1} \\mathbf{X})^\\perp\\) iff \\(C(\\mathbf{X}) = C(\\mathbf{V}^{-1} \\mathbf{X})\\)\nShow that \\(\\mathbf{n}\\in C(\\mathbf{X})^\\perp\\) iff \\(\\mathbf{n}\\in C(\\mathbf{V}^{-1}\\mathbf{X})^\\perp\\) so \\(\\mathbf{P}_\\mathbf{V}\\mathbf{n}= 0\\)\n\n\nSee Proposition 2.7.5 and Proof in Christensen"
  },
  {
    "objectID": "resources/slides/06-GLS.html#some-intuition",
    "href": "resources/slides/06-GLS.html#some-intuition",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Some Intuition",
    "text": "Some Intuition\nFor the linear model \\(\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\) with \\(\\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}_n\\) and \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\sigma^2 \\mathbf{V}\\), we can always write\n\\[\\begin{align} \\boldsymbol{\\epsilon}& = \\mathbf{P}\\boldsymbol{\\epsilon}+ (\\mathbf{I}- \\mathbf{P})\\boldsymbol{\\epsilon}\\\\\n                   & = \\boldsymbol{\\epsilon}_\\mathbf{X}+ \\boldsymbol{\\epsilon}_N   \n\\end{align}\\]\n\nwe can recover \\(\\boldsymbol{\\epsilon}_N\\) from the data \\(\\mathbf{Y}\\) but not \\(\\boldsymbol{\\epsilon}_\\mathbf{X}\\): \\[\\begin{align} \\mathbf{P}\\mathbf{Y}& = \\mathbf{P}( \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}_\\mathbf{X}+ \\boldsymbol{\\epsilon}_n )\\\\\n                   & =  \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}_\\mathbf{X}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\\\\n    (\\mathbf{I}_n - \\mathbf{P}) \\mathbf{Y}& =  \\boldsymbol{\\epsilon}_N = \\hat{\\boldsymbol{\\epsilon}} = \\mathbf{e}\n\\end{align}\\]\nCan \\(\\boldsymbol{\\epsilon}_\\textsf{N}\\) help us estimate \\(\\mathbf{X}\\boldsymbol{\\beta}\\)? What if \\(\\boldsymbol{\\epsilon}_N\\) could tell us something about \\(\\boldsymbol{\\epsilon}_X\\)?\nYes if they were highly correlated! But if they were independent or uncorrelated then knowing \\(\\boldsymbol{\\epsilon}_\\textsf{N}\\) doesn’t help us!"
  },
  {
    "objectID": "resources/slides/06-GLS.html#intuition-continued",
    "href": "resources/slides/06-GLS.html#intuition-continued",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Intuition Continued",
    "text": "Intuition Continued\n\nFor what matrices are \\(\\boldsymbol{\\epsilon}_\\mathbf{X}\\) and \\(\\boldsymbol{\\epsilon}_N\\) uncorrelated?\nUnder \\(\\mathbf{V}= \\mathbf{I}_n\\): \\[\\begin{align}\n\\textsf{E}[\\boldsymbol{\\epsilon}_X \\boldsymbol{\\epsilon}_N] & = \\mathbf{P}\\textsf{E}[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T](\\mathbf{I}-\\mathbf{P}) \\\\\n                & = \\sigma^2 \\mathbf{P}(\\mathbf{I}- \\mathbf{P}) = \\mathbf{0}\n\\end{align}\\] so they are uncorrelated\nFor the \\(\\mathbf{V}\\) in the theorem, introduce\n\n\\(\\mathbf{Z}_\\mathbf{X}\\) where \\(\\textsf{E}[\\mathbf{Z}_\\mathbf{X}]= \\mathbf{0}_n\\) and \\(\\textsf{Cov}[\\mathbf{Z}_\\mathbf{X}] = \\boldsymbol{\\Psi}\\)\n\\(\\mathbf{Z}_\\textsf{N}\\) where \\(\\textsf{E}[\\mathbf{Z}_\\textsf{N}]= \\mathbf{0}_n\\) and \\(\\textsf{Cov}[\\mathbf{Z}_\\textsf{N}] = \\boldsymbol{\\Phi}\\)\n\\(\\mathbf{Z}_\\mathbf{X}\\) and \\(\\mathbf{Z}_\\textsf{N}\\) are uncorrelated, \\(\\textsf{E}[\\mathbf{Z}_\\mathbf{X}\\mathbf{Z}_\\textsf{N}] = \\mathbf{0}\\)\n\\(\\boldsymbol{\\epsilon}= \\mathbf{X}\\mathbf{Z}_\\mathbf{X}+ \\mathbf{H}\\mathbf{Z}_\\textsf{N}\\) so that \\(\\boldsymbol{\\epsilon}\\) has the desired mean and covariance \\(\\mathbf{V}\\) in the theorem"
  },
  {
    "objectID": "resources/slides/06-GLS.html#intuition-continued-1",
    "href": "resources/slides/06-GLS.html#intuition-continued-1",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Intuition Continued",
    "text": "Intuition Continued\nAs a consequence we have\n\n\\(\\boldsymbol{\\epsilon}_\\mathbf{X}= \\mathbf{P}\\boldsymbol{\\epsilon}= \\mathbf{X}\\mathbf{Z}_\\mathbf{X}\\)\n\\(\\boldsymbol{\\epsilon}_\\textsf{N}= (\\mathbf{I}_n - \\mathbf{P})\\boldsymbol{\\epsilon}= \\mathbf{H}\\mathbf{Z}_\\textsf{N}\\)\n\\(\\boldsymbol{\\epsilon}_\\mathbf{X}\\) and \\(\\boldsymbol{\\epsilon}_\\textsf{N}\\) are uncorrelated \\[\\begin{align}\n\\textsf{E}[\\boldsymbol{\\epsilon}_\\mathbf{X}\\boldsymbol{\\epsilon}_\\textsf{N}] & = \\textsf{E}[\\mathbf{X}\\mathbf{Z}_\\mathbf{X}\\mathbf{Z}_\\textsf{N}^T \\mathbf{H}^T] \\\\\n                  & = \\mathbf{X}\\mathbf{0}\\mathbf{H}^T \\\\\n                  & = \\mathbf{0}\n\\end{align}\\]\nso that \\(\\boldsymbol{\\epsilon}_\\mathbf{X}\\) and \\(\\boldsymbol{\\epsilon}_\\textsf{N}\\) are uncorrelated with \\(\\mathbf{V}= \\mathbf{X}\\boldsymbol{\\Psi}\\mathbf{X}^T + \\mathbf{H}\\boldsymbol{\\Phi}\\mathbf{H}\\) ^T$\nAlternative Statement of Theorem: \\(\\hat{\\boldsymbol{\\beta}}= \\hat{\\boldsymbol{\\beta}}_\\mathbf{V}\\) for all \\(\\mathbf{Y}\\) under \\(\\textsf{Cov}[\\mathbf{Y}] = \\sigma^2 \\mathbf{V}\\) iff \\(\\mathbf{P}\\mathbf{Y}\\) and \\((\\mathbf{I}- \\mathbf{P})\\mathbf{Y}\\) are uncorrelated"
  },
  {
    "objectID": "resources/slides/06-GLS.html#equivalence-of-gls-estimators",
    "href": "resources/slides/06-GLS.html#equivalence-of-gls-estimators",
    "title": "Generalized Least Squares, BLUES & BUES",
    "section": "Equivalence of GLS estimators",
    "text": "Equivalence of GLS estimators\nThe following corollary to the theorem establishes when two GLS estimators for different \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}]\\) are equivalent :\n\nCorollarySuppose \\(\\mathbf{V}= \\mathbf{X}\\boldsymbol{\\Psi}\\mathbf{X}^ T + \\boldsymbol{\\Phi}\\mathbf{H}\\boldsymbol{\\Omega}\\mathbf{H}^T \\boldsymbol{\\Phi}\\). Then \\(\\hat{\\boldsymbol{\\beta}}_\\mathbf{V}= \\hat{\\boldsymbol{\\beta}}_\\boldsymbol{\\Phi}\\)\n\n\n\nCan you construct an equivalent representation based on zero correlation of \\(\\mathbf{P}_\\boldsymbol{\\Phi}\\mathbf{Y}\\) and \\((\\mathbf{I}_n - \\mathbf{P}_\\boldsymbol{\\Phi})\\mathbf{Y}\\) when \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] = \\sigma^2 \\mathbf{V}?\\)\n\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#outline",
    "href": "resources/slides/11-shrinkage.html#outline",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Outline",
    "text": "Outline\n\nLasso\nBayesian Lasso\n\n\n\nReadings (see reading link)\n\nSeber & Lee Chapter Chapter 12\nTibshirani (JRSS B 1996)\nPark & Casella (JASA 2008)\nHans (Biometrika 2010)\nCarvalho, Polson & Scott (Biometrika 2010)"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#lasso-estimator",
    "href": "resources/slides/11-shrinkage.html#lasso-estimator",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "LASSO Estimator",
    "text": "LASSO Estimator\n\n\nTibshirani (JRSS B 1996) proposed estimating coefficients through \\(L_1\\) constrained least squares via the Least Absolute Shrinkage and Selection Operator or lasso \\[\\hat{\\boldsymbol{\\beta}}_{L} = \\mathop{\\mathrm{argmin}}_{\\boldsymbol{\\beta}} \\left\\{ \\|\\mathbf{Y}_c - \\mathbf{X}_s \\boldsymbol{\\beta}\\|^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_1 \\right\\}\\]\n\n\\(\\mathbf{Y}_c\\) is the centered \\(\\mathbf{Y}\\), \\(\\mathbf{Y}_c =    \\mathbf{Y}- \\bar{\\mathbf{Y}} \\mathbf{1}\\)\n\\(\\mathbf{X}_s\\) is the centered and standardized \\(\\mathbf{X}\\) matrix so that the diagonal elements of \\(\\mathbf{X}_s^T\\mathbf{X}_s = c\\).\nuse the scale function but standardization usually handled within packages\n\n\n\n\nControl how large coefficients may grow \\[\\arg \\min_{\\boldsymbol{\\beta}} (\\mathbf{Y}_c - \\mathbf{X}_s \\boldsymbol{\\beta})^T (\\mathbf{Y}_c - \\mathbf{X}_s\\boldsymbol{\\beta})\\]\n\\[ \\text{ subject to } \\sum |\\beta_j| \\le t\\]\n\n\n\nimage from Machine Learning with R"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#lasso-solutions",
    "href": "resources/slides/11-shrinkage.html#lasso-solutions",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Lasso Solutions",
    "text": "Lasso Solutions\nThe entire path of solutions can be easily found using the ``Least Angle Regression’’ Algorithm of Efron et al (Annals of Statistics 2004)\n\nlibrary(lars); datasets::longley\nlongley.lars = lars(as.matrix(longley[,-7]), longley[,7], type=\"lasso\")"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#coefficients",
    "href": "resources/slides/11-shrinkage.html#coefficients",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Coefficients",
    "text": "Coefficients\n\nround(coef(longley.lars),4)\n\n      GNP.deflator     GNP Unemployed Armed.Forces Population   Year\n [1,]       0.0000  0.0000     0.0000       0.0000     0.0000 0.0000\n [2,]       0.0000  0.0327     0.0000       0.0000     0.0000 0.0000\n [3,]       0.0000  0.0362    -0.0037       0.0000     0.0000 0.0000\n [4,]       0.0000  0.0372    -0.0046      -0.0010     0.0000 0.0000\n [5,]       0.0000  0.0000    -0.0124      -0.0054     0.0000 0.9068\n [6,]       0.0000  0.0000    -0.0141      -0.0071     0.0000 0.9438\n [7,]       0.0000  0.0000    -0.0147      -0.0086    -0.1534 1.1843\n [8,]      -0.0077  0.0000    -0.0148      -0.0087    -0.1708 1.2289\n [9,]       0.0000 -0.0121    -0.0166      -0.0093    -0.1303 1.4319\n[10,]       0.0000 -0.0253    -0.0187      -0.0099    -0.0951 1.6865\n[11,]       0.0151 -0.0358    -0.0202      -0.0103    -0.0511 1.8292"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#selecting-a-solution-from-the-path",
    "href": "resources/slides/11-shrinkage.html#selecting-a-solution-from-the-path",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Selecting a Solution from the Path",
    "text": "Selecting a Solution from the Path\n\n\n\nsummary(longley.lars)\n\nLARS/LASSO\nCall: lars(x = as.matrix(longley[, -7]), y = longley[, 7], type = \"lasso\")\n   Df     Rss        Cp\n0   1 185.009 1976.7120\n1   2   6.642   59.4712\n2   3   3.883   31.7832\n3   4   3.468   29.3165\n4   5   1.563   10.8183\n5   4   1.339    6.4068\n6   5   1.024    5.0186\n7   6   0.998    6.7388\n8   7   0.907    7.7615\n9   6   0.847    5.1128\n10  7   0.836    7.0000\n\n\n\n\nFor \\(p\\) predictors, \\[C_p = \\frac{\\textsf{SSE}_p}{s^2} -n + 2p\\]\n\\(s^2\\) is the residual variance from the full model\n\\(\\textsf{SSE}_p\\) is the sum of squared errors for the model with \\(p\\) predictors (RSS)\nif the model includes all the predictors with non-zero coefficients, then \\(C_p \\approx p\\)\nchoose minimum \\(C_p \\approx p\\)\nin practice use Cross-validation or Generalized Cross Validation (GCV) to choose \\(\\lambda\\)"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#features-and-issues",
    "href": "resources/slides/11-shrinkage.html#features-and-issues",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Features and Issues",
    "text": "Features and Issues\n\nCombines shrinkage (like Ridge Regression) with Variable Selection to deal with collinearity\nCan be used for prediction or variable selection\nnot invariant under linear transformations of the predictors\ntypically no uncertainty estimates for the coefficients or predictions\nignores uncertainty in the choice of \\(\\lambda\\)\nmay overshrink large coefficients"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#bayesian-lasso",
    "href": "resources/slides/11-shrinkage.html#bayesian-lasso",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Bayesian LASSO",
    "text": "Bayesian LASSO\n\nEquivalent to finding posterior mode with a Double Laplace Prior \\[\n\\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\beta}} -\\frac{\\phi}{2} \\{ \\| \\mathbf{Y}_c - \\mathbf{X}_s \\boldsymbol{\\beta}\\|^2 + \\lambda^* \\|\\boldsymbol{\\beta}\\|_1 \\}\n\\]\nPark & Casella (JASA 2008) and Hans (Biometrika 2010) propose Bayesian versions of the Lasso \\[\\begin{eqnarray*}\n\\mathbf{Y}\\mid \\alpha, \\boldsymbol{\\beta}, \\phi & \\sim & \\textsf{N}(\\mathbf{1}_n \\alpha + \\mathbf{X}^s \\boldsymbol{\\beta}^s, \\mathbf{I}_n/\\phi) \\\\\n\\boldsymbol{\\beta}\\mid \\alpha, \\phi, \\boldsymbol{\\tau}& \\sim & \\textsf{N}(\\mathbf{0}, \\textsf{diag}(\\boldsymbol{\\tau}^2)/\\phi)  \\\\\n\\tau_1^2 \\ldots, \\tau_p^2 \\mid \\alpha, \\phi & \\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}& \\textsf{Exp}(\\lambda^2/2)  \\\\\n  p(\\alpha, \\phi) & \\propto& 1/\\phi  \\\\\n\\end{eqnarray*}\\]\nGeneralizes Ridge Priors to allow different prior variances for each coefficient"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#double-exponential-or-double-laplace-prior",
    "href": "resources/slides/11-shrinkage.html#double-exponential-or-double-laplace-prior",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Double Exponential or Double Laplace Prior",
    "text": "Double Exponential or Double Laplace Prior\n\nMarginal distribution of \\(\\beta_j\\) \\[\\begin{eqnarray*}\n\\boldsymbol{\\beta}\\mid \\alpha, \\phi, \\boldsymbol{\\tau}& \\sim & \\textsf{N}(\\mathbf{0}, \\textsf{diag}(\\boldsymbol{\\tau}^2)/\\phi)  \\\\\n\\tau_1^2 \\ldots, \\tau_p^2 \\mid \\alpha, \\phi & \\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}& \\textsf{Exp}(\\lambda^2/2)  \\\\\np(\\beta_j \\mid \\phi, \\lambda) & = & \\int_0^\\infty p(\\beta_i \\mid \\phi, \\tau^2_j) p(\\tau^2_j \\mid \\phi, \\lambda) \\, d\\tau^2 \\\\\n\\end{eqnarray*}\\]\nCan show that \\(\\beta_j \\mid \\phi, \\lambda \\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}DE(\\lambda \\sqrt{\\phi})\\) \\[\\int_0^\\infty \\frac{1}{\\sqrt{2 \\pi t}}\ne^{-\\frac{1}{2} \\phi \\frac{\\beta^2}{t }}\n\\, \\frac{\\lambda^2}{2} e^{- \\frac{\\lambda^2 t}{2}}\\, dt =\n\\frac{\\lambda \\phi^{1/2}}{2} e^{-\\lambda \\phi^{1/2} |\\beta|}\n\\]\nScale Mixture of Normals (Andrews and Mallows 1974)"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#gibbs-sampler",
    "href": "resources/slides/11-shrinkage.html#gibbs-sampler",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Gibbs Sampler",
    "text": "Gibbs Sampler\n\nIntegrate out \\(\\alpha\\): \\(\\alpha \\mid \\mathbf{Y}, \\phi \\sim \\textsf{N}(\\bar{y},\n  1/(n \\phi)\\)\n\n\\(\\boldsymbol{\\beta}\\mid \\boldsymbol{\\tau}, \\phi, \\lambda, \\mathbf{Y}_c \\sim \\textsf{N}(, )\\)\n\n\\(\\phi \\mid \\boldsymbol{\\tau}, \\boldsymbol{\\beta}, \\lambda, \\mathbf{Y}_c \\sim \\mathbf{G}( , )\\)\n\n\\(1/\\tau_j^2 \\mid \\boldsymbol{\\beta}, \\phi, \\lambda, \\mathbf{Y}\\sim \\textsf{InvGaussian}(\n, )\\)\n\\(X \\sim \\textsf{InvGaussian}(\\mu,  \\lambda)\\) has density \\[\nf(x) =  \\sqrt{\\frac{\\lambda^2}{2 \\pi}}  x^{-3/2} e^{- \\frac{1}{2} \\frac{\n  \\lambda^2( x - \\mu)^2} {\\mu^2 x}} \\qquad x &gt; 0\n\\]\nHomework: Derive the full conditionals for \\(\\boldsymbol{\\beta}^s\\), \\(\\phi\\), \\(1/\\tau^2\\)\nsee Casella & Park"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#horseshoe-priors",
    "href": "resources/slides/11-shrinkage.html#horseshoe-priors",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Horseshoe Priors",
    "text": "Horseshoe Priors\nCarvalho, Polson & Scott (2010) propose an alternative shrinkage prior \\[\\begin{align*}\n\\boldsymbol{\\beta}\\mid \\phi & \\sim \\textsf{N}(\\mathbf{0}_p, \\frac{\\textsf{diag}(\\tau^2)}{ \\phi\n    }) \\\\\n\\tau \\mid \\lambda & \\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}C^+(0, \\lambda) \\\\\n\\lambda & \\sim \\textsf{C}^+(0, 1/\\phi) \\\\\np(\\alpha, \\phi) & \\propto 1/\\phi\n\\end{align*}\\]\n\n\\(C^+(0, \\lambda)\\) is the half-Cauchy distribution with scale \\(\\lambda\\) \\[\np(\\tau \\mid \\lambda) = \\frac{2}{\\pi} \\frac{\\lambda}{\\lambda^2 + \\tau_j^2}\n\\]\n\\(\\textsf{C}^+(0, 1/\\phi)\\) is the half-Cauchy distribution with scale \\(1/\\phi\\)"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#special-case",
    "href": "resources/slides/11-shrinkage.html#special-case",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Special Case",
    "text": "Special Case\n\n\nIn the case \\(\\lambda = \\phi = 1\\) and with \\(\\mathbf{X}^t\\mathbf{X}= \\mathbf{I}\\), \\(\\mathbf{Y}^* =\n\\mathbf{X}^T\\mathbf{Y}\\) \\[\\begin{align*}\nE[\\beta_i \\mid \\mathbf{Y}] & = \\textsf{E}_{\\kappa_i \\mid \\mathbf{Y}}[ \\textsf{E}_{\\beta_i \\mid \\kappa_i, \\mathbf{Y}}[\\beta_i \\mid \\mathbf{Y}] \\\\\n& = \\int_0^1 (1 - \\kappa_i) y^*_i p(\\kappa_i \\mid \\mathbf{Y})\n\\ d\\kappa_i \\\\\n& = (1 - \\textsf{E}[\\kappa \\mid y^*_i]) y^*_i\n\\end{align*}\\] where \\(\\kappa_i = 1/(1 + \\tau_i^2)\\) is the shrinkage factor (like in James-Stein)\n\nHalf-Cauchy prior induces a Beta(1/2, 1/2) distribution on \\(\\kappa_i\\) a priori (change of variables)"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#features-and-issues-1",
    "href": "resources/slides/11-shrinkage.html#features-and-issues-1",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Features and Issues",
    "text": "Features and Issues\n\n\n\nthe posterior mode also induces shrinkage and variable selection if the mode is at zero\nthe posterior mean is a shrinkage estimator (no selection)\nthe tails of the distribution are heavier than the Laplace prior (like a Cauchy distribution) so that there is less shrinkage of large \\(|\\hat{\\boldsymbol{\\beta}}|\\).\nDesirable in the orthogonal case, where lasso is more like ridge regression (related to bounded influence)\nMCMC is slow to mix using programs like stan but specialized R packages like horseshoe and monomvn::bhsare available"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#bounded-influence-and-posterior-mean",
    "href": "resources/slides/11-shrinkage.html#bounded-influence-and-posterior-mean",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Bounded Influence and Posterior Mean",
    "text": "Bounded Influence and Posterior Mean\n\n\nPosterior mean of \\(\\beta\\) may also be written as \\[E[\\beta_i \\mid y^*_i] = y^*_i + \\frac{d} {d y} \\log m(y^*_i)\\] where \\(m(y)\\) is the predictive density \\(y^*_i\\) under the prior (known \\(\\lambda\\))\n\nHS has Bounded Influence: \\[\\lim_{|y| \\to \\infty} \\frac{d}{dy} \\log m(y) = 0\\]\n\\(\\lim_{|y_i^*| \\to \\infty} E[\\beta_i \\mid y^*_i) \\to y^*_i\\) (the MLE)\nsince the MLE \\(\\to \\beta_i^*\\) as \\(n \\to \\infty\\), the HS is asymptotically consistent\n\n\n\n\nthe DE also has bounded influence, but the bound does not decay to zero in tails so that the posterior mean does not shrink to the MLE"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#comparison",
    "href": "resources/slides/11-shrinkage.html#comparison",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Comparison",
    "text": "Comparison\n\n\n\nDiabetes data (from the lars package)\n64 predictors: 10 main effects, 2-way interactions and quadratic terms\n\n\nsample size of 442\nsplit into training and test sets\ncompare MSE for out-of-sample prediction using OLS, lasso and horseshoe priors\nRoot MSE for prediction for left out data based on 25 different random splits with 100 test cases"
  },
  {
    "objectID": "resources/slides/11-shrinkage.html#summary",
    "href": "resources/slides/11-shrinkage.html#summary",
    "title": "Shrinkage Estimators and Hierarchical Bayes",
    "section": "Summary",
    "text": "Summary\nThe literature on shrinkage estimators (with or without selection) is vast\n\nElastic Net (Zou & Hastie 2005)\nSCAD (Fan & Li 2001)\nGeneralized Double Pareto Prior (Armagan, Dunson & Lee 2013)\nSpike-and-Slab Lasso (Rockova & George 2018)\n\n\nFor Bayes, choice of estimator\n\nposterior mean (easy via MCMC)\nposterior mode (optimization)\nposterior median (via MCMC)\n\n\n\nProperties?\n\nFan & Li (JASA 2001) discuss variable selection via non-concave penalties and oracle properties (next time …)\n\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/09-bayes-freq-risk.html#outline",
    "href": "resources/slides/09-bayes-freq-risk.html#outline",
    "title": "Bayesian Estimation and Frequentist Risk",
    "section": "Outline",
    "text": "Outline\n\nFrequentist Risk of Bayes estimators\nBayes and Penalized Loss Functions\nGeneralized Ridge Regression\nHierarchical Bayes and Other Penalties\n\n\nReadings:\n\nChristensen Chapter 2.9 and Chapter 15\nSeber & Lee Chapter 10.7.3 and Chapter 12"
  },
  {
    "objectID": "resources/slides/09-bayes-freq-risk.html#frequentist-risk-of-bayes-estimators",
    "href": "resources/slides/09-bayes-freq-risk.html#frequentist-risk-of-bayes-estimators",
    "title": "Bayesian Estimation and Frequentist Risk",
    "section": "Frequentist Risk of Bayes Estimators",
    "text": "Frequentist Risk of Bayes Estimators\nQuadratic loss for estimating \\(\\boldsymbol{\\beta}\\) using estimator \\(\\mathbf{a}\\) \\[ L(\\boldsymbol{\\beta}, \\mathbf{a}) =  ( \\boldsymbol{\\beta}- \\mathbf{a})^T(\\boldsymbol{\\beta}-\\mathbf{a})\\]\n\nConsider our expected loss (before we see the data) of taking an ``action’’ \\(\\mathbf{a}\\) (i.e. reporting \\(\\mathbf{a}\\) as the estimate of \\(\\boldsymbol{\\beta}\\)) \\[ \\textsf{E}_{\\mathbf{Y}\\mid \\boldsymbol{\\beta}}[L(\\boldsymbol{\\beta}, \\mathbf{a})] =  \\textsf{E}_{\\mathbf{Y}\\mid \\boldsymbol{\\beta}}[( \\boldsymbol{\\beta}- \\mathbf{a})^T(\\boldsymbol{\\beta}-\\mathbf{a})]\\] where the expectation is over the data \\(\\mathbf{Y}\\) given the true value of \\(\\boldsymbol{\\beta}\\)."
  },
  {
    "objectID": "resources/slides/09-bayes-freq-risk.html#expectation-of-quadratic-forms",
    "href": "resources/slides/09-bayes-freq-risk.html#expectation-of-quadratic-forms",
    "title": "Bayesian Estimation and Frequentist Risk",
    "section": "Expectation of Quadratic Forms",
    "text": "Expectation of Quadratic Forms\n\nTheorem: Christensen Thm 1.3.2If \\(\\mathbf{W}\\) is a random variable with mean \\(\\boldsymbol{\\mu}\\) and covariance matrix \\(\\boldsymbol{\\Sigma}\\) then \\[\\textsf{E}[\\mathbf{W}^T\\mathbf{A}\\mathbf{W}] = \\textsf{tr}(\\mathbf{A}\\boldsymbol{\\Sigma}) + \\boldsymbol{\\mu}^T\\mathbf{A}\\boldsymbol{\\mu}\\]\n\n\n\n\nProof\\[\\begin{eqnarray*}\n(\\mathbf{W}- \\boldsymbol{\\mu})^T\\mathbf{A}(\\mathbf{W}- \\boldsymbol{\\mu}) & = & \\mathbf{W}^T\\mathbf{A}\\mathbf{W}- 2\\boldsymbol{\\mu}^T\\mathbf{A}\\mathbf{W}+ \\boldsymbol{\\mu}^T\\mathbf{A}\\boldsymbol{\\mu}\\\\\n\\textsf{E}[(\\mathbf{W}- \\boldsymbol{\\mu})^T\\mathbf{A}(\\mathbf{W}- \\boldsymbol{\\mu})] & = & \\textsf{E}[\\mathbf{W}^T\\mathbf{A}\\mathbf{W}]  - 2 \\boldsymbol{\\mu}^T\\mathbf{A}\\textsf{E}[\\mathbf{W}] + \\boldsymbol{\\mu}^T\\mathbf{A}\\boldsymbol{\\mu}\n\\end{eqnarray*}\\]\nRearranging we have \\[\\textsf{E}[\\mathbf{W}^T\\mathbf{A}\\mathbf{W}] =  \\textsf{E}[(\\mathbf{W}- \\boldsymbol{\\mu})^T\\mathbf{A}(\\mathbf{W}- \\boldsymbol{\\mu})] + \\boldsymbol{\\mu}^T\\mathbf{A}\\boldsymbol{\\mu}\\]"
  },
  {
    "objectID": "resources/slides/09-bayes-freq-risk.html#steps-to-evaluate-frequentist-risk",
    "href": "resources/slides/09-bayes-freq-risk.html#steps-to-evaluate-frequentist-risk",
    "title": "Bayesian Estimation and Frequentist Risk",
    "section": "Steps to Evaluate Frequentist Risk",
    "text": "Steps to Evaluate Frequentist Risk\n\nMSE: \\(\\textsf{E}_\\mathbf{Y}[( \\boldsymbol{\\beta}- \\mathbf{a})^T(\\boldsymbol{\\beta}-\\mathbf{a}) = \\textsf{tr}(\\boldsymbol{\\Sigma}_\\mathbf{a}) + (\\boldsymbol{\\beta}- \\textsf{E}_{\\mathbf{Y}\\mid \\boldsymbol{\\beta}}[\\mathbf{a}])^T(\\boldsymbol{\\beta}- \\textsf{E}_{\\mathbf{Y}\\mid \\boldsymbol{\\beta}}[\\mathbf{a}])\\)\nBias of \\(\\mathbf{a}\\): \\(\\textsf{E}_{\\mathbf{Y}\\mid \\boldsymbol{\\beta}}[\\mathbf{a}- \\boldsymbol{\\beta}] = \\textsf{E}_{\\mathbf{Y}\\mid \\boldsymbol{\\beta}}[\\mathbf{a}] - \\boldsymbol{\\beta}\\)\nCovariance of \\(\\mathbf{a}\\): \\(\\textsf{Cov}_{\\mathbf{Y}\\mid \\boldsymbol{\\beta}}[\\mathbf{a}- \\textsf{E}[\\mathbf{a}]\\)\nMultivariate analog of MSE = Bias\\(^2\\) + Variance in the univariate case"
  },
  {
    "objectID": "resources/slides/09-bayes-freq-risk.html#mean-square-error-of-ols-estimator",
    "href": "resources/slides/09-bayes-freq-risk.html#mean-square-error-of-ols-estimator",
    "title": "Bayesian Estimation and Frequentist Risk",
    "section": "Mean Square Error of OLS Estimator",
    "text": "Mean Square Error of OLS Estimator\n\nMSE of OLS \\(\\textsf{E}_\\mathbf{Y}[( \\boldsymbol{\\beta}- \\hat{\\boldsymbol{\\beta}})^T(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})\\)\nOLS is unbiased os mean of \\(\\boldsymbol{\\beta}- \\hat{\\boldsymbol{\\beta}}\\) is \\(\\mathbf{0}_p\\)\ncovariance is \\(\\textsf{Cov}[\\boldsymbol{\\beta}- \\hat{\\boldsymbol{\\beta}}] = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1}\\) \\[\\begin{eqnarray*}\n\\textsf{MSE}(\\boldsymbol{\\beta}) \\equiv \\textsf{E}_\\mathbf{Y}[( \\boldsymbol{\\beta}- \\hat{\\boldsymbol{\\beta}})^T(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}}) & = &\\sigma^2\n\\textsf{tr}[(\\mathbf{X}^T\\mathbf{X})^{-1}]  \\\\\n& = & \\sigma^2 \\textsf{tr}\\mathbf{U}\\Lambda^{-1} \\mathbf{U}^T \\\\\n& = & \\sigma^2 \\sum_{j=1}^p \\lambda_j^{-1}\n\\end{eqnarray*}\\] where \\(\\lambda_j\\) are eigenvalues of \\(\\mathbf{X}^T\\mathbf{X}\\).\nIf smallest \\(\\lambda_j \\to 0\\) then MSE \\(\\to \\infty\\)"
  },
  {
    "objectID": "resources/slides/09-bayes-freq-risk.html#mean-square-error-using-the-g-prior",
    "href": "resources/slides/09-bayes-freq-risk.html#mean-square-error-using-the-g-prior",
    "title": "Bayesian Estimation and Frequentist Risk",
    "section": "Mean Square Error using the \\(g\\)-prior",
    "text": "Mean Square Error using the \\(g\\)-prior\n\nposterior mean is \\(\\hat{\\boldsymbol{\\beta}}_g = \\frac{g}{1+g} \\hat{\\boldsymbol{\\beta}}\\) (minimizes Bayes risk under squared error loss)\nbias of \\(\\hat{\\boldsymbol{\\beta}}_g\\): \\[\\begin{align*}\n\\textsf{E}_{\\mathbf{Y}\\mid \\boldsymbol{\\beta}}[\\boldsymbol{\\beta}- \\hat{\\boldsymbol{\\beta}}_g] & = \\boldsymbol{\\beta}\\left(1 - \\frac{g}{1+g}\\right)  = \\frac{1}{1+g} \\boldsymbol{\\beta}\n\\end{align*}\\]\ncovariance of \\(\\hat{\\boldsymbol{\\beta}}_g\\): \\(\\textsf{Cov}(\\hat{\\boldsymbol{\\beta}}_g) = \\frac{g^2}{(1+g)^2} \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1}\\)\nMSE of \\(\\hat{\\boldsymbol{\\beta}}_g\\): \\[\\begin{align*}\n\\textsf{MSE}(\\boldsymbol{\\beta}) = \\frac{g^2}{(1+g)^2} \\sigma^2 \\textsf{tr}(\\mathbf{X}^T\\mathbf{X})^{-1} + \\frac{1}{(1+g)^2} \\|\\boldsymbol{\\beta}\\|^2 \\\\\n= \\frac{1}{(1+g)^2} \\left( g^2 \\sigma^2 \\sum_{j=1}^p\\lambda_j^{-1} + \\|\\boldsymbol{\\beta}\\|^2 \\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "resources/slides/09-bayes-freq-risk.html#can-bayes-estimators-have-smaller-mse-than-ols",
    "href": "resources/slides/09-bayes-freq-risk.html#can-bayes-estimators-have-smaller-mse-than-ols",
    "title": "Bayesian Estimation and Frequentist Risk",
    "section": "Can Bayes Estimators have smaller MSE than OLS?",
    "text": "Can Bayes Estimators have smaller MSE than OLS?\n\nMSE of OLS is \\(\\textsf{E}_\\mathbf{Y}[( \\boldsymbol{\\beta}- \\hat{\\boldsymbol{\\beta}})^T(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}}) = \\sigma^2\n  \\textsf{tr}[(\\mathbf{X}^T\\mathbf{X})^{-1}]\\) (OLS has minimum MSE under squared error loss out of all unbiased estimators)\nMSE of \\(g\\)-prior estimator is\n\\[\\textsf{MSE}_g(\\boldsymbol{\\beta}) = \\frac{1}{(1+g)^2} \\left( g^2 \\sigma^2 \\textsf{tr}[(\\mathbf{X}^T\\mathbf{X})^{-1}] + \\|\\boldsymbol{\\beta}\\|^2 \\right)\\]\nfor fixed \\(\\boldsymbol{\\beta}\\), what values of \\(g\\) is the MSE of \\(\\hat{\\boldsymbol{\\beta}}_g\\) lower than that of \\(\\hat{\\boldsymbol{\\beta}}\\)?\nfor fixed \\(g\\), what values of \\(\\boldsymbol{\\beta}\\) is the MSE of \\(\\hat{\\boldsymbol{\\beta}}_g\\) lower than that of \\(\\hat{\\boldsymbol{\\beta}}\\)?\nis there a value of \\(g\\) that minimizes the MSE of \\(\\hat{\\boldsymbol{\\beta}}_g\\)?\nwhat is the MSE of \\(\\hat{\\boldsymbol{\\beta}}_g\\) under the “optimal” \\(g\\)?\nis the MSE of \\(\\hat{\\boldsymbol{\\beta}}_g\\) using the “optimal” \\(g\\) always lower than that of \\(\\hat{\\boldsymbol{\\beta}}\\)?"
  },
  {
    "objectID": "resources/slides/09-bayes-freq-risk.html#mean-square-error-under-ridge-priors",
    "href": "resources/slides/09-bayes-freq-risk.html#mean-square-error-under-ridge-priors",
    "title": "Bayesian Estimation and Frequentist Risk",
    "section": "Mean Square Error under Ridge Priors",
    "text": "Mean Square Error under Ridge Priors\n\nMSE with OLS and \\(g\\)-prior estimators depend on the eigenvalues of \\(\\mathbf{X}^T\\mathbf{X}\\) and can be infinite if the smallest eigenvalue is zero.\nRidge regression estimator \\(\\hat{\\boldsymbol{\\beta}}_\\kappa = (\\mathbf{X}^T\\mathbf{X}+  \\kappa \\mathbf{I}_p)^{-1} \\mathbf{X}^T\\mathbf{Y}\\) has finite MSE for all \\(\\kappa &gt; 0\\). (\\(k = 0\\) is OLS)\nMSE of Ridge estimator \\(\\textsf{E}_{\\mathbf{Y}\\mid \\boldsymbol{\\beta}}[( \\boldsymbol{\\beta}- \\hat{\\boldsymbol{\\beta}}_\\kappa)^T(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}}_\\kappa) = \\textsf{E}[(\\boldsymbol{\\alpha}- \\mathbf{a})^T(\\boldsymbol{\\alpha}- \\mathbf{a})]\\)\nbias of \\(a_j = \\frac{\\lambda_j}{\\lambda_j + \\kappa} \\hat{\\alpha}_j\\) is \\(\\frac{\\kappa}{\\lambda_j + \\kappa} {\\alpha}_j\\)\nvariance \\(a_j = \\sigma^2 \\frac{\\lambda_j^2}{(\\lambda_j + \\kappa)^2}\\) \\[\\textsf{MSE}_R = \\sigma^2 \\sum_{j=1}^p \\frac{\\lambda_j^2}{(\\lambda_j + \\kappa)^2} + \\sum_{j=1}^p \\frac{\\kappa^2}{(\\lambda_j + \\kappa)^2} \\alpha_j^2\\]\ncan show that the deriviate of the \\(\\textsf{MSE}_R\\) with respect to \\(\\kappa\\) is negative at \\(k = 0\\) so that there exists a \\(\\kappa\\) so the MSE of the Ridge estimator is always less than that of OLS."
  },
  {
    "objectID": "resources/slides/09-bayes-freq-risk.html#penalized-regression",
    "href": "resources/slides/09-bayes-freq-risk.html#penalized-regression",
    "title": "Bayesian Estimation and Frequentist Risk",
    "section": "Penalized Regression",
    "text": "Penalized Regression\n\nRidge regression is a special case of penalized regression where the penalty is \\(\\kappa \\|\\boldsymbol{\\beta}\\|^2\\) for some \\(\\kappa &gt; 0\\). (let \\(\\kappa^* = \\kappa/\\phi\\))\nposterior mode maximizes the posterior density or log posterior density \\[\\begin{align*}\n\\hat{\\boldsymbol{\\beta}}_R = \\arg \\max_{\\boldsymbol{\\beta}} \\cal{L}(\\boldsymbol{\\beta}) & = \\log p(\\boldsymbol{\\beta}\\mid \\mathbf{Y}) \\propto \\log p(\\mathbf{Y}\\mid \\boldsymbol{\\beta}) + \\log p(\\boldsymbol{\\beta}) \\\\\n& = -\\frac{\\phi}{2} \\|\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}\\|^2 - \\frac{\\kappa}{2} \\|\\boldsymbol{\\beta}\\|^2 \\\\\n& = -\\frac{\\phi}{2} \\left( \\|\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}\\|^2 + \\kappa^* \\|\\boldsymbol{\\beta}\\|^2 \\right)\n\\end{align*}\\]\nmaximizing the posterior mode is equivalent to minimizing the penalized loss function \\[\\begin{align*}\n\\hat{\\boldsymbol{\\beta}}_R & =  \\arg \\max_{\\boldsymbol{\\beta}} -\\left(\\|\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}\\|^2 + \\kappa^* \\|\\boldsymbol{\\beta}\\|^2 \\right) \\\\\n& = \\arg \\min_{\\boldsymbol{\\beta}} \\left(\\|\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}\\|^2 + \\kappa^* \\|\\boldsymbol{\\beta}\\|^2 \\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "resources/slides/09-bayes-freq-risk.html#scaling-and-centering",
    "href": "resources/slides/09-bayes-freq-risk.html#scaling-and-centering",
    "title": "Bayesian Estimation and Frequentist Risk",
    "section": "Scaling and Centering",
    "text": "Scaling and Centering\nNote: usually use Ridge regression after centering and scaling the columns of \\(\\mathbf{X}\\) so that the penalty is the same for all variables. \\(\\mathbf{Y}_c = (\\mathbf{I}- \\mathbf{P}_1) \\mathbf{Y}\\) and \\(X_c\\) the centered and standardized \\(\\mathbf{X}\\) matrix\n\nalternatively as a prior, we are assuming that the \\(\\boldsymbol{\\beta}_j\\) are iid \\(\\textsf{N}(0, 1/\\kappa^*)\\) so that the prior for \\(\\boldsymbol{\\beta}\\) is \\(\\textsf{N}(\\mathbf{0}_p, \\mathbf{I}_p/\\kappa^* )\\)\nif the units/scales of the variables are different, then the variance or penality should be different for each variable.\nstandardizing the \\(\\mathbf{X}\\) so that \\(\\mathbf{X}_c^T\\mathbf{X}_c\\) is a constant times the correlation matrix of \\(\\mathbf{X}\\) ensures that all \\(\\boldsymbol{\\beta}\\)’s have the same scale\ncentering the data forces the intercept to be 0 (so no shrinkage or penality)"
  },
  {
    "objectID": "resources/slides/09-bayes-freq-risk.html#alternative-motivation",
    "href": "resources/slides/09-bayes-freq-risk.html#alternative-motivation",
    "title": "Bayesian Estimation and Frequentist Risk",
    "section": "Alternative Motivation",
    "text": "Alternative Motivation\n\nIf \\(\\hat{\\boldsymbol{\\beta}}\\) is unconstrained expect high variance with nearly singular \\(\\mathbf{X}_c\\) \nControl how large coefficients may grow \\[\\arg \\min_{\\boldsymbol{\\beta}} (\\mathbf{Y}_c - \\mathbf{X}_c \\boldsymbol{\\beta})^T (\\mathbf{Y}_c - \\mathbf{X}_c\\boldsymbol{\\beta})\\] subject to \\[ \\sum \\beta_j^2 \\le t\\]\nEquivalent Quadratic Programming Problem \\[\\hat{\\boldsymbol{\\beta}}_{R} = \\arg \\min_{\\boldsymbol{\\beta}} \\| \\mathbf{Y}_c - \\mathbf{X}_c \\boldsymbol{\\beta}\\|^2 + \\kappa^* \\|\\boldsymbol{\\beta}\\|^2\\]\ndifferent approaches to selecting \\(\\kappa^*\\) from frequentist ane Bayesian perspectives"
  },
  {
    "objectID": "resources/slides/09-bayes-freq-risk.html#plot-of-constrained-problem",
    "href": "resources/slides/09-bayes-freq-risk.html#plot-of-constrained-problem",
    "title": "Bayesian Estimation and Frequentist Risk",
    "section": "Plot of Constrained Problem",
    "text": "Plot of Constrained Problem"
  },
  {
    "objectID": "resources/slides/09-bayes-freq-risk.html#generalized-ridge-regression",
    "href": "resources/slides/09-bayes-freq-risk.html#generalized-ridge-regression",
    "title": "Bayesian Estimation and Frequentist Risk",
    "section": "Generalized Ridge Regression",
    "text": "Generalized Ridge Regression\n\nrather than a common penalty for all variables, consider a different penalty for each variable\nas a prior, we are assuming that the \\(\\boldsymbol{\\beta}_j\\) are iid \\(\\textsf{N}(0, \\frac{\\kappa_j}{\\phi})\\) so that the prior for \\(\\boldsymbol{\\beta}\\) is \\(\\textsf{N}(\\mathbf{0}_p, \\phi^{-1} \\mathbf{K}^{-1})\\) where \\(\\mathbf{K}= \\textsf{diag}(\\kappa_1, \\ldots, \\kappa_p)\\)\nhard enough to choose a single penalty, how to choose \\(p\\) penalties?\nplace independent priors on each of the \\(\\kappa_j\\)’s\na hierarchical Bayes model\nif we can integrate out the \\(\\kappa_j\\)’s we have a new prior for \\(\\beta_j\\)\nthis leads to a new penalty!\nexamples include the Lasso (Double Exponential Prior) and Double Pareto Priors\n\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/02-mles.html#outline",
    "href": "resources/slides/02-mles.html#outline",
    "title": "Maximum Likelihood Estimation",
    "section": "Outline",
    "text": "Outline\n\nLikelihood Function\nProjections\nMaximum Likelihood Estimates\n\n\nReadings: Christensen Chapter 1-2, Appendix A, and Appendix B"
  },
  {
    "objectID": "resources/slides/02-mles.html#normal-model",
    "href": "resources/slides/02-mles.html#normal-model",
    "title": "Maximum Likelihood Estimation",
    "section": "Normal Model",
    "text": "Normal Model\nTake an random vector \\(\\mathbf{Y}\\in \\mathbb{R}^n\\) which is observable and decompose\n\\[ \\mathbf{Y}= \\boldsymbol{\\mu}+ \\boldsymbol{\\epsilon}\\]\n\n\\(\\boldsymbol{\\mu}\\in \\mathbb{R}^n\\) (unknown, fixed)\n\n\\(\\boldsymbol{\\epsilon}\\in \\mathbb{R}^n\\) unobservable error vector (random)\n\n\nUsual assumptions?\n\n\\(E[\\epsilon_i] = 0 \\ \\forall i \\Leftrightarrow \\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}\\) \\(\\quad \\Rightarrow \\textsf{E}[\\mathbf{Y}] = \\boldsymbol{\\mu}\\) (mean vector)\n\\(\\epsilon_i\\) independent with \\(\\textsf{Var}(\\epsilon_i) = \\sigma^2\\) and \\(\\textsf{Cov}(\\epsilon_i, \\epsilon_j) = 0\\)\nMatrix version \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] \\equiv \\left[ (\\textsf{E}\\left[(\\epsilon_i -\\textsf{E}[\\epsilon_i])(\\epsilon_j - \\textsf{E}[\\epsilon_j])\\right]\\right]_{ij} = \\sigma^2 \\mathbf{I}_n\n\\quad \\Rightarrow \\textsf{Cov}[\\mathbf{Y}] = \\sigma^2 \\mathbf{I}_n\\) (errors are uncorrelated)\n\\(\\boldsymbol{\\epsilon}_i \\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}\\textsf{N}(0, \\sigma^2)\\) implies that \\(Y_i \\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}\\textsf{N}(\\mu_i, \\sigma^2)\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#likelihood-function",
    "href": "resources/slides/02-mles.html#likelihood-function",
    "title": "Maximum Likelihood Estimation",
    "section": "Likelihood Function",
    "text": "Likelihood Function\nThe likelihood function for \\(\\boldsymbol{\\mu}, \\sigma^2\\) is proportional to the sampling distribution of the data\n\\[\\begin{eqnarray*}\n{\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2) & \\propto & \\prod_{i = 1}^n \\frac{1}{\\sqrt{(2 \\pi\n                                 \\sigma^2)}} \\exp{- \\frac{1}{2}\n                                 \\left\\{ \\frac{( Y_i\n                                 - \\mu_i)^2}{\\sigma^2} \\right\\}}\n                                 \\\\\n& \\propto & ({2 \\pi} \\sigma^2)^{-n/2}\n\\exp{\\left\\{ - \\frac 1 2  \\frac{ \\sum_i(Y_i - \\mu_i)^2 )}{\\sigma^2}\n\\right\\}}   \\\\\n   & \\propto & (\\sigma^2)^{-n/2}\n\\exp{\\left\\{ - \\frac 1 2 \\frac{\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2}{\\sigma^2}\n\\right\\}} \\\\\n  & \\propto &  (2 \\pi)^{-n/2}\n| \\mathbf{I}_n\\sigma^2|^{-1/2}\n\\exp{\\left\\{ - \\frac 1 2 \\frac{\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2}{\\sigma^2}\n\\right\\}}  \n\\end{eqnarray*}\\]\n\nLast line is the density of \\(\\mathbf{Y}\\sim \\textsf{N}_n\\left(\\boldsymbol{\\mu}, \\sigma^2 \\mathbf{I}_n\\right)\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#mles",
    "href": "resources/slides/02-mles.html#mles",
    "title": "Maximum Likelihood Estimation",
    "section": "MLEs",
    "text": "MLEs\nFind values of \\(\\hat{\\boldsymbol{\\mu}}\\) and \\({\\hat{\\sigma}}^2\\) that maximize the likelihood \\({\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2)\\) for \\(\\boldsymbol{\\mu}\\in \\mathbb{R}^n\\) and \\(\\sigma^2 \\in \\mathbb{R}^+\\) \\[\\begin{eqnarray*}\n{\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2)\n    & \\propto & (\\sigma^2)^{-n/2}\n\\exp{\\left\\{ - \\frac 1 2 \\frac{\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2}{\\sigma^2}\n\\right\\}} \\\\\n\\log({\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2) )\n   & \\propto & -\\frac{n}{2} \\log(\\sigma^2)  - \\frac 1 2 \\frac{\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2}{\\sigma^2}\n\\\\\n\\end{eqnarray*}\\] or equivalently the log likelihood\n\nClearly, \\(\\hat{\\boldsymbol{\\mu}}= \\mathbf{Y}\\) but \\({\\hat{\\sigma}}^2= 0\\) is outside the parameter space\nIf \\(\\boldsymbol{\\mu}= \\mathbf{X}\\boldsymbol{\\beta}\\), can show that \\(\\hat{\\boldsymbol{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) is the MLE/OLS estimator of \\(\\boldsymbol{\\beta}\\) and \\(\\hat{\\boldsymbol{\\mu}}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\) if \\(\\mathbf{X}\\) is full column rank.\nshow via projections"
  },
  {
    "objectID": "resources/slides/02-mles.html#projections",
    "href": "resources/slides/02-mles.html#projections",
    "title": "Maximum Likelihood Estimation",
    "section": "Projections",
    "text": "Projections\ntake any point \\(\\mathbf{y}\\in \\mathbb{R}^n\\) and “project” it onto \\(C(\\mathbf{X}) = \\boldsymbol{{\\cal M}}\\)\n\nany point already in \\(\\boldsymbol{{\\cal M}}\\) stays the same\nso if \\(\\mathbf{P}_\\mathbf{X}\\) is a projection onto the column space of \\(\\mathbf{X}\\) then for \\(\\mathbf{m}\\in C(\\mathbf{X})\\) \\(\\mathbf{P}_\\mathbf{X}\\mathbf{m}= \\mathbf{m}\\)\n\\(\\mathbf{P}_\\mathbf{X}\\) is a linear transformation from \\(\\mathbb{R}^n \\to \\mathbb{R}^n\\)\nmaps vectors in \\(\\mathbb{R}^n\\) into \\(C(\\mathbf{X})\\)\nif \\(\\mathbf{z}\\in \\mathbb{R}^n\\) then \\(\\mathbf{P}_\\mathbf{X}\\mathbf{z}= \\mathbf{X}\\mathbf{a}\\in C(\\mathbf{X})\\) for some \\(\\mathbf{a}\\in \\mathbb{R}^p\\)\n\n\n\n\n\n\n\n\nExample\n\n\nFor \\(\\mathbf{X}\\in \\mathbb{R}^{n \\times p}\\), rank \\(p\\), \\(\\mathbf{P}_\\mathbf{X}= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}\\) is a projection onto the \\(p\\) dimensional subspace \\(\\boldsymbol{{\\cal M}}= C(\\mathbf{X})\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#idempotent-matrix",
    "href": "resources/slides/02-mles.html#idempotent-matrix",
    "title": "Maximum Likelihood Estimation",
    "section": "Idempotent Matrix",
    "text": "Idempotent Matrix\nWhat if we project a projection?\n\n\\(\\mathbf{P}_\\mathbf{X}\\mathbf{z}= \\mathbf{X}\\mathbf{a}\\in C(\\mathbf{X})\\)\n\\(\\mathbf{P}_\\mathbf{X}\\mathbf{X}\\mathbf{a}= \\mathbf{X}\\mathbf{a}\\)\nsince \\(\\mathbf{P}_\\mathbf{X}^2 \\mathbf{z}=  \\mathbf{P}_\\mathbf{X}\\mathbf{z}\\) for all \\(\\mathbf{z}\\in \\mathbb{R}^n\\) we have \\(\\mathbf{P}_\\mathbf{X}^2 = \\mathbf{P}_\\mathbf{X}\\)\n\n\n\nDefinition: ProjectionFor a matrix \\(\\mathbf{P}\\) in \\(\\mathbb{R}^{n \\times n}\\) is a projection matrix if \\(\\mathbf{P}^2 = \\mathbf{P}\\). That is all projections \\(\\mathbf{P}\\) are idempotent matrix.\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\nFor \\(\\mathbf{X}\\in \\mathbb{R}^{n \\times p}\\), rank \\(p\\), if \\(\\mathbf{P}_\\mathbf{X}= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}\\) use the definition to show that it is a projection onto the \\(p\\) dimensional subspace \\(\\boldsymbol{{\\cal M}}= C(\\mathbf{X})\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#null-space",
    "href": "resources/slides/02-mles.html#null-space",
    "title": "Maximum Likelihood Estimation",
    "section": "Null Space",
    "text": "Null Space\n\nDefinition: Orthogonal ComplementThe set of all vectors that are orthogonal to a given subspace \\(\\boldsymbol{{\\cal M}}\\) is called the orthogonal complement of the subspace denoted as \\(\\boldsymbol{{\\cal M}}^\\perp\\). Under the usual inner product, \\(\\boldsymbol{{\\cal M}}^\\perp \\equiv \\{\\mathbf{n}\\in \\mathbb{R}^n \\ni \\mathbf{m}^T\\mathbf{n}= 0 {\\text{ for }} \\mathbf{m}\\in \\boldsymbol{{\\cal M}}\\}\\)\n\n\n\n\nDefinition: Null SpaceFor a matrix \\(\\mathbf{A}\\), the null space of \\(\\mathbf{A}\\) is defined as \\(N(\\mathbf{A}) = \\{\\mathbf{n}\\ni \\mathbf{A}\\mathbf{n}= \\mathbf{0}\\}\\)\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\nShow that \\(C(\\mathbf{X})^\\perp\\) (the orthogonal complement of \\(C(\\mathbf{X})\\)) is the null space of \\(\\mathbf{X}^T\\), \\(\\, N(\\mathbf{X}^T)\\)."
  },
  {
    "objectID": "resources/slides/02-mles.html#orthogonal-projection",
    "href": "resources/slides/02-mles.html#orthogonal-projection",
    "title": "Maximum Likelihood Estimation",
    "section": "Orthogonal Projection",
    "text": "Orthogonal Projection\n\nDefinition: Orthogonal ProjectionsFor a vector space \\({\\cal V}\\) with an inner product \\(\\langle \\mathbf{x}, \\mathbf{y}\\rangle\\) for \\(\\mathbf{x}, \\mathbf{y}\\in {\\cal V}\\), \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are orthogonal if \\(\\langle \\mathbf{x}, \\mathbf{y}\\rangle = 0\\). A projection \\(\\mathbf{P}\\) is an orthogonal projection onto a subspace \\(\\boldsymbol{{\\cal M}}\\) of \\({\\cal V}\\) if for any \\(\\mathbf{m}\\in {\\cal V}\\), \\(\\mathbf{P}\\mathbf{m}= \\mathbf{m}\\) and for any \\(\\mathbf{n}\\in \\boldsymbol{{\\cal M}}^\\perp\\), \\(\\mathbf{P}\\mathbf{n}= \\mathbf{0}\\).\nThe null space of \\(\\mathbf{P}\\) is the orthogonal complement of \\(\\boldsymbol{{\\cal M}}\\)\n\n\n\nFor \\(\\mathbb{R}^N\\) with the inner product, \\(\\langle \\mathbf{x}, \\mathbf{y}\\rangle = \\mathbf{x}^T\\mathbf{y}\\), \\(\\mathbf{P}\\) is an orthogonal projection onto \\(\\boldsymbol{{\\cal M}}\\) if \\(\\mathbf{P}\\) is a projection (\\(\\mathbf{P}^2 = \\mathbf{P}\\)) and it is symmetric (\\(\\mathbf{P}= \\mathbf{P}^T\\))\n\n\n\n\n\n\n\n\nExercise\n\n\nShow that \\(\\mathbf{P}_\\mathbf{X}\\) is an orthogonal projection on \\(C(\\mathbf{X})\\)."
  },
  {
    "objectID": "resources/slides/02-mles.html#decompsition",
    "href": "resources/slides/02-mles.html#decompsition",
    "title": "Maximum Likelihood Estimation",
    "section": "Decompsition",
    "text": "Decompsition\n\nFor any \\(\\mathbf{y}\\in \\mathbb{R}^n\\), we can write it uniquely as a vector \\[ \\mathbf{y}= \\mathbf{m}+ \\mathbf{n}, \\quad \\mathbf{m}\\in \\boldsymbol{{\\cal M}}\\quad \\mathbf{n}\\in \\boldsymbol{{\\cal M}}^\\perp\\]\nwrite \\(\\mathbf{y}= \\mathbf{P}\\mathbf{y}+ (\\mathbf{y}- \\mathbf{P}\\mathbf{y}) = \\mathbf{P}\\mathbf{y}+ (\\mathbf{I}- \\mathbf{P})\\mathbf{y}\\)\nclaim that if \\(\\mathbf{P}\\) is an orthogonal projection, \\((\\mathbf{I}- \\mathbf{P})\\) is an orthogonal projection onto \\(\\boldsymbol{{\\cal M}}^\\perp\\)\nif \\(\\mathbf{n}\\in \\boldsymbol{{\\cal M}}^\\perp\\), then \\((\\mathbf{I}- \\mathbf{P})\\mathbf{n}= \\mathbf{n}- \\mathbf{P}\\mathbf{n}= \\mathbf{n}\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#back-to-mles",
    "href": "resources/slides/02-mles.html#back-to-mles",
    "title": "Maximum Likelihood Estimation",
    "section": "Back to MLEs",
    "text": "Back to MLEs\n\n\\(\\mathbf{Y}\\sim \\textsf{N}(\\boldsymbol{\\mu}, \\sigma^2 \\mathbf{I}_n)\\) with \\(\\boldsymbol{\\mu}= \\mathbf{X}\\boldsymbol{\\beta}\\) and \\(\\mathbf{X}\\) full column rank\nClaim: Maximum Likelihood Estimator (MLE) of \\(\\boldsymbol{\\mu}\\) is \\(\\mathbf{P}_\\mathbf{X}\\mathbf{Y}\\)\nLog Likelihood: \\[ \\log {\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2) =\n-\\frac{n}{2} \\log(\\sigma^2)\n- \\frac 1 2 \\frac{\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2}{\\sigma^2}\n\\]\nDecompose \\(\\mathbf{Y}= \\mathbf{P}_\\mathbf{X}\\mathbf{Y}+ (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\)\n\nUse \\(\\mathbf{P}_\\mathbf{X}\\boldsymbol{\\mu}= \\boldsymbol{\\mu}\\)\n\nSimplify \\(\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#expand",
    "href": "resources/slides/02-mles.html#expand",
    "title": "Maximum Likelihood Estimation",
    "section": "Expand",
    "text": "Expand\n\\[\\begin{eqnarray*}\n    \\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2 & = & \\|  { (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}+ \\mathbf{P}_x \\mathbf{Y}} -\n    \\boldsymbol{\\mu}\\|^2  \\\\\n  & = & \\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}+ \\mathbf{P}_x \\mathbf{Y}- {\\mathbf{P}_\\mathbf{X}}\\boldsymbol{\\mu}\\|^2  \\\\\n  & = & {\\|(\\mathbf{I}-\\mathbf{P}_\\mathbf{x})}\\mathbf{Y}+  {\\mathbf{P}_\\mathbf{X}}(\\mathbf{Y}- \\boldsymbol{\\mu})\n  \\|^2 \\\\\n& = & {\\|(\\mathbf{I}-\\mathbf{P}_\\mathbf{x})\\mathbf{Y}\\|^2} +  {\\|\n   {\\mathbf{P}_\\mathbf{X}}(\\mathbf{Y}- \\boldsymbol{\\mu}) \\|^2}  + {\\small{2 (\\mathbf{Y}-\n\\boldsymbol{\\mu})^T \\mathbf{P}_\\mathbf{X}^T (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}}}\\\\\n& = & \\|(\\mathbf{I}-\\mathbf{P}_\\mathbf{x})\\mathbf{Y}\\|^2 +  \\| {\\mathbf{P}_\\mathbf{X}}(\\mathbf{Y}- \\boldsymbol{\\mu}) \\|^2 + {0}\n\\\\\n& = & \\|(\\mathbf{I}-\\mathbf{P}_\\mathbf{x})\\mathbf{Y}\\|^2 +  \\| {\\mathbf{P}_\\mathbf{X}}\\mathbf{Y}- \\boldsymbol{\\mu}\\|^2\n  \\end{eqnarray*}\\]\n\nCrossproduct term is zero: \\[\\begin{eqnarray*}\n  \\mathbf{P}_\\mathbf{X}^T (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) & = &  \\mathbf{P}_\\mathbf{X}(\\mathbf{I}- \\mathbf{P}_\\mathbf{X})  \\\\\n  & = &  \\mathbf{P}_\\mathbf{X}- \\mathbf{P}_\\mathbf{X}\\mathbf{P}_\\mathbf{X}\\\\\n& = &  \\mathbf{P}_\\mathbf{X}- \\mathbf{P}_\\mathbf{X}\\\\\n& = & \\mathbf{0}\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "resources/slides/02-mles.html#log-likelihood",
    "href": "resources/slides/02-mles.html#log-likelihood",
    "title": "Maximum Likelihood Estimation",
    "section": "Log Likelihood",
    "text": "Log Likelihood\nSubstitute decomposition into log likelihood \\[\\begin{eqnarray*}\n\\log {\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2)  & = &\n-\\frac{n}{2} \\log(\\sigma^2) - \\frac 1 2 \\frac{\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2}{\\sigma^2}  \\\\\n  & = & -\\frac{n}{2} \\log(\\sigma^2)  - \\frac 1 2 \\left( \\frac{\\|(\\mathbf{I}- \\mathbf{P}_\\mathbf{X})\n  \\mathbf{Y}\\|^2}{\\sigma^2} + \\frac{\\| \\mathbf{P}_\\mathbf{X}\\mathbf{Y}- \\boldsymbol{\\mu}\\|^2 } {\\sigma^2} \\right)   \\\\\n& = &  \\underbrace { -\\frac{n}{2} \\log(\\sigma^2)  - \\frac 1 2  \\frac{\\|(\\mathbf{I}- \\mathbf{P}_\\mathbf{X})\n  \\mathbf{Y}\\|^2}{\\sigma^2} }  +  \\underbrace{- \\frac 1 2  \\frac{\\| \\mathbf{P}_\\mathbf{X}\\mathbf{Y}-\n  \\boldsymbol{\\mu}\\|^2 } {\\sigma^2}}   \\\\\n& = &  \\text{ constant with respect to } \\boldsymbol{\\mu}\\qquad  \\leq 0\n\\end{eqnarray*}\\]\n\nMaximize with respect to \\(\\boldsymbol{\\mu}\\) for each \\(\\sigma^2\\)\nRHS is largest when \\(\\boldsymbol{\\mu}= \\mathbf{P}_\\mathbf{X}\\mathbf{Y}\\) for any choice of \\(\\sigma^2\\) \\[\\therefore \\quad \\hat{\\boldsymbol{\\mu}}= \\mathbf{P}_\\mathbf{X}\\mathbf{Y}\\] is the MLE of \\(\\boldsymbol{\\mu}\\) (fitted values \\(\\hat{\\mathbf{Y}}= \\mathbf{P}_\\mathbf{X}\\mathbf{Y}\\))"
  },
  {
    "objectID": "resources/slides/02-mles.html#mle-of-boldsymbolbeta",
    "href": "resources/slides/02-mles.html#mle-of-boldsymbolbeta",
    "title": "Maximum Likelihood Estimation",
    "section": "MLE of \\(\\boldsymbol{\\beta}\\)",
    "text": "MLE of \\(\\boldsymbol{\\beta}\\)\n\\[{\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2)   =  -\\frac{n}{2} \\log(\\sigma^2)  - \\frac 1 2 \\left( \\frac{\\|(\\mathbf{I}- \\mathbf{P}_\\mathbf{X})\n  \\mathbf{Y}\\|^2}{\\sigma^2} + \\frac{\\| \\mathbf{P}_\\mathbf{X}\\mathbf{Y}- \\boldsymbol{\\mu}\\|^2 } {\\sigma^2} \\right)\\]\n\nRewrite as likeloood function for \\(\\boldsymbol{\\beta}, \\sigma^2\\): \\[{\\cal{L}}(\\boldsymbol{\\beta}, \\sigma^2 )  =  -\\frac{n}{2} \\log(\\sigma^2)  - \\frac 1 2 \\left( \\frac{\\|(\\mathbf{I}- \\mathbf{P}_\\mathbf{X})\n  \\mathbf{Y}\\|^2}{\\sigma^2} + \\frac{\\| \\mathbf{P}_\\mathbf{X}\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}\\|^2 } {\\sigma^2}\n\\right)\\]\n\n\n\nSimilar argument to show that RHS is maximized by minimizing \\[\\| \\mathbf{P}_\\mathbf{X}\n\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}\\|^2\\] \nTherefore \\(\\hat{\\boldsymbol{\\beta}}\\) is a MLE of \\(\\boldsymbol{\\beta}\\) if and only if satisfies \\[\\mathbf{P}_\\mathbf{X}\\mathbf{Y}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\]\nIf \\(\\mathbf{X}^T\\mathbf{X}\\) is full rank, the MLE of \\(\\boldsymbol{\\beta}\\) is \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}= \\hat{\\boldsymbol{\\beta}}\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#mle-of-sigma2",
    "href": "resources/slides/02-mles.html#mle-of-sigma2",
    "title": "Maximum Likelihood Estimation",
    "section": "MLE of \\(\\sigma^2\\)",
    "text": "MLE of \\(\\sigma^2\\)\n\nPlug-in MLE of \\(\\hat{\\boldsymbol{\\mu}}\\) for \\(\\boldsymbol{\\mu}\\) \\[ \\log {\\cal{L}}(\\hat{\\boldsymbol{\\mu}}, \\sigma^2)  =   -\\frac{n}{2} \\log \\sigma^2 - \\frac 1 2\n\\frac{\\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\|^2  }{\\sigma^2}\\]\nDifferentiate with respect to \\(\\sigma^2\\) \\[\\frac{\\partial \\, \\log {\\cal{L}}(\\hat{\\boldsymbol{\\mu}}, \\sigma^2)}{\\partial \\, \\sigma^2} =  -\\frac{n}{2} \\frac{1}{\\sigma^2}  +  \\frac 1 2\n\\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\|^2 \\left(\\frac{1}{\\sigma^2}\\right)^2 \\]\nSet derivative to zero and solve for MLE \\[\\begin{eqnarray*}\n0 & = &  -\\frac{n}{2} \\frac{1}{{\\hat{\\sigma}}^2}  +  \\frac 1 2\n\\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\|^2 \\left(\\frac{1}{{\\hat{\\sigma}}^2}\\right)^2  \\\\\n\\frac{n}{2} {\\hat{\\sigma}}^2& = & \\frac 1 2\n\\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\|^2 \\\\\n{\\hat{\\sigma}}^2& = & \\frac{\\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\|^2}{n}\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "resources/slides/02-mles.html#mle-estimate-of-sigma2",
    "href": "resources/slides/02-mles.html#mle-estimate-of-sigma2",
    "title": "Maximum Likelihood Estimation",
    "section": "MLE Estimate of \\(\\sigma^2\\)",
    "text": "MLE Estimate of \\(\\sigma^2\\)\nMaximum Likelihood Estimate of \\(\\sigma^2\\) \\[\\begin{eqnarray*}\n    {\\hat{\\sigma}}^2& = & \\frac{\\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\|^2}{n} \\\\\n      & = & \\frac{\\mathbf{Y}^T(\\mathbf{I}- \\mathbf{P}_\\mathbf{X})^T(\\mathbf{I}-\\mathbf{P}_\\mathbf{X}) \\mathbf{Y}}{n} \\\\\n& = & \\frac{ \\mathbf{Y}^T(\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}}{n} \\\\\n& = & \\frac{\\mathbf{e}^T\\mathbf{e}} {n}\n  \\end{eqnarray*}\\] where \\(\\mathbf{e}= (\\mathbf{I}- \\mathbf{P}_\\mathbf{X})\\mathbf{Y}\\) are the residuals from the regression of \\(\\mathbf{Y}\\) on \\(\\mathbf{X}\\)\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/10-james-stein.html#outline",
    "href": "resources/slides/10-james-stein.html#outline",
    "title": "James-Stein Estimation and Shrinkage",
    "section": "Outline",
    "text": "Outline\n\nFrequentist Risk in Orthogonal Regression\nJames-Stein Estimation\n\n\nReadings:\n\nSeber & Lee Chapter Chapter 12"
  },
  {
    "objectID": "resources/slides/10-james-stein.html#orthogonal-regression",
    "href": "resources/slides/10-james-stein.html#orthogonal-regression",
    "title": "James-Stein Estimation and Shrinkage",
    "section": "Orthogonal Regression",
    "text": "Orthogonal Regression\n\nConsider the model \\(\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{e}\\) where \\(\\mathbf{X}\\) is \\(n \\times p\\) with \\(n &gt; p\\) and \\(\\mathbf{X}^T\\mathbf{X}= \\mathbf{I}_p\\).\nIf \\(\\mathbf{X}\\) has orthogonal columns, then \\(\\hat{\\boldsymbol{\\beta}}= \\mathbf{X}^T\\mathbf{Y}\\) is the OLS estimator of \\(\\boldsymbol{\\beta}\\).\nThe OLS estimator is unbiased and has minimum variance among all\nThe MSE for estimating \\(\\boldsymbol{\\beta}\\) is \\(\\textsf{E}_\\mathbf{Y}[( \\boldsymbol{\\beta}- \\hat{\\boldsymbol{\\beta}})^T(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}})] = \\sigma^2\n  \\textsf{tr}[(\\mathbf{X}^T\\mathbf{X})^{-1}] = p\\sigma^2\\)\nCan always take a general regression problem and transform design so that the model matrix has orthogonal columns \\[\\mathbf{X}\\boldsymbol{\\beta}= \\mathbf{U}\\boldsymbol{\\Delta}\\mathbf{V}^T \\boldsymbol{\\beta}= \\mathbf{U}\\boldsymbol{\\alpha}\\] where new parameters are \\(\\boldsymbol{\\alpha}= \\boldsymbol{\\Delta}\\mathbf{V}^T \\boldsymbol{\\beta}\\) and \\(\\mathbf{U}^T\\mathbf{U}= \\mathbf{I}_p\\).\nOrthogonal polynomials, Fourier bases and wavelet regression are other examples.\n\\(\\hat{\\boldsymbol{\\alpha}} = \\mathbf{U}^T\\mathbf{Y}\\) and MSE of \\(\\hat{\\boldsymbol{\\alpha}}\\) is \\(p\\sigma^2\\)\nso WLOG we will assume that \\(\\mathbf{X}\\) has orthogonal columns"
  },
  {
    "objectID": "resources/slides/10-james-stein.html#shrinkage-estimators",
    "href": "resources/slides/10-james-stein.html#shrinkage-estimators",
    "title": "James-Stein Estimation and Shrinkage",
    "section": "Shrinkage Estimators",
    "text": "Shrinkage Estimators\n\nthe \\(g\\)-prior and Ridge prior are equivalent in the orthogonal case \\[\\boldsymbol{\\beta}\\sim \\textsf{N}(\\mathbf{0}_p, \\sigma^2 \\mathbf{I}_p/\\kappa)\\] using the ridge parameterization of the prior \\(\\kappa = 1/g\\)\nBayes estimator in this case is \\[\\hat{\\boldsymbol{\\beta}}_\\kappa = \\frac{1}{1+\\kappa} \\hat{\\boldsymbol{\\beta}}\\]\nMSE of \\(\\hat{\\boldsymbol{\\beta}}_\\kappa\\) is \\[\\textsf{MSE}(\\hat{\\boldsymbol{\\beta}}_\\kappa) = \\frac{1}{(1+\\kappa)^2} \\sigma^2 p + \\frac{\\kappa^2}{(1 + \\kappa)^2} \\sum_{j=1}^{p} \\beta_j^2\\]\nsquared bias term grows with \\(\\kappa\\) and variance term decreases with \\(\\kappa\\)"
  },
  {
    "objectID": "resources/slides/10-james-stein.html#shrinkage",
    "href": "resources/slides/10-james-stein.html#shrinkage",
    "title": "James-Stein Estimation and Shrinkage",
    "section": "Shrinkage",
    "text": "Shrinkage\n\n\n\nin principle, with the right choice of \\(\\kappa\\) we can get a better estimator and reduce the MSE\nwhile not unbiased, what we pay for bias we can make up for with a reduction in variance\nthe variance-bias decomposition of MSE based on the plot suggests there is an optimal value of \\(\\kappa\\) the improves over OLS in terms of MSE\n“optimal” \\(\\kappa\\) \\[\\kappa = \\frac{p \\sigma^2}{\\|\\boldsymbol{\\beta}^*\\|^2}\\] where \\(\\boldsymbol{\\beta}^*\\) is the true value of \\(\\boldsymbol{\\beta}\\)\nbut never know that in practice!"
  },
  {
    "objectID": "resources/slides/10-james-stein.html#estimating-kappa",
    "href": "resources/slides/10-james-stein.html#estimating-kappa",
    "title": "James-Stein Estimation and Shrinkage",
    "section": "Estimating \\(\\kappa\\)",
    "text": "Estimating \\(\\kappa\\)\n\nif we use the optimal \\(\\kappa\\), the shrinkage estimator can be written as \\[\\tilde{\\boldsymbol{\\beta}}= \\frac{\\|\\boldsymbol{\\beta}\\|^2}{p \\sigma^2 + \\|\\boldsymbol{\\beta}\\|^2} \\hat{\\boldsymbol{\\beta}}\\] or \\[\\tilde{\\boldsymbol{\\beta}}= \\left(1 - \\frac{p\\sigma^2}{p\\sigma^2 + \\|\\boldsymbol{\\beta}\\|^2} \\right) \\hat{\\boldsymbol{\\beta}}\\]\nbut note that \\(\\textsf{E}\\|\\hat{\\boldsymbol{\\beta}}\\|^2 = p \\sigma^2 + \\|\\boldsymbol{\\beta}\\|^2\\) (the denominator)\nplugging in \\(\\|\\hat{\\boldsymbol{\\beta}}\\|^2\\) for the denominator leads to an estimator that we can compute! \\[\\tilde{\\boldsymbol{\\beta}}= \\left(1 - \\frac{p\\sigma^2}{\\|\\hat{\\boldsymbol{\\beta}}\\|^2} \\right) \\hat{\\boldsymbol{\\beta}}\\]"
  },
  {
    "objectID": "resources/slides/10-james-stein.html#james-stein-estimators",
    "href": "resources/slides/10-james-stein.html#james-stein-estimators",
    "title": "James-Stein Estimation and Shrinkage",
    "section": "James-Stein Estimators",
    "text": "James-Stein Estimators\nin James and Stein (1961) proposed a shrinkage estimator that dominated the MLE for the mean of a multivariate normal distribution \\[\\tilde{\\boldsymbol{\\beta}}_{JS} = \\left(1 - \\frac{(p-2)\\sigma^2}{\\|\\hat{\\boldsymbol{\\beta}}\\|^2} \\right) \\hat{\\boldsymbol{\\beta}}\\] (equivalent to our orthogonal regression case; just multiply everything by \\(\\mathbf{X}^T\\) to show)\n\nthey showed that this is the best (in terms of smallest MSE) of all estimators of the form \\(\\left(1- \\frac{b}{\\|\\hat{\\boldsymbol{\\beta}}\\|^2} \\right) \\hat{\\boldsymbol{\\beta}}\\)\nit is possible to show that the MSE of the James-Stein estimator is \\[\\textsf{MSE}(\\tilde{\\boldsymbol{\\beta}}_{JS}) = 2\\sigma^2\\] which is less than the MSE of the OLS estimator if \\(p &gt; 2\\)! (more on this in STA732)"
  },
  {
    "objectID": "resources/slides/10-james-stein.html#negative-shrinkage",
    "href": "resources/slides/10-james-stein.html#negative-shrinkage",
    "title": "James-Stein Estimation and Shrinkage",
    "section": "Negative Shrinkage?",
    "text": "Negative Shrinkage?\n\n\n\none potential problem with the James-Stein estimator \\[\\tilde{\\boldsymbol{\\beta}}_{JS} = \\left(1 - \\frac{(p-2)\\sigma^2}{\\|\\hat{\\boldsymbol{\\beta}}\\|^2} \\right) \\hat{\\boldsymbol{\\beta}}\\] is that the term in the parentheses can be negative if \\(\\|\\hat{\\boldsymbol{\\beta}}\\|^2 &lt; (p-2)\\sigma^2\\)\nHow likely is this to happen?\nSuppose that each of the parameters \\(\\beta_j\\) are actually zero, then \\(\\hat{\\boldsymbol{\\beta}}\\sim \\textsf{N}(\\mathbf{0}_p, \\sigma^2 \\mathbf{I}_p)\\) then \\(\\|\\hat{\\boldsymbol{\\beta}}\\|^2 /\\sigma^2 \\sim  \\chi^2_p\\)\ncompute the probability that \\(\\chi^2_p &lt; (p-2)\\)\nso if the model is full of small effects, the James-Stein can lead to negative shrinkage!"
  },
  {
    "objectID": "resources/slides/10-james-stein.html#positive-part-james-stein-estimator",
    "href": "resources/slides/10-james-stein.html#positive-part-james-stein-estimator",
    "title": "James-Stein Estimation and Shrinkage",
    "section": "Positive Part James-Stein Estimator",
    "text": "Positive Part James-Stein Estimator\n\nany shrinkage estimator of the form \\(\\tilde{\\boldsymbol{\\beta}}= (1 - b) \\hat{\\boldsymbol{\\beta}}\\) is inadmissible if the shrinkage factor is negative or greater than one (there is a better estimator)\nBaranchik (1964) proposed the positive part James-Stein estimator \\[\\tilde{\\boldsymbol{\\beta}}_{PPJS} = \\left(1 - \\frac{(p-2)\\sigma^2}{\\|\\hat{\\boldsymbol{\\beta}}\\|^2} \\right)^+ \\hat{\\boldsymbol{\\beta}}\\] where \\((x)^+ = \\max(x, 0)\\)\nThis is the same as the James-Stein estimator if the shrinkage factor is positive and zero otherwise. (related to testing the null hypothesis that all the \\(\\beta_j\\) are zero)\nit turns out this is also inadmissible! ie there is another estimator that has smaller MSE!\nif we care about admissibility, we may want to stick to Bayes rules that do not depend on the data!\nmore on admissibility & minimaxity in STA732!"
  },
  {
    "objectID": "resources/slides/10-james-stein.html#bayes-and-admissibilty",
    "href": "resources/slides/10-james-stein.html#bayes-and-admissibilty",
    "title": "James-Stein Estimation and Shrinkage",
    "section": "Bayes and Admissibilty",
    "text": "Bayes and Admissibilty\n\nBayes rules based on proper priors are generally always admissible (see Christian Robert (2007) The Bayesian Choice for more details)\nunique Bayes rules are admissible\nGeneralized Bayes rules based on improper priors may not be inadmissible, but this will depend on the loss function and the prior\nunder regularity conditions, limits of Bayes rules will be admissible\nthe Positive-Part James-Stein estimator fails to be admissible under squared error loss as Bayes risk is not continuous"
  },
  {
    "objectID": "resources/slides/10-james-stein.html#positive-part-james-stein-estimator-and-testimators",
    "href": "resources/slides/10-james-stein.html#positive-part-james-stein-estimator-and-testimators",
    "title": "James-Stein Estimation and Shrinkage",
    "section": "Positive Part James-Stein Estimator and Testimators",
    "text": "Positive Part James-Stein Estimator and Testimators\n\nthe positive part James-Stein estimator can be shown to be related to testimators where if we fail to reject the hypothesis that all the \\(\\beta_j\\) are zero at some level, we set all coefficients to zero, and otherwise we shrink the coefficients by an amount that depends on how large the test statistic (\\(\\|\\hat{\\boldsymbol{\\beta}}\\|^2\\)) is.\nnote this can shrink all the coefficients to zero if the majority are small so increased bias for large coefficients that are not zero!\nthis is a form of model selection where we are selecting the model that has all the coefficients zero!"
  },
  {
    "objectID": "resources/slides/10-james-stein.html#lasso-estimator",
    "href": "resources/slides/10-james-stein.html#lasso-estimator",
    "title": "James-Stein Estimation and Shrinkage",
    "section": "LASSO Estimator",
    "text": "LASSO Estimator\n\n\n\nan alternative estimator that allows for shrinkage and selection is the LASSO (Least Absolute Shrinkage and Selection Operator).\nthe LASSO replaces the penalty term in the ridge regression with an \\(L_1\\) penalty term \\[\\hat{\\boldsymbol{\\beta}}_{LASSO} = \\mathop{\\mathrm{argmin}}_{\\boldsymbol{\\beta}} \\left\\{ \\|\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}\\|^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_1 \\right\\}\\]\nthe LASSO can also be shown to be the posterior mode of a Bayesian model with independent Laplace or double exponential prior distributions on the coefficients.\nas the double exponential prior is a “scale” mixture of normals, this provides a generalization of the ridge regression.\n\n\n\nfrom Machine Learning with R\n\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/example.html",
    "href": "resources/slides/example.html",
    "title": "Custom blocks and crossreferencing",
    "section": "",
    "text": "Custom blocks and crossreferencing\nWith this filter, you can define custom div classes (environments) that come with numbering, such as theorems, examples, exercises. The filter supports output formats pdf and html.\n\n\nNumbering\nNumbering is (currently) within section for single documents, or within chapter for books. Grouped classes share the same counter, and the same default style.\nNumbered custom blocks can be cross-referenced with \\ref.\nDefault numbering can be switched off for the whole class by setting the numbered to false, or for an individual block by adding the class unnumbered.\nCrossreferences my need a re-run to update.\n\n\nBoxes can be nested\nHowever, inner boxes are not numbered – it would be hard to put them in a sequence with outer boxes anyway.\n\n\n\nThe default style for custom divs is foldbox: a collapsible similar to quarto’s callouts, with a collapse button at the bottom that makes it easier collapse long boxes, and box open to the right. It comes with the variant foldbox.simple, with closed box and no additional close button. (needs a fix for the moment)\n\n\nCustom styles\n\ncreate an API for user defined block styles\nprovide an example\nand documentation\n\n\n\nCustom list of blocks\nGenerate .qmd files that contains a list of selected block classes, intermitted with headers from the document for easy customization and commenting. This way, one can make a list of all definitions, examples, or {theorems, propositions and lemmas} etc., edit it later and attach to the main document. If you edit, make sure to rename the autogenerated list first, otherwise it will get overwritten in the next run and all work is lost …\nCurrently, you need to give a key listin for any class or group of classes that should appear in a list of things. The value of this key is an array of names, also if only one list is to be generated. These names are turned into files list-of-name.qmd. I am considering replacing the yaml interface by a sub-key to custom-numbered-classes. This would allow to define arbitrary classes that can be attached to any custom div block, such as .important.\n\n\n\nPseudomath examples\n\n\nF\\(\\alpha\\)ncybox\nA box is called f\\(\\alpha\\)ncybox if it looks quite fancy.\nIn this context, by fancy we mean that the title of the box appears as a clickable button when rendered as html, where clickable implies that it throws a small shadow that becomes bigger when hovering over it.\n\n\n\nBy Definition \\(\\ref{fancy}\\), foldboxes are fancyboxes.\n\n\nStudents are lured into clicking on the title and unfolding the fancybox.\n\n\nThis extension has been written by a teacher who hopes that students read the course notes…\n\nTheorem \\(\\ref{TeacherHopes}\\) is suggested by Conjecture \\(\\ref{TeachersHope}\\), but it cannot be proven theoretically. It does give rise to more conjectures, though.\n\nThe teacher mentioned in Theorem \\(\\ref{TeacherHopes}\\) is a statistician who got addicted to quarto due to James J Balamuta’s web-r extension, and desparately wanted to have a common counter for theorem and alike. She got also convinced that everything is possible in quarto by the many nice extensions from Shafayet Khan Shafee."
  },
  {
    "objectID": "resources/slides/18-bvs.html#us-air-example",
    "href": "resources/slides/18-bvs.html#us-air-example",
    "title": "Bayesian Model Averaging and Variable Selection",
    "section": "US Air Example",
    "text": "US Air Example\n\nlibrary(BAS)\ndata(usair, package=\"HH\")\npoll.bma = bas.lm(log(SO2) ~ temp + log(mfgfirms) +\n                             log(popn) + wind +\n                             precip + raindays,\n                  data=usair,\n                  prior=\"JZS\",  #Jeffrey-Zellner-Siow\n                  alpha=nrow(usair), # n\n                  n.models=2^6,\n                  modelprior = uniform(),\n                  method=\"deterministic\")"
  },
  {
    "objectID": "resources/slides/18-bvs.html#summary",
    "href": "resources/slides/18-bvs.html#summary",
    "title": "Bayesian Model Averaging and Variable Selection",
    "section": "Summary",
    "text": "Summary\n\nsummary(poll.bma, n.models=4)\n\n              P(B != 0 | Y) model 1   model 2   model 3   model 4\nIntercept        1.00000000 1.00000 1.0000000 1.0000000 1.0000000\ntemp             0.91158530 1.00000 1.0000000 1.0000000 1.0000000\nlog(mfgfirms)    0.31718916 0.00000 0.0000000 0.0000000 1.0000000\nlog(popn)        0.09223957 0.00000 0.0000000 0.0000000 0.0000000\nwind             0.29394451 0.00000 0.0000000 0.0000000 1.0000000\nprecip           0.28384942 0.00000 1.0000000 0.0000000 1.0000000\nraindays         0.22903262 0.00000 0.0000000 1.0000000 0.0000000\nBF                       NA 1.00000 0.3286643 0.2697945 0.2655873\nPostProbs                NA 0.29410 0.0967000 0.0794000 0.0781000\nR2                       NA 0.29860 0.3775000 0.3714000 0.5427000\ndim                      NA 2.00000 3.0000000 3.0000000 5.0000000\nlogmarg                  NA 3.14406 2.0313422 1.8339656 1.8182487"
  },
  {
    "objectID": "resources/slides/18-bvs.html#plots-of-coefficients",
    "href": "resources/slides/18-bvs.html#plots-of-coefficients",
    "title": "Bayesian Model Averaging and Variable Selection",
    "section": "Plots of Coefficients",
    "text": "Plots of Coefficients\n\n beta = coef(poll.bma)\n par(mfrow=c(2,3));  plot(beta, subset=2:7,ask=F)"
  },
  {
    "objectID": "resources/slides/18-bvs.html#posterior-distribution-with-uniform-prior-on-model-space",
    "href": "resources/slides/18-bvs.html#posterior-distribution-with-uniform-prior-on-model-space",
    "title": "Bayesian Model Averaging and Variable Selection",
    "section": "Posterior Distribution with Uniform Prior on Model Space",
    "text": "Posterior Distribution with Uniform Prior on Model Space\n\nimage(poll.bma, rotate=FALSE)"
  },
  {
    "objectID": "resources/slides/18-bvs.html#posterior-distribution-with-bb11-prior-on-model-space",
    "href": "resources/slides/18-bvs.html#posterior-distribution-with-bb11-prior-on-model-space",
    "title": "Bayesian Model Averaging and Variable Selection",
    "section": "Posterior Distribution with BB(1,1) Prior on Model Space",
    "text": "Posterior Distribution with BB(1,1) Prior on Model Space\n\npoll.bb.bma = bas.lm(log(SO2) ~ temp + log(mfgfirms) +\n                                log(popn) + wind +\n                                precip + raindays,\n                     data=usair,\n                     prior=\"JZS\",\n                     alpha=nrow(usair),\n                     n.models=2^6,  #enumerate\n                     modelprior=beta.binomial(1,1))"
  },
  {
    "objectID": "resources/slides/18-bvs.html#posterior-distribution-with-bb11-prior-on-model-space-1",
    "href": "resources/slides/18-bvs.html#posterior-distribution-with-bb11-prior-on-model-space-1",
    "title": "Bayesian Model Averaging and Variable Selection",
    "section": "Posterior Distribution with BB(1,1) Prior on Model Space",
    "text": "Posterior Distribution with BB(1,1) Prior on Model Space\n\nimage(poll.bb.bma, rotate=FALSE)"
  },
  {
    "objectID": "resources/slides/18-bvs.html#diabetes-example",
    "href": "resources/slides/18-bvs.html#diabetes-example",
    "title": "Bayesian Model Averaging and Variable Selection",
    "section": "Diabetes Example",
    "text": "Diabetes Example\n\nset.seed(8675309)\nsource(\"yX.diabetes.train.txt\")\ndiabetes.train = as.data.frame(diabetes.train)\nsource(\"yX.diabetes.test.txt\")\ndiabetes.test = as.data.frame(diabetes.test)\ncolnames(diabetes.test)[1] = \"y\"\n\nstr(diabetes.train)\n\n'data.frame':   342 obs. of  65 variables:\n $ y      : num  -0.0147 -1.0005 -0.1444 0.6987 -0.2222 ...\n $ age    : num  0.7996 -0.0395 1.7913 -1.8703 0.113 ...\n $ sex    : num  1.064 -0.937 1.064 -0.937 -0.937 ...\n $ bmi    : num  1.296 -1.081 0.933 -0.243 -0.764 ...\n $ map    : num  0.459 -0.553 -0.119 -0.77 0.459 ...\n $ tc     : num  -0.9287 -0.1774 -0.9576 0.256 0.0826 ...\n $ ldl    : num  -0.731 -0.402 -0.718 0.525 0.328 ...\n $ hdl    : num  -0.911 1.563 -0.679 -0.757 0.171 ...\n $ tch    : num  -0.0544 -0.8294 -0.0544 0.7205 -0.0544 ...\n $ ltg    : num  0.4181 -1.4349 0.0601 0.4765 -0.6718 ...\n $ glu    : num  -0.371 -1.936 -0.545 -0.197 -0.979 ...\n $ age^2  : num  -0.312 -0.867 1.925 2.176 -0.857 ...\n $ bmi^2  : num  0.4726 0.1185 -0.0877 -0.6514 -0.2873 ...\n $ map^2  : num  -0.652 -0.573 -0.815 -0.336 -0.652 ...\n $ tc^2   : num  -0.091 -0.6497 -0.0543 -0.6268 -0.6663 ...\n $ ldl^2  : num  -0.289 -0.521 -0.3 -0.45 -0.555 ...\n $ hdl^2  : num  -0.0973 0.8408 -0.3121 -0.2474 -0.5639 ...\n $ tch^2  : num  -0.639 -0.199 -0.639 -0.308 -0.639 ...\n $ ltg^2  : num  -0.605 0.78 -0.731 -0.567 -0.402 ...\n $ glu^2  : num  -0.578 1.8485 -0.4711 -0.6443 -0.0258 ...\n $ age:sex: num  0.69 -0.139 1.765 1.609 -0.284 ...\n $ age:bmi: num  0.852 -0.142 1.489 0.271 -0.271 ...\n $ age:map: num  0.0349 -0.3346 -0.5862 1.1821 -0.3025 ...\n $ age:tc : num  -0.978 -0.246 -1.927 -0.72 -0.244 ...\n $ age:ldl: num  -0.803 -0.203 -1.504 -1.2 -0.182 ...\n $ age:hdl: num  -0.7247 0.0147 -1.2661 1.6523 0.1046 ...\n $ age:tch: num  -0.254 -0.176 -0.31 -1.598 -0.216 ...\n $ age:ltg: num  0.0644 -0.2142 -0.163 -1.1657 -0.3474 ...\n $ age:glu: num  -0.636 -0.239 -1.359 0.071 -0.438 ...\n $ sex:bmi: num  1.304 0.935 0.915 0.142 0.635 ...\n $ sex:map: num  0.258 0.289 -0.381 0.5 -0.697 ...\n $ sex:tc : num  -1.02 0.131 -1.051 -0.274 -0.112 ...\n $ sex:ldl: num  -0.927 0.236 -0.913 -0.638 -0.452 ...\n $ sex:hdl: num  -0.647 -1.188 -0.377 1.189 0.238 ...\n $ sex:tch: num  -0.411 0.47 -0.411 -1.062 -0.296 ...\n $ sex:ltg: num  0.2988 1.2093 -0.0866 -0.6032 0.4857 ...\n $ sex:glu: num  -0.6171 1.6477 -0.8069 -0.0239 0.7283 ...\n $ bmi:map: num  0.189 0.191 -0.477 -0.195 -0.702 ...\n $ bmi:tc : num  -1.5061 -0.0595 -1.1853 -0.3231 -0.3239 ...\n $ bmi:ldl: num  -1.267 0.183 -0.976 -0.407 -0.536 ...\n $ bmi:hdl: num  -0.869 -1.41 -0.286 0.586 0.251 ...\n $ bmi:tch: num  -0.505 0.505 -0.484 -0.614 -0.388 ...\n $ bmi:ltg: num  0.1014 1.1613 -0.4085 -0.5893 0.0716 ...\n $ bmi:glu: num  -0.862 1.693 -0.89 -0.337 0.358 ...\n $ map:tc : num  -0.687 -0.148 -0.131 -0.451 -0.21 ...\n $ map:ldl: num  -0.5407 0.0388 -0.1034 -0.6114 -0.036 ...\n $ map:hdl: num  -0.235 -0.672 0.254 0.745 0.252 ...\n $ map:tch: num  -0.29 0.207 -0.258 -0.835 -0.29 ...\n $ map:ltg: num  -0.214 0.428 -0.427 -0.811 -0.748 ...\n $ map:glu: num  -0.541 0.659 -0.314 -0.23 -0.812 ...\n $ tc:ldl : num  -0.144 -0.551 -0.139 -0.509 -0.581 ...\n $ tc:hdl : num  0.8363 -0.3457 0.6304 -0.2579 -0.0392 ...\n $ tc:tch : num  -0.405 -0.326 -0.404 -0.295 -0.451 ...\n $ tc:ltg : num  -0.901 -0.259 -0.571 -0.392 -0.569 ...\n $ tc:glu : num  0.0202 0.0196 0.2073 -0.396 -0.4283 ...\n $ ldl:hdl: num  0.889 -0.446 0.705 -0.207 0.26 ...\n $ ldl:tch: num  -0.463 -0.243 -0.463 -0.21 -0.506 ...\n $ ldl:ltg: num  -0.6536 0.2724 -0.3783 -0.0708 -0.5638 ...\n $ ldl:glu: num  -0.0194 0.4995 0.1032 -0.4013 -0.6234 ...\n $ hdl:tch: num  0.703 -0.5 0.692 0.171 0.651 ...\n $ hdl:ltg: num  0.0179 -1.9846 0.3839 0.0399 0.3043 ...\n $ hdl:glu: num  0.654 -2.948 0.689 0.452 0.113 ...\n $ tch:ltg: num  -0.592 0.531 -0.574 -0.253 -0.537 ...\n $ tch:glu: num  -0.371 1.114 -0.362 -0.522 -0.34 ...\n $ ltg:glu: num  -0.584 2.184 -0.468 -0.526 0.183 ..."
  },
  {
    "objectID": "resources/slides/18-bvs.html#mcmc-with-bas",
    "href": "resources/slides/18-bvs.html#mcmc-with-bas",
    "title": "Bayesian Model Averaging and Variable Selection",
    "section": "MCMC with BAS",
    "text": "MCMC with BAS\n\nlibrary(BAS)\ndiabetes.bas = bas.lm(y ~ ., data=diabetes.train,\n                      prior = \"JZS\",\n                      method=\"MCMC\",\n                      n.models = 10000,\n                      MCMC.iterations=500000,\n                      thin = 10,\n                      initprobs=\"eplogp\",\n                      force.heredity=FALSE)\n\n\n\n   user  system elapsed \n 10.538   0.574  11.113 \n\n\n[1] \"number of unique models 5905\"\n\n\n\nincrease MCMC.iterations?\ncheck diagnostics"
  },
  {
    "objectID": "resources/slides/18-bvs.html#estimates-of-posterior-probabilities",
    "href": "resources/slides/18-bvs.html#estimates-of-posterior-probabilities",
    "title": "Bayesian Model Averaging and Variable Selection",
    "section": "Estimates of Posterior Probabilities",
    "text": "Estimates of Posterior Probabilities\n\nrelative frequencies \\(\\hat{P}_{RF}(\\boldsymbol{\\gamma}\\mid \\mathbf{Y}) = \\frac{\\text{# times } \\boldsymbol{\\gamma}\\in S }{S}\\)\n\nergodic average converges to \\(p(\\boldsymbol{\\gamma}\\mid \\mathbf{Y})\\) as \\(S \\to \\infty\\)\nasymptoptically unbaised\n\nrenormalized posterior probabilities \\(\\hat{P}_{RN}(\\boldsymbol{\\gamma}\\mid \\mathbf{Y}) = \\frac{p(\\mathbf{Y}\\mid \\boldsymbol{\\gamma}) p(\\boldsymbol{\\gamma})} {\\sum_{\\boldsymbol{\\gamma}\\in S} p(\\mathbf{Y}\\mid \\boldsymbol{\\gamma}) p(\\boldsymbol{\\gamma})}\\)\n\nalso asymptoptically unbaised\nFisher consistent (e.g if we happen to enumerate all models in \\(S\\) iterations we recover the truth)\n\nif we run long enough the two should agree\nalso look at other summaries i.e posterior inclusion probabilities \\[\\hat{p}(\\gamma_j = 1 \\mid \\mathbf{Y}) = \\sum_S \\gamma_j \\hat{P}(\\boldsymbol{\\gamma}\\mid \\mathbf{Y})\\]"
  },
  {
    "objectID": "resources/slides/18-bvs.html#diagnostic-plot",
    "href": "resources/slides/18-bvs.html#diagnostic-plot",
    "title": "Bayesian Model Averaging and Variable Selection",
    "section": "Diagnostic Plot",
    "text": "Diagnostic Plot\n\ndiagnostics(diabetes.bas, type=\"pip\")\n\n\n\nmodel probabilities converge much slower!"
  },
  {
    "objectID": "resources/slides/18-bvs.html#out-of-sample-prediction",
    "href": "resources/slides/18-bvs.html#out-of-sample-prediction",
    "title": "Bayesian Model Averaging and Variable Selection",
    "section": "Out of Sample Prediction",
    "text": "Out of Sample Prediction\n\nWhat is the optimal value to predict \\(\\mathbf{Y}^{\\text{test}}\\) given \\(\\mathbf{Y}\\) under squared error?\nIterated expectations leads to BMA for \\(\\textsf{E}[\\mathbf{Y}^{\\text{test}} \\mid \\mathbf{Y}]\\)\nPrediction under model averaging \\[\\hat{Y} = \\sum_S (\\hat{\\alpha}_\\boldsymbol{\\gamma}+ \\mathbf{X}_{\\boldsymbol{\\gamma}}^{\\text{test}} \\hat{\\boldsymbol{\\beta}}_{\\boldsymbol{\\gamma}}) \\hat{p}(\\boldsymbol{\\gamma}\\mid \\mathbf{Y})\\]\n\n\n\npred.bas = predict(diabetes.bas,\n                   newdata=diabetes.test,\n                   estimator=\"BMA\",\n                   se=TRUE)\nmean((pred.bas$fit- diabetes.test$y)^2)\n\n[1] 0.4556414"
  },
  {
    "objectID": "resources/slides/18-bvs.html#credible-intervals-coverage",
    "href": "resources/slides/18-bvs.html#credible-intervals-coverage",
    "title": "Bayesian Model Averaging and Variable Selection",
    "section": "Credible Intervals & Coverage",
    "text": "Credible Intervals & Coverage\n\nposterior predictive distribution \\[\np(\\mathbf{y}^{\\text{test}} \\mid \\mathbf{y}) = \\sum_\\boldsymbol{\\gamma}p(\\mathbf{y}^{\\text{test}} \\mid \\mathbf{y}, \\boldsymbol{\\gamma})p(\\boldsymbol{\\gamma}\\mid \\mathbf{y})\n\\]\nintegrate out \\(\\alpha\\) and \\(\\boldsymbol{\\beta_\\gamma}\\) to get a normal predictive given \\(\\phi\\) and \\(\\boldsymbol{\\gamma}\\) (and \\(\\mathbf{y}\\))\nintegrate out \\(\\phi\\) to get a t distribution given \\(\\boldsymbol{\\gamma}\\) and \\(\\mathbf{y}\\)\ncredible intervals via sampling\n\nsample a model from \\(p(\\boldsymbol{\\gamma}\\mid \\mathbf{y})\\)\nconditional on a model sample \\(y \\sim p(\\mathbf{y}^{\\text{test}} \\mid \\mathbf{y}, \\boldsymbol{\\gamma})\\)\ncompute quantiles from sample \\(y\\)\n\n\n\n\nci.bas = confint(pred.bas);\ncoverage = mean(diabetes.test$y &gt; ci.bas[,1] & diabetes.test$y &lt; ci.bas[,2])\ncoverage\n\n[1] 1"
  },
  {
    "objectID": "resources/slides/18-bvs.html#prediction-intervals",
    "href": "resources/slides/18-bvs.html#prediction-intervals",
    "title": "Bayesian Model Averaging and Variable Selection",
    "section": "95% Prediction intervals",
    "text": "95% Prediction intervals\n\nplot(ci.bas)\n\nNULL\n\npoints(diabetes.test$y, col=2, pch=15)"
  },
  {
    "objectID": "resources/slides/18-bvs.html#selection-and-prediction",
    "href": "resources/slides/18-bvs.html#selection-and-prediction",
    "title": "Bayesian Model Averaging and Variable Selection",
    "section": "Selection and Prediction",
    "text": "Selection and Prediction\n\nBMA - optimal for squared error loss Bayes \\[\\textsf{E}[\\| \\mathbf{Y}^{\\text{test}} - a\\|^2 \\mid \\mathbf{y}] = \\textsf{E}[\\| \\mathbf{Y}^{\\text{test}} - \\textsf{E}[\\mathbf{Y}^{\\text{test}}\\mid \\mathbf{y}] \\|^2 \\mid \\mathbf{y}]  + \\| \\textsf{E}[\\mathbf{Y}^{\\text{test}}\\mid \\mathbf{y}] - a\\|^2  \\]\nWhat if we want to use only a single model for prediction under squared error loss?\nHPM: Highest Posterior Probability model is optimal for selection, but not prediction\nMPM: Median Probability model (select model where PIP &gt; 0.5) (optimal under certain conditions; nested models)\nBPM: Best Probability Model - Model closest to BMA under loss (usually includes more predictors than HPM or MPM)"
  },
  {
    "objectID": "resources/slides/18-bvs.html#example",
    "href": "resources/slides/18-bvs.html#example",
    "title": "Bayesian Model Averaging and Variable Selection",
    "section": "Example",
    "text": "Example\n\npred.bas = predict(diabetes.bas,\n                   newdata=diabetes.test,\n                   estimator=\"BPM\",\n                   se=TRUE)\n#MSE\nmean((pred.bas$fit- diabetes.test$y)^2)\n\n[1] 0.4740667\n\n#Coverage\nci.bas = confint(pred.bas)\nmean(diabetes.test$y &gt; ci.bas[,1] &\n     diabetes.test$y &lt; ci.bas[,2])\n\n[1] 0.98"
  },
  {
    "objectID": "resources/slides/18-bvs.html#theory---consistency-of-g-priors",
    "href": "resources/slides/18-bvs.html#theory---consistency-of-g-priors",
    "title": "Bayesian Model Averaging and Variable Selection",
    "section": "Theory - Consistency of g-priors",
    "text": "Theory - Consistency of g-priors\n\ndesire that posterior probability of model goes to 1 as \\(n \\to \\infty\\)\n\ndoes not alwyas hold if the null model is true (may be highest posterior probability model)\nneed prior on \\(g\\) to depend on \\(n\\) (rules out EB and fixed g-priors with \\(g \\ne n\\))\nasymptotically BMA collapses to the true model\n\nother quantities may converge i.e. posterior mean\nwhat if the true model \\(\\boldsymbol{\\gamma}_T\\) is not in \\(\\Gamma\\)? What can we say?\n\n\\(\\boldsymbol{{\\cal M}}\\)-complete; BMA converges to the model that is “closest” to the truth in Kullback-Leibler divergence\n\\(\\boldsymbol{{\\cal M}}\\)-closed; realize that \\((p_\\boldsymbol{\\gamma}) = 0 \\ \\forall \\boldsymbol{\\gamma}\\in \\mathbf{G}\\) and is nonsense but know \\(\\boldsymbol{\\gamma}_T\\), however want to use models in \\(\\mathbf{G}\\) only\n\\(\\boldsymbol{{\\cal M}}\\)-open; realize that \\((p_\\boldsymbol{\\gamma}) = 0 \\ \\forall \\boldsymbol{\\gamma}\\in \\mathbf{G}\\) and is nonsense but know \\(\\boldsymbol{\\gamma}_T\\)\nlatter is related to “stacking” which is a frequentist method of ensemble learning using cross-validation; see Clyde & Iversen (2013) for the curious to motivate via decision theory"
  },
  {
    "objectID": "resources/slides/18-bvs.html#summary-1",
    "href": "resources/slides/18-bvs.html#summary-1",
    "title": "Bayesian Model Averaging and Variable Selection",
    "section": "Summary",
    "text": "Summary\n\nChoice of prior on \\(\\boldsymbol{\\beta_\\gamma}\\)\n\northogonally invariant priors - multivariate Spike & Slab\nproducts of independent Spike & Slab priors\nnon-semi-conjugate\n\npriors on the models (sensitivity)\ncomputation (MCMC, “stochastic search”, variational, orthogonal data augmentation, reversible jump-MCMC)\nposterior summaries - select a model or “average” over all models\n\n\nOther aspects of model selection?\n\ntransformations of \\(\\mathbf{Y}\\)\nfunctions of \\(\\mathbf{X}\\): interactions or nonlinear functions such as splines kernels\nchoice of error distribution\n\n\n\n\n\nhttps://sta702-F23.github.io/website/"
  },
  {
    "objectID": "resources/slides/15-CI.html#outline",
    "href": "resources/slides/15-CI.html#outline",
    "title": "Confidence & Credible Regions",
    "section": "Outline",
    "text": "Outline\n\nConfidence Interverals from Test Statistics\nPivotal Quantities\nConfidence intervals for parameters\nPrediction Intervals\nBayesian Credible Regions and Intervals\n\n\nReadings:\n\nChristensen Appendix C, Chapter 3"
  },
  {
    "objectID": "resources/slides/15-CI.html#goals",
    "href": "resources/slides/15-CI.html#goals",
    "title": "Confidence & Credible Regions",
    "section": "Goals",
    "text": "Goals\nFor the regression model \\(\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}\\) we usually want to do more than just testing that \\(\\boldsymbol{\\beta}\\) is zero\n\nwhat is a plausible range for \\(\\beta_j\\)?\nwhat is a plausible set of values for \\(\\beta_j\\) and \\(\\beta_k\\)?\nwhat is a a plausible range of values for \\(\\mathbf{x}^T\\boldsymbol{\\beta}\\) for a particular \\(\\mathbf{x}\\)?\nwhat is a plausible range of values for \\(\\mathbf{Y}_{n+1}\\) for a given value of \\(\\mathbf{x}_{n+1}\\)?\n\n\nLook at confidence intervals, confidence regions, prediction regions and Bayesian credible regions/intervals"
  },
  {
    "objectID": "resources/slides/15-CI.html#confidence-sets",
    "href": "resources/slides/15-CI.html#confidence-sets",
    "title": "Confidence & Credible Regions",
    "section": "Confidence Sets",
    "text": "Confidence Sets\nFor a random variable \\(\\mathbf{Y}\\sim \\mathbf{P}\\in \\{P_{\\boldsymbol{\\theta}}: \\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}\\}\\)\n\nDefinition: Confidence RegionA set valued function \\(C\\) is a \\((1 - \\alpha) \\times 100\\%\\) confidence region for \\(\\boldsymbol{\\theta}\\) if \\[P_{\\boldsymbol{\\theta}}(\\{\\boldsymbol{\\theta}\\in C(\\mathbf{Y})\\}) = 1- \\alpha \\, \\forall \\, \\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}\\]\n\n\n\nIn this case we say \\(C(Y)\\) is a \\(1 - \\alpha\\) confidence region for the parameter \\(\\boldsymbol{\\theta}\\)\nthere is some true value of \\(\\boldsymbol{\\theta}\\), and the confidence region will cover it with probability \\(1- \\alpha\\) no matter what it is.\nthe randomness is due to \\(\\mathbf{Y}\\) and \\(C(\\mathbf{Y})\\)\nonce we observe \\(\\mathbf{Y}\\) everything is fixed, so region may not include the true \\(\\boldsymbol{\\theta}\\)"
  },
  {
    "objectID": "resources/slides/15-CI.html#hypothesis-tests-and-rejectionacceptance-regions",
    "href": "resources/slides/15-CI.html#hypothesis-tests-and-rejectionacceptance-regions",
    "title": "Confidence & Credible Regions",
    "section": "Hypothesis Tests and Rejection/Acceptance Regions",
    "text": "Hypothesis Tests and Rejection/Acceptance Regions\nRecall for a level \\(\\alpha\\) test of a point null hypothesis\n\nwe reject \\(H\\) with probability \\(\\alpha\\) when \\(H\\) is true\nfor each test we can construct:\n\na rejection region \\(R(\\boldsymbol{\\theta}) \\subset \\cal{Y}\\), the \\(Y\\) values for which we reject \\(H\\)\nan acceptance region \\(A(\\boldsymbol{\\theta})  \\subset \\cal{Y}\\), the \\(Y\\) values for which we accept \\(H\\)\n\nthese sets are complements of each other (for non-randomized tests)\n\n\n\\[\\Pr(\\mathbf{Y}\\in A(\\boldsymbol{\\theta}) \\mid \\boldsymbol{\\theta}) =  1 - \\alpha\\]"
  },
  {
    "objectID": "resources/slides/15-CI.html#duality-of-hypothesis-testingconfidence-regions",
    "href": "resources/slides/15-CI.html#duality-of-hypothesis-testingconfidence-regions",
    "title": "Confidence & Credible Regions",
    "section": "Duality of Hypothesis-Testing/Confidence Regions",
    "text": "Duality of Hypothesis-Testing/Confidence Regions\nSuppose we have a level \\(\\alpha\\) test for every possible valuse of \\(\\boldsymbol{\\theta}\\)\n\nfor each \\(\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}\\), let \\(A(\\boldsymbol{\\theta})\\) be the acceptance region of the test \\(\\mathbf{Y}\\sim P_{\\boldsymbol{\\theta}}\\)\nthen \\(P(\\mathbf{Y}\\in A(\\boldsymbol{\\theta}) \\mid \\boldsymbol{\\theta}) = 1 - \\alpha\\) for each \\(\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}\\)\nThis collection of hypothesis tests can be “inverted” to construct a confidence region for θ, as follows:\ndefine \\(C(\\mathbf{Y}) = \\{ \\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}: \\mathbf{Y}\\in A(\\boldsymbol{\\theta}) \\}\\)\nthis is the set of \\(\\boldsymbol{\\theta}\\) values that are not rejected when \\(\\mathbf{Y}= \\mathbf{y}\\) is observed\nthen \\(C\\) is a \\(1 - \\alpha\\) confidence region for \\(\\boldsymbol{\\theta}\\)"
  },
  {
    "objectID": "resources/slides/15-CI.html#confidence-intervals-for-regression-parameters",
    "href": "resources/slides/15-CI.html#confidence-intervals-for-regression-parameters",
    "title": "Confidence & Credible Regions",
    "section": "Confidence Intervals for Regression Parameters",
    "text": "Confidence Intervals for Regression Parameters\nFor the linear model \\(\\mathbf{Y}\\sim \\textsf{N}(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I})\\), confidence intervals for \\(\\beta_j\\) can be constructed from inverting the approriate \\(t\\)-test.\n\nsuppose you are testing \\(H: \\beta_j = 0\\)\nif \\(H\\) is true, then\n\n\\(\\hat{\\beta}_j - \\beta_j \\sim \\textsf{N}(0, \\sigma^2 v_{jj})\\) where \\(v_{jj}\\) is the \\(j\\)th diagonal element of \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\)\n\\(s^2 \\sim \\sigma^2 \\chi^2_{n-p}/(n-p)\\)\n\\(\\hat{\\beta}_j\\) and \\(s^2\\) are independent\n\ntherefore if \\(H\\) is true \\[t_j = \\frac{\\hat{\\beta}_j - \\beta_j}{s\\sqrt{v_{jj}}} \\sim t_{n-p}\\]"
  },
  {
    "objectID": "resources/slides/15-CI.html#acceptance-region-confidence-interval",
    "href": "resources/slides/15-CI.html#acceptance-region-confidence-interval",
    "title": "Confidence & Credible Regions",
    "section": "Acceptance Region & Confidence Interval",
    "text": "Acceptance Region & Confidence Interval\n\ndefine the acceptance region \\(A(\\beta_j) = \\{ \\hat{\\beta}_j, s^2: |t_j| &lt; t_{n-p, 1 - \\alpha/2}\\}\\)\n\nwe have that \\(H\\) is accepted if \\[t_j \\in A(\\beta_j) \\Leftrightarrow \\frac{|\\hat{\\beta}_j - \\beta_j|}{s\\sqrt{v_{jj}}} &lt; t_{n-p, 1-\\alpha/2}\\]\nNow construct a confidence interval for the true value by inverting the tests: \\[\\begin{align*}\nC(\\hat{\\beta}_j, s^2) & =  (\\hat{\\beta_j}, s^2) \\in A(\\beta_j) \\\\\n& = \\left\\{ \\beta_j: |\\hat{\\beta}_j - \\beta_j| &lt; s\\sqrt{v_{jj}} t_{n-p, 1 - \\alpha/2} \\right\\}\\\\\n& = \\left\\{ \\beta_j: \\hat{\\beta}_j - s\\sqrt{v_{jj}} t_{n-p, 1 - \\alpha/2} &lt; \\beta_j &lt; \\hat{\\beta}_j + s\\sqrt{v_{jj}} t_{n-p, 1 - \\alpha/2} \\right\\} \\\\\n& = \\hat{\\beta}_j \\pm s\\sqrt{v_{jj}}\\, t_{n-p, 1 - \\alpha/2}\n\\end{align*}\\]\nfor \\(\\alpha = 0.05\\) and large \\(n\\), \\(t_{n-p,0.975} \\approx 2\\), so CI is approximately \\(\\hat{\\beta}_j \\pm 2s\\sqrt{v_{jj}}\\)"
  },
  {
    "objectID": "resources/slides/15-CI.html#confidence-intervals-for-linear-functions",
    "href": "resources/slides/15-CI.html#confidence-intervals-for-linear-functions",
    "title": "Confidence & Credible Regions",
    "section": "Confidence Intervals for Linear Functions",
    "text": "Confidence Intervals for Linear Functions\nFor a linear function of the parameters \\(\\lambda = \\mathbf{a}^T \\boldsymbol{\\beta}\\) we can construct a confidence interval by inverting the appropriate \\(t\\)-test\n\nmost important example \\(\\mathbf{a}^T\\boldsymbol{\\beta}= \\mathbf{x}^T\\boldsymbol{\\beta}= \\textsf{E}[\\mathbf{Y}\\mid \\mathbf{x}]\\)\nsuppose you are testing \\(H: \\mathbf{a}^T\\boldsymbol{\\beta}= m\\)\nIf \\(H\\) is true, \\(\\mathbf{a}^T\\boldsymbol{\\beta}- m \\sim \\textsf{N}(0, \\sigma^2 v)\\) where \\(v=\\mathbf{a}^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{a})\\)\n\\(s^2 \\sim \\sigma^2 \\chi^2_{n-p}/(n-p)\\) independent of \\(\\mathbf{a}^T\\hat{\\boldsymbol{\\beta}}\\)\nthen \\(t = \\frac{\\mathbf{a}^T\\hat{\\boldsymbol{\\beta}} - m}{s\\sqrt{v}} \\sim t_{n-p}\\)\na \\(1-\\alpha\\) confidence interval for \\(\\mathbf{a}^T\\boldsymbol{\\beta}\\) is \\[\\mathbf{a}^T\\hat{\\boldsymbol{\\beta}} \\pm s\\sqrt{v}\\, t_{n-p, 1 - \\alpha/2}\\]"
  },
  {
    "objectID": "resources/slides/15-CI.html#prediction-regions-and-intervals",
    "href": "resources/slides/15-CI.html#prediction-regions-and-intervals",
    "title": "Confidence & Credible Regions",
    "section": "Prediction Regions and Intervals",
    "text": "Prediction Regions and Intervals\nRelated to CI for \\(\\textsf{E}[Y \\mid \\mathbf{x}] = \\mathbf{x}^T\\boldsymbol{\\beta}\\), we may wish to construct a prediction interval for a new observation \\(Y^*\\) at \\(\\mathbf{x}_*\\)\n\na \\(1-\\alpha\\) prediction interval for \\(Y^*\\) is a set valued function of \\(\\mathbf{Y}\\), \\(C(\\mathbf{Y})\\) such that \\[\\Pr(\\mathbf{Y}^* \\in C(\\mathbf{Y}) \\mid \\boldsymbol{\\beta},\\sigma^2) = 1 - \\alpha\\] where the distribution is computed using the distribution of \\(\\mathbf{Y}^*\\)\nthis use the idea of a pivotal quantity: a function of the data and the parameters that has a known distribution that does not depend on any unknown parameters.\nfor prediction, \\(Y^* = \\mathbf{x}_*^T\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}^*\\) where \\(\\boldsymbol{\\epsilon}^* \\sim \\textsf{N}(0, \\sigma^2)\\) independent of \\(\\boldsymbol{\\epsilon}\\)\n\n\n\\[\\begin{align*}\n\\textsf{E}[Y^* - \\mathbf{x}_*^T\\hat{\\boldsymbol{\\beta}}]   & = \\mathbf{x}_*^T\\boldsymbol{\\beta}- \\mathbf{x}_*^T\\boldsymbol{\\beta}= 0 \\\\\n\\textsf{Var}(Y^* - \\mathbf{x}_*^T\\hat{\\boldsymbol{\\beta}}) & = \\textsf{Var}(\\boldsymbol{\\epsilon}^*)  + \\textsf{Var}(\\mathbf{x}_*^T\\hat{\\boldsymbol{\\beta}})   \n                         = \\sigma^2 + \\sigma^2 \\mathbf{x}_*^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{x}_* \\\\\nY^* - \\mathbf{x}_*^T\\hat{\\boldsymbol{\\beta}}& \\sim \\textsf{N}(0, \\sigma^2(1 + \\mathbf{x}_*^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{x}_*))\n\\end{align*}\\]"
  },
  {
    "objectID": "resources/slides/15-CI.html#pivotal-quantity-and-prediction-intervals",
    "href": "resources/slides/15-CI.html#pivotal-quantity-and-prediction-intervals",
    "title": "Confidence & Credible Regions",
    "section": "Pivotal Quantity and Prediction Intervals",
    "text": "Pivotal Quantity and Prediction Intervals\nSince \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(s^2\\) are independent, we can construct a pivotal quantity for \\(Y^* - \\mathbf{x}_*^T\\hat{\\boldsymbol{\\beta}}\\): \\[\\frac{Y^* - \\mathbf{x}_*^T\\hat{\\boldsymbol{\\beta}}}{s\\sqrt{1 + \\mathbf{x}_*^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{x}_*}} \\sim t_{n-p}\\]\n\ntherefore \\[\\Pr\\left(\\frac{|Y^* - \\mathbf{x}_*^T\\hat{\\boldsymbol{\\beta}}|}{s\\sqrt{1 + \\mathbf{x}_*^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{x}_*}} &lt; t_{n-p, 1-\\alpha/2} \\right) = 1 - \\alpha\\]\nRearranging gives a \\(1-\\alpha\\) prediction interval for \\(Y^*\\): \\[\\mathbf{x}_*^T\\hat{\\boldsymbol{\\beta}}\\pm s\\sqrt{1 + \\mathbf{x}_*^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{x}_*} t_{n-p, 1-\\alpha/2}\\]"
  },
  {
    "objectID": "resources/slides/15-CI.html#joint-confidence-regions-for-boldsymbolbeta",
    "href": "resources/slides/15-CI.html#joint-confidence-regions-for-boldsymbolbeta",
    "title": "Confidence & Credible Regions",
    "section": "Joint Confidence Regions for \\(\\boldsymbol{\\beta}\\)",
    "text": "Joint Confidence Regions for \\(\\boldsymbol{\\beta}\\)\n\nwe can construct a joint confidence region for \\(\\boldsymbol{\\beta}\\) based on inverting a test \\(H: \\boldsymbol{\\beta}= \\boldsymbol{\\beta}_0\\). Recall:\n\n\n\\[\\begin{align*}\n\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}& \\sim \\textsf{N}(0, \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1}) \\\\\n(\\mathbf{X}^T\\mathbf{X})^{-1/2}(\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}) & \\sim \\textsf{N}(0, \\sigma^2 \\mathbf{I}) \\\\\n(\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta})^T(\\mathbf{X}^T\\mathbf{X})^{-1}(\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}) & \\sim \\sigma^2 \\chi^2_p\n\\end{align*}\\]\n\nsince \\(s^2\\) is independent of \\(\\hat{\\boldsymbol{\\beta}}\\) we can construct a CI based on the \\(F\\)-distribution \\[\\frac{(\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}_0)^T(\\mathbf{X}^T\\mathbf{X})^{-1}(\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}_0)/p}{s^2} \\sim F_{p, n-p}\\]\ninverting the \\(F\\)-test gives a \\(1-\\alpha\\) confidence region for \\(\\boldsymbol{\\beta}\\): \\[\\{ \\boldsymbol{\\beta}: (\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}_0)^T(\\mathbf{X}^T\\mathbf{X})^{-1}(\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}_0)/s^2 &lt; p F_{p, n-p, 1-\\alpha} \\}\\]"
  },
  {
    "objectID": "resources/slides/15-CI.html#bayesian-credible-regions",
    "href": "resources/slides/15-CI.html#bayesian-credible-regions",
    "title": "Confidence & Credible Regions",
    "section": "Bayesian Credible Regions",
    "text": "Bayesian Credible Regions\n\nIn a Bayesian setting, we have a posterior distribution for \\(\\boldsymbol{\\beta}\\) given the data \\(\\mathbf{Y}\\)\na set \\(C \\in \\mathbb{R}^p\\) is a \\(1-\\alpha\\) posterior credible region (sometimes called a Bayesian confidence region) if \\(\\Pr(\\boldsymbol{\\beta}\\in C \\mid \\mathbf{Y}) = 1 - \\alpha\\)\nlots of sets have this property, but we usually want the most probable values of \\(\\boldsymbol{\\beta}\\) given the data\nthis motivates looking at the highest posterior density (HPD) region which is a \\(1-\\alpha\\) credible set \\(C\\) such that the values in \\(C\\) have higher posterior density than those outside of \\(C\\)\nthe HPD region is the smallest region that contains \\(1-\\alpha\\) of the posterior probability"
  },
  {
    "objectID": "resources/slides/15-CI.html#bayesian-credible-regions-1",
    "href": "resources/slides/15-CI.html#bayesian-credible-regions-1",
    "title": "Confidence & Credible Regions",
    "section": "Bayesian Credible Regions",
    "text": "Bayesian Credible Regions\n\nFor a normal prior and normal likelihood, the posterior for \\(\\boldsymbol{\\beta}\\) conditional on \\(\\sigma^2\\) is normal with say posterior mean \\(\\mathbf{b}_n\\) and posterior precision \\(\\boldsymbol{\\Phi}_n\\)\nthe posterior density as a function of \\(\\boldsymbol{\\beta}\\) for a fixed \\(\\sigma^2\\) is \\[p(\\boldsymbol{\\beta}\\mid \\mathbf{Y}) \\propto \\exp\\left\\{ -(\\boldsymbol{\\beta}- \\mathbf{b}_n)^T \\boldsymbol{\\Phi}_n (\\boldsymbol{\\beta}- \\mathbf{b}_n)/2 \\right\\}\\]\nso a highest posterior density region has the form \\[C = \\{ \\boldsymbol{\\beta}: (\\boldsymbol{\\beta}- \\mathbf{b}_n)^T\\boldsymbol{\\Phi}_n^{-1}(\\boldsymbol{\\beta}- \\mathbf{b}_n)  &lt; q \\}\\]\n\n\n\\[\\begin{align*}\n\\boldsymbol{\\beta}- \\boldsymbol{\\beta}_n \\mid \\sigma^2 & \\sim \\textsf{N}(0, \\boldsymbol{\\Phi}_n^{-1}) \\\\\n\\boldsymbol{\\Phi}_n^{1/2}(\\boldsymbol{\\beta}- \\boldsymbol{\\beta}_n) \\mid \\sigma^2 & \\sim \\textsf{N}(0, \\mathbf{I}) \\\\\n(\\boldsymbol{\\beta}- \\boldsymbol{\\beta}_n)^T \\boldsymbol{\\Phi}_n (\\boldsymbol{\\beta}- \\boldsymbol{\\beta}_n) \\mid \\sigma^2 & \\sim  \\chi^2_p\n\\end{align*}\\]\n\nsetting \\(q = \\chi^2_{p, 1-\\alpha}\\) gives a Credible Region for \\(\\Pr(\\boldsymbol{\\beta}\\in C \\mid \\mathbf{Y}) = 1 - \\alpha\\)"
  },
  {
    "objectID": "resources/slides/15-CI.html#bayesian-hpd-regions-for-unknown-sigma2",
    "href": "resources/slides/15-CI.html#bayesian-hpd-regions-for-unknown-sigma2",
    "title": "Confidence & Credible Regions",
    "section": "Bayesian HPD Regions For Unknown \\(\\sigma^2\\)",
    "text": "Bayesian HPD Regions For Unknown \\(\\sigma^2\\)\n\nFor unknown \\(\\sigma^2\\) we need to integrate out \\(\\sigma^2\\) to get the marginal posterior for \\(\\boldsymbol{\\beta}\\)\nfor conjugate priors, \\(\\boldsymbol{\\beta}\\mid \\phi \\sim \\textsf{N}(\\mathbf{b}_0, (\\phi \\boldsymbol{\\Phi}_0)^{-1})\\) and \\(\\phi \\sim \\mathbf{G}(a_0/2, b_0/2)\\), then \\[\\begin{align*}\n\\boldsymbol{\\beta}\\mid \\phi, \\mathbf{Y}& \\sim \\textsf{N}(\\mathbf{b}_n, (\\phi \\boldsymbol{\\Phi}_n)^{-1}) \\\\\n\\phi \\mid \\mathbf{Y}& \\sim \\mathbf{G}(a_n/2, b_n/2) \\\\\n\\boldsymbol{\\beta}\\mid \\mathbf{Y}& \\sim \\textsf{St}(a_n, \\mathbf{b}_n, \\hat{\\sigma}^2\\boldsymbol{\\Phi}_n^{-1})\n\\end{align*}\\] where \\(\\textsf{St}(a_n, \\mathbf{b}_n, \\hat{\\sigma}^2\\boldsymbol{\\Phi}_n^{-1})\\) is a multivariate Student-t distribution with \\(a_n\\) degrees of freedom location \\(\\mathbf{b}_n\\) and scale matrix \\(\\hat{\\sigma}^2\\boldsymbol{\\Phi}_n^{-1}\\) with \\(\\hat{\\sigma}^2 = b_n/a_n\\)\ndensity of \\(\\boldsymbol{\\beta}\\) is \\[p(\\boldsymbol{\\beta}\\mid \\mathbf{Y}) \\propto \\left(1 + \\frac{(\\boldsymbol{\\beta}- \\mathbf{b}_n)^T\\boldsymbol{\\Phi}_n(\\boldsymbol{\\beta}- \\mathbf{b}_n)}{a_n \\hat{\\sigma}^2 } \\right)^{-(a_n + p)/2}\\]"
  },
  {
    "objectID": "resources/slides/15-CI.html#reference-posterior-distribution",
    "href": "resources/slides/15-CI.html#reference-posterior-distribution",
    "title": "Confidence & Credible Regions",
    "section": "Reference Posterior Distribution",
    "text": "Reference Posterior Distribution\nFor the reference prior \\(\\pi(\\boldsymbol{\\beta},\\phi) \\propto 1/\\phi\\) and the likelihood \\(p(\\mathbf{Y}\\mid \\boldsymbol{\\beta})\\), the posterior is proportional to the likelihood times \\(\\phi^{-1}\\)\n\n(generalized) posterior distribution: \\[\\begin{align*}\n\\boldsymbol{\\beta}\\mid \\phi, \\mathbf{Y}& \\sim \\textsf{N}(\\hat{\\boldsymbol{\\beta}}, (\\phi \\mathbf{X}^T\\mathbf{X})^{-1}) \\\\\n\\phi \\mid \\mathbf{Y}& \\sim \\mathbf{G}((n-p)/2, \\textsf{SSE}/2)\n\\end{align*}\\] if \\(n &gt; p\\)\nmarginal posterior distribution for \\(\\boldsymbol{\\beta}\\) is multivariate Student-t with \\(n-p\\) degrees of freedom, location \\(\\hat{\\boldsymbol{\\beta}}\\) and scale matrix \\(\\hat{\\sigma}^2\\mathbf{X}^T\\mathbf{X}^{-1}\\)"
  },
  {
    "objectID": "resources/slides/15-CI.html#duality",
    "href": "resources/slides/15-CI.html#duality",
    "title": "Confidence & Credible Regions",
    "section": "Duality",
    "text": "Duality\n\nthe posterior density \\(\\boldsymbol{\\beta}\\) is a monotonically decreasing function of \\(Q(\\boldsymbol{\\beta}) \\equiv (\\boldsymbol{\\beta}- \\hat{\\boldsymbol{\\beta}})^T\\mathbf{X}^T\\mathbf{X}(\\boldsymbol{\\beta}- \\hat{\\boldsymbol{\\beta}})\\) so contours of \\(p(\\boldsymbol{\\beta}\\mid \\mathbf{Y})\\) are ellipsoidal in the parameter space of \\(\\boldsymbol{\\beta}\\)\nthe quantity \\(Q(\\boldsymbol{\\beta})/p \\hat{\\sigma}^2\\) is distributed a posteriori \\[ Q(\\boldsymbol{\\beta})/p \\hat{\\sigma}^2 \\sim F(p, n-p)\\] and the ellipsoidal contour of \\(p(\\boldsymbol{\\beta}\\mid \\mathbf{Y})\\) is defined as \\(\\frac{Q(\\boldsymbol{\\beta})}{p \\hat{\\sigma}^2} = F(p, n-p, \\alpha)\\). (Box & Tiao 1973)\nthen HPD regions for \\(\\boldsymbol{\\beta}\\) are the same as confidence regions for \\(\\boldsymbol{\\beta}\\) based on the \\(F\\)-distribution\nmarginals of \\(\\beta_j\\), \\(\\mathbf{x}^T\\boldsymbol{\\beta}\\) and \\(Y^*\\) are also univariate Student-t with \\(n-p\\) degrees of freedom\ndifference is in the interpretation of the regions i.e posterior probability that \\(\\boldsymbol{\\beta}\\) is in the given the data vs the probability a priori that the region covers the true \\(\\boldsymbol{\\beta}\\)\n\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": " Schedule",
    "section": "",
    "text": "Please refresh often in case links/content has been updated\n\n\n\n\n\n\n\n\nWeek\nDate\nLesson\nReading\nSlides\nLabs\nHomework\n\n\n\n\nWEEK 1\nTues, Aug 26\nLecture 1: Introduction to Linear Models\n\n\n\n\n\n\n\n\n\n\nThur, Aug 28\nLecture 2: MLEs & Projections\n\n\n\n\n hw-01\n\n\n\n\nFri, Aug 29\n\n\n\n\n\n\n\n\n\n\n\nWEEK 2\nTues, Sept 3\nLecture 3: Rank Deficient Models\n\n\n\n\n\n\n\n\n\n\nThur, Sept 5\nLecture 4: Best Linear Unbiased Estimation and Gauss-Markov Theorem\n\n\n\n\n hw-02\n\n\n\n\nFri, Sept 6\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 3\nTues, Sept 10\nLecture 5: BLUES for Prediction and MVUE\n\n\n\n\n\n\n\n\n\n\nThur, Sept 12\nLecture 6: Generalized Linear Squares\n\n\n\n\n hw-03\n\n\n\n\nFri, Sept 13\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 4\nTues, Sept 17\nLecture 7: Sampling Distributions & Distribution Theory\n\n\n\n\n\n\n\n\n\n\nThur, Sept 19\nLecture 8: Bayesian Estimation\n\n\n\n\n hw-04\n\n\n\n\nFri, Sept 20\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 5\nTues, Sept 24\nLecture 9: Bayes and Frequentist Risk\n\n\n\n\n\n\n\n\n\n\nThur, Sept 26\nLecture 10: James-Stein\n\n\n\n\n hw-05\n\n\n\n\nFri, Sept 28\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 6\nTues, Oct 1\nLecture 11: Shrinkage Estimators and Hierarchical Bayes\n\n\n\n\n\n\n\n\n\n\nThu, Oct 3\nLecture 12: Shrinkage/Selection and Oracle Properties\n\n\n\n\n hw-06\n\n\n\n\nFri, Oct 4\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 7\nTues, Oct 8\nLecture 13: Testing Hypotheses\n\n\n\n\n\n\n\n\n\n\nThu, Oct 10\nLecture 14: Testing Submodels\n\n\n\n\n hw-07\n\n\n\n\nFri, Oct 11\nReview in Lab (see Canvas)\n\n\n\n\n\n\n\n\n\n\nWEEK 8\nTue, Oct 15\nNO CLASS FALL BREAK\n\n\n\n\n\n\n\n\n\n\n\n\nThur, Oct 17\nNO CLASS - Use time for review\n\n\n\n\n\n\n\n\n\n\n\n\nFri, Oct 18\nMidterm 1\n\n\n\n\n\n\n\n\n\n\nWEEK 9\nTue, Oct 22\nLecture 15: Confidence Regions and Intervals\n\n\n\n\n\n\n\n\n\n\nThur, Oct 24\nLecture 16: Intro to Bayesian Hypothesis Testing\n\n\n\n\n hw-08\n\n\n\n\nFri, Oct 25\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 10\nTues, Oct 29\nLecture 17: Bayesian Model Averaging\n\n\n\n\n\n\n\n\n\n\nThur, Oct 31\nLecture 18: BMA and Bayesian Variable Selection\n\n\n\n\n\n\n\n\n\n\nFri, Nov 1\n\n\n\n\n\n\n\n\n hw-09\n\n\nWeek 11\nTues, Nov 5\nLecture 19:\n\n\n\n\n\n\n\n\n\n\n\n\nThurs, Nov 7\nLecture 20:\n\n\n\n\n\n\n\n\n\n\n\n\nFri, Nov 8\n\n\n\n\n\n\n\n\n hw-10\n\n\nWeek 12\nTues, Nov 12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThurs, Nov 14\nMidterm2\n\n\n\n\n\n\n\n\n\n\n\n\nFri, Nov 15\n\n\n\n\n\n\n\n\n hw-11\n\n\nWeek 12\nTues, Nov 19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThurs, Nov 21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFri, Nov 22\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 13\nTues, Nov 26\n\n\n\n\n\n\n\n\n hw-12\n\n\n\n\nThur, Nov 30\nNO CLASS THANKSGIVING\n\n\n\n\n\n\n\n\n\n\nWeek 14\n\n\nGraduate Reading Period\n\n\n\n\n\n\n\n\n\n\nFinals Period\nSunt, Dec 15 9am-12pm (in classroom)"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "STA721: Linear Models (Fall 2024)",
    "section": "Course Description",
    "text": "Course Description\nThis course introduces students to linear models and extensions for model building from the frequentist (OLS/MLE and penalized likelihoods) and Bayesian paradigms, with an emphasis on a geometric perspective. Course topics include optimal estimation and prediction, distributional assumptions and model checking, hypothesis testing and model selection including Bayes factors and intrinsic Bayes factors, and Bayesian Model Averaging. Students should have a strong background in matrix algebra and distribution theory. Co-requisite: STA 602L, 702L or equivalent."
  },
  {
    "objectID": "index.html#course-info",
    "href": "index.html#course-info",
    "title": "STA721: Linear Models (Fall 2024)",
    "section": "Course Info",
    "text": "Course Info\n\nTextbooks\n\n\n\nTextbook\nOrdering Information\n\n\n\n\n\nPlane Answers to Complex Questions Ronald Christensen (2011) 4th Edition Springer-Verlag, NY. The textbook is freely available as an eBook thru the Duke Library. You’re welcomed to read on screen or print it out. If you prefer a paperback version you can buy it at the cost of printing from Springer or purchase a hardback version at your favorite vendor.\n\n\n\nLinear Regression Analysis, George A.F Seber and Alan J. Lee (2003) 2nd Edition, Wiley eBook in Duke Library. Duke Library is aware the link to the ebook is broken See the\n\n\n\nCanvas site for a pdf version until the links are fixed.\n\n\nLecture\n   Tuesday and Thursday\n   1:25pm - 2:40pm\n   Old Chemistry 123 \n\n\nLabs\n   Fridays\n  10:05am - 11:20pm\n   LINK 088 (Clasroom 4) \n\n\nFinal Exam\n   December 15\n   9:00am - 12:00pm\n   Old Chemistry 123 \n\n\nInstructional Team and Office Hours\n\n\n\nRole\nName\nEmail\nOffice Hours\nLocation\n\n\n\n\nInstructor\nDr Merlise Clyde\n\nTues 2:45 - 3:45  or by appointment \n223E Old Chem\n\n\nTA\nBongjung Sung\n\nMon 9:30-11:30am\n203B Old Chem"
  },
  {
    "objectID": "index.html#course-topics",
    "href": "index.html#course-topics",
    "title": "STA721: Linear Models (Fall 2024)",
    "section": "Course Topics",
    "text": "Course Topics\nCourse topics will be drawn (but subject to change) from\n\nMotivation for Studying Linear Models as Foundation\nRandom Vectors and Matrices\nMultivariate Normal Distribution Theory\nConditional Normal Distribution Theory\nLinear Models via Coordinate free representations (examples)\nMaximum Likelihood Estimation & Projections\nInterval Estimation: Distribution of Quadratic Forms\nGauss-Markov Theorem & Optimality of OLS/GLS\nFormulation of Bayesian Inference\nSubjective and Default Priors\nRelated Shrinkage Methods and Penalized Likelihoods (Ridge regression, lasso)\nModel Selection (comparison of classical and Bayesian approaches)\nBayes Factors\nBayesian Model Averaging\nModel Checking: Residual Analysis & Diagnostics\nRobust Methods for Outliers\nGeneralized Linear Model\nHierarchical Models\n\nPlease check the website for updates, slides and current readings."
  },
  {
    "objectID": "HW/hw-02.html",
    "href": "HW/hw-02.html",
    "title": "Homework 2",
    "section": "",
    "text": "See Gradescope for any updates on due dates."
  },
  {
    "objectID": "HW/hw-02.html#due-1100pm-thurs-sept-12",
    "href": "HW/hw-02.html#due-1100pm-thurs-sept-12",
    "title": "Homework 2",
    "section": "",
    "text": "See Gradescope for any updates on due dates."
  },
  {
    "objectID": "HW/hw-02.html#rstudio",
    "href": "HW/hw-02.html#rstudio",
    "title": "Homework 2",
    "section": "RStudio",
    "text": "RStudio\nYou all should have R and RStudio installed on your computers by now or access to the Department RStudio Server https://rstudio.stat.duke.edu. If you need to setup your local version, first install the latest version of R here: https://cran.rstudio.com - remember to select the right installer for your operating system). Next, install the latest version of RStudio here: https://www.rstudio.com/products/rstudio/download/. Scroll down to the “Installers for Supported Platforms” section and find the right installer for your operating system."
  },
  {
    "objectID": "HW/hw-02.html#r-rnw",
    "href": "HW/hw-02.html#r-rnw",
    "title": "Homework 2",
    "section": "R & RNW",
    "text": "R & RNW\nYou are required to use the .Rnw format to type up this report report. To get started see Knitr with LaTeX article by Karl Broman. Feel free to include images of technical derivations if you are short of time to typeset in LaTeX. The macros.tex file should help with shortcuts. You may need to change some of your Project or Global options to get your document to compile with KnitR. If so please post under Ed in the General category or ask in Lab."
  },
  {
    "objectID": "HW/hw-02.html#getting-started-with-github-classroom",
    "href": "HW/hw-02.html#getting-started-with-github-classroom",
    "title": "Homework 2",
    "section": "Getting started with Github Classroom",
    "text": "Getting started with Github Classroom\nGo to Github classroom to accept the invitation to create a repository for this assignment at HW2\nThis will create a private repo in the STA721-F24 organization on Github with your github user name. Note: You may receive an email invitation to join STA702-F24 on your behalf.\nHit refresh, to see the link for the repo, and then click on it to go to the STA721-F24 organization in Github.\nFollow the instructions in the README in your repo and in the hw2.Rnw file to complete the assignment and push to GitHub. If you encounter issues with Gradescope submission, it is critical that you have pushed your final assignment to GitHub by the due date to validate timestamps."
  },
  {
    "objectID": "HW/hw-02.html#gradescope-submission",
    "href": "HW/hw-02.html#gradescope-submission",
    "title": "Homework 2",
    "section": "Gradescope Submission",
    "text": "Gradescope Submission\nYou MUST submit both your final .Rnw .pdf files to the repo on Github and upload the pdf to Gradescope here: https://www.gradescope.com/courses/843802/assignments\nMake sure to to pdf; ask the TA about knitting to pdf if you cannot figure it out. Be sure to submit under the right assignment entry by the due date!"
  },
  {
    "objectID": "HW/hw-02.html#grading",
    "href": "HW/hw-02.html#grading",
    "title": "Homework 2",
    "section": "Grading",
    "text": "Grading\nTotal: 20 points."
  },
  {
    "objectID": "HW/hw-04.html",
    "href": "HW/hw-04.html",
    "title": "Homework 4",
    "section": "",
    "text": "See Gradescope for any updates on due dates.\nCreate your repo from GitHub Classroom"
  },
  {
    "objectID": "HW/hw-04.html#due-1100pm-thurs-sept-27",
    "href": "HW/hw-04.html#due-1100pm-thurs-sept-27",
    "title": "Homework 4",
    "section": "",
    "text": "See Gradescope for any updates on due dates.\nCreate your repo from GitHub Classroom"
  },
  {
    "objectID": "HW/hw-04.html#rstudio",
    "href": "HW/hw-04.html#rstudio",
    "title": "Homework 4",
    "section": "RStudio",
    "text": "RStudio\nYou all should have R and RStudio installed on your computers by now or access to the Department RStudio Server https://rstudio.stat.duke.edu. If you need to setup your local version, first install the latest version of R here: https://cran.rstudio.com - remember to select the right installer for your operating system). Next, install the latest version of RStudio here: https://www.rstudio.com/products/rstudio/download/. Scroll down to the “Installers for Supported Platforms” section and find the right installer for your operating system."
  },
  {
    "objectID": "HW/hw-04.html#getting-started-with-github-classroom",
    "href": "HW/hw-04.html#getting-started-with-github-classroom",
    "title": "Homework 4",
    "section": "Getting started with Github Classroom",
    "text": "Getting started with Github Classroom\nGo to Github classroom to accept the invitation to create a repository for this assignment.\nThis will create a private repo in the STA721-F24 organization on Github with your github user name. Note: You may receive an email invitation to join STA702-F24 on your behalf.\nHit refresh, to see the link for the repo, and then click on it to go to the STA721-F24 organization in Github.\nFollow the instructions in the README in your repo and in the hw#.Rnw file to complete the assignment and push to GitHub. If you encounter issues with Gradescope submission, it is critical that you have pushed your final assignment to GitHub by the due date to validate timestamps."
  },
  {
    "objectID": "HW/hw-04.html#r-rnw",
    "href": "HW/hw-04.html#r-rnw",
    "title": "Homework 4",
    "section": "R & RNW",
    "text": "R & RNW\nYou are required to use the .Rnw format to type up this report report if there are any R problems. To get started see Knitr with LaTeX article by Karl Broman. Feel free to include images of technical derivations if you are short of time to typeset in LaTeX. The macros.tex file should help with shortcuts. You may need to change some of your Project or Global options to get your document to compile with KnitR. If so please post under Ed in the General category or ask in Lab."
  },
  {
    "objectID": "HW/hw-04.html#gradescope-submission",
    "href": "HW/hw-04.html#gradescope-submission",
    "title": "Homework 4",
    "section": "Gradescope Submission",
    "text": "Gradescope Submission\nYou MUST submit both your final .Rnw .pdf files to the repo on Github and upload the pdf to Gradescope here: https://www.gradescope.com/courses/843802/assignments\nMake sure to to pdf; ask the TA about knitting to pdf if you cannot figure it out. Be sure to submit under the right assignment entry by the due date!"
  },
  {
    "objectID": "HW/hw-04.html#grading",
    "href": "HW/hw-04.html#grading",
    "title": "Homework 4",
    "section": "Grading",
    "text": "Grading\nTotal: 20 points."
  },
  {
    "objectID": "HW/hw-08.html",
    "href": "HW/hw-08.html",
    "title": "Homework 8",
    "section": "",
    "text": "See Gradescope for any updates on due dates.\nCreate your repo for the HW from GitHub Classroom"
  },
  {
    "objectID": "HW/hw-08.html#due-1000pm-sunday-november-3",
    "href": "HW/hw-08.html#due-1000pm-sunday-november-3",
    "title": "Homework 8",
    "section": "",
    "text": "See Gradescope for any updates on due dates.\nCreate your repo for the HW from GitHub Classroom"
  },
  {
    "objectID": "HW/hw-08.html#rstudio",
    "href": "HW/hw-08.html#rstudio",
    "title": "Homework 8",
    "section": "RStudio",
    "text": "RStudio\nYou all should have R and RStudio installed on your computers by now or access to the Department RStudio Server https://rstudio.stat.duke.edu. If you need to setup your local version, first install the latest version of R here: https://cran.rstudio.com - remember to select the right installer for your operating system). Next, install the latest version of RStudio here: https://www.rstudio.com/products/rstudio/download/. Scroll down to the “Installers for Supported Platforms” section and find the right installer for your operating system."
  },
  {
    "objectID": "HW/hw-08.html#getting-started-with-github-classroom",
    "href": "HW/hw-08.html#getting-started-with-github-classroom",
    "title": "Homework 8",
    "section": "Getting started with Github Classroom",
    "text": "Getting started with Github Classroom\nGo to Github classroom to accept the invitation to create a repository for this assignment.\nThis will create a private repo in the STA721-F24 organization on Github with your github user name. Note: You may receive an email invitation to join STA702-F24 on your behalf.\nHit refresh, to see the link for the repo, and then click on it to go to the STA721-F24 organization in Github.\nFollow the instructions in the README in your repo and in the hw#.Rnw file to complete the assignment and push to GitHub. If you encounter issues with Gradescope submission, it is critical that you have pushed your final assignment to GitHub by the due date to validate timestamps."
  },
  {
    "objectID": "HW/hw-08.html#r-rnw",
    "href": "HW/hw-08.html#r-rnw",
    "title": "Homework 8",
    "section": "R & RNW",
    "text": "R & RNW\nYou are required to use the .Rnw format to type up this report report if there are any R problems. To get started see Knitr with LaTeX article by Karl Broman. Feel free to include images of technical derivations if you are short of time to typeset in LaTeX. The macros.tex file should help with shortcuts. You may need to change some of your Project or Global options to get your document to compile with KnitR. If so please post under Ed in the General category or ask in Lab."
  },
  {
    "objectID": "HW/hw-08.html#gradescope-submission",
    "href": "HW/hw-08.html#gradescope-submission",
    "title": "Homework 8",
    "section": "Gradescope Submission",
    "text": "Gradescope Submission\nYou MUST submit both your final .Rnw .pdf files to the repo on Github and upload the pdf to Gradescope here: https://www.gradescope.com/courses/843802/assignments\nMake sure to to pdf; ask the TA about knitting to pdf if you cannot figure it out. Be sure to submit under the right assignment entry by the due date!"
  },
  {
    "objectID": "HW/hw-08.html#grading",
    "href": "HW/hw-08.html#grading",
    "title": "Homework 8",
    "section": "Grading",
    "text": "Grading\nTotal: 20 points."
  },
  {
    "objectID": "HW/hw-06.html",
    "href": "HW/hw-06.html",
    "title": "Homework 5",
    "section": "",
    "text": "See Gradescope for any updates on due dates.\nCreate your repo for the HW from GitHub Classroom"
  },
  {
    "objectID": "HW/hw-06.html#due-1000pm-fri-oct-4",
    "href": "HW/hw-06.html#due-1000pm-fri-oct-4",
    "title": "Homework 5",
    "section": "",
    "text": "See Gradescope for any updates on due dates.\nCreate your repo for the HW from GitHub Classroom"
  },
  {
    "objectID": "HW/hw-06.html#rstudio",
    "href": "HW/hw-06.html#rstudio",
    "title": "Homework 5",
    "section": "RStudio",
    "text": "RStudio\nYou all should have R and RStudio installed on your computers by now or access to the Department RStudio Server https://rstudio.stat.duke.edu. If you need to setup your local version, first install the latest version of R here: https://cran.rstudio.com - remember to select the right installer for your operating system). Next, install the latest version of RStudio here: https://www.rstudio.com/products/rstudio/download/. Scroll down to the “Installers for Supported Platforms” section and find the right installer for your operating system."
  },
  {
    "objectID": "HW/hw-06.html#getting-started-with-github-classroom",
    "href": "HW/hw-06.html#getting-started-with-github-classroom",
    "title": "Homework 5",
    "section": "Getting started with Github Classroom",
    "text": "Getting started with Github Classroom\nGo to Github classroom to accept the invitation to create a repository for this assignment.\nThis will create a private repo in the STA721-F24 organization on Github with your github user name. Note: You may receive an email invitation to join STA702-F24 on your behalf.\nHit refresh, to see the link for the repo, and then click on it to go to the STA721-F24 organization in Github.\nFollow the instructions in the README in your repo and in the hw#.Rnw file to complete the assignment and push to GitHub. If you encounter issues with Gradescope submission, it is critical that you have pushed your final assignment to GitHub by the due date to validate timestamps."
  },
  {
    "objectID": "HW/hw-06.html#r-rnw",
    "href": "HW/hw-06.html#r-rnw",
    "title": "Homework 5",
    "section": "R & RNW",
    "text": "R & RNW\nYou are required to use the .Rnw format to type up this report report if there are any R problems. To get started see Knitr with LaTeX article by Karl Broman. Feel free to include images of technical derivations if you are short of time to typeset in LaTeX. The macros.tex file should help with shortcuts. You may need to change some of your Project or Global options to get your document to compile with KnitR. If so please post under Ed in the General category or ask in Lab."
  },
  {
    "objectID": "HW/hw-06.html#gradescope-submission",
    "href": "HW/hw-06.html#gradescope-submission",
    "title": "Homework 5",
    "section": "Gradescope Submission",
    "text": "Gradescope Submission\nYou MUST submit both your final .Rnw .pdf files to the repo on Github and upload the pdf to Gradescope here: https://www.gradescope.com/courses/843802/assignments\nMake sure to to pdf; ask the TA about knitting to pdf if you cannot figure it out. Be sure to submit under the right assignment entry by the due date!"
  },
  {
    "objectID": "HW/hw-06.html#grading",
    "href": "HW/hw-06.html#grading",
    "title": "Homework 5",
    "section": "Grading",
    "text": "Grading\nTotal: 20 points."
  },
  {
    "objectID": "reading/17-bma.html",
    "href": "reading/17-bma.html",
    "title": "Lecture 17: Bayesian Model Averaging",
    "section": "",
    "text": "Mixture of g-priors Liang et al (2008)\nA First Course in Bayesian Statistical Methods by Peter D. Hoff\n\nSection 9.1: The linear regression model (review)\nSection 9.2: Bayesian estimation for a regression model\nSection 9.3: Model Selection (linear regression)\n\nBayesian Data Analysis (Third Edition) by Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin\n\nSection 14.2: Bayesian analysis of the classical regression model\n\nThe Bayesian Choice (Second Edition) by Christian Robert\nLiang, F., Paulo, R, Molina, G. Clyde, M, Berger, J (2008) Mixtures of g-priors for Bayesian Variable Selection. Journal of the American Statistical Association, 103: 410-423\nHoeting, J.A, Madigan, D. Raftery,A. Volinsky, C.T. (1999) Bayesian Model Averaging: A Tutorial Statistical Science, 14: 382-401\nPorwal, A & Raftery, A.E. Comparing methods for statistical inference with model uncertainty. Proceedings of the National Academy of Sciences, 119(16), e2120737119"
  },
  {
    "objectID": "reading/17-bma.html#readings",
    "href": "reading/17-bma.html#readings",
    "title": "Lecture 17: Bayesian Model Averaging",
    "section": "",
    "text": "Mixture of g-priors Liang et al (2008)\nA First Course in Bayesian Statistical Methods by Peter D. Hoff\n\nSection 9.1: The linear regression model (review)\nSection 9.2: Bayesian estimation for a regression model\nSection 9.3: Model Selection (linear regression)\n\nBayesian Data Analysis (Third Edition) by Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin\n\nSection 14.2: Bayesian analysis of the classical regression model\n\nThe Bayesian Choice (Second Edition) by Christian Robert\nLiang, F., Paulo, R, Molina, G. Clyde, M, Berger, J (2008) Mixtures of g-priors for Bayesian Variable Selection. Journal of the American Statistical Association, 103: 410-423\nHoeting, J.A, Madigan, D. Raftery,A. Volinsky, C.T. (1999) Bayesian Model Averaging: A Tutorial Statistical Science, 14: 382-401\nPorwal, A & Raftery, A.E. Comparing methods for statistical inference with model uncertainty. Proceedings of the National Academy of Sciences, 119(16), e2120737119"
  },
  {
    "objectID": "reading/05-BLUE-MVUE.html",
    "href": "reading/05-BLUE-MVUE.html",
    "title": "Best Linear Unbiased Estimation in Prediction, MVUEs and BUEs",
    "section": "",
    "text": "We will continue our discussion of OLS/MLE estimators with exploring conditions for when the Best Linear Unbiased Estimators exist, with a special focus on out of sample prediction in the non-full rank case. We will outline the proof for why MLEs are also Minimum Variance Unbiased Estimators or “BUE” out of all unbiased estimators linear or non-linear under the additional assumption of normality of the errors. This opens up the question of what estimators are unbiased for linear models and if there are other nonlinear unbiased estimators that are better than OLS, a topic that has received recent attention.\nReadings:\n\nChristensen Chapter 2, Appendix B\nSeber & Lee Chapter 3\n\nFor the curious\n\nWhat Estimators are Unbiased for Linear Models (2023) and references within\nAnderson, T.W. (1962). Least squares and best unbiased estimates. The Annals of Mathematical Statistics, 33(1): 266–272\nHansen, B.E. (2022) A modern gauss-markov theorem. Econometrica"
  },
  {
    "objectID": "reading/01-introduction.html",
    "href": "reading/01-introduction.html",
    "title": "Lecture 1 Readings",
    "section": "",
    "text": "links to eBooks are on the Home page of the website or Resources page."
  },
  {
    "objectID": "reading/01-introduction.html#introduction-to-linear-models",
    "href": "reading/01-introduction.html#introduction-to-linear-models",
    "title": "Lecture 1 Readings",
    "section": "Introduction to Linear Models",
    "text": "Introduction to Linear Models\n\nreview the course website and syllabus for policies"
  },
  {
    "objectID": "reading/01-introduction.html#vector-spaces",
    "href": "reading/01-introduction.html#vector-spaces",
    "title": "Lecture 1 Readings",
    "section": "Vector Spaces",
    "text": "Vector Spaces\nChristensen: Read\n\nChapter 1: pages 1-3\nAppendix A in Christensen pages 411-413\nAppendix B section B.1\n\nSee also Seber & Lee Chapter 1."
  },
  {
    "objectID": "reading/11-shrinkage.html",
    "href": "reading/11-shrinkage.html",
    "title": "Shrinkage and Hierarchical Bayes",
    "section": "",
    "text": "We discuss properties of the shrinkage estimators that provide both shrinkage and selection from frequentist and Bayesian paradigms.\nReadings:\n\nSeber & Lee Chapter 12\nTibshirani (1996) “Regression Shrinkage and Selection via the Lasso”\nPark & Casella (2008) “The Bayesian Lasso”\nHans (2010) “Model uncertainty and variable selection in Bayesian lasso regression”\nCarvalho, Polson & Scott (2009) “Handling Sparsity via the Horseshoe”\nArmagan, Dunson & Lee (2013) “Generalized Double Pareto Shrinkage”\nRobert, C. (2007) “The Bayesian Choice” Chapter 5"
  },
  {
    "objectID": "reading/09-bayes-freq-risk.html",
    "href": "reading/09-bayes-freq-risk.html",
    "title": "Bayesian and Frequentist Risk",
    "section": "",
    "text": "We study frequentist properties of Bayes estimators of the regression coefficients under the \\(g\\)-prior and independent normal prior that leads to ridge regression. In particular, there are values of the hper-parameters in the prior that lead to Bayes estimators dominating OLS in terms of MSE.\nReadings:\n\nChristensen Chapter 2.9 and Chapter 15\nSeber & Lee Chapter 3.12, 10.7.3 and 12\nHoerl, A.E. and Kennard, R.W. (1970) Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1):55–67\nZellner, A. (1986) On assessing prior distributions and Bayesian regression analysis with \\(g\\)-prior distributions. In Bayesian Inference and Decision Techniques: Essays in Honor of Bruno de Finetti, eds. P. K. Goel and A. Zellner, 233–243. Amsterdam: North-Holland"
  },
  {
    "objectID": "reading/02-mles.html",
    "href": "reading/02-mles.html",
    "title": "MLEs & Projections",
    "section": "",
    "text": "Readings:\n\nChristensen Chapter 1-2, Appendix A, and Appendix B\nSeber & Lee Chapter 3, Appendix B"
  },
  {
    "objectID": "reading/07-sampling.html",
    "href": "reading/07-sampling.html",
    "title": "Sampling Dsistributions & Distribution Theory",
    "section": "",
    "text": "Readings:\n\nChristensen Chapter 1, 2.91 and Appendix C\nSeber & Lee Chapter 3.3 3.5"
  },
  {
    "objectID": "reading/18-bvs.html",
    "href": "reading/18-bvs.html",
    "title": "Lecture 18: Bayesian Model Averaging & Bayesian Variable Selection",
    "section": "",
    "text": "Mixture of g-priors Liang et al (2008)\nBayesian Data Analysis (Third Edition) by Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin\n\nSection 14.2: Bayesian analysis of the classical regression model\n\nThe Bayesian Choice (Second Edition) by Christian Robert\nLiang, F., Paulo, R, Molina, G. Clyde, M, Berger, J (2008) Mixtures of g-priors for Bayesian Variable Selection. Journal of the American Statistical Association, 103: 410-423\nHoeting, J.A, Madigan, D. Raftery,A. Volinsky, C.T. (1999) Bayesian Model Averaging: A Tutorial Statistical Science, 14: 382-401\nPorwal, A & Raftery, A.E. Comparing methods for statistical inference with model uncertainty. Proceedings of the National Academy of Sciences, 119(16), e2120737119"
  },
  {
    "objectID": "reading/18-bvs.html#readings",
    "href": "reading/18-bvs.html#readings",
    "title": "Lecture 18: Bayesian Model Averaging & Bayesian Variable Selection",
    "section": "",
    "text": "Mixture of g-priors Liang et al (2008)\nBayesian Data Analysis (Third Edition) by Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin\n\nSection 14.2: Bayesian analysis of the classical regression model\n\nThe Bayesian Choice (Second Edition) by Christian Robert\nLiang, F., Paulo, R, Molina, G. Clyde, M, Berger, J (2008) Mixtures of g-priors for Bayesian Variable Selection. Journal of the American Statistical Association, 103: 410-423\nHoeting, J.A, Madigan, D. Raftery,A. Volinsky, C.T. (1999) Bayesian Model Averaging: A Tutorial Statistical Science, 14: 382-401\nPorwal, A & Raftery, A.E. Comparing methods for statistical inference with model uncertainty. Proceedings of the National Academy of Sciences, 119(16), e2120737119"
  },
  {
    "objectID": "reading/14-testing-submodels.html",
    "href": "reading/14-testing-submodels.html",
    "title": "Testing Hypotheses about Submodels",
    "section": "",
    "text": "We continue discussion of \\(F\\)-tests with testing hypotheses about submodels of a linear model and connections to \\(t\\)-tests for individual coefficients as well as likelihood ratio tests.\nReadings:\n\nChristensen Chapter 3, Appendix C"
  }
]