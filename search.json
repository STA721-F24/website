[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": " Resources",
    "section": "",
    "text": "Primary Textbooks\nThese textbooks are great resources for some of the topics we will cover.\n\nPlane Answers to Complex Questions, Ronald Christensen. eBook in Duke Library\nLinear Regression Analysis, George A.F Seber and Alan J. Lee eBook in Duke Library. Duke Library is aware the link is broken\nThe Linear Model and Hypothesis, George A. F. Seber eBook in Duke Library\n\n\n\nSupplementary Textbooks on Linear/Matrix Algebra\n\nGilbert Strang’s Online Course at MIT\nVideo Lectures\n\nIntroduction to Linear Algebra. Strang, Gilbert. 4th ed. Wellesley, MA: Wellesley-Cambridge Press, 2009. ISBN: 9780980232714. Buy @ Amazon\n\nMatrix Algebra from a Statistician’s Perspective. Harville, David A. eBook in Duke Library\n\n\n\nR and R Markdown Resources\nQuarto/R Markdown/LaTeX can be used to create high quality reports and presentations with embedded chunks of R code and LaTeX equations! You are required to use Quarto in RStudio to type up your homework assignments that involve Data Analysis/Simulation for this course, but you are welcome to use any word processor of your choice for those. To learn more about Quarto/R Markdown and for other resources for programming in R, see the links below.\n\nUsing R in Quarto for Documents\nR for Data Science (by Hadley Wickham & Garrett Grolemund)\nIntroduction to R Markdown (Article by Garrett Grolemund)\nIntroduction to R Markdown (Slides by Andrew Cho)\nR Markdown Cheat Sheet\nData Visualization with ggplot2 Cheat Sheet\nOther Useful Cheat Sheets\nA very (very!) basic R Markdown template\n\n\n\nLaTeX\nYou may also use LaTeX to type up your assignments. You may find it easier to create your TeX and LaTeX documents using online editors such as Overleaf (simply create a free account and you are good to go!). However, that need not be the case. If you prefer to create them locally/offline on your personal computers, you will need to download a TeX distribution (the most popular choices are MiKTeX for Windows and MacTeX for macOS) plus an editor (I personally prefer TeXstudio but feel free to download any editor of your choice). Follow the links below for some options, and to also learn how to use LaTeX.\n\nLearn LaTeX in 30 minutes\nChoosing a LaTeX Compiler.\n\n\n\nInteresting Articles\nI will add articles I find interesting below. These are articles I find useful as supplementary readings for topics covered in class, or as good sources that cover concepts I think you should know, but which we may not have time to cover. I strongly suggest you find time to (at the very least) take a “quick peek” at each article.\n\nEfron, B., 1986. Why isn’t everyone a Bayesian?. The American Statistician, 40(1), pp. 1-5.\nGelman, A., 2008. Objections to Bayesian statistics. Bayesian Analysis, 3(3), pp. 445-449.\nDiaconis, P., 1977. Finite forms of de Finetti’s theorem on exchangeability. Synthese, 36(2), pp. 271-281.\nGelman, A., Meng, X. L. and Stern, H., 1996. Posterior predictive assessment of model fitness via realized discrepancies. Statistica sinica, pp. 733-760.\nDunson, D. B., 2018. Statistics in the big data era: Failures of the machine. Statistics & Probability Letters, 136, pp. 4-9."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": " Schedule",
    "section": "",
    "text": "Please refresh often in case links/content has been updated\n\n\n\n\n\n\n\n\nWeek\nDate\nLesson\nReading\nSlides\nLabs\nHomework\n\n\n\n\nWEEK 1\nTues, Aug 26\nLecture 0: Course Overview and Introduction\n\n\n\n\n\n\n\n\n\n\nThur, Aug 28\n\n\n\n\n\n\n\n hw-01\n\n\n\n\nFri, Aug 29\n\n\n\n\n\n\n\n\n\n\n\nWEEK 2\nTues, Sept 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThur, Sept 5\n\n\n\n\n\n\n\n\n hw-02\n\n\n\n\nFri, Sept 6\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 3\nTues, Sept 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThur, Sept 12\n\n\n\n\n\n\n\n\n hw-03\n\n\n\n\nFri, Sept 13\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 4\nTues, Sept 17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThur, Sept 19\n\n\n\n\n\n\n\n\n hw-04\n\n\n\n\nFri, Sept 20\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 5\nTues, Sept 24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThur, Sept 26\n\n\n\n\n\n\n\n\n hw-05\n\n\n\n\nFri, Sept 28\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 6\nTues, Oct 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThu, Oct 3\n\n\n\n\n\n\n\n\n hw-06\n\n\n\n\nFri, Oct 4\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 7\nTue, Oct 8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThu, Oct 10\n\n\n\n\n\n\n\n\n hw-07\n\n\n\n\nFri, Oct 11\nReview for Midterm I\n\n\n\n\n\n\n\n\n\n\nWEEK 8\nTue, Oct 15\nNO CLASS FALL BREAK\n\n\n\n\n\n\n\n\n\n\n\n\nThur, Oct 17\nMidterm 1\n\n\n\n\n\n\n\n\n\n\n\n\nFri, Oct 18\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 9\nTue, Oct 22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThur, Oct 24\n\n\n\n\n\n\n\n\n hw-08\n\n\n\n\nFri, Oct 25\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 10\nTues, Oct 29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThur, Oct 31\n\n\n\n\n\n\n\n\n hw-09\n\n\n\n\nFri, Nov 1\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 11\nTues, Nov 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThurs, Nov 7\n\n\n\n\n\n\n\n\n hw-10\n\n\n\n\nFri, Nov 8\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 12\nTues, Nov 12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThurs, Nov 14\nMidterm2\n\n\n\n\n\n\n hw-11\n\n\n\n\nFri, Nov 15\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 12\nTues, Nov 19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThurs, Nov 21\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 13\nTues, Nov 26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThur, Nov 30\nNO CLASS THANKSGIVING\n\n\n\n\n\n\n\n\n\n\nWeek 14\n\n\nGraduate Reading Period\n\n\n\n\n\n\n\n\n\n\nFinals Period\nSunt, Dec 15 9am-12pm (in classroom)"
  },
  {
    "objectID": "resources/slides/01-introduction.html#introduction-to-sta721",
    "href": "resources/slides/01-introduction.html#introduction-to-sta721",
    "title": "Introduction to STA721",
    "section": "Introduction to STA721",
    "text": "Introduction to STA721\n\nCourse: Theory and Application of linear models from both a frequentist (classical) and Bayesian perspective\nPrerequisites: linear algebra and a mathematical statistics course covering likelihoods and distribution theory (normal, t, F, chi-square, gamma distributions)\nIntroduce R programming as needed in the lab\nIntroduce Bayesian methods, but assume that you are co-registered in 702 or have taken it previously\nmore info on Course website https://sta721-F24.github.io/website/\n\nschedule and slides, HW, etc\ncritical dates (Midterms and Finals)\noffice hours\n\nCanvas for grades, email, announcements\n\n\nPlease let me know if there are broken links for slides, etc!"
  },
  {
    "objectID": "resources/slides/01-introduction.html#notation",
    "href": "resources/slides/01-introduction.html#notation",
    "title": "Introduction to STA721",
    "section": "Notation",
    "text": "Notation\n\nscalors are \\(a\\) (italics or math italics)\nvectors are in bold lower case, \\(\\mathbf{a}\\), with the exception of random variables\nall vectors are column vectors \\[\\mathbf{a}= \\left[\\begin{array}{c}\n    a_1 \\\\\n    a_2 \\\\\n    \\vdots \\\\\n    a_n\n     \\end{array} \\right]\n\\]\n\n\\(\\mathbf{1}_n\\) is a \\(n \\times 1\\) vector of all ones\n\ninner product \\(\\langle    \\mathbf{a}, \\mathbf{a}\\rangle = \\mathbf{a}^T\\mathbf{a}= \\|\\mathbf{a}\\|^2 = \\sum_{i=1}^n a_i^2\\); \\(\\langle    \\mathbf{a}, \\mathbf{b}\\rangle = \\mathbf{a}^T\\mathbf{b}\\)\nlength or norm of \\(\\mathbf{a}\\) is \\(\\|\\mathbf{a}\\|\\)"
  },
  {
    "objectID": "resources/slides/01-introduction.html#matrices",
    "href": "resources/slides/01-introduction.html#matrices",
    "title": "Introduction to STA721",
    "section": "Matrices",
    "text": "Matrices\n\nMatrices are represented in bold \\(\\mathbf{A}= (a_{ij})\\) \\[\\mathbf{A}= \\left[\\begin{array}{cccc}\n    a_{11} & a_{12} & \\cdots & a_{1m}  \\\\\n    a_{21} & a_{22} & \\cdots & a_{2m} \\\\\n    \\vdots & \\vdots & \\vdots & \\vdots\\\\\n    a_{n1} & a_{n2} & \\cdots & a_{nm}\n     \\end{array} \\right]\n\\]\n\nidentity matrix \\(\\mathbf{I}_n\\) square matrix with diagonal elements 1 and off diagonal 0\ntrace: if \\(\\mathbf{A}\\) is \\(n \\times m\\) \\(\\textsf{tr}(\\mathbf{A}) = \\sum_i^{\\max n,m } a_{ii}\\)\ndeterminant: for \\(\\mathbf{A}\\) is \\(n \\times n\\) then the determinant is \\(\\det(A)\\)\ninverse: if \\(\\mathbf{A}\\) is nonsingular \\(\\mathbf{A}&gt; 0\\), then its inverse is \\(\\mathbf{A}^{-1}\\)"
  },
  {
    "objectID": "resources/slides/01-introduction.html#statistical-models",
    "href": "resources/slides/01-introduction.html#statistical-models",
    "title": "Introduction to STA721",
    "section": "Statistical Models",
    "text": "Statistical Models\nOhm’s Law: \\(Y\\) is voltage across a resistor of \\(r\\) ohms and \\(X\\) is the amperes of the current through the resistor (in theory) \\[Y = rX\\]\n\nSimple linear regression for observational data \\[Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\text{  for  } i = 1,\n\\ldots, n\\]\nRewrite in vectors: \\[\\begin{eqnarray*}\n\\left[\n\\begin{array}{c}  y_1 \\\\ \\vdots \\\\  y_n \\end{array}\n\\right]   =  &\n\\left[ \\begin{array}{c}  1 \\\\ \\vdots \\\\ 1 \\end{array}  \\right]   \\beta_0 +\n\\left[ \\begin{array}{c}  x_1 \\\\ \\vdots \\\\  x_n \\end{array}\n\\right] \\beta_1 +\n\\left[ \\begin{array}{c}  \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n  \\end{array}\n\\right]\n=  &\n\\left[ \\begin{array}{cc}  1 &  x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_n\\end{array}  \\right]\n\\left[ \\begin{array}{c}  \\beta_0  \\\\  \\beta_1 \\end{array}\n\\right] +\n\\left[ \\begin{array}{c}  \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n  \\end{array}\n\\right] \\\\\n\\\\\n\\mathbf{Y}= & \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "resources/slides/01-introduction.html#nonlinear-models",
    "href": "resources/slides/01-introduction.html#nonlinear-models",
    "title": "Introduction to STA721",
    "section": "Nonlinear Models",
    "text": "Nonlinear Models\nGravitational Law: \\(F = \\alpha/d^\\beta\\) where \\(d\\) is distance between 2 objects and \\(F\\) is the force of gravity between them\n\nlog transformations \\[\\log(F) = \\log(\\alpha) - \\beta \\log(d)\\]\ncompare to noisy experimental data \\(Y_i =\\log(F_i)\\) observed at \\(x_i = \\log(d_i)\\)\nwrite \\(\\mathbf{X}= [\\mathbf{1}_n \\, \\mathbf{x}]\\)\n\\(\\boldsymbol{\\beta}= (\\log(\\alpha), -\\beta)^T\\)\nmodel with additive error on log scale \\(\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{e}\\)\ntest if \\(\\beta = 2\\)\nerror assumptions?"
  },
  {
    "objectID": "resources/slides/01-introduction.html#intrinsically-nonlinear-models",
    "href": "resources/slides/01-introduction.html#intrinsically-nonlinear-models",
    "title": "Introduction to STA721",
    "section": "Intrinsically Nonlinear Models",
    "text": "Intrinsically Nonlinear Models\nRegression function may be an intrinsically nonlinear function of \\(t_i\\) (time) and parameters \\(\\boldsymbol{\\theta}\\) \\[Y_i = f(t_i, \\boldsymbol{\\theta}) + \\epsilon_i\\]"
  },
  {
    "objectID": "resources/slides/01-introduction.html#quadratic-linear-regression",
    "href": "resources/slides/01-introduction.html#quadratic-linear-regression",
    "title": "Introduction to STA721",
    "section": "Quadratic Linear Regression",
    "text": "Quadratic Linear Regression\nTaylor’s Theorem: \\[f(t_i, \\boldsymbol{\\theta}) = f(t_0, \\boldsymbol{\\theta}) + (t_i - t_0) f'(t_0, \\boldsymbol{\\theta}) + (t_i - t_0)^2\n\\frac{f^{''}(t_0, \\boldsymbol{\\theta})}{2}  + R(t_i, \\boldsymbol{\\theta})\\]\n\n\\[Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i \\text{  for  } i = 1, \\ldots, n\\]\n\n\nRewrite in vectors: \\[\\begin{eqnarray*}\n\\left[\n\\begin{array}{c}  y_1 \\\\ \\vdots \\\\  y_n \\end{array}\n  \\right]   =  &\n\\left[ \\begin{array}{ccc}  1 &  x_1 & x_1^2 \\\\ \\vdots & \\vdots \\\\ 1 &\n     x_n &  x_n^2\\end{array}  \\right]\n\\left[ \\begin{array}{c}  \\beta_0  \\\\  \\beta_1 \\\\ \\beta_2 \\end{array}\n\\right] +\n\\left[ \\begin{array}{c}  \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n  \\end{array}\n\\right] \\\\\n& \\\\\n\\mathbf{Y}= & \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\n\\end{eqnarray*}\\]\n\n\nQuadratic in \\(x\\), but linear in \\(\\beta\\)’s - how do we know this model is adequate?"
  },
  {
    "objectID": "resources/slides/01-introduction.html#linear-regression-models",
    "href": "resources/slides/01-introduction.html#linear-regression-models",
    "title": "Introduction to STA721",
    "section": "Linear Regression Models",
    "text": "Linear Regression Models\nResponse \\(Y_i\\) and \\(p\\) predictors \\(x_{i1}, x_{i2}, \\dots x_ip\\) \\[Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\ldots \\beta_{p}\n  x_{ip} + \\epsilon_i\\]\n\nDesign matrix \\[\\mathbf{X}=\n\\left[\\begin{array}{cccc}\n1 & x_{11} & \\ldots & x_{1p} \\\\\n1 & x_{21}  & \\ldots & x_{2p} \\\\\n\\vdots & \\vdots  & \\vdots & \\vdots \\\\\n1 & x_{n1} & \\ldots & x_{np} \\\\\n\\end{array} \\right] = \\left[ \\begin{array}{cc}\n1 & \\mathbf{x}_1^T  \\\\\n\\vdots & \\vdots \\\\\n1 & \\mathbf{x}_n^T\n\\end{array} \\right] =\n\\left[\\begin{array}{cccc}\n\\mathbf{1}_n & \\mathbf{X}_1 & \\mathbf{X}_2 \\cdots \\mathbf{X}_p\n\\end{array} \\right]\n\\]\nmatrix version \\[\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\epsilon\\] what should go into \\(\\mathbf{X}\\) and do we need all columns of \\(\\mathbf{X}\\) for inference about \\(\\mathbf{Y}\\)?"
  },
  {
    "objectID": "resources/slides/01-introduction.html#linear-model",
    "href": "resources/slides/01-introduction.html#linear-model",
    "title": "Introduction to STA721",
    "section": "Linear Model",
    "text": "Linear Model\n\n\\(\\mathbf{Y}= \\mathbf{X}\\, \\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\)\n\\(\\mathbf{Y}\\) (\\(n \\times 1\\)) vector of random response (observe \\(\\mathbf{y}\\)); \\(\\mathbf{Y}, \\mathbf{y}\\in \\mathbb{R}^n\\)\n\\(\\mathbf{X}\\) (\\(n \\times p\\)) design matrix (observe)\n\\(\\boldsymbol{\\beta}\\) (\\(p \\times 1\\)) vector of coefficients (unknown)\n\\(\\boldsymbol{\\epsilon}\\) (\\(n \\times 1\\)) vector of “errors” (unobservable)\n\n\nGoals:\n\nWhat goes into \\(\\mathbf{X}\\)? (model building, model selection - post-selection inference?)\nWhat if multiple models are “good”? (model averaging or ensembles) \nWhat about the future? (Prediction)\nUncertainty Quantification - assumptions about \\(\\boldsymbol{\\epsilon}\\)\n\n\n\nAll models are wrong, but some may be useful (George Box)"
  },
  {
    "objectID": "resources/slides/01-introduction.html#ordinary-least-squares",
    "href": "resources/slides/01-introduction.html#ordinary-least-squares",
    "title": "Introduction to STA721",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\nGoal: Find the best fitting “line” or “hyper-plane” that minimizes \\[\\sum_i  (Y_i - \\mathbf{x}_i^T \\boldsymbol{\\beta})^2 = (\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}) = \\| \\mathbf{Y}-\n\\mathbf{X}\\boldsymbol{\\beta}\\|^2 \\]\n\nOptimization problem - seek \\(\\boldsymbol{\\beta}\\ni \\mathbf{X}\\boldsymbol{\\beta}\\) is close to \\(\\mathbf{Y}\\) in squared error\nMay over-fit \\(\\Rightarrow\\) add other criteria that provide a penalty Penalized Least Squares\nRobustness to extreme points \\(\\Rightarrow\\) replace quadratic loss with other functions\n\nno notion of uncertainty of estimates\n\nno structure of problem (repeated measures on individual, randomization restrictions, etc)\n\n\nNeed Distribution Assumptions of \\(\\mathbf{Y}\\) (or \\(\\boldsymbol{\\epsilon}\\)) for testing and uncertainty measures \\(\\Rightarrow\\) Likelihood and Bayesian inference"
  },
  {
    "objectID": "resources/slides/01-introduction.html#random-vectors",
    "href": "resources/slides/01-introduction.html#random-vectors",
    "title": "Introduction to STA721",
    "section": "Random Vectors",
    "text": "Random Vectors\n\nLet \\(Y_1, \\ldots Y_n\\) be random variables in \\(\\mathbb{R}\\) Then \\[\\mathbf{Y}\\equiv\n\\left[ \\begin{array}{c}\nY_1 \\\\\n\\vdots \\\\\nY_n\n\\end{array} \\right]\\] is a random vector in \\(\\mathbb{R}^n\\)\nExpectations of random vectors are defined element-wise: \\[\\textsf{E}[\\mathbf{Y}] \\equiv\n\\textsf{E}\\left[ \\begin{array}{c}\nY_1 \\\\\n\\vdots \\\\\nY_n\n\\end{array} \\right] \\equiv\n\\left[ \\begin{array}{c}\n\\textsf{E}[Y_1] \\\\\n\\vdots \\\\\n\\textsf{E}[Y_n]\n\\end{array} \\right] =\n\\left[ \\begin{array}{c}\n\\mu_1 \\\\\n\\vdots \\\\\n\\mu_n\n\\end{array} \\right]\n\\equiv \\boldsymbol{\\mu}\\in \\mathbb{R}^n\n\\] where mean or expected value \\(\\textsf{E}[Y_i] = \\mu_i\\)"
  },
  {
    "objectID": "resources/slides/01-introduction.html#model-space",
    "href": "resources/slides/01-introduction.html#model-space",
    "title": "Introduction to STA721",
    "section": "Model Space",
    "text": "Model Space\nWe will work with inner product spaces: a vector spaces, say \\(\\mathbb{R}^n\\) equipped with an inner product \\(\\langle \\mathbf{x},\\mathbf{y}\\rangle \\equiv \\mathbf{x}^T\\mathbf{y}, \\quad \\mathbf{x}, \\mathbf{y}\\in \\mathbb{R}^n\\)\n\n\nDefinition: SubspaceA set \\(\\boldsymbol{{\\cal M}}\\) is a subspace of \\(\\mathbb{R}^n\\) if is a subset of \\(\\mathbb{R}^n\\) and also a vector space.\nThat is, if \\(\\mathbf{x}_1 \\in \\boldsymbol{{\\cal M}}\\) and \\(\\mathbf{x}_2 \\in \\boldsymbol{{\\cal M}}\\), then \\(b_1\\mathbf{x}_1 + b_2 \\mathbf{x}_2 \\in \\boldsymbol{{\\cal M}}\\) for all \\(b_1, b_2 \\in \\mathbb{R}\\)\n\n\n\n\n\nDefinition: Column SpaceThe column space of \\(\\mathbf{X}\\) is \\(C(\\mathbf{X}) = \\mathbf{X}\\boldsymbol{\\beta}\\) for \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\)\n\n\n\n\nIf \\(\\mathbf{X}\\) is full column rank, then the columns of \\(\\mathbf{X}\\) form a basis for \\(C(\\mathbf{X})\\) and \\(C(\\mathbf{X})\\) is a p-dimensional subspace of \\(\\mathbb{R}^n\\)\n\n\nIf we have just a single model matrix \\(\\mathbf{X}\\), then the subspace \\(\\boldsymbol{{\\cal M}}\\) is the model space."
  },
  {
    "objectID": "resources/slides/01-introduction.html#philosophy",
    "href": "resources/slides/01-introduction.html#philosophy",
    "title": "Introduction to STA721",
    "section": "Philosophy",
    "text": "Philosophy\n\nfor many problems frequentist and Bayesian methods will give similar answers (more a matter of taste in interpretation)\n\nFor small problems, Bayesian methods allow us to incorporate prior information which provides better calibrated answers\n\nfor problems with complex designs and/or missing data Bayesian methods are often easier to implement (do not need to rely on asymptotics) \n\nFor problems involving hypothesis testing or model selection frequentist and Bayesian methods can be strikingly different.\n\nFrequentist methods often faster (particularly with “big data”) so great for exploratory analysis and for building a “data-sense”\n\nBayesian methods sit on top of Frequentist Likelihood\n\n\nImportant to understand advantages and problems of each perspective!\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "resources/slides/list-of-mathstuff.html",
    "href": "resources/slides/list-of-mathstuff.html",
    "title": "STA721-F24: Linear Models",
    "section": "",
    "text": "Definition: Subspace\nDefinition: Column Space"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": " Syllabus",
    "section": "",
    "text": "When in doubt about anything at all, ask questions!!!\n\nPrerequisites\nALL students are expected to be familiar with all the topics covered within the required prerequisites to be in this course. That is - mathematical statistics and probability, linear algebra, and multivariate calculus. Students are also expected to be familiar with R and are encouraged to learn LaTeX during the course.\n\n\nWorkload\nWork hours will include time spent going through the preassigned readings, attending lectures and lab sessions, and doing all graded work.\n\n\nGraded Work\nGraded work for the course will consist of homework assignments, lab exercises, two midterms and a final exam. Regrade requests for problem sets and lab exercises must be done via Gradescope AT MOST 24 hours after grades are released! Regrade requests for quizzes, midterm, and final exams must be done via Gradescope AT MOST 12 hours after grades are released! Always write in complete sentences and show your steps.\nStudents’ final grades will be determined as shown below:\n\nComponent Percentage\n\n\nComponent\nPercentage\n\n\n\n\nHomework\n20%\n\n\nMidterm\n25%\n\n\nMidterm II\n25%\n\n\nParticipation\n5%\n\n\nFinal Exam\n25%\n\n\n\nThere are no make-ups for any of the graded work except for medical or familial emergencies or for reasons approved by the instructor BEFORE the due date. See the instructor in advance of relevant due dates to discuss possible alternatives.\nGrades may be curved at the end of the semester. Cumulative averages of 90% – 100% are guaranteed at least an A-, 80% – 89% at least a B-, and 70% – 79% at least a C-, however the exact ranges for letter grades will be determined at the end of the course.\n\n\nDescriptions of graded work\n\nProblem sets\nHomework will be handed out on a weekly basis. They will be based on both the lectures and labs and will be announced every Thursday or Friday – be sure to check the website regularly! Also, please note that any work that is not legible by the instructor or TAs will not be graded (given a score of 0). Every write-up must be clearly written in full sentences and clear English. Any assignment that is completely unclear to the instructors and/or TAs, may result in a grade of a 0. For programming exercises, we will be using R/knitr with \\(\\LaTeX\\) for preparing assignments using github classroom for data analysis.\nEach student MUST write up and turn in her or his own answers. You are encouraged to talk to each other regarding homework problems or to the instructor/TA. However, the write-up, solution, and code must be entirely your own work. No sharing of solutions or code! The assignments must be submitted on Gradescope under Assignments. Note that you will not be able to make online submissions after the due date, so be sure to submit before or by the Gradescope-specified deadline. You may resubmit, so when in doubt submit work early. In certain situations if there are issues with submissions, the TA may review your GitHub repository prior to the due date.\nSolutions will be curated from student solutions with proper attribution. Every week the TAs will select a representative correct solution for the assigned problems and put them together into one solutions set with each answer being attributed to the student who wrote it. If you would like to OPT OUT of having your homework solutions used for the class solutions, please let the Instructor and TAs know in advance.\nFinally, your lowest homework score will be dropped!\n\n\nLab exercises\nThe objective of the lab assignments is to give you more hands-on experience with Bayesian data analysis. Attend the lab session and learn a concept or two and some R from the TA, and then work on the computational part of the problem sets. Each lab assignment should be submitted in timely fashion. You are REQUIRED to use R/knitr (or R/Rmarkdown in some cases).\n\n\nMidterm Exams\nThere will be two inclass midterm exams. Detailed instructions on the midterm will be made available later but please check dates on the calendar well in advance!\n\n\nFinal Exam\nThere will be a final exam after the reading week. If you miss any quiz or the midterm, your grade will depend more on the final exam score since there are no make-up exams. You cannot miss the final exam! Please check the important dates on the homepage for the date and time of the final before making plans to return home at the end of the semester. Detailed instructions on the final will be made available later.\n\n\n\nLate Submission Policy\n\nno late submission of homework or lab assignments, however we will drop the lowest score in each.\n\n\n\nCourse Topics\nFor a detailed day-by-day list of topics, please refer to the Course Schedule\n\n\nAcademic integrity\nDuke University is a community dedicated to scholarship, leadership, and service and to the principles of honesty, fairness, respect, and accountability. Citizens of this community commit to reflect upon and uphold these principles in all academic and nonacademic endeavors, and to protect and promote a culture of integrity.\nRemember the Duke Community Standard that you have agreed to abide by:\n\nTo uphold the Duke Community Standard:\n\n\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\nCheating or plagiarism on any graded assessments, lying about an illness or absence and other forms of academic dishonesty are a breach of trust with classmates and faculty, violate the Duke Community Standard, and will not be tolerated. Such incidences will result in a 0 grade for all parties involved. Additionally, there may be penalties to your final class grade along with being reported to the Office of Student Conduct. Review the academic dishonesty policies at https://studentaffairs.duke.edu/conduct/z-policies/academic-dishonesty.\n\n\nDiversity & Inclusiveness\nThis course is designed so that students from all backgrounds and perspectives all feel welcome both in and out of class. Please feel free to talk to me (in person or via email) if you do not feel well-served by any aspect of this class, or if some aspect of class is not welcoming or accessible to you. My goal is for you to succeed in this course, therefore, let me know immediately if you feel you are struggling with any part of the course more than you know how to manage. Doing so will not affect your grades, but it will allow me to provide the resources to help you succeed in the course.\n\n\nDisability Statement\nStudents with disabilities who believe that they may need accommodations in the class are encouraged to contact the Student Disabilities Access Office at 919-668-1267 or disabilities@aas.duke.edu as soon as possible to better ensure that such accommodations are implemented in a timely fashion.\n\n\nOther Information\nIt can be a lot more pleasant oftentimes to get one-on-one answers and help. Make use of the teaching team’s office hours, we’re here to help! Do not hesitate to talk to me during office hours or by appointment to discuss a problem set or any aspect of the course. Questions related to course assignments and honesty policy should be directed to me. When the teaching team has announcements for you we will send an email to your Duke email address. Be sure to check your email daily.\nMost of the course components will be held in person, but occasionally may need to be held online using Zoom meetings. If you have any concerns, issues or challenges, let the instructor know as soon as possible. Also, all students are strongly encouraged to rely on the forums in Sakai, for interacting among yourself and asking other students questions. You can also ask the instructor or the TAs questions on there and we will try to respond as soon as possible. If you experience any technical issues with joining or using the forums, let the instructor know.\n\n\nProfessionalism\nTry as much as possible to refrain from texting or using your computer for anything other than coursework during class and labs. Again, the more engaged you are, the quicker you will be able to get through the materials. You are responsible for everything covered in the lecture videos, lecture notes/slides, and in the assigned readings."
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "STA721Linear Models (Fall 2024)",
    "section": "Course Description",
    "text": "Course Description\nThis course introduces students to linear models and extensions for model building from the frequentist (OLS/MLE and penalized likelihoods) and Bayesian paradigms, with an emphasis on a geometric perspective. Course topics include optimal estimation and prediction, distributional assumptions and model checking, hypothesis testing and model selection including Bayes factors and intrinsic Bayes factors, and Bayesian Model Averaging. Students should have a strong background in matrix algebra and distribution theory. Co-requisite: STA 602L, 702L or equivalent."
  },
  {
    "objectID": "index.html#course-info",
    "href": "index.html#course-info",
    "title": "STA721Linear Models (Fall 2024)",
    "section": "Course Info",
    "text": "Course Info\n\nLecture\n   Tuesday and Thursday\n   1:25pm - 2:40pm\n   Old Chemistry 123 \n\n\nLabs\n   Fridays\n  10:05am - 11:20pm\n   LINK 088 (Clasroom 4) \n\n\nFinal Exam\n   December 15\n   9:00am - 12:00pm\n   Old Chemistry 123 \n\n\nInstructional Team and Office Hours\n\n\n\nRole\nName\nEmail\nOffice Hours\nLocation\n\n\n\n\nInstructor\nDr Merlise Clyde\n\nTues 2:45 - 3:45  or by appointment \n223E Old Chem\n\n\nTA\nBongjung Sung\n\nMon 9:30-11:30am\n203B Old Chem"
  }
]