{
  "hash": "5aaecf8e8c92d24beb5e8c55e4df942e",
  "result": {
    "engine": "knitr",
    "markdown": "---\nsubtitle: \"STA 721: Lecture 3\"\ntitle: \"Rank Deficient Models\"\nauthor: \"Merlise Clyde (clyde@duke.edu)\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    smaller: true\n    logo: ../../img/icon.png\n    footer: <https://sta721-F24.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"   \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\nnumber-sections: false\nfilters:\n  - custom-numbered-blocks  \ncustom-numbered-blocks:\n  groups:\n    thmlike:\n      colors: [948bde, 584eab]\n      boxstyle: foldbox.simple\n      collapse: false\n      listin: [mathstuff]\n    todos: default  \n  classes:\n    Theorem:\n      group: thmlike\n    Corollary:\n      group: thmlike\n    Conjecture:\n      group: thmlike\n      collapse: true  \n    Definition:\n      group: thmlike\n      colors: [d999d3, a01793]\n    Feature: \n       numbered: false\n    TODO:\n      label: \"To do\"\n      colors: [e7b1b4, 8c3236]\n      group: todos\n      listin: [stilltodo]\n    DONE:\n      label: \"Done\"\n      colors: [cce7b1, 86b754]  \n      group: todos  \n---\n\n\n\n\n\n## Outline\n\n\\usepackage{xcolor}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator{\\sgn}{sgn}\n\\newcommand{\\e}{\\mathbf{e}}\n\\newcommand{\\Mb}{\\mathbf{M}}\n\\renewcommand{\\P}{\\mathbf{P}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\R}{\\textsf{R}}\n\\newcommand{\\mat}[1] {\\mathbf{#1}}\n\\newcommand{\\E}{\\textsf{E}}\n\\newcommand{\\SE}{\\textsf{SE}}\n\\newcommand{\\SSE}{\\textsf{SSE}}\n\\newcommand{\\RSS}{\\textsf{RSS}}\n\\newcommand{\\FSS}{\\textsf{FSS}}\n\\renewcommand{\\SS}{\\textsf{SS}}\n\\newcommand{\\MSE}{\\textsf{MSE}}\n\\newcommand{\\SSR}{\\textsf{SSR}}\n\\newcommand{\\Be}{\\textsf{Beta}}\n\\newcommand{\\St}{\\textsf{St}}\n\\newcommand{\\Ca}{\\textsf{C}}\n\\newcommand{\\Lv}{\\textsf{LÃ©vy}}\n\\newcommand{\\Exp}{\\textsf{Exp}}\n\\newcommand{\\GDP}{\\textsf{GDP}}\n\\newcommand{\\NcSt}{\\textsf{NcSt}}\n\\newcommand{\\Bin}{\\textsf{Bin}}\n\\newcommand{\\NB}{\\textsf{NegBin}}\n\\newcommand{\\Mult}{\\textsf{MultNom}}\n\\renewcommand{\\NG}{\\textsf{NG}}\n\\newcommand{\\N}{\\textsf{N}}\n\\newcommand{\\Ber}{\\textsf{Ber}}\n\\newcommand{\\Poi}{\\textsf{Poi}}\n\\newcommand{\\Gam}{\\textsf{Gamma}}\n\\newcommand{\\BB}{\\textsf{BB}}\n\\newcommand{\\BF}{\\textsf{BF}}\n\\newcommand{\\Gm}{\\textsf{G}}\n\\newcommand{\\Un}{\\textsf{Unif}}\n\\newcommand{\\Ex}{\\textsf{Exp}}\n\\newcommand{\\DE}{\\textsf{DE}}\n\\newcommand{\\tr}{\\textsf{tr}}\n\\newcommand{\\cF}{{\\cal{F}}}\n\\newcommand{\\cL}{{\\cal{L}}}\n\\newcommand{\\cI}{{\\cal{I}}}\n\\newcommand{\\cB}{{\\cal{B}}}\n\\newcommand{\\cP}{{\\cal{P}}}\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\pperp}{\\mathrel{{\\rlap{$\\,\\perp$}\\perp\\,\\,}}}\n\\newcommand{\\OFP}{(\\Omega,\\cF, \\P)}\n\\newcommand{\\eps}{\\boldsymbol{\\epsilon}}\n\\newcommand{\\Psib}{\\boldsymbol{\\Psi}}\n\\newcommand{\\1}{\\mathbf{1}_n}\n\\newcommand{\\gap}{\\vspace{8mm}}\n\\newcommand{\\ind}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}}\n\\newcommand{\\iid}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}}\n\\newcommand{\\simiid}{\\ensuremath{\\mathrel{\\mathop{\\sim}\\limits^{\\rm\niid}}}}\n\\newcommand{\\eqindis}{\\mathrel{\\mathop{=}\\limits^{\\rm D}}}\n\\newcommand{\\SSZ}{S_{zz}}\n\\newcommand{\\SZW}{S_{zw}}\n\\newcommand{\\Var}{\\textsf{Var}}\n\\newcommand{\\corr}{\\textsf{corr}}\n\\newcommand{\\cov}{\\textsf{cov}}\n\\newcommand{\\diag}{\\textsf{diag}}\n\\newcommand{\\var}{\\textsf{var}}\n\\newcommand{\\Cov}{\\textsf{Cov}}\n\\newcommand{\\Sam}{{\\cal S}}\n\\newcommand{\\VS}{{\\cal V}}\n\\def\\H{\\mathbf{H}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\tY}{\\tilde{\\mathbf{Y}}}\n\\newcommand{\\Yhat}{\\hat{\\mathbf{Y}}}\n\\newcommand{\\yhat}{\\hat{\\mathbf{y}}}\n\\newcommand{\\Yobs}{\\mathbf{Y}_{{\\cal S}}}\n\\newcommand{\\barYobs}{\\bar{Y}_{{\\cal S}}}\n\\newcommand{\\barYmiss}{\\bar{Y}_{{\\cal S}^c}}\n\\def\\bv{\\mathbf{b}}\n\\def\\X{\\mathbf{X}}\n\\def\\XtX{\\X^T\\X}\n\\def\\tX{\\tilde{\\mathbf{X}}}\n\\def\\x{\\mathbf{x}}\n\\def\\xbar{\\bar{\\mathbf{x}}}\n\\def\\Xbar{\\bar{\\mathbf{X}}}\n\\def\\Xg{\\mathbf{X}_{\\boldsymbol{\\gamma}}}\n\\def\\Ybar{\\bar{\\Y}}\n\\def\\ybar{\\bar{y}}\n\\def\\y{\\mathbf{y}}\n\\def\\Yf{\\mathbf{Y_f}}\n\\def\\W{\\mathbf{W}}\n\\def\\L{\\mathbf{L}}\n\\def\\m{\\mathbf{m}}\n\\def\\n{\\mathbf{n}}\n\\def\\w{\\mathbf{w}}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\z{\\mathbf{z}}\n\\def\\v{\\mathbf{v}}\n\\def\\u{\\mathbf{u}}\n\n\\def\\zero{\\mathbf{0}}\n\\def\\one{\\mathbf{1}}\n\\newcommand{\\taub}{\\boldsymbol{\\tau}}\n\\newcommand{\\betav}{\\boldsymbol{\\beta}}\n\\newcommand{\\alphav}{\\boldsymbol{\\alpha}}\n\\newcommand{\\bfomega}{\\boldsymbol{\\omega}}\n\\newcommand{\\bfchi}{\\boldsymbol{\\chi}}\n\\newcommand{\\bfLambda}{\\boldsymbol{\\Lambda}}\n\\newcommand{\\Lmea}{{\\cal L}} \n\\newcommand{\\scale}{\\bflambda} \n\\newcommand{\\Scale}{\\bfLambda} \n\\newcommand{\\bfscale}{\\bflambda} \n\\newcommand{\\mean}{\\bfchi}\n\\newcommand{\\loc}{\\bfchi}\n\\newcommand{\\bfmean}{\\bfchi}\n\\newcommand{\\bfx}{\\x}\n\\renewcommand{\\k}{g}\n\\newcommand{\\Gen}{{\\cal G}}\n\\newcommand{\\Levy}{L{\\'e}vy}\n\\def\\bfChi    {\\boldsymbol{\\Chi}}\n\\def\\bfOmega  {\\boldsymbol{\\Omega}}\n\\newcommand{\\Po}{\\mbox{\\sf P}}\n\\newcommand{\\A}{\\mathbf{A}}\n\\def\\a{\\mathbf{a}}\n\\def\\K{\\mathbf{K}}\n\\newcommand{\\B}{\\mathbf{B}}\n\\def\\b{\\boldsymbol{\\beta}}\n\\def\\bhat{\\hat{\\boldsymbol{\\beta}}}\n\\def\\btilde{\\tilde{\\boldsymbol{\\beta}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\bg{\\boldsymbol{\\beta_\\gamma}}\n\\def\\bgnot{\\boldsymbol{\\beta_{(-\\gamma)}}}\n\\def\\mub{\\boldsymbol{\\mu}}\n\\def\\tmub{\\tilde{\\boldsymbol{\\mu}}}\n\\def\\muhat{\\hat{\\boldsymbol{\\mu}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\tk{\\boldsymbol{\\theta}_k}\n\\def\\tj{\\boldsymbol{\\theta}_j}\n\\def\\Mk{\\boldsymbol{{\\cal M}}_k}\n\\def\\M{\\boldsymbol{{\\cal M}}}\n\\def\\Mj{\\boldsymbol{{\\cal M}}_j}\n\\def\\Mi{\\boldsymbol{{\\cal M}}_i}\n\\def\\Mg{{\\boldsymbol{{\\cal M}_\\gamma}}}\n\\def\\Mnull{\\boldsymbol{{\\cal M}}_{N}}\n\\def\\gMPM{\\boldsymbol{\\gamma}_{\\text{MPM}}}\n\\def\\gHPM{\\boldsymbol{\\gamma}_{\\text{HPM}}}\n\\def\\Mfull{\\boldsymbol{{\\cal M}}_{F}}\n\\def\\NS{\\boldsymbol{{\\cal N}}}\n\\def\\tg{\\boldsymbol{\\theta}_{\\boldsymbol{\\gamma}}}\n\\def\\g{\\boldsymbol{\\gamma}}\n\\def\\eg{\\boldsymbol{\\eta}_{\\boldsymbol{\\gamma}}}\n\\def\\G{\\mathbf{G}}\n\\def\\cM{\\cal M}\n\\def\\D{\\boldsymbo{\\Delta}}\n\\def\\Dbf{\\mathbf{D}}\n\\def \\shat{{\\hat{\\sigma}}^2}\n\\def\\uv{\\mathbf{u}}\n\\def\\l {\\lambda}\n\\def\\d{\\delta}\n\\def\\Sigmab{\\boldsymbol{\\Sigma}}\n\\def\\Phib{\\boldsymbol{\\Phi}}\n\\def\\Lambdab{\\boldsymbol{\\Lambda}}\n\\def\\lambdab{\\boldsymbol{\\lambda}}\n\\def\\Mg{{\\cal M}_\\gamma}\n\\def\\S{{\\cal{S}}}\n\\def\\Sbf{{\\mathbf{S}}}\n\\def\\qg{p_{\\boldsymbol{\\gamma}}}\n\\def\\pg{p_{\\boldsymbol{\\gamma}}}\n\\def\\T{\\boldsymbol{\\Theta}}\n\\def\\Tb{\\boldsymbol{\\Theta}}\n\\def\\t{\\mathbf{t}}\n\n\n- Rank Deficient Models \n- Generalized Inverses, Projections and MLEs/OLS\n- Class of Unbiased Estimators\n\n\n\n. . .\n\nReadings: \n - Christensen Chapter 2 and Appendix B\n - Seber & Lee Chapter 3\n\n## Geometric View\n\n![](img/OLS.jpg)\n\n## Non-Full Rank Case\n\n- Model: $\\Y = \\mub + \\eps$ \n\n- Assumption: $\\mub \\in C(\\X)$ for $\\X \\in \\bbR^{n \\times p}$\n\n- What if the rank of $\\X$, $r(\\X) \\equiv r \\ne p$?\n\n- Still have result that the OLS/MLE solution satisfies \n$$\\P_\\X \\Y = \\X \\bhat$$\n\n- How can we characterize $\\P$ and $\\bhat$ in this case?   3 cases\n\n. . .\n\n1) $p \\le n$, $r(\\X) \\ne p$ $\\Rightarrow r(\\X) < p$\n2) $p \\gt n$, $r(\\X) \\ne p$\n3) $p \\gt n$, $r(\\X) = p$\n\n. . .\n\nFocus on the first case for OLS/MLE for now...\n\n## Model Space\n\n- $\\M = C(\\X)$ is an $r$-dimensional subspace of $\\bbR^n$\n\n- $\\M$ has an $(n - r)$-dimensional orthogonal complement $\\NS$\n\n- each $\\y \\in \\bbR^n$ has a unique representation as \n$$ \\y = \\yhat + \\e$$ \nfor $\\yhat \\in \\M$ and $\\e \\in \\NS$\n\n- $\\yhat$  is the orthogonal projection of $\\y$ onto $\\M$ and is the OLS/MLE estimate of $\\mub$ that satisfies\n$$\\P_\\X \\y = \\X \\bhat$$\n\n- $\\XtX$ is not invertible so need another way to represent $\\P_\\X$ and $\\bhat$\n\n\n\n## Spectral Decomposition (SD)\n\nEvery symmetric  $n \\times n$ matrix, $\\Sbf$, has an eigen decomposition  $\\Sbf = \\U \\Lambdab \\U^T$\n\n- $\\lambdab$ is a diagonal matrix with eigenvalues $(\\lambda_1, \\ldots, \\lambda_n)$ of $\\Sbf$\n- $\\U$ is a $n \\times n$ *orthogonal* matrix $\\U^T\\U = \\U \\U^T = \\I_n$ ( $\\U^{-1} = \\U^T$)\n- the columns of $\\U$ from an Orthonormal Basis (ONB)  for $\\bbR^n$\n- the columns of $\\U$ associated with non-zero eigenvalues form an ONB for $C(\\Sbf)$ \n- the number of non-zero eigenvalues is the rank of $\\Sbf$\n- the columns of $\\U$ associated with zero eigenvalues form an ONB for $C(\\Sbf)^\\perp$ \n- $\\Sbf^d = \\U \\Lambdab^d \\U^T$ (matrix powers)\n\n## Positive Definite and Non-Negative Definite Matrices \n\n::: {.Definition #PD .unnumbered}\n## B.21 Positive Definite and Non-Negative Definite\nA symmetric matrix $\\Sbf$ is *positive definite* ($\\Sbf \\gt 0$) if and only if $\\x^T\\Sbf \\x > 0$ for $\\x \\in \\bbR^n$,  $\\x \\ne \\zero_n$,  and *positive semi-definite*  or *non-negative definite* ($\\Sbf \\ge 0$) if and only if $\\x^T\\Sbf \\x \\ge 0$ for $\\x \\in \\bbR^n$, $\\x \\ne \\zero_n$\n:::\n\n\n\n::: {.callout-note }\n## Exercise\nShow that a symmetric matrix $\\Sbf$ is positive definite if and only if its eigenvalues are all strictly greater than zero, and positive semi-definite if all the eigenvalues are non-negative.\n:::\n\n## Projections \n\nLet $\\P$ be an orthogonal projection matrix onto $\\M$, then \n\n1. the eigenvalues of $\\P$,\n$\\lambda_i$, are\neither zero or one \n\n2. the trace of $\\P$ is the rank of $\\P$ \n\n3. the dimension of the subspace that $\\P$ projects onto is the rank of $\\P$\n4. the columns of $\\U_r = [u_1, u_2, \\ldots u_r]$ form an ONB for the $C(\\P)$\n5. the projection $\\P$ has the representation $\\P = \\U_r \\U_r^T = \\sum_{i = 1}^r u_i u_i^T$ (the sum of $r$ rank $1$ projections)\n6. the projection $\\I_n - \\P = \\I - \\U_r \\U_r^T = \\U_\\perp \\U_\\perp^T$\nwhere $\\U_\\perp = [u_{r+1}, \\ldots u_n]$ is an orthogonal projection onto $\\NS$\n\n\n\n. . .\n\nMLE/OLS: \n\n- $\\P_X \\y = \\U_r \\U_r^T \\y = \\U_r \\btilde$\n- Claim $\\btilde$ is a MLE/OLS estimate of $\\b$ where $\\tX = \\U_r$.\n\n## Singular Value Decomposition & Connections to Spectral Decompositions\n\nA matrix $\\X \\in \\bbR^{n \\times p}$, $p \\le n$ has a *singular value decomposition* \n$$\\X = \\U_p \\Dbf \\V^T$$\n\n- $\\U_p$ is a $n \\times p$ matrix with the first $p$ eigenvectors in $\\U$ associated with the $p$ largest eigenvectors of $\\X \\X^T = \\U \\Lambdab \\U^T$ with $\\U_p^T\\U_p = I_p$ \n- $\\V$ is a $p \\times p$ orthogonal matrix associated with the $p$ eigenvectors of $\\X^T\\X = \\V \\Lambdab_p \\V^T$ where $\\Lambdab_p$ is the diagonal matrix of eigenvalues associated with the $p$ largest eigenvalues of $\\Lambdab$\n- $\\Dbf$ = $\\Lambdab_p^{1/2}$ are the singular values\n- if $\\X$ has rank $r < p$, then $C(\\X) = C(\\U_p) = C(\\U_r)$, where $\\U_r$ are the eigenvectors of $\\U$ or $\\U_p$ associated with the non-zero eigenvalues. \n \n \n##  MLE/OLS for non-full rank case\n\n- if $\\X^T\\X$ is invertible, $\\P_X = \\X (\\XtX)^{-1} \\X^T$ and $\\bhat$ is the unique estimator that satisfies $\\P_\\X\\y = \\X \\bhat$ or $\\bhat = (\\XtX)^{-1} \\X^T\\y$\n\n- if $\\X^T\\X$ is not invertible, replace $\\X$ by $\\tX$ that is rank $r$\n\n- or represent $\\P_\\X = \\X (\\XtX)^{-} \\X^T$ where $(\\XtX)^{-}$ is a generalized inverse of $\\X^T\\X$ and $\\bhat = (\\XtX)^{-}\\X^T \\y$\n\n## Generalized Inverses\n\n::: {.Definition #Generalized-Inverse .unnumbered}\n## Generalized-Inverse (B.36)\nA generalized inverse of any matrix $\\A$: $\\A^{-}$ satisfies   $\\A \\A^- \\A = \\A$\n:::\n\n\n- A generalized inverse of $\\A$ symmetric always exists!\n\n. . .\n\n\n::: {.Theorem #Th-Generalized-Inverse .unnumbered}\n## Christensen B.39\n  If $\\G_1$ and $\\G_2$ are generalized inverses of $\\A$ then $\\G_1 \\A \\G_2$ is also a generalized inverse of $\\A$\n:::  \n\n \n- if $\\A$ is symmetric, then $\\A^-$ need not be!\n\n## Orthogonal Projections in General\n::: {.callout-note title=\"Lemma B.43\" appearance=\"default\"}\n\n  If $\\G$ and $\\H$ are generalized inverses of $\\XtX$ then\n\\begin{align*}\n \\X \\G \\X^T \\X  & = \\X \\H \\X^T \\X = \\X \\\\\n \\X \\G \\X^T & = \\X \\H \\X^T\n\\end{align*}\n\n:::\n\n. . .\n\n::: {.Theorem .unnumbered}\n## B.44 \n$\\X(\\XtX)^-\\X^T$ is an orthogonal projection onto $C(\\X)$.\n:::\n\n. . .\n\n::: {.Proof .unnumbered}\nWe need to show that (i) $\\P\\m = \\m$ for $\\m \\in C(\\X)$ and (ii) $\\P \\n = 0$ for $\\n \\in C(\\X)^\\perp$.\n\ni) For $\\m \\in C(\\X)$, write $\\m = \\X \\bv$.  Then $\\P \\m = \\P \\X \\bv = \\X(\\XtX)^-\\X^T \\X \\bv$ and by Lemma B43, we have that $\\X(\\XtX)^-\\X^T \\X \\bv = \\X \\bv = \\m$\n\nii) For $\\n \\perp C(\\X)$, $\\P \\n = \\X(\\XtX)^-\\X^T \\n = \\zero_n$ as $C(\\X)^\\perp = N(\\X^T)$.\n\n:::\n\n## MLEs & OLS\n\nMLE/OLS satisfies\n\n- $\\P \\y = \\X \\bhat$ \n- $\\P \\y = \\X(\\XtX)^-\\X^T \\X \\bhat = \\X \\bhat$ (does not depend on  choice of generalized inverse)\n- $\\bhat = (\\XtX)^-\\X^T \\y$ \n- $\\bhat$ is not unique - does depend on choice of generalized inverse unless $\\X$ is full rank\n\n## Moore-Penrose Generalized Inverse:\n\n-  Decompose symmetric $\\A = \\U \\Lambdab \\U^T$  (i.e $\\XtX$)\n-  $\\A^-_{MP} = \\U \\Lambdab^- \\U^T$  \n-  $\\Lambdab^-$ is diagonal with \n$$ \\lambda_i^- = \\left\\{\n    \\begin{array}{l}\n   1/\\lambda_i \\text{ if } \\lambda_i \\neq 0 \\\\\n   0 \\quad \\, \\text{  if } \\lambda_i = 0\n    \\end{array}\n\\right.$$  \n-  Symmetric  $\\A^-_{MP} = (\\A^-_{MP})^T$  \n- Reflexive  $\\A^-_{MP}\\A \\A^-_{MP} = \\A^-_{MP}$  \n\n. . .\n\n- Can you construct another generalized inverse of $\\XtX$ ?\n\n- Can you find the Moore-Penrose generalized inverse of $\\X \\in \\bbR^{n \\times p}$?\n\n## Properties of OLS (full rank case)\n\nHow good is $\\bhat$ as an estimator of $\\beta$\n\n- $\\bhat = (\\XtX)^{-1}\\X^T \\Y =  (\\XtX)^{-1}\\X^T \\X \\b + (\\XtX)^{-1}\\X^T\\eps$\n- don't know $\\eps$, but can talk about behavior on average over\n  - different runs of an experiment\n  - different samples from a population\n  - different values of $\\eps$\n- with minimal assumption $\\E[\\eps] = \\zero_n$, \n\\begin{align*}\n\\E[\\bhat] & = \\E[(\\XtX)^{-1}\\X^T\\X\\b + (\\XtX)^{-1}\\X^T\\eps]\\\\\n& = (\\XtX)^{-1}\\X^T\\E[\\eps] \\\\\n& = \\b\n\\end{align*}\n- *Bias* of $\\bhat$, $\\text{Bias}[\\bhat] =  \\E[\\bhat - \\b] = \\zero_p$ - $\\bhat$ is an unbiased estimator of $\\b$ if $\\mub \\in C(\\X)$\n\n\n\n\n\n## Class of Unbiased Estimators\n\nClass of linear statistical models:\n\\begin{align*}\n\\Y & = \\X \\b + \\eps \\\\\n\\eps & \\sim P \\\\\nP & \\in \\cal{P}\n\\end{align*}\n\n. . .\n\nAn estimator $\\btilde$ is unbiased for $\\b$ if $\\E_P[\\btilde] = \\b \\quad \\forall \\b \\in \\bbR^p$ and $P \\in \\cal{P}$\n\n. . .\n\nExamples:\n\n\n. . .\n\n$\\cal{P}_1= \\{P = \\N(\\zero_n ,\\I_n)\\}$\n\n. . .\n\n$\\cal{P}_2 = \\{P = \\N(\\zero_n ,\\sigma^2 \\I_n), \\sigma^2 >0\\}$\n\n. . .\n\n$\\cal{P}_3 = \\{P = \\N(\\zero_n ,\\Sigmab), \\Sigmab \\in \\cal{\\S}^+ \\}$ ($\\cal{\\S}^+$ is the set of all $n \\times n$ symmetric positive definite matrices.)\n\n. . . \n\n$\\cal{P}_4$ is the set of distributions with $\\E_P[\\eps] = \\zero_n$ and  $\\E_P[\\eps \\eps^T] \\gt 0$ \n\n. . . \n\n$\\cal{P}_5$ is the set of distributions with $\\E_P[\\eps] = \\zero_n$ and  $\\E_P[\\eps \\eps^T] \\ge 0$  \n\n\n## Linear Unbiased Estimation\n\n::: {.callout-note title=\"Exercise\" appearance=\"default\"}\n\n1.  Explain why an estimator that is unbiased for the model with parameter space $\\b \\in \\bbR^p$ and $P \\in  \\cal{P}_{k+1}$ is unbiased for the model with parameter space $\\b \\in \\bbR^p$ and $P \\in  \\cal{P}_{k}$ .\n\n2.  Find an estimator that is unbiased for $\\b \\in \\bbR^p$ and $P \\in  \\cal{P}_{1}$ that but is biased for $\\b \\in \\bbR^p$ and $P \\in  \\cal{P}_{2}$.\n:::\n\n. . .\n\n\nRestrict attention to **linear** unbiased estimators \n\n. . .\n\n::: {.Definition .unnumbered}\n## Linear Unbiased Estimators (LUEs)\nAn estimator $\\btilde$ is a **Linear Unbiased Estimator** (LUE) of $\\b$ if\n\n1) linearity: $\\btilde = \\A \\Y$ for $\\A \\in \\bbR^{p \\times n}$\n2) unbiasedness: $\\E[\\btilde] = \\b$ for all $\\b \\in \\bbR^p$\n\n\n:::\n\n. . .\n\n- Are there other LUEs besides the OLS/MLE estimator?\n- Which is \"best\"? (and in what sense?)\n\n::: footer\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}