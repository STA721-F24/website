---
subtitle: "STA 721: Lecture 5"
title: "Best Linear Unbiased Estimators in Prediction, MVUEs and BUEs"
author: "Merlise Clyde (clyde@duke.edu)"
institute: "Duke University"
format: 
  revealjs:
    theme: [simple, custom.scss]
    slide-number: true
    incremental: true
    scrollable: false
    controls: true
    fragments: true
    preview-links: auto
    smaller: true
    logo: ../../img/icon.png
    footer: <https://sta721-F24.github.io/website/>
    chalkboard: 
      boardmarker-width: 1
      chalk-width: 2
      chalk-effect: 0
    embed-resources: false
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"   
editor: 
  markdown: 
    wrap: 72
execute: 
  echo: false
number-sections: false
filters:
  - custom-numbered-blocks  
custom-numbered-blocks:
  groups:
    thmlike:
      colors: [948bde, 584eab]
      boxstyle: foldbox.simple
      collapse: false
      listin: [mathstuff]
    todos: default  
  classes:
    Theorem:
      group: thmlike
      numbered: false
    Lemma:
      group: thmlike
      numbered: false
    Corollary:
      group: thmlike
      numbered: false
    Proposition:
      group: thmlike
      numbered: false      
    Proof:
      group: thmlike
      numbered: false
      collapse: true  
    Exercise:
      group: thmlike
      numbered: false
    Definition:
      group: thmlike
      numbered: false
      colors: [d999d3, a01793]
    Feature: 
       numbered: false
    TODO:
      label: "To do"
      colors: [e7b1b4, 8c3236]
      group: todos
      listin: [stilltodo]
    DONE:
      label: "Done"
      colors: [cce7b1, 86b754]  
      group: todos  
---


```{r setup, include=FALSE}
# R options
#  

options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warm = 1,
  width=72
  )
# Set dpi and height for images
library(knitr)
knitr::opts_chunk$set(fig.height = 2.65, dpi = 300,fig.align='center',fig.show='hold',size='footnotesize', small.mar=TRUE) 
# For nonsese...
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
```

## Outline
{{< include macros.qmd >}}

- Gauss-Markov Theorem for non-full rank $\X$ (recap)
- Best Linear Unbiased Estimators for Prediction 
- MVUE
- Discussion of recent papers on Best Unbiased Estimators beyond linearity



. . .

Readings: 

- Christensen Chapter 2 (Appendix B as needed)
 
- Seber & Lee Chapter 3
 
- For the curious: 

  - Andersen (1962) [Least squares and best unbiased estimates](https://www.jstor.org/stable/2237657) 
  
  - Hansen (2022)  [A modern gauss-markov theorem](https://users.ssc.wisc.edu/~bhansen/papers/ecnmt_2022.pdf)

  -  [What Estimators are Unbiased for Linear Models](https://arxiv.org/pdf/2212.14185) (2023) and references within


## Identifiability
::: {.Definition .unnumbered}
## Identifiable
$\b$ and $\sigma^2$ are identifiable if the distribution of $\Y$, $f_\Y(\y;
\b_1, \sigma^2_1) = f_\Y(\y;
\b_2, \sigma^2_2)$ implies that $(\b_1, \sigma^2_1)^T =  (\b_2, \sigma^2_2)^T$
:::

- For linear models, equivalent definition is that $\b$ is identifiable
if for any $\b_1$ and $\b_2$, $\mu(\b_1)  = \mu(\b_2)$ or $\X\b_1 =\X\b_2$ implies that
$\b_1 = \b_2$.  

- If $r(\X) = p$ then $\b$ is identifiable
 
- If $\X$ is not full rank, there exists $\b_1 \neq \b_2$, but $\X\b_1 =
\X\b_2$ and hence $\b$ is not identifiable!

- identifiable linear functions of $\b$, $\Lambdab^T\b$ that have an unbiased estimator are historically referred to as **estimable** in linear models.

## BLUE of $\Lambdab^T \b$

If $\Lambdab^T= \B\X$ for some matrix $\B$ (or $\Lambdab = \X^T\B$ then

- $\E[\B\P\Y] = \E[\B\X\bhat] = \E[\Lambdab^T \bhat] = \Lambdab^T\b$ 
- identifiable as it is a function of $\mub$, linear and unbiased
- The unique OLS estimate of $\Lambdab^T\b$ is $\Lambdab^T\bhat$
- $\B\P\Y = \Lambdab^T\bhat$ is the BLUE of $\Lambdab^T\b$
\begin{align*}
 & \E[\|\B\P\Y - \B\mub\|^2]  \le \E[\|\A\Y - \B\mub\|^2] \\
 \Leftrightarrow & \\
& \E[\|\Lambdab^T\bhat - \Lambdab^T\b)\|^2]  \le \E[\|\L^T\btilde - \Lambdab^T\b\|^2]
\end{align*}
for LUE $\A\Y = \L^T\btilde$ of $\Lambdab^T\b$


## Non-Identifiable Example 
One-way ANOVA model 
$$\mu_{ij} = \mu + \tau_j \qquad \mub = (
    \mu_{11}, \ldots,\mu_{n_1 1},\mu_{12},\ldots, \mu_{n_2,2},\ldots, \mu_{1J},
\ldots,
\mu_{n_J J})^T $$ 

- Let $\b_{1} = (\mu, \tau_1, \ldots, \tau_J)^T$ 
- Let $\b_{2} = (\mu - 42, \tau_1 + 42, \ldots, \tau_J + 42)^T$ 
- Then $\mub_{1} = \mub_{2}$ even though  $\b_1 \neq \b_2$ 
- $\b$ is not identifiable 
- yet  $\mub$ is identifiable, where $\mub = \X \b$  (a linear
  combination of $\b$)

## LUEs of Individual $\beta_j$
::: {.Proposition}
For $\mub = \X \b = \sum_j \X_j \beta_j$
$\beta_j$ is **not identifiable**  if and only if there exists $\alpha_j$
such that $\X_j = \sum_{i \neq j} \X_i \alpha_i$
:::

. . .

One-way Anova Model: $Y_{ij} = \mu + \tau_j + \epsilon_{ij}$
$$\mub =  \left[
    \begin{array}{lllll}
\one_{n_1} & \one_{n_1} & \zero_{n_1} &  \ldots & \zero_{n_1} \\
\one_{n_2} & \zero_{n_2} & \one_{n_2} &  \ldots & \zero_{n_2} \\
\vdots & \vdots & \ddots & \vdots \\
\one_{n_J} & \zero_{n_J} & \zero_{n_J} &  \ldots & \one_{n_J} \\
    \end{array} \right]
 \left(   \begin{array}{l}
      \mu \\
      \tau_1 \\
   \tau_2 \\
 \vdots \\
\tau_J
    \end{array} \right)
$$

- Are any parameters $\mu$ or $\tau_j$ identifiable?

## Examples:
- A $j$th element of $\b$: $\lambdab = (0, 0, \ldots,1, 0, \ldots, 0)^T$,  $$\lambdab^T\b = \beta_j$$

- Difference between two treatements: $\tau_1 - \tau_2$:  $\lambdab = (0, 1, -1, \ldots, 0, \ldots, 0)^T$,  $$\lambdab^T\b = \tau_1 - \tau_2$$

- Estimation at observed $\x_i$: $\lambdab = \x_i$  $$\mu_i = \x_i^T \b$$ 
- Estimation or prediction at a new point $\x_*$:
$\lambdab = \x_*$, $$\mu_* = \x_*^T \b$$

## Another Non-Full Rank Example


```{r}
#| echo: true
x1 = -4:4
x2 = c(-2, 1, -1, 2, 0, 2, -1, 1, -2)
x3 = 3*x1  -2*x2
x4 = x2 - x1 + 4
Y = 1+x1+x2+x3+x4 + c(-.5,.5,.5,-.5,0,.5,-.5,-.5,.5)
dev.set = data.frame(Y, x1, x2, x3, x4)

# Order 1
lm1234 = lm(Y ~ x1 + x2 + x3 + x4, data=dev.set)
round(coefficients(lm1234), 4)

# Order 2
lm3412 = lm(Y ~ x3 + x4 + x1 + x2, data = dev.set)
round(coefficients(lm3412), 4)
```

## In Sample Predictions

```{r}
#| echo: true
cbind(dev.set, predict(lm1234), predict(lm3412))
```


- Both models agree for estimating the mean at the observed $\X$ points!

## Out of Sample

```{r}
test.set = data.frame(
    x1 = c(3, 6, 6, 0, 0, 1),
    x2 = c(1, 2, 2, 0, 0, 2),
    x3 = c(7,14, 14,0, 0, 3),
    x4 = c(2, 4, 0, 4, 0, 4))
```

```{r}
#| warning: false
#| echo: true
out = data.frame(test.set,
      Y1234=predict(lm1234, new=test.set),
      Y3412=predict(lm3412, new=test.set))
out
```

- Agreement for cases 1, 3, and 4 only!  

- Can we determine that without finding the predictions and comparing?

- Conditions for general $\Lambdab$ or $\lambdab$ without findingn $\B$ ($\b^T$)? 

## Conditions for LUE of $\lambdab$

- GM requires that $\lambdab^T = \bv^T\X \Leftrightarrow \lambdab = \X^T \bv$   therefore $\lambdab \in C(\X^T)$ 
- Suppose we have an arbitrary $\lambdab = \lambdab_* + \u$, where $\lambdab_*  \in C(\X^T)$ and $\u \in C(\X^T)^\perp$ (orthogonal complement)
- Let  $\P_{\X^T}$ denote an orthogonal projection onto $C(\X^T)$ then $\I - \P_{\X^T}$ is an orthogonal  projection  onto $C(\X^T)^\perp$ 

- $(\I - \P_{\X^T})\lambdab = (\I - \P_{\X^T})\lambdab_* + (\I - \P_{\X^T})\u = \zero_p + \u$ 
- so if $\lambdab \in C(\X^T)$ we will have $(\I - \P_{\X^T})\lambdab = \zero_p$!

- Note this is really just a generalization of Proposition 2.1.6 in Christensen  that $\beta_j$ is identifiable iff there exist scalars such that $\X_j \neq \sum_{i \neq j} \X_i \alpha_i$

---

::: {.Exercise}
a) Is $\P_{\X^T} = (\X^T\X)(\X^T\X)^{-}$ a projection onto $C(\X^T)$?
b) is the expression for $\P_{\X^T}$ unique?
c) Is $\P_{\X^T}$ an orthogonal projection in general?
d) Is $\P_{\X^T}$ using the Moore-Penrose generalized inverse an orthogonal projection?
:::

## Prediction Example Again

For prediction at a new $\x_*$, this is implemented in the `R` package `estimability`
```{r}
#| echo: true
require("estimability" )
cbind(epredict(lm1234, test.set), epredict(lm3412, test.set))
```

Rows 2, 5, and 6  do not have a unique best linear unbiased estimator, $\x_*^T \b$



## MVUE: Minimum Variance Unbiased Estimators
- Gauss-Markov Theorem says that OLS has minimum variance in the
    class of all Linear Unbiased estimators for $\E[\eps] = \zero_n$ and $\Cov[\eps] = \sigma^2 |_n$
- Requires just first and second moments 
- Additional assumption of normality and full rank,  OLS of $\b$ is the same as MLEs and have
  minimum variance out of **ALL**  unbiased estimators (MVUE); not
  just linear estimators  (section 2.5 in Christensen)
- requires Complete Sufficient Statististics and Rao-Blackwell Theorem - next semester in STA732)

- so Best Unbiased Estimators (BUE) not just BLUE!

## What about ?

- are there nonlinear estimators that are better than OLS under the assumptions ?

- [Anderson (1962)](https://www.jstor.org/stable/2237657) showed OLS is not generally the MVUE with $\E[\eps] = \zero_n$ and $\Cov[\eps] = \sigma^2 \I_n$ 

- pointed out that linear-plus-quadratic (LPQ) estimators can outperform the OLS estimator for certain error distributions.


- Other assumptions on $\Cov[\eps] = \Sigmab$?  
    - Generalized Least Squares are BLUE (not necessarily equivalent to OLS)

- more recently  [Hansen (2022)](https://users.ssc.wisc.edu/~bhansen/papers/ecnmt_2022.pdf) concludes that OLS is BUE over the broader class of  linear models with $\Cov[\eps]$  finite and $\E[\eps] = \zero_n$

- lively ongoing debate! - see [What Estimators are Unbiased for Linear Models](https://arxiv.org/pdf/2212.14185) (2023) and references within




## Next Up

- GLS under assumptions $\E[\eps] = \zero_n$ and $\Cov[\eps] = \Sigmab^2$

- Oblique projections and orthogonality with other inner products on $\bbR^n$

- MLEs in Multivariate Normal setting 

- Gauss-Markov

