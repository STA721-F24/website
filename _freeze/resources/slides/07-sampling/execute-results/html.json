{
  "hash": "4a0d1e42c7a7fe99893346e4f925cd69",
  "result": {
    "engine": "knitr",
    "markdown": "---\nsubtitle: \"STA 721: Lecture 7\"\ntitle: \"Sampling Distributions and Distribution Theory\"\nauthor: \"Merlise Clyde (clyde@duke.edu)\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    smaller: true\n    logo: ../../img/icon.png\n    footer: <https://sta721-F24.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"   \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\nnumber-sections: false\nfilters:\n  - custom-numbered-blocks  \ncustom-numbered-blocks:\n  groups:\n    thmlike:\n      colors: [948bde, 584eab]\n      boxstyle: foldbox.simple\n      collapse: false\n      listin: [mathstuff]\n    todos: default  \n  classes:\n    Theorem:\n      group: thmlike\n      numbered: false\n    Lemma:\n      group: thmlike\n      numbered: false\n    Corollary:\n      group: thmlike\n      numbered: false\n    Proposition:\n      group: thmlike\n      numbered: false      \n    Proof:\n      group: thmlike\n      numbered: false\n      collapse: false   \n    Exercise:\n      group: thmlike\n      numbered: false\n    Definition:\n      group: thmlike\n      numbered: false\n      colors: [d999d3, a01793]\n    Feature: \n       numbered: false\n    TODO:\n      label: \"To do\"\n      colors: [e7b1b4, 8c3236]\n      group: todos\n      listin: [stilltodo]\n    DONE:\n      label: \"Done\"\n      colors: [cce7b1, 86b754]  \n      group: todos  \n---\n\n\n\n\n\n## Outline\n\n\\usepackage{xcolor}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator{\\sgn}{sgn}\n\\newcommand{\\e}{\\mathbf{e}}\n\\newcommand{\\Mb}{\\mathbf{M}}\n\\renewcommand{\\P}{\\mathbf{P}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\R}{\\textsf{R}}\n\\newcommand{\\mat}[1] {\\mathbf{#1}}\n\\newcommand{\\E}{\\textsf{E}}\n\\newcommand{\\SE}{\\textsf{SE}}\n\\newcommand{\\SSE}{\\textsf{SSE}}\n\\newcommand{\\RSS}{\\textsf{RSS}}\n\\newcommand{\\FSS}{\\textsf{FSS}}\n\\renewcommand{\\SS}{\\textsf{SS}}\n\\newcommand{\\MSE}{\\textsf{MSE}}\n\\newcommand{\\SSR}{\\textsf{SSR}}\n\\newcommand{\\Be}{\\textsf{Beta}}\n\\newcommand{\\St}{\\textsf{St}}\n\\newcommand{\\Ca}{\\textsf{C}}\n\\newcommand{\\Lv}{\\textsf{LÃ©vy}}\n\\newcommand{\\Exp}{\\textsf{Exp}}\n\\newcommand{\\GDP}{\\textsf{GDP}}\n\\newcommand{\\NcSt}{\\textsf{NcSt}}\n\\newcommand{\\Bin}{\\textsf{Bin}}\n\\newcommand{\\NB}{\\textsf{NegBin}}\n\\newcommand{\\Mult}{\\textsf{MultNom}}\n\\renewcommand{\\NG}{\\textsf{NG}}\n\\newcommand{\\N}{\\textsf{N}}\n\\newcommand{\\Ber}{\\textsf{Ber}}\n\\newcommand{\\Poi}{\\textsf{Poi}}\n\\newcommand{\\Gam}{\\textsf{Gamma}}\n\\newcommand{\\BB}{\\textsf{BB}}\n\\newcommand{\\BF}{\\textsf{BF}}\n\\newcommand{\\Gm}{\\textsf{G}}\n\\newcommand{\\Un}{\\textsf{Unif}}\n\\newcommand{\\Ex}{\\textsf{Exp}}\n\\newcommand{\\DE}{\\textsf{DE}}\n\\newcommand{\\tr}{\\textsf{tr}}\n\\newcommand{\\cF}{{\\cal{F}}}\n\\newcommand{\\cL}{{\\cal{L}}}\n\\newcommand{\\cI}{{\\cal{I}}}\n\\newcommand{\\cB}{{\\cal{B}}}\n\\newcommand{\\cP}{{\\cal{P}}}\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\pperp}{\\mathrel{{\\rlap{$\\,\\perp$}\\perp\\,\\,}}}\n\\newcommand{\\OFP}{(\\Omega,\\cF, \\P)}\n\\newcommand{\\eps}{\\boldsymbol{\\epsilon}}\n\\def\\ehat{\\hat{\\boldsymbol{\\epsilon}}}\n\\newcommand{\\Psib}{\\boldsymbol{\\Psi}}\n\\newcommand{\\Omegab}{\\boldsymbol{\\Omega}}\n\\newcommand{\\1}{\\mathbf{1}_n}\n\\newcommand{\\gap}{\\vspace{8mm}}\n\\newcommand{\\ind}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}}\n\\newcommand{\\iid}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}}\n\\newcommand{\\simiid}{\\ensuremath{\\mathrel{\\mathop{\\sim}\\limits^{\\rm\niid}}}}\n\\newcommand{\\eqindis}{\\mathrel{\\mathop{=}\\limits^{\\rm D}}}\n\\newcommand{\\SSZ}{S_{zz}}\n\\newcommand{\\SZW}{S_{zw}}\n\\newcommand{\\Var}{\\textsf{Var}}\n\\newcommand{\\corr}{\\textsf{corr}}\n\\newcommand{\\cov}{\\textsf{cov}}\n\\newcommand{\\diag}{\\textsf{diag}}\n\\newcommand{\\var}{\\textsf{var}}\n\\newcommand{\\Cov}{\\textsf{Cov}}\n\\newcommand{\\Sam}{{\\cal S}}\n\\newcommand{\\VS}{{\\cal V}}\n\\def\\H{\\mathbf{H}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\tY}{\\tilde{\\mathbf{Y}}}\n\\newcommand{\\Yhat}{\\hat{\\mathbf{Y}}}\n\\newcommand{\\yhat}{\\hat{\\mathbf{y}}}\n\\newcommand{\\Yobs}{\\mathbf{Y}_{{\\cal S}}}\n\\newcommand{\\barYobs}{\\bar{Y}_{{\\cal S}}}\n\\newcommand{\\barYmiss}{\\bar{Y}_{{\\cal S}^c}}\n\\def\\bv{\\mathbf{b}}\n\\def\\X{\\mathbf{X}}\n\\def\\XtX{\\X^T\\X}\n\\def\\tX{\\tilde{\\mathbf{X}}}\n\\def\\x{\\mathbf{x}}\n\\def\\xbar{\\bar{\\mathbf{x}}}\n\\def\\Xbar{\\bar{\\mathbf{X}}}\n\\def\\Xg{\\mathbf{X}_{\\boldsymbol{\\gamma}}}\n\\def\\Ybar{\\bar{\\Y}}\n\\def\\ybar{\\bar{y}}\n\\def\\y{\\mathbf{y}}\n\\def\\Yf{\\mathbf{Y_f}}\n\\def\\W{\\mathbf{W}}\n\\def\\L{\\mathbf{L}}\n\\def\\m{\\mathbf{m}}\n\\def\\n{\\mathbf{n}}\n\\def\\w{\\mathbf{w}}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\z{\\mathbf{z}}\n\\def\\v{\\mathbf{v}}\n\\def\\u{\\mathbf{u}}\n\n\\def\\zero{\\mathbf{0}}\n\\def\\one{\\mathbf{1}}\n\\newcommand{\\taub}{\\boldsymbol{\\tau}}\n\\newcommand{\\betav}{\\boldsymbol{\\beta}}\n\\newcommand{\\alphav}{\\boldsymbol{\\alpha}}\n\\newcommand{\\bfomega}{\\boldsymbol{\\omega}}\n\\newcommand{\\bfchi}{\\boldsymbol{\\chi}}\n\\newcommand{\\bfLambda}{\\boldsymbol{\\Lambda}}\n\\newcommand{\\Lmea}{{\\cal L}} \n\\newcommand{\\scale}{\\bflambda} \n\\newcommand{\\Scale}{\\bfLambda} \n\\newcommand{\\bfscale}{\\bflambda} \n\\newcommand{\\mean}{\\bfchi}\n\\newcommand{\\loc}{\\bfchi}\n\\newcommand{\\bfmean}{\\bfchi}\n\\newcommand{\\bfx}{\\x}\n\\renewcommand{\\k}{g}\n\\newcommand{\\Gen}{{\\cal G}}\n\\newcommand{\\Levy}{L{\\'e}vy}\n\\def\\bfChi    {\\boldsymbol{\\Chi}}\n\\def\\bfOmega  {\\boldsymbol{\\Omega}}\n\\newcommand{\\Po}{\\mbox{\\sf P}}\n\\newcommand{\\A}{\\mathbf{A}}\n\\def\\a{\\mathbf{a}}\n\\def\\K{\\mathbf{K}}\n\\newcommand{\\B}{\\mathbf{B}}\n\\def\\b{\\boldsymbol{\\beta}}\n\\def\\bhat{\\hat{\\boldsymbol{\\beta}}}\n\\def\\btilde{\\tilde{\\boldsymbol{\\beta}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\bg{\\boldsymbol{\\beta_\\gamma}}\n\\def\\bgnot{\\boldsymbol{\\beta_{(-\\gamma)}}}\n\\def\\mub{\\boldsymbol{\\mu}}\n\\def\\tmub{\\tilde{\\boldsymbol{\\mu}}}\n\\def\\muhat{\\hat{\\boldsymbol{\\mu}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\tk{\\boldsymbol{\\theta}_k}\n\\def\\tj{\\boldsymbol{\\theta}_j}\n\\def\\Mk{\\boldsymbol{{\\cal M}}_k}\n\\def\\M{\\boldsymbol{{\\cal M}}}\n\\def\\Mj{\\boldsymbol{{\\cal M}}_j}\n\\def\\Mi{\\boldsymbol{{\\cal M}}_i}\n\\def\\Mg{{\\boldsymbol{{\\cal M}_\\gamma}}}\n\\def\\Mnull{\\boldsymbol{{\\cal M}}_{N}}\n\\def\\gMPM{\\boldsymbol{\\gamma}_{\\text{MPM}}}\n\\def\\gHPM{\\boldsymbol{\\gamma}_{\\text{HPM}}}\n\\def\\Mfull{\\boldsymbol{{\\cal M}}_{F}}\n\\def\\NS{\\boldsymbol{{\\cal N}}}\n\\def\\tg{\\boldsymbol{\\theta}_{\\boldsymbol{\\gamma}}}\n\\def\\g{\\boldsymbol{\\gamma}}\n\\def\\eg{\\boldsymbol{\\eta}_{\\boldsymbol{\\gamma}}}\n\\def\\G{\\mathbf{G}}\n\\def\\cM{\\cal M}\n\\def\\D{\\boldsymbol{\\Delta}}\n\\def\\Dbf{\\mathbf{D}}\n\\def \\shat{{\\hat{\\sigma}}^2}\n\\def\\uv{\\mathbf{u}}\n\\def\\l {\\lambda}\n\\def\\d{\\delta}\n\\def\\deltab{\\mathbf{\\delta}}\n\\def\\Sigmab{\\boldsymbol{\\Sigma}}\n\\def\\Phib{\\boldsymbol{\\Phi}}\n\\def\\Lambdab{\\boldsymbol{\\Lambda}}\n\\def\\lambdab{\\boldsymbol{\\lambda}}\n\\def\\Mg{{\\cal M}_\\gamma}\n\\def\\S{{\\cal{S}}}\n\\def\\Sbf{{\\mathbf{S}}}\n\\def\\qg{p_{\\boldsymbol{\\gamma}}}\n\\def\\pg{p_{\\boldsymbol{\\gamma}}}\n\\def\\T{\\boldsymbol{\\Theta}}\n\\def\\Tb{\\boldsymbol{\\Theta}}\n\\def\\t{\\mathbf{t}}\n\n\n\n\n\n- distributions of $\\bhat$, $\\Yhat$, $\\hat{\\eps}$ under normality \n\n- Unbiased Estimation of $\\sigma^2$\n\n- sampling distribution of $\\hat{\\sigma^2}$\n\n- independence\n\n. . .\n\nReadings:\n\n  -   Christensen Chapter 1, 2.91 and Appendix C\n  -   Seber & Lee Chapter 3.3 - 3.5\n\n\n## Multivariate Normal\n\nUnder the linear model $\\Y = \\X\\b + \\eps$, $\\E[\\eps] = \\zero_n$ and $\\Cov[\\eps] = \\sigma^2 \\I_n$, we had\n\n- $\\E[\\bhat] = \\b$ \n- $\\E[\\Yhat] = \\P_\\X \\Y = \\X\\b$ \n- $\\E[\\ehat] = (\\I_n - \\P_\\X) \\Y = \\zero_n$\n- distributions if $\\epsilon_i \\sim \\N(0, \\sigma^2)$?\n\n. . .\n\n For a $d$ dimensional **multivariate normal** random vector, we write\n  $\\Y \\sim \\N_d(\\mub, \\Sigmab)$ \n  \n  - $\\E[\\Y] = \\mub$:  $d$ dimensional vector with means $E[Y_j]$ \n  \n  - $\\Cov[\\Y] = \\Sigmab$: $d \\times d$ matrix with diagonal elements\n    that are the variances of $Y_j$ and off diagonal elements that are\n    the covariances $\\E[(Y_j - \\mu_j)(Y_k - \\mu_k)]$ \n    \n  -  If $\\Sigmab$ is positive definite ($\\x'\\Sigmab \\x > 0$ for any $\\x \\ne\n  0$ in $\\bbR^d$) then $\\Y$ has  a density$^\\dagger$ \n$$p(\\Y) = (2 \\pi)^{-d/2} |\\Sigmab|^{-1/2} \\exp(-\\frac{1}{2}(\\Y - \\mub)^T\n\\Sigmab^{-1} (\\Y - \\mub))$$\n\n\n::: footer\n$\\dagger$ density with respect to Lebesgue\n  measure on $\\bbR^d$ \n:::  \n\n## Transformations of Normal Random Variables\n\nIf $\\Y \\sim \\N_n(\\mub, \\Sigmab)$   then for $\\A$ $m \\times n$\n$$\\A \\Y \\sim \\N_m(\\A \\mub, \\A \\Sigmab \\A^T)$$ \n\n. . . \n\n\n- $\\bhat = (\\XtX)^{-1}\\X^T\\Y \\sim \\N(\\b, \\sigma^2 (\\XtX)^{-1})$\n\n- $\\Yhat = \\P_\\X \\Y \\sim \\N(\\X\\b, \\sigma^2 \\P_\\X)$\n\n- $\\ehat = (\\I_n - \\P_\\X)\\Y \\sim \\N(\\zero, \\sigma^2 (\\I_n - \\P_\\X))$\n\n\n. . .\n\n$\\A \\Sigmab \\A^T$ does not have to be positive definite! \\pause\n\n## Singular Case\n\n  If the covariance is singular then there is no density (on $\\bbR^n$), but claim that\n  $\\Y$ still has a multivariate normal distribution!  \n\n. . . \n\n::: {.Definition}\n## Multivariate Normal \n  $\\Y \\in \\bbR^n$ has a  multivariate normal distribution $\\N(\\mub,\n  \\Sigmab)$ if for any $\\v \\in \\bbR^n$ $\\v^T\\Y$ has a univariate normal\n  distribution with mean $\\v^T\\mub$ and variance $\\v^T\\Sigmab \\v$\n:::\n\n. . . \n\n::: {.Proof}\nUse moment generating or characteristic functions which uniquely characterize distribution to show that $\\v^T\\Y$ has a univariate normal distribution.\n:::\n\n- both $\\Yhat$ and $\\ehat$ have multivariate normal distributions even though they do not have densities!   (singular distributions)\n\n## Distribution of MLE of $\\sigma^2$\n\nRecall we found the MLE of $\\sigma^2$\n$$\\shat = \\frac{\\ehat^T\\ehat} {n}$$\n\n- let $\\RSS = \\| \\ehat \\|^2 = \\ehat^T\\ehat$\n\n- then\n\\begin{align*}\n\\| \\ehat \\|^2  & = \\ehat^T\\ehat \\\\\n               & = \\eps^T(\\I_n - \\P_\\X)^T (\\I_n - \\P_\\X) \\eps \\\\\n               & = \\eps^T(\\I_n - \\P_\\X) \\eps \\\\\n               & = \\eps^N \\N \\N^T \\eps \\\\\n               & = \\e^T\\e \n\\end{align*}\n\n- $\\N$ is the matrix of the $(n - p)$ eigen vectors from the spectral decomposition of $(\\I_n - \\P_\\X)$ associated with the non-zero eigen-values.\n\n::: footer\n:::\n\n## Distribution of $\\RSS$\n\nSince $\\eps \\sim \\N(\\zero_n, \\sigma^2 \\I_n)$ and $\\N \\in \\bbR^{n \\times (n - p)}$, \n$$\\N^T \\eps = \\e \\sim \\N(\\zero_{n - p}, \\sigma^2\\N^T\\N ) = \\N(\\zero_{n - p}, \\sigma^2\\I_{n - p} )$$\n\n. . .\n\n\n\\begin{align*}\n\\RSS & =  \\sum_{i = 1}^{n-p} e_i^2 \\\\\n     & \\eqindis  \\sum_{i = 1}^{n-p} (\\sigma z_i)^2  \\quad \\text{ where } \\Z \\sim \\N(\\zero_{n-p}, \\I_{n-p}) \\\\\n     & = \\sigma^2 \\sum_{i = 1}^{n-p} z_i^2 \\\\\n     &\\eqindis  \\sigma^2 \\chi^2_{n-p}\n     \n\\end{align*}\n\n. . .\n\n**Background Theory:** If $\\Z \\sim \\N_d(\\zero_d, \\I_d)$, then $\\Z^T\\Z \\sim \\chi^2_{d}$\n\n## Unbiased Estimate of $\\sigma^2$\n\n- Expected value of a $\\chi^2_d$ random variable is $d$ (the degrees of freedom)\n\n- $\\E[\\RSS] = \\E[\\sigma^2 \\chi^2_{n-p}] = \\sigma^2 (n-p)$\n- the expected value of the MLE is $$\\shat = \\E[\\RSS]/n = \\sigma^2 \\frac{(n-p)}{n}$$ so is biased\n\n- an unbiased estimator of $\\sigma^2$, is $s^2 = \\RSS/(n-p)$\n\n- note:  we can find the expectation of $\\shat$ or $s^2$ based on the covariance of $\\eps$ without assuming normality by exploiting properties of the trace.\n\n## Distribution of $\\bhat$\n\n$\\bhat \\sim \\N\\left(\\b, \\sigma^2( \\XtX)^{-1}\\right)$\n\n\n\n- do not know $\\sigma^2$\n\n- Need a distribution that does not depend on unknown parameters for deriving confidence intervals and hypothesis tests for $\\b$.\n\n\n- what if we plug in $s^2$ or $\\shat$ for $\\sigma^2$? \n\n- won't be multivariate normal\n\n- need to reflect uncertainty in estimating $\\sigma^2$\n\n- first show that $\\bhat$ and $s^2$ are independent\n\n## Independence of $\\bhat$ and $s^2$\n\nIf the distribution of $\\Y$ is normal, then $\\bhat$ and $s^2$ are statistically independent. \n\n\n- The derivation of this result basically has three steps:\n\n  1) $\\bhat$ and $\\ehat$ or $\\e$ have zero covariance\n  2) $\\bhat$ and $\\ehat$ or $\\e$ are independent\n  3) Conclude $\\bhat$ and $\\RSS$ (or $s^2$) are independent\n\n\n. . .\n\n**Step 1:**  \n\n. . .\n\n\\begin{align*}\n\\Cov[\\bhat, \\ehat] & = \\E[(\\bhat - \\b) \\ehat^T] \\\\\n                   & = \\E[(\\XtX)^{-1}\\X^T\\eps \\eps^T (\\I - \\P_\\X)] \\\\\n                   & = \\sigma^2 (\\XtX)^{-1}\\X^T (\\I - \\P_\\X) \\\\\n                   & = \\zero\n\\end{align*}\n\n\n\n## Zero Covariance $\\Leftrightarrow$ Independence in Multivariate Normals\n\n**Step 2:**  $\\bhat$ and $\\ehat$ are independent\n\n::: {.Theorem}\n## Zero Correlation and Independence\nFor a random vector $\\W \\sim \\N(\\mub, \\Sigmab)$ partitioned as\n$$\n\\W = \\left[\n  \\begin{array}{c}\n\\W_1  \\\\ \\W_2 \\end{array} \\right]  \\sim \\N\\left( \\left[\n  \\begin{array}{c} \\mub_1  \\\\ \\mub_2 \\end{array} \\right],\n  \\left[ \\begin{array}{cc}\n\\Sigmab_{11} &  \\Sigmab_{12}  \\\\\n\\Sigmab_{21} & \\Sigmab_{22} \\end{array} \\right]\n \\right)\n $$  \nthen $\\Cov(\\W_1, \\W_2) = \\Sigmab_{12} = \\Sigmab_{21}^T = \\zero$  if and\nonly if $\\W_1$ and $\\W_2$ are independent.\n:::\n\n## Proof:  Independence implies Zero Covariance \n\nEasy direction \n\n- $\\Cov[\\W_1, \\W_2] = \\E[(\\W_1 - \\mub_1)(\\W_2 - \\mub_2)^T]$\n\n- since they are independent\n\\begin{align*}\n\\Cov[\\W_1, \\W_2] & = \\E[(\\W_1 - \\mub_1)] \\E[(\\W_2 - \\mub_2)^T] \\\\\n & = \\zero \\zero^T \\\\\n & = \\zero\n \\end{align*}\n\n. . . \n\nso $\\W_1$ and $\\W_2$ are uncorrelated\n\n## Zero Covariance Implies  Independence\n::: {.Proof}\n\nAssume $\\Sigmab_{12} = \\zero$:\n\n\n\n- Choose an\n$$\\A = \\left[\n  \\begin{array}{ll}\n    \\A_1 & \\zero \\\\\n    \\zero & \\A_2\n  \\end{array}\n\\right]$$\n such that $\\A_1 \\A_1^T = \\Sigmab_{11}$, $\\A_2 \\A_2^T = \\Sigmab_{22}$\n\n\n\n- Partition  \n$$ \\Z = \\left[\n  \\begin{array}{c}\n    \\Z_1 \\\\ \\Z_2\n  \\end{array}\n\\right] \\sim \\N\\left(\n\\left[\n  \\begin{array}{c}\n    \\zero_1 \\\\ \\zero_2\n  \\end{array}\n\\right],\n\\left[\n  \\begin{array}{ll}\n    \\I_1 &\\zero \\\\\n\\zero & \\I_2\n  \\end{array}\n\\right]\n \\right)  \\text{ and } \\mub = \\left[\n  \\begin{array}{c}\n    \\mub_1 \\\\ \\mub_2\n  \\end{array}\n\\right]$$  \n\n- then\n       $\\W \\eqindis \\A \\Z + \\mub \\sim  \\N(\\mub, \\Sigmab)$\n:::\n\n---\n\n::: {.Proof}\n### continued\n\n$\\W \\eqindis \\A \\Z + \\mub \\sim  \\N(\\mub, \\Sigmab)$\n\n\\begin{align*}\n\\left[\n  \\begin{array}{c}\n    \\W_1 \\\\ \\W_2\n  \\end{array}\n\\right]  & \\eqindis \n\\left[  \\begin{array}{cc}\n    \\A_1 & \\zero \\\\ \n    \\zero & \\A_2\n  \\end{array}\n\\right]\n\\left[\n  \\begin{array}{c}\n    \\Z_1 \\\\ \\Z_2\n  \\end{array}\n\\right]\n+ \\left[\n  \\begin{array}{c}\n    \\mub_1 \\\\ \\mub_2\n  \\end{array}\n\\right] \\\\\n& =  \\left[\n  \\begin{array}{c}\n    \\A_1\\Z_1 + \\mub_1 \\\\ \\A_2\\Z_2 +\\mub_2\n  \\end{array}\n\\right]\n\\end{align*}\n\n- But $\\Z_1$ and $\\Z_2$ are independent \n- Functions of $\\Z_1$ and $\\Z_2$ are independent \n- Therefore $\\W_1$ and $\\W_2$ are independent  \n:::\n\n. . .\n\nFor Multivariate Normal Zero Covariance implies independence!\n\n---\n\n::: {.Corollary}\nIf $\\Y \\sim \\N( \\mub, \\sigma^2 \\I_n)$ and $\\A \\B^T = \\zero$\nthen $\\A \\Y$ and $\\B \\Y$ are independent.\n:::\n\n. . .\n\n::: {.Proof}\n$$\n\\left[\n  \\begin{array}{c}\n    \\W_1 \\\\ \\W_2\n  \\end{array}\n\\right]  = \\left[\n  \\begin{array}{c}\n    \\A \\\\ \\B\n  \\end{array}\n\\right]  \\Y =  \\left[\n  \\begin{array}{c}\n    \\A \\Y \\\\ \\B  \\Y\n  \\end{array}\n\\right]\n$$ \n\n\n-  $\\Cov(\\W_1, \\W_2) = \\Cov(\\A \\Y, \\B \\Y) = \\sigma^2 \\A \\B^T$\n \n- $\\A \\Y$ and $\\B \\Y$ are independent if $\\A \\B^T = \\zero$\n:::\n\n- Since $\\bhat = (\\XtX)^{-1}\\Y$ and $\\ehat = (\\I- \\P_\\X)\\Y$ have zero covariance, using the corollary we have that $\\bhat$ and $\\ehat$ are independent to show Step 2.\n\n## Step 3: \n\nShow $\\bhat$ and $\\RSS$ are independent\n\n- $\\bhat = (\\XtX)^{-1}\\Y$ and $\\ehat = (\\I- \\P_\\X)\\Y$ are independent\n\n- functions of independent random variables are independent so\n$\\bhat$ and $\\RSS = \\ehat^T\\ehat$ are independent\n\n- so $\\bhat$ and $s^2 = \\RSS/(n-p)$ are independent\n\n\n. . .\n\nThis result will be critical for creating confidence regions and intervals for $\\b$ and linear combinations of $\\b$, $\\lambda^T \\b$ as well as testing hypotheses\n\n## Next Class\n\n- shrinkage estimators \n\n- Bayes and penalized loss functions\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}