{
  "hash": "c776329d306bc6dc5e6c11f0fe5fcaf4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayesian Model Uncertainty\"\nauthor: \"Merlise Clyde\"\nsubtitle: \"STA721: Lecture 19\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    smaller: true\n    logo: ../../img/icon.png\n    footer: <https://sta702-F23.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"    \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\nnumber-sections: false\nfilters:\n  - custom-numbered-blocks  \ncustom-numbered-blocks:\n  groups:\n    thmlike:\n      colors: [948bde, 584eab]\n      boxstyle: foldbox.simple\n      collapse: false\n      listin: [mathstuff]\n    todos: default  \n  classes:\n    Theorem:\n      group: thmlike\n    Corollary:\n      group: thmlike\n    Conjecture:\n      group: thmlike\n      collapse: true  \n    Definition:\n      group: thmlike\n      colors: [d999d3, a01793]\n    Feature: default\n    TODO:\n      label: \"To do\"\n      colors: [e7b1b4, 8c3236]\n      group: todos\n      listin: [stilltodo]\n    DONE:\n      label: \"Done\"\n      colors: [cce7b1, 86b754]  \n      group: todos  \n---\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n## Recap Diabetes Data\n\n\n\\usepackage{xcolor}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator{\\sgn}{sgn}\n\\newcommand{\\e}{\\mathbf{e}}\n\\newcommand{\\Mb}{\\mathbf{M}}\n\\renewcommand{\\P}{\\mathbf{P}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\R}{\\textsf{R}}\n\\newcommand{\\mat}[1] {\\mathbf{#1}}\n\\newcommand{\\pen}{\\textsf{pen}}\n\\newcommand{\\E}{\\textsf{E}}\n\\newcommand{\\SE}{\\textsf{SE}}\n\\newcommand{\\SSE}{\\textsf{SSE}}\n\\newcommand{\\RSS}{\\textsf{RSS}}\n\\newcommand{\\FSS}{\\textsf{FSS}}\n\\renewcommand{\\SS}{\\textsf{SS}}\n\\newcommand{\\MSE}{\\textsf{MSE}}\n\\newcommand{\\SSR}{\\textsf{SSR}}\n\\newcommand{\\Be}{\\textsf{Beta}}\n\\newcommand{\\St}{\\textsf{St}}\n\\newcommand{\\Ca}{\\textsf{C}}\n\\newcommand{\\Lv}{\\textsf{LÃ©vy}}\n\\newcommand{\\Exp}{\\textsf{Exp}}\n\\newcommand{\\GDP}{\\textsf{GDP}}\n\\newcommand{\\NcSt}{\\textsf{NcSt}}\n\\newcommand{\\Bin}{\\textsf{Bin}}\n\\newcommand{\\NB}{\\textsf{NegBin}}\n\\newcommand{\\Mult}{\\textsf{MultNom}}\n\\renewcommand{\\NG}{\\textsf{NG}}\n\\newcommand{\\N}{\\textsf{N}}\n\\newcommand{\\Ber}{\\textsf{Ber}}\n\\newcommand{\\Poi}{\\textsf{Poi}}\n\\newcommand{\\Gam}{\\textsf{Gamma}}\n\\newcommand{\\BB}{\\textsf{BB}}\n\\newcommand{\\BF}{\\textsf{BF}}\n\\newcommand{\\Gm}{\\textsf{G}}\n\\newcommand{\\Un}{\\textsf{Unif}}\n\\newcommand{\\Ex}{\\textsf{Exp}}\n\\newcommand{\\DE}{\\textsf{DE}}\n\\newcommand{\\tr}{\\textsf{tr}}\n\\newcommand{\\cF}{{\\cal{F}}}\n\\newcommand{\\cL}{{\\cal{L}}}\n\\newcommand{\\cI}{{\\cal{I}}}\n\\newcommand{\\cB}{{\\cal{B}}}\n\\newcommand{\\cP}{{\\cal{P}}}\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\pperp}{\\mathrel{{\\rlap{$\\,\\perp$}\\perp\\,\\,}}}\n\\newcommand{\\OFP}{(\\Omega,\\cF, \\P)}\n\\newcommand{\\eps}{\\boldsymbol{\\epsilon}}\n\\def\\ehat{\\hat{\\boldsymbol{\\epsilon}}}\n\\newcommand{\\Psib}{\\boldsymbol{\\Psi}}\n\\newcommand{\\Omegab}{\\boldsymbol{\\Omega}}\n\\newcommand{\\1}{\\mathbf{1}_n}\n\\newcommand{\\gap}{\\vspace{8mm}}\n\\newcommand{\\ind}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}}\n\\newcommand{\\iid}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}}\n\\newcommand{\\simiid}{\\ensuremath{\\mathrel{\\mathop{\\sim}\\limits^{\\rm\niid}}}}\n\\newcommand{\\eqindis}{\\mathrel{\\mathop{=}\\limits^{\\rm D}}}\n\\newcommand{\\SSZ}{S_{zz}}\n\\newcommand{\\SZW}{S_{zw}}\n\\newcommand{\\Var}{\\textsf{Var}}\n\\newcommand{\\corr}{\\textsf{corr}}\n\\newcommand{\\cov}{\\textsf{cov}}\n\\newcommand{\\diag}{\\textsf{diag}}\n\\newcommand{\\var}{\\textsf{var}}\n\\newcommand{\\Cov}{\\textsf{Cov}}\n\\newcommand{\\Sam}{{\\cal S}}\n\\newcommand{\\VS}{{\\cal V}}\n\\def\\H{\\mathbf{H}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\tY}{\\tilde{\\mathbf{Y}}}\n\\newcommand{\\Yhat}{\\hat{\\mathbf{Y}}}\n\\newcommand{\\yhat}{\\hat{\\mathbf{y}}}\n\\newcommand{\\Yobs}{\\mathbf{Y}_{{\\cal S}}}\n\\newcommand{\\barYobs}{\\bar{Y}_{{\\cal S}}}\n\\newcommand{\\barYmiss}{\\bar{Y}_{{\\cal S}^c}}\n\\def\\bv{\\mathbf{b}}\n\\def\\X{\\mathbf{X}}\n\\def\\XtX{\\X^T\\X}\n\\def\\tX{\\tilde{\\mathbf{X}}}\n\\def\\x{\\mathbf{x}}\n\\def\\xbar{\\bar{\\mathbf{x}}}\n\\def\\Xbar{\\bar{\\mathbf{X}}}\n\\def\\Xg{\\mathbf{X}_{\\boldsymbol{\\gamma}}}\n\\def\\Ybar{\\bar{\\Y}}\n\\def\\ybar{\\bar{y}}\n\\def\\y{\\mathbf{y}}\n\\def\\Yf{\\mathbf{Y_f}}\n\\def\\W{\\mathbf{W}}\n\\def\\L{\\mathbf{L}}\n\\def\\m{\\mathbf{m}}\n\\def\\n{\\mathbf{n}}\n\\def\\w{\\mathbf{w}}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\z{\\mathbf{z}}\n\\def\\v{\\mathbf{v}}\n\\def\\u{\\mathbf{u}}\n\n\\def\\zero{\\mathbf{0}}\n\\def\\one{\\mathbf{1}}\n\\newcommand{\\taub}{\\boldsymbol{\\tau}}\n\\newcommand{\\betav}{\\boldsymbol{\\beta}}\n\\newcommand{\\alphav}{\\boldsymbol{\\alpha}}\n\\newcommand{\\bfomega}{\\boldsymbol{\\omega}}\n\\newcommand{\\bfchi}{\\boldsymbol{\\chi}}\n\\newcommand{\\bfLambda}{\\boldsymbol{\\Lambda}}\n\\newcommand{\\Lmea}{{\\cal L}} \n\\newcommand{\\scale}{\\bflambda} \n\\newcommand{\\Scale}{\\bfLambda} \n\\newcommand{\\bfscale}{\\bflambda} \n\\newcommand{\\mean}{\\bfchi}\n\\newcommand{\\loc}{\\bfchi}\n\\newcommand{\\bfmean}{\\bfchi}\n\\newcommand{\\bfx}{\\x}\n\\renewcommand{\\k}{g}\n\\newcommand{\\Gen}{{\\cal G}}\n\\newcommand{\\Levy}{L{\\'e}vy}\n\\def\\bfChi    {\\boldsymbol{\\Chi}}\n\\def\\bfOmega  {\\boldsymbol{\\Omega}}\n\\newcommand{\\Po}{\\mbox{\\sf P}}\n\\newcommand{\\A}{\\mathbf{A}}\n\\def\\a{\\mathbf{a}}\n\\def\\K{\\mathbf{K}}\n\\newcommand{\\B}{\\mathbf{B}}\n\\def\\b{\\boldsymbol{\\beta}}\n\\def\\bhat{\\hat{\\boldsymbol{\\beta}}}\n\\def\\btilde{\\tilde{\\boldsymbol{\\beta}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\bg{\\boldsymbol{\\beta_\\gamma}}\n\\def\\bgnot{\\boldsymbol{\\beta_{(-\\gamma)}}}\n\\def\\mub{\\boldsymbol{\\mu}}\n\\def\\tmub{\\tilde{\\boldsymbol{\\mu}}}\n\\def\\muhat{\\hat{\\boldsymbol{\\mu}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\tk{\\boldsymbol{\\theta}_k}\n\\def\\tj{\\boldsymbol{\\theta}_j}\n\\def\\Mk{\\boldsymbol{{\\cal M}}_k}\n\\def\\M{\\boldsymbol{{\\cal M}}}\n\\def\\Mj{\\boldsymbol{{\\cal M}}_j}\n\\def\\Mi{\\boldsymbol{{\\cal M}}_i}\n\\def\\Mg{{\\boldsymbol{{\\cal M}_\\gamma}}}\n\\def\\Mnull{\\boldsymbol{{\\cal M}}_{N}}\n\\def\\gMPM{\\boldsymbol{\\gamma}_{\\text{MPM}}}\n\\def\\gHPM{\\boldsymbol{\\gamma}_{\\text{HPM}}}\n\\def\\Mfull{\\boldsymbol{{\\cal M}}_{F}}\n\\def\\NS{\\boldsymbol{{\\cal N}}}\n\\def\\tg{\\boldsymbol{\\theta}_{\\boldsymbol{\\gamma}}}\n\\def\\g{\\boldsymbol{\\gamma}}\n\\def\\eg{\\boldsymbol{\\eta}_{\\boldsymbol{\\gamma}}}\n\\def\\G{\\mathbf{G}}\n\\def\\cM{\\cal M}\n\\def\\D{\\boldsymbol{\\Delta}}\n\\def\\Dbf{\\mathbf{D}}\n\\def \\shat{{\\hat{\\sigma}}^2}\n\\def\\uv{\\mathbf{u}}\n\\def\\l {\\lambda}\n\\def\\d{\\delta}\n\\def\\deltab{\\mathbf{\\delta}}\n\\def\\Sigmab{\\boldsymbol{\\Sigma}}\n\\def\\Phib{\\boldsymbol{\\Phi}}\n\\def\\Lambdab{\\boldsymbol{\\Lambda}}\n\\def\\lambdab{\\boldsymbol{\\lambda}}\n\\def\\Mg{{\\cal M}_\\gamma}\n\\def\\S{{\\cal{S}}}\n\\def\\Sbf{{\\mathbf{S}}}\n\\def\\qg{p_{\\boldsymbol{\\gamma}}}\n\\def\\pg{p_{\\boldsymbol{\\gamma}}}\n\\def\\T{\\boldsymbol{\\Theta}}\n\\def\\Tb{\\boldsymbol{\\Theta}}\n\\def\\t{\\mathbf{t}}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(8675309)\nsource(\"yX.diabetes.train.txt\")\ndiabetes.train = as.data.frame(diabetes.train)\nsource(\"yX.diabetes.test.txt\")\ndiabetes.test = as.data.frame(diabetes.test)\ncolnames(diabetes.test)[1] = \"y\"\n\nstr(diabetes.train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t342 obs. of  65 variables:\n $ y      : num  -0.0147 -1.0005 -0.1444 0.6987 -0.2222 ...\n $ age    : num  0.7996 -0.0395 1.7913 -1.8703 0.113 ...\n $ sex    : num  1.064 -0.937 1.064 -0.937 -0.937 ...\n $ bmi    : num  1.296 -1.081 0.933 -0.243 -0.764 ...\n $ map    : num  0.459 -0.553 -0.119 -0.77 0.459 ...\n $ tc     : num  -0.9287 -0.1774 -0.9576 0.256 0.0826 ...\n $ ldl    : num  -0.731 -0.402 -0.718 0.525 0.328 ...\n $ hdl    : num  -0.911 1.563 -0.679 -0.757 0.171 ...\n $ tch    : num  -0.0544 -0.8294 -0.0544 0.7205 -0.0544 ...\n $ ltg    : num  0.4181 -1.4349 0.0601 0.4765 -0.6718 ...\n $ glu    : num  -0.371 -1.936 -0.545 -0.197 -0.979 ...\n $ age^2  : num  -0.312 -0.867 1.925 2.176 -0.857 ...\n $ bmi^2  : num  0.4726 0.1185 -0.0877 -0.6514 -0.2873 ...\n $ map^2  : num  -0.652 -0.573 -0.815 -0.336 -0.652 ...\n $ tc^2   : num  -0.091 -0.6497 -0.0543 -0.6268 -0.6663 ...\n $ ldl^2  : num  -0.289 -0.521 -0.3 -0.45 -0.555 ...\n $ hdl^2  : num  -0.0973 0.8408 -0.3121 -0.2474 -0.5639 ...\n $ tch^2  : num  -0.639 -0.199 -0.639 -0.308 -0.639 ...\n $ ltg^2  : num  -0.605 0.78 -0.731 -0.567 -0.402 ...\n $ glu^2  : num  -0.578 1.8485 -0.4711 -0.6443 -0.0258 ...\n $ age:sex: num  0.69 -0.139 1.765 1.609 -0.284 ...\n $ age:bmi: num  0.852 -0.142 1.489 0.271 -0.271 ...\n $ age:map: num  0.0349 -0.3346 -0.5862 1.1821 -0.3025 ...\n $ age:tc : num  -0.978 -0.246 -1.927 -0.72 -0.244 ...\n $ age:ldl: num  -0.803 -0.203 -1.504 -1.2 -0.182 ...\n $ age:hdl: num  -0.7247 0.0147 -1.2661 1.6523 0.1046 ...\n $ age:tch: num  -0.254 -0.176 -0.31 -1.598 -0.216 ...\n $ age:ltg: num  0.0644 -0.2142 -0.163 -1.1657 -0.3474 ...\n $ age:glu: num  -0.636 -0.239 -1.359 0.071 -0.438 ...\n $ sex:bmi: num  1.304 0.935 0.915 0.142 0.635 ...\n $ sex:map: num  0.258 0.289 -0.381 0.5 -0.697 ...\n $ sex:tc : num  -1.02 0.131 -1.051 -0.274 -0.112 ...\n $ sex:ldl: num  -0.927 0.236 -0.913 -0.638 -0.452 ...\n $ sex:hdl: num  -0.647 -1.188 -0.377 1.189 0.238 ...\n $ sex:tch: num  -0.411 0.47 -0.411 -1.062 -0.296 ...\n $ sex:ltg: num  0.2988 1.2093 -0.0866 -0.6032 0.4857 ...\n $ sex:glu: num  -0.6171 1.6477 -0.8069 -0.0239 0.7283 ...\n $ bmi:map: num  0.189 0.191 -0.477 -0.195 -0.702 ...\n $ bmi:tc : num  -1.5061 -0.0595 -1.1853 -0.3231 -0.3239 ...\n $ bmi:ldl: num  -1.267 0.183 -0.976 -0.407 -0.536 ...\n $ bmi:hdl: num  -0.869 -1.41 -0.286 0.586 0.251 ...\n $ bmi:tch: num  -0.505 0.505 -0.484 -0.614 -0.388 ...\n $ bmi:ltg: num  0.1014 1.1613 -0.4085 -0.5893 0.0716 ...\n $ bmi:glu: num  -0.862 1.693 -0.89 -0.337 0.358 ...\n $ map:tc : num  -0.687 -0.148 -0.131 -0.451 -0.21 ...\n $ map:ldl: num  -0.5407 0.0388 -0.1034 -0.6114 -0.036 ...\n $ map:hdl: num  -0.235 -0.672 0.254 0.745 0.252 ...\n $ map:tch: num  -0.29 0.207 -0.258 -0.835 -0.29 ...\n $ map:ltg: num  -0.214 0.428 -0.427 -0.811 -0.748 ...\n $ map:glu: num  -0.541 0.659 -0.314 -0.23 -0.812 ...\n $ tc:ldl : num  -0.144 -0.551 -0.139 -0.509 -0.581 ...\n $ tc:hdl : num  0.8363 -0.3457 0.6304 -0.2579 -0.0392 ...\n $ tc:tch : num  -0.405 -0.326 -0.404 -0.295 -0.451 ...\n $ tc:ltg : num  -0.901 -0.259 -0.571 -0.392 -0.569 ...\n $ tc:glu : num  0.0202 0.0196 0.2073 -0.396 -0.4283 ...\n $ ldl:hdl: num  0.889 -0.446 0.705 -0.207 0.26 ...\n $ ldl:tch: num  -0.463 -0.243 -0.463 -0.21 -0.506 ...\n $ ldl:ltg: num  -0.6536 0.2724 -0.3783 -0.0708 -0.5638 ...\n $ ldl:glu: num  -0.0194 0.4995 0.1032 -0.4013 -0.6234 ...\n $ hdl:tch: num  0.703 -0.5 0.692 0.171 0.651 ...\n $ hdl:ltg: num  0.0179 -1.9846 0.3839 0.0399 0.3043 ...\n $ hdl:glu: num  0.654 -2.948 0.689 0.452 0.113 ...\n $ tch:ltg: num  -0.592 0.531 -0.574 -0.253 -0.537 ...\n $ tch:glu: num  -0.371 1.114 -0.362 -0.522 -0.34 ...\n $ ltg:glu: num  -0.584 2.184 -0.468 -0.526 0.183 ...\n```\n\n\n:::\n:::\n\n\n\n## Credible Intervals under BMA\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncoef.diabetes = coefficients(diabetes.bas)\nci.coef.bas = confint(coef.diabetes, level=0.95)\nplot(ci.coef.bas)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNULL\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](19-bvs_files/figure-revealjs/CI-coef-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n\n- uses Monte Carlo simulations from the posteriors of the coefficients\n\n- uses HPD intervals from the CODA package to compute intervals\n\n## Out of Sample Prediction\n\n- What is the optimal value to predict $\\Y^{\\text{test}}$ given $\\Y$ under squared error?\n\n- BMA is optimal prediction for squared error loss with Bayes\n$$\\E[\\| \\Y^{\\text{test}} - a\\|^2 \\mid \\y] = \\E[\\| \\Y^{\\text{test}} - \\E[\\Y^{\\text{test}}\\mid \\y] \\|^2 \\mid \\y]  + \\| \\E[\\Y^{\\text{test}}\\mid \\y] - a\\|^2  $$\n\n- Iterated expectations leads to BMA for $\\E[\\Y^{\\text{test}} \\mid \\Y]$\n\n- Prediction under model averaging\n$$\\hat{Y} = \\sum_S (\\hat{\\alpha}_\\g + \\Xg^{\\text{test}} \\hat{\\b}_{\\g}) \\hat{p}(\\g \\mid \\Y)$$\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4556414\n```\n\n\n:::\n:::\n\n\n\n## Credible Intervals & Coverage\n\n- posterior predictive distribution \n$$\np(\\y^{\\text{test}} \\mid \\y) = \\sum_\\g p(\\y^{\\text{test}} \\mid \\y, \\g)p(\\g \\mid \\y)\n$$\n- integrate out $\\alpha$ and $\\bg$ to get a normal predictive given $\\phi$ and $\\g$ (and $\\y$)\n\n- integrate out $\\phi$ to get a t distribution given $\\g$ and $\\y$\n\n- credible intervals via sampling \n\n    - sample a model from $p(\\g \\mid \\y)$\n    - conditional on a model sample $y \\sim p(\\y^{\\text{test}} \\mid \\y, \\g)$\n    - compute HPD or quantiles from samples of $y$\n    \n. . .\n\n\n## 95% Prediction intervals\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nci.bas = confint(pred.bas);\ncoverage = mean(diabetes.test$y > ci.bas[,1] & diabetes.test$y < ci.bas[,2])\ncoverage\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.99\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(ci.bas)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNULL\n```\n\n\n:::\n\n```{.r .cell-code}\npoints(diabetes.test$y, col=2, pch=15)\n```\n\n::: {.cell-output-display}\n![](19-bvs_files/figure-revealjs/pred-in-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n\n## Selection and Prediction\n\n- BMA is optimal for squared error loss Bayes\n\n\n \n\n- What if we want to use only a single model for prediction under squared error loss?\n\n- HPM: Highest Posterior Probability model is optimal for selection, but not prediction\n\n- MPM: Median Probability model (select model where PIP > 0.5)\n (optimal under certain conditions; nested models)\n \n-  BPM: Best Probability Model - Model closest to BMA under loss\n      (usually includes more predictors than HPM or MPM)\n\n- costs of using variables in prediction?      \n\n## Example\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred.bas = predict(diabetes.bas,\n                   newdata=diabetes.test,\n                   estimator=\"BMA\",\n                   se=TRUE)\nmean((pred.bas$fit- diabetes.test$y)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4556414\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred.bas = predict(diabetes.bas,\n                   newdata=diabetes.test,\n                   estimator=\"BPM\",\n                   se=TRUE)\n#MSE\nmean((pred.bas$fit- diabetes.test$y)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4740667\n```\n\n\n:::\n\n```{.r .cell-code}\n#Coverage\nci.bas = confint(pred.bas)\nmean(diabetes.test$y > ci.bas[,1] &\n     diabetes.test$y < ci.bas[,2])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.98\n```\n\n\n:::\n:::\n\n\n\n \n## Theory - Consistency  of g-priors \n\n\n- desire that posterior probability of model goes to 1 as $n \\to \\infty$\n\n     - does not always hold if the null model is true  (may be highest posterior probability model)\n     - need prior on $g$ to depend on $n$  (rules out EB and fixed g-priors with $g \\ne n$)\n     - asymptotically BMA collapses to the true model\n\n- other quantities may converge i.e.  posterior mean \n\n## Model Paradigms\n\n- what if the true model $\\g_T$ is not in $\\Gamma$?   What can we say?\n\n- $\\M$-complete; BMA converges to the model that is \"closest\" to the truth in Kullback-Leibler divergence\n- $\\M$-closed; \n  - know $\\g_T \\notin \\G$ so that $(p_\\g) = 0 \\ \\forall \\g \\in \\G$  but  want to use models in $\\G$\n  - Predictive distribution $p(\\Y^* \\mid \\Y, \\g_T)$ is available\n  \n- $\\M$-open; \n  - know $\\g_T \\notin \\G$ so that $(p_\\g) = 0 \\ \\forall \\g \\in \\G$  but  want to use models in $\\G$\n  - Predictive distribution $p(\\Y^* \\mid \\Y, \\g_T)$ is not available. (too complicated to use, etc)\n\n\n## $\\M$-Open and $M$-Complete Prediction\n\nClyde & Iversen (2013) [pdf](https://www.researchgate.net/publication/261252831_Bayesian_Model_Averaging_in_the_M-Open_Framework) motivate a solution via decision theory\n\n- Use the models in $\\G$ to predict $\\Y^*$ given $\\Y$ under squared error loss \n    $$E[\\Y^*, \\sum_{\\g \\in \\G} \\omega_\\g \\hat{\\Y}^*_\\g \\mid \\Y] = \\int ( \\Y^* -  \\sum_{\\g \\in \\G} \\omega_\\g \\hat{\\Y}^*_\\g )^2 p(\\Y^* \\mid \\Y)$$\n- Still use a weighted sum of predictions or densities from models in $\\G$ \n    but now the weights are not probabilities but are chosen to minimize the loss function\n  - uses additional constraints of penalties on the weights as part of the loss function\n  - need to approximate the predictive distribution for $\\Y^* \\mid \\Y$ (via an approximate Dirichlet Process Model)\n  - latter is related to \"stacking\"  (Wolpert 1972) which is a frequentist method of ensemble learning using cross-validation;\n     \n\n\n## Summary\n\n\n-  Choice of prior on $\\bg$ \n      \n    - multivariate Spike & Slab\n    - products of independent Spike & Slab priors\n    - intermediates block g-priors\n    - non-semi-conjugate \n    - non-local priors\n    - shrinkage priors without point-masses\n    \n-  priors on the models (sensitivity)\n-  computation (MCMC, \"stochastic search\", adaptive MH, variational, orthogonal \n   data augmentation, reversible jump-MCMC)\n-  decision theory - select a model or \"average\" over all models\n- asymptotic properties - large $n$ and large $p > n$\n\n## Other aspects of model selection?\n\n - transformations of $\\Y$\n - functions of $\\X$: interactions or nonlinear functions such as splines kernels\n - choice of error distribution\n - outliers \n ",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}