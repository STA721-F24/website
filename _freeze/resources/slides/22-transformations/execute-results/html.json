{
  "hash": "466d45702639bc1b58253430e897b94e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Transformations and Normality Assumptions\"\nauthor: \"Merlise Clyde\"\nsubtitle: \"STA 721: Lecture 22\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    smaller: true\n    logo: ../../img/icon.png\n    footer: <https://sta721-F24.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"    \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\nnumber-sections: false\nfilters:\n  - custom-numbered-blocks  \ncustom-numbered-blocks:\n  groups:\n    thmlike:\n      colors: [948bde, 584eab]\n      boxstyle: foldbox.simple\n      collapse: false\n      listin: [mathstuff]\n    todos: default  \n  classes:\n    Theorem:\n      group: thmlike\n    Corollary:\n      group: thmlike\n    Conjecture:\n      group: thmlike\n      collapse: true  \n    Definition:\n      group: thmlike\n      colors: [d999d3, a01793]\n    Feature: default\n    TODO:\n      label: \"To do\"\n      colors: [e7b1b4, 8c3236]\n      group: todos\n      listin: [stilltodo]\n    DONE:\n      label: \"Done\"\n      colors: [cce7b1, 86b754]  \n      group: todos  \n---\n\n\n\n\n\n## Outline\n\n\\usepackage{xcolor}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator{\\sgn}{sgn}\n\\newcommand{\\e}{\\mathbf{e}}\n\\newcommand{\\Mb}{\\mathbf{M}}\n\\renewcommand{\\P}{\\mathbf{P}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\R}{\\textsf{R}}\n\\newcommand{\\mat}[1] {\\mathbf{#1}}\n\\newcommand{\\pen}{\\textsf{pen}}\n\\newcommand{\\E}{\\textsf{E}}\n\\newcommand{\\SE}{\\textsf{SE}}\n\\newcommand{\\SSE}{\\textsf{SSE}}\n\\newcommand{\\RSS}{\\textsf{RSS}}\n\\newcommand{\\FSS}{\\textsf{FSS}}\n\\renewcommand{\\SS}{\\textsf{SS}}\n\\newcommand{\\MSE}{\\textsf{MSE}}\n\\newcommand{\\SSR}{\\textsf{SSR}}\n\\newcommand{\\Be}{\\textsf{Beta}}\n\\newcommand{\\St}{\\textsf{St}}\n\\newcommand{\\Ca}{\\textsf{C}}\n\\newcommand{\\Lv}{\\textsf{LÃ©vy}}\n\\newcommand{\\Exp}{\\textsf{Exp}}\n\\newcommand{\\GDP}{\\textsf{GDP}}\n\\newcommand{\\NcSt}{\\textsf{NcSt}}\n\\newcommand{\\Bin}{\\textsf{Bin}}\n\\newcommand{\\NB}{\\textsf{NegBin}}\n\\newcommand{\\Mult}{\\textsf{MultNom}}\n\\renewcommand{\\NG}{\\textsf{NG}}\n\\newcommand{\\N}{\\textsf{N}}\n\\newcommand{\\Ber}{\\textsf{Ber}}\n\\newcommand{\\Poi}{\\textsf{Poi}}\n\\newcommand{\\Gam}{\\textsf{Gamma}}\n\\newcommand{\\BB}{\\textsf{BB}}\n\\newcommand{\\BF}{\\textsf{BF}}\n\\newcommand{\\Gm}{\\textsf{G}}\n\\newcommand{\\Un}{\\textsf{Unif}}\n\\newcommand{\\Ex}{\\textsf{Exp}}\n\\newcommand{\\DE}{\\textsf{DE}}\n\\newcommand{\\tr}{\\textsf{tr}}\n\\newcommand{\\cF}{{\\cal{F}}}\n\\newcommand{\\cL}{{\\cal{L}}}\n\\newcommand{\\cI}{{\\cal{I}}}\n\\newcommand{\\cB}{{\\cal{B}}}\n\\newcommand{\\cP}{{\\cal{P}}}\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\pperp}{\\mathrel{{\\rlap{$\\,\\perp$}\\perp\\,\\,}}}\n\\newcommand{\\OFP}{(\\Omega,\\cF, \\P)}\n\\newcommand{\\eps}{\\boldsymbol{\\epsilon}}\n\\def\\ehat{\\hat{\\boldsymbol{\\epsilon}}}\n\\newcommand{\\Psib}{\\boldsymbol{\\Psi}}\n\\newcommand{\\Omegab}{\\boldsymbol{\\Omega}}\n\\newcommand{\\1}{\\mathbf{1}_n}\n\\newcommand{\\gap}{\\vspace{8mm}}\n\\newcommand{\\ind}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}}\n\\newcommand{\\iid}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}}\n\\newcommand{\\simiid}{\\ensuremath{\\mathrel{\\mathop{\\sim}\\limits^{\\rm\niid}}}}\n\\newcommand{\\eqindis}{\\mathrel{\\mathop{=}\\limits^{\\rm D}}}\n\\newcommand{\\SSZ}{S_{zz}}\n\\newcommand{\\SZW}{S_{zw}}\n\\newcommand{\\Var}{\\textsf{Var}}\n\\newcommand{\\corr}{\\textsf{corr}}\n\\newcommand{\\cov}{\\textsf{cov}}\n\\newcommand{\\diag}{\\textsf{diag}}\n\\newcommand{\\var}{\\textsf{var}}\n\\newcommand{\\Cov}{\\textsf{Cov}}\n\\newcommand{\\Sam}{{\\cal S}}\n\\newcommand{\\VS}{{\\cal V}}\n\\def\\H{\\mathbf{H}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\tY}{\\tilde{\\mathbf{Y}}}\n\\newcommand{\\Yhat}{\\hat{\\mathbf{Y}}}\n\\newcommand{\\yhat}{\\hat{\\mathbf{y}}}\n\\newcommand{\\Yobs}{\\mathbf{Y}_{{\\cal S}}}\n\\newcommand{\\barYobs}{\\bar{Y}_{{\\cal S}}}\n\\newcommand{\\barYmiss}{\\bar{Y}_{{\\cal S}^c}}\n\\def\\bv{\\mathbf{b}}\n\\def\\X{\\mathbf{X}}\n\\def\\XtX{\\X^T\\X}\n\\def\\tX{\\tilde{\\mathbf{X}}}\n\\def\\x{\\mathbf{x}}\n\\def\\xbar{\\bar{\\mathbf{x}}}\n\\def\\Xbar{\\bar{\\mathbf{X}}}\n\\def\\Xg{\\mathbf{X}_{\\boldsymbol{\\gamma}}}\n\\def\\Ybar{\\bar{\\Y}}\n\\def\\ybar{\\bar{y}}\n\\def\\y{\\mathbf{y}}\n\\def\\Yf{\\mathbf{Y_f}}\n\\def\\W{\\mathbf{W}}\n\\def\\L{\\mathbf{L}}\n\\def\\m{\\mathbf{m}}\n\\def\\n{\\mathbf{n}}\n\\def\\w{\\mathbf{w}}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\z{\\mathbf{z}}\n\\def\\v{\\mathbf{v}}\n\\def\\u{\\mathbf{u}}\n\n\\def\\zero{\\mathbf{0}}\n\\def\\one{\\mathbf{1}}\n\\newcommand{\\taub}{\\boldsymbol{\\tau}}\n\\newcommand{\\betav}{\\boldsymbol{\\beta}}\n\\newcommand{\\alphav}{\\boldsymbol{\\alpha}}\n\\newcommand{\\bfomega}{\\boldsymbol{\\omega}}\n\\newcommand{\\bfchi}{\\boldsymbol{\\chi}}\n\\newcommand{\\bfLambda}{\\boldsymbol{\\Lambda}}\n\\newcommand{\\Lmea}{{\\cal L}} \n\\newcommand{\\scale}{\\bflambda} \n\\newcommand{\\Scale}{\\bfLambda} \n\\newcommand{\\bfscale}{\\bflambda} \n\\newcommand{\\mean}{\\bfchi}\n\\newcommand{\\loc}{\\bfchi}\n\\newcommand{\\bfmean}{\\bfchi}\n\\newcommand{\\bfx}{\\x}\n\\renewcommand{\\k}{g}\n\\newcommand{\\Gen}{{\\cal G}}\n\\newcommand{\\Levy}{L{\\'e}vy}\n\\def\\bfChi    {\\boldsymbol{\\Chi}}\n\\def\\bfOmega  {\\boldsymbol{\\Omega}}\n\\newcommand{\\Po}{\\mbox{\\sf P}}\n\\newcommand{\\A}{\\mathbf{A}}\n\\def\\a{\\mathbf{a}}\n\\def\\K{\\mathbf{K}}\n\\newcommand{\\B}{\\mathbf{B}}\n\\def\\b{\\boldsymbol{\\beta}}\n\\def\\bhat{\\hat{\\boldsymbol{\\beta}}}\n\\def\\btilde{\\tilde{\\boldsymbol{\\beta}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\bg{\\boldsymbol{\\beta_\\gamma}}\n\\def\\bgnot{\\boldsymbol{\\beta_{(-\\gamma)}}}\n\\def\\mub{\\boldsymbol{\\mu}}\n\\def\\tmub{\\tilde{\\boldsymbol{\\mu}}}\n\\def\\muhat{\\hat{\\boldsymbol{\\mu}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\tk{\\boldsymbol{\\theta}_k}\n\\def\\tj{\\boldsymbol{\\theta}_j}\n\\def\\Mk{\\boldsymbol{{\\cal M}}_k}\n\\def\\M{\\boldsymbol{{\\cal M}}}\n\\def\\Mj{\\boldsymbol{{\\cal M}}_j}\n\\def\\Mi{\\boldsymbol{{\\cal M}}_i}\n\\def\\Mg{{\\boldsymbol{{\\cal M}_\\gamma}}}\n\\def\\Mnull{\\boldsymbol{{\\cal M}}_{N}}\n\\def\\gMPM{\\boldsymbol{\\gamma}_{\\text{MPM}}}\n\\def\\gHPM{\\boldsymbol{\\gamma}_{\\text{HPM}}}\n\\def\\Mfull{\\boldsymbol{{\\cal M}}_{F}}\n\\def\\NS{\\boldsymbol{{\\cal N}}}\n\\def\\tg{\\boldsymbol{\\theta}_{\\boldsymbol{\\gamma}}}\n\\def\\g{\\boldsymbol{\\gamma}}\n\\def\\eg{\\boldsymbol{\\eta}_{\\boldsymbol{\\gamma}}}\n\\def\\G{\\mathbf{G}}\n\\def\\cM{\\cal M}\n\\def\\D{\\boldsymbol{\\Delta}}\n\\def\\Dbf{\\mathbf{D}}\n\\def \\shat{{\\hat{\\sigma}}^2}\n\\def\\uv{\\mathbf{u}}\n\\def\\l {\\lambda}\n\\def\\d{\\delta}\n\\def\\deltab{\\mathbf{\\delta}}\n\\def\\Sigmab{\\boldsymbol{\\Sigma}}\n\\def\\Phib{\\boldsymbol{\\Phi}}\n\\def\\Lambdab{\\boldsymbol{\\Lambda}}\n\\def\\lambdab{\\boldsymbol{\\lambda}}\n\\def\\Mg{{\\cal M}_\\gamma}\n\\def\\S{{\\cal{S}}}\n\\def\\Sbf{{\\mathbf{S}}}\n\\def\\qg{p_{\\boldsymbol{\\gamma}}}\n\\def\\pg{p_{\\boldsymbol{\\gamma}}}\n\\def\\T{\\boldsymbol{\\Theta}}\n\\def\\Tb{\\boldsymbol{\\Theta}}\n\\def\\t{\\mathbf{t}}\n\\def\\pause{\\vspace{1mm}}\n\n\n\n-   Normality \\& Transformations\n-   Box-Cox\n-   Variance Stabilizing Transformations\n-   Nonlinear Regression\n\n. . .\n\nReadings: Christensen  Chapter 13, Seber & Lee Chapter 10  \\& Wakefield Chapter 6\n\n## Model Assumptions\n\nLinear Model:\n $$ \\Y = \\mub + \\eps $$  \nAssumptions:  \n\\begin{eqnarray*}\n   \\mub \\in C(\\X) & \\Leftrightarrow & \\mub = \\X \\b  \\\\\n   \\eps  & \\sim &  \\N(\\zero_n, \\sigma^2 \\I_n) \n\\end{eqnarray*}\n\n\n\n\n-  Normal Distribution for $\\Y$ with constant variance or fixed covariance\n- linearity of $\\mub$ in $\\X$\n-  Computational Advantages of Normal Models\n-  Robustify with heavy tailed  error distributions\n\n\n\n\n## Checking via QQ-Plots\n\n-  QQ-Plots are a graphical tool to assess normality\n\n-  Order residuals $e_i$: $e_{(1)} \\le e_{(2)} \\ldots \\le e_{(n)}$  sample\n    order statistics or sample quantiles (standardized - divide by $\\sqrt{1 - h_{ii}}$ \n-  Let $z_{(1)} \\le z_{(2)} \\ldots z_{(n)}$ denote the expected\n  order statistics of a sample of size $n$ from a standard normal\n  distribution ``theoretical quantiles''  \n-  If the $e_i$ are normal then $\\E[e_{(i)}/\\sqrt{1 - h_{ii}}] = \\sigma z_{(i)}$   \n-  Expect that points in a scatter plot of $e_{(i)}/\\sqrt{1 - h_{ii}}$ and $z_{(i)}$\n  should be on a straight line.  \n-  Judgment call - use simulations to gain experience!\n\n## Animal Example\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(Animals, package=\"MASS\")\nplot(brain ~ body, data=Animals, xlab=\"Body Weight (kg)\", ylab=\"Brain Weight (g)\", main=\"Original Units\")\n```\n\n::: {.cell-output-display}\n![](22-transformations_files/figure-revealjs/animal-brain weight-1.png){fig-align='center' width=6in height=6in}\n:::\n:::\n\n\n\n::: {.footer}\n:::\n\n## Residual Plots\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbrains.lm <- lm(brain ~ body, data=Animals)\npar(mfrow=c(2,2))\nplot(brains.lm)\n```\n\n::: {.cell-output-display}\n![](22-transformations_files/figure-revealjs/animal-residuals-1.png){fig-align='center' width=6in height=6in}\n:::\n:::\n\n\n\n::: {.footer}\n:::\n\n## Normality Assumption\n\nRecall: \n\n\\begin{eqnarray*}\n \\e & = &(\\I - \\P_\\X) \\Y  \\\\\n & = & (\\I - \\P_\\X)(\\X \\bhat + \\eps)  \\\\\n & = &  (\\I - \\P_\\X)\\eps  \n    \\end{eqnarray*}\n    \n. . . \n\n$$e_i = \\epsilon_i - \\sum_{j=1}^n h_{ij} \\epsilon_j$$  \n\n- Lyapunov CLT (independent but not identically distributed) implies that  residuals will be approximately normal\n(even for modest $n$),\nif the errors are not normal  \n\n\n\n\n\n- *Supernormality of residuals*\n\n\n- clearly not the case here!\n\n## Box-Cox Transformation\n\nBox and Cox (1964) suggested a family of power transformations for $Y > 0$  \n$$\nU(\\Y, \\lambda) =  Y^{(\\lambda)} = \\left\\{\n   \\begin{array}{ll}\n     \\frac{(Y^\\lambda -1)}{\\lambda} & \\lambda \\neq 0 \\\\\n \\log(Y) & \\lambda = 0\n   \\end{array} \\right.\n$$  \n\n\n-  Estimate $\\lambda$ by maximum Likelihood  \n$$\\cL(\\lambda, \\b, \\sigma^2) \\propto \\prod f(y_i \\mid \\lambda, \\b,\n\\sigma^2)$$\n\n-   $U(\\Y, \\lambda) = Y^{(\\lambda)} \\sim \\N(\\X\\b, \\sigma^2)$\n \n-  Jacobian term is $\\prod_i y_i^{\\lambda - 1}$ for all $\\lambda$  \n-  Profile Likelihood based on substituting MLE $\\b$ and $\\sigma^2$\n  for each value of $\\lambda$ is\n$$\\log(\\cL(\\lambda) \\propto (\\lambda -1)\n\\sum_i \\log(Y_i) - \\frac{n}{2} \\log(\\SSE (\\lambda))$$\n\n\n## Profile Likelihood\n\n-  Profile Likelihood is a function of $\\lambda$ obtained by\n  substituting the MLE of $\\b$ and $\\sigma^2$ for each value of\n  $\\lambda$\n  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nMASS::boxcox(brains.lm, lambda=seq(-2, 2, by=0.1))\n```\n\n::: {.cell-output-display}\n![](22-transformations_files/figure-revealjs/boxcox-1.png){fig-align='center' width=5in height=5in}\n:::\n:::\n\n\n  \n::: {.footer}\n:::\n\n## Residuals after Transformation of Response\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(lm(log(brain) ~ body, data=Animals))\n```\n\n::: {.cell-output-display}\n![](22-transformations_files/figure-revealjs/log-transform-1.png){fig-align='center' width=5in height=5in}\n:::\n\n::: {.cell-output-display}\n![](22-transformations_files/figure-revealjs/log-transform-2.png){fig-align='center' width=5in height=5in}\n:::\n\n::: {.cell-output-display}\n![](22-transformations_files/figure-revealjs/log-transform-3.png){fig-align='center' width=5in height=5in}\n:::\n\n::: {.cell-output-display}\n![](22-transformations_files/figure-revealjs/log-transform-4.png){fig-align='center' width=5in height=5in}\n:::\n:::\n\n\n\n::: {.footer}\n:::\n\n## Residuals after Transformation of Response and Predictor\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogbrain.lm <- lm(log(brain) ~ log(body), data=Animals)\nplot(logbrain.lm)\n```\n\n::: {.cell-output-display}\n![](22-transformations_files/figure-revealjs/log-log-1.png){fig-align='center' width=5in height=5in}\n:::\n\n::: {.cell-output-display}\n![](22-transformations_files/figure-revealjs/log-log-2.png){fig-align='center' width=5in height=5in}\n:::\n\n::: {.cell-output-display}\n![](22-transformations_files/figure-revealjs/log-log-3.png){fig-align='center' width=5in height=5in}\n:::\n\n::: {.cell-output-display}\n![](22-transformations_files/figure-revealjs/log-log-4.png){fig-align='center' width=5in height=5in}\n:::\n:::\n\n\n\n::: {.footer}\n:::\n\n\n## Transformed Data\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(brain ~ body, data=Animals, xlab=\"Body Weight (kg)\", ylab=\"Brain Weight (g)\", log=\"xy\", main=\"Logarithmic Scale\")\n```\n\n::: {.cell-output-display}\n![](22-transformations_files/figure-revealjs/log-log orignal scale-1.png){fig-align='center' width=5in height=5in}\n:::\n:::\n\n\n\n::: {.footer}\n:::\n\n## Test that Dinosaurs are from a Different Population (outliers)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogbrains.nodino.lm = lm(log(brain) ~ log(body) + \n                           I(row.names(Animals) == \"Triceratops\") +  \n                           I(row.names(Animals) == \"Brachiosaurus\") + \n                           I(row.names(Animals) == \"Dipliodocus\"), \n                         data=Animals)\n\nanova(logbrain.lm, logbrains.nodino.lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: log(brain) ~ log(body)\nModel 2: log(brain) ~ log(body) + I(row.names(Animals) == \"Triceratops\") + \n    I(row.names(Animals) == \"Brachiosaurus\") + I(row.names(Animals) == \n    \"Dipliodocus\")\n  Res.Df    RSS Df Sum of Sq     F    Pr(>F)\n1     26 60.988                             \n2     23 12.117  3    48.871 30.92 3.031e-08\n```\n\n\n:::\n:::\n\n\n\n\n---\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nAnimals = cbind(Animals, diag(28)); colnames(Animals)[3:30] = rownames(Animals)\nbrains.bas = bas.lm(log(brain) ~ log(body) + . - body, data=Animals, \n                    prior=\"hyper-g-n\", a=3,modelprior=beta.binomial(1,28), method=\"MCMC\", n.models=10000, MCMC.it=2^17)\nimage(brains.bas, rotate=FALSE)\n```\n\n::: {.cell-output-display}\n![](22-transformations_files/figure-revealjs/BAS-1.png){fig-align='center' width=5in height=7in}\n:::\n:::\n\n\n\n::: {.footer}\n:::\n\n## Variance Stabilizing Transformations\n\n\n-  If $Y - \\mu$ (approximately) $N(0, h(\\mu))$ \n-  Delta Method implies that\n$$g(Y) \\stackrel{\\cdot}{\\sim}\\N( g(\\mu),  g'(\\mu)^2 h(\\mu))$$ \n\n-  Find function $g$ such that $g'(\\mu)^2 h(\\mu)$ is constant\n$$g(Y) \\sim N(g(\\mu), c)$$\n\n\n\n-  Poisson Counts (need $Y > 3$), $g$ is the square root transformation \n-  Binomial: $\\arcsin(\\sqrt{Y})$\n\n. . .\n\nNote: transformation for normality may not be the same as the variance\nstabilizing transformation; boxcox assumes mean function is correct \n\n. . .\n\nGeneralized Linear Models are preferable to transforming data, but may still be useful for \nstarting values for MCMC\n\n## Nonlinear Regression\n\n- Drug concentration of caldralazine  at time $X_i$ in a cardiac\nfailure patient given a single 30mg dose  $(D = 30)$ given by \n$$\n\\mu(\\b) = \\left[\\frac{D}{V} \\exp(-\\kappa_e x_i) \\right]\n$$\nwith $\\b = (V, \\kappa_e)$  $V = volume$ and $\\kappa_e$ is the\nelimination rate \n\n- If $Y_i  =  \\left[\\frac{D}{V} \\exp(-\\kappa_e x_i) \\right] \\times \\epsilon_i$ with $\\log(\\epsilon_i) \\iid\nN(0, \\sigma^2)$ then the model is *intrinisically* linear (can transform\nto linear model)\n\n. . .\n\n\\begin{eqnarray*}\n\\log(\\mu(\\b)) & = & \\log\\left[\\frac{D}{V} \\exp(-\\kappa_e x_i) \\right]  =  \\log[D] - \\log(V) -\\kappa_e x_i \\\\\nlog(Y_i) - \\log[30] & = &\\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\end{eqnarray*}\nwhere $\\epsilon_i$ has a log normal distribution\n\n## Nonlinear Least Squares Example\n::: {.columns}\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx = c(2,4,6,8,10,24,28, 32)\ny = c(1.63, 1.01, .73, .55, .41, .01, .06, .02)\n\nconc.lm = lm(I(log(y) - log(30)) ~ x)\n\nvhat = exp(-coef(conc.lm)[1])\nkhat = -coef(conc.lm)[2]\n\nvhat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n   16.66331 \n```\n\n\n:::\n\n```{.r .cell-code}\nkhat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        x \n0.1521064 \n```\n\n\n:::\n:::\n\n\n:::\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(x, y)\nlines(x, (30/vhat)*exp(-khat*x))\n```\n\n::: {.cell-output-display}\n![](22-transformations_files/figure-revealjs/nonlinear plot-1.png){fig-align='center' width=5in height=5in}\n:::\n:::\n\n\n:::\n::::\n## Additive Error Model\n\n-  If $\\Y = \\left[\\frac{D}{V} \\exp(-\\kappa_e x_i) \\right] + \\epsilon_i$ model is intrinisically nonlinear\nand cannot transform to a linear model.\n\n- need to use nonlinear least squares to estimate $\\b$ and $\\sigma^2$\n\n- or MCMC to estimate the posterior distribution of $\\b$ and $\\sigma^2$\n\n\n## Intrinsically Linear Model nls\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf = data.frame(y=y, x=x)\nlogconc.nlm = nls( log(y) ~ log((30/V)*exp(-k*x)), data=df, start=list(V=vhat, k=khat))\nsummary(logconc.nlm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFormula: log(y) ~ log((30/V) * exp(-k * x))\n\nParameters:\n              Estimate Std. Error t value Pr(>|t|)\nV.(Intercept) 16.66331    7.11923   2.341 0.057796\nk.x            0.15211    0.02368   6.423 0.000673\n\nResidual standard error: 0.7411 on 6 degrees of freedom\n\nNumber of iterations to convergence: 0 \nAchieved convergence tolerance: 4.056e-09\n```\n\n\n:::\n:::\n\n\n\n## Intrinsically Nonlinear Model\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFormula: y ~ (30/V) * exp(-k * x)\n\nParameters:\n  Estimate Std. Error t value Pr(>|t|)\nV 13.06506    0.60899   21.45 6.69e-07\nk  0.18572    0.01124   16.52 3.14e-06\n\nResidual standard error: 0.05126 on 6 degrees of freedom\n\nNumber of iterations to convergence: 4 \nAchieved convergence tolerance: 7.698e-06\n```\n\n\n:::\n:::\n\n\n\n## Functions of Interest\nInterest is in\n\n-  clearance: $V \\kappa_e$ \n-  elimination half-life $x_{1/2} = \\log 2/\\kappa_e$ \n\n-    Use properties of MLEs: asymptotically  $\\hat{\\b} \\sim N\\left(\\b,\n    I(\\hat{\\b})^{-1}\\right)$ \\pause\n-  Asymptotic Distributions\n\n\n- Bayes obtain the posterior directly for parameters and functions of parameters!    \n  - Priors? \n  - Constraints on Distributions?\n  - Bayes Factor for testing normal vs log-normal models?\n\n## Summary\n\n\n-  Optimal transformation for normality (MLE) depends on choice\n    of mean function \\pause\n-  May not be the same as the variance stabilizing transformation \\pause\n-  Nonlinear Models as suggested by Theory or Generalized Linear\n   Models are alternatives \\pause\n-  ``normal'' estimates may be useful approximations for large $p$\n   or for starting values for more complex models  (where convergence\n   may be sensitive to starting values)\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}