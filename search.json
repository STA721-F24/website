[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": " Resources",
    "section": "",
    "text": "Primary Textbooks\nThese textbooks are great resources for some of the topics we will cover.\n\nPlane Answers to Complex Questions, Ronald Christensen. eBook in Duke Library\nLinear Regression Analysis, George A.F Seber and Alan J. Lee eBook in Duke Library. Duke Library is aware the link is broken\nThe Linear Model and Hypothesis, George A. F. Seber eBook in Duke Library\n\n\n\nSupplementary Textbooks on Linear/Matrix Algebra\n\nGilbert Strang’s Online Course at MIT\nVideo Lectures\n\nIntroduction to Linear Algebra. Strang, Gilbert. 4th ed. Wellesley, MA: Wellesley-Cambridge Press, 2009. ISBN: 9780980232714. Buy @ Amazon\n\nMatrix Algebra from a Statistician’s Perspective. Harville, David A. eBook in Duke Library\n\n\n\nR and R Markdown Resources\nQuarto/R Markdown/LaTeX can be used to create high quality reports and presentations with embedded chunks of R code and LaTeX equations! You are required to use Quarto in RStudio to type up your homework assignments that involve Data Analysis/Simulation for this course, but you are welcome to use any word processor of your choice for those. To learn more about Quarto/R Markdown and for other resources for programming in R, see the links below.\n\nUsing R in Quarto for Documents\nR for Data Science (by Hadley Wickham & Garrett Grolemund)\nIntroduction to R Markdown (Article by Garrett Grolemund)\nIntroduction to R Markdown (Slides by Andrew Cho)\nR Markdown Cheat Sheet\nData Visualization with ggplot2 Cheat Sheet\nOther Useful Cheat Sheets\nA very (very!) basic R Markdown template\n\n\n\nLaTeX\nYou may also use LaTeX to type up your assignments. You may find it easier to create your TeX and LaTeX documents using online editors such as Overleaf (simply create a free account and you are good to go!). However, that need not be the case. If you prefer to create them locally/offline on your personal computers, you will need to download a TeX distribution (the most popular choices are MiKTeX for Windows and MacTeX for macOS) plus an editor (I personally prefer TeXstudio but feel free to download any editor of your choice). Follow the links below for some options, and to also learn how to use LaTeX.\n\nLearn LaTeX in 30 minutes\nChoosing a LaTeX Compiler.\n\n\n\nInteresting Articles\nI will add articles I find interesting below. These are articles I find useful as supplementary readings for topics covered in class, or as good sources that cover concepts I think you should know, but which we may not have time to cover. I strongly suggest you find time to (at the very least) take a “quick peek” at each article.\n\nEfron, B., 1986. Why isn’t everyone a Bayesian?. The American Statistician, 40(1), pp. 1-5.\nGelman, A., 2008. Objections to Bayesian statistics. Bayesian Analysis, 3(3), pp. 445-449.\nDiaconis, P., 1977. Finite forms of de Finetti’s theorem on exchangeability. Synthese, 36(2), pp. 271-281.\nGelman, A., Meng, X. L. and Stern, H., 1996. Posterior predictive assessment of model fitness via realized discrepancies. Statistica sinica, pp. 733-760.\nDunson, D. B., 2018. Statistics in the big data era: Failures of the machine. Statistics & Probability Letters, 136, pp. 4-9."
  },
  {
    "objectID": "reading/01-introduction.html",
    "href": "reading/01-introduction.html",
    "title": "Lecture 1 Readings",
    "section": "",
    "text": "links to eBooks are on the Home page of the website or Resources page."
  },
  {
    "objectID": "reading/01-introduction.html#introduction-to-linear-models",
    "href": "reading/01-introduction.html#introduction-to-linear-models",
    "title": "Lecture 1 Readings",
    "section": "Introduction to Linear Models",
    "text": "Introduction to Linear Models\n\nreview the course website and syllabus for policies"
  },
  {
    "objectID": "reading/01-introduction.html#vector-spaces",
    "href": "reading/01-introduction.html#vector-spaces",
    "title": "Lecture 1 Readings",
    "section": "Vector Spaces",
    "text": "Vector Spaces\nChristensen: Read\n\nChapter 1: pages 1-3\nAppendix A in Christensen pages 411-413\nAppendix B section B.1\n\nSee also Seber & Lee Chapter 1."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": " Schedule",
    "section": "",
    "text": "Please refresh often in case links/content has been updated\n\n\n\n\n\n\n\n\nWeek\nDate\nLesson\nReading\nSlides\nLabs\nHomework\n\n\n\n\nWEEK 1\nTues, Aug 26\nLecture 1: Introduction to Linear Models\n\n\n\n\n\n\n\n\n\n\nThur, Aug 28\nLecture 2: MLEs & Projections\n\n\n\n\n hw-01\n\n\n\n\nFri, Aug 29\n\n\n\n\n\n\n\n\n\n\n\nWEEK 2\nTues, Sept 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThur, Sept 5\n\n\n\n\n\n\n\n\n hw-02\n\n\n\n\nFri, Sept 6\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 3\nTues, Sept 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThur, Sept 12\n\n\n\n\n\n\n\n\n hw-03\n\n\n\n\nFri, Sept 13\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 4\nTues, Sept 17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThur, Sept 19\n\n\n\n\n\n\n\n\n hw-04\n\n\n\n\nFri, Sept 20\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 5\nTues, Sept 24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThur, Sept 26\n\n\n\n\n\n\n\n\n hw-05\n\n\n\n\nFri, Sept 28\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 6\nTues, Oct 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThu, Oct 3\n\n\n\n\n\n\n\n\n hw-06\n\n\n\n\nFri, Oct 4\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 7\nTue, Oct 8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThu, Oct 10\n\n\n\n\n\n\n\n\n hw-07\n\n\n\n\nFri, Oct 11\nReview for Midterm I\n\n\n\n\n\n\n\n\n\n\nWEEK 8\nTue, Oct 15\nNO CLASS FALL BREAK\n\n\n\n\n\n\n\n\n\n\n\n\nThur, Oct 17\nMidterm 1\n\n\n\n\n\n\n\n\n\n\n\n\nFri, Oct 18\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 9\nTue, Oct 22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThur, Oct 24\n\n\n\n\n\n\n\n\n hw-08\n\n\n\n\nFri, Oct 25\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK 10\nTues, Oct 29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThur, Oct 31\n\n\n\n\n\n\n\n\n hw-09\n\n\n\n\nFri, Nov 1\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 11\nTues, Nov 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThurs, Nov 7\n\n\n\n\n\n\n\n\n hw-10\n\n\n\n\nFri, Nov 8\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 12\nTues, Nov 12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThurs, Nov 14\nMidterm2\n\n\n\n\n\n\n hw-11\n\n\n\n\nFri, Nov 15\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 12\nTues, Nov 19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThurs, Nov 21\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 13\nTues, Nov 26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThur, Nov 30\nNO CLASS THANKSGIVING\n\n\n\n\n\n\n\n\n\n\nWeek 14\n\n\nGraduate Reading Period\n\n\n\n\n\n\n\n\n\n\nFinals Period\nSunt, Dec 15 9am-12pm (in classroom)"
  },
  {
    "objectID": "hw-01.html",
    "href": "hw-01.html",
    "title": "Homework 1",
    "section": "",
    "text": "(see [Gradescope]https://www.gradescope.com/courses/843802/assignments) for any updates on due dates)"
  },
  {
    "objectID": "hw-01.html#due-1100pm-thurs-sept-5",
    "href": "hw-01.html#due-1100pm-thurs-sept-5",
    "title": "Homework 1",
    "section": "",
    "text": "(see [Gradescope]https://www.gradescope.com/courses/843802/assignments) for any updates on due dates)"
  },
  {
    "objectID": "hw-01.html#rstudio",
    "href": "hw-01.html#rstudio",
    "title": "Homework 1",
    "section": "RStudio",
    "text": "RStudio\nYou all should have R and RStudio installed on your computers by now or access to the Department Server https://rstudio.stat.duke.edu. If you need to setup your local version, first install the latest version of R here: https://cran.rstudio.com (remember to select the right installer for your operating system). Next, install the latest version of RStudio here: https://www.rstudio.com/products/rstudio/download/. Scroll down to the “Installers for Supported Platforms” section and find the right installer for your operating system."
  },
  {
    "objectID": "hw-01.html#r-quarto",
    "href": "hw-01.html#r-quarto",
    "title": "Homework 1",
    "section": "R & Quarto",
    "text": "R & Quarto\nYou are required to use the .qmd format to type up this report report. To get started see technical writing with Quarto. Feel free to include images of technical derivations if you are short of time to typeset in LaTeX. The macros.tex file should help with shortcuts."
  },
  {
    "objectID": "hw-01.html#getting-started-with-github-classroom",
    "href": "hw-01.html#getting-started-with-github-classroom",
    "title": "Homework 1",
    "section": "Getting started with Github Classroom",
    "text": "Getting started with Github Classroom\nGo to Github classroom to accept the invitation to create a repository for this assignment at HW1\nThis will create a private repo in the STA721-F24 organization on Github with your github user name. Note: You may receive an email invitation to join STA702-F24 on your behalf.\nHit refresh, to see the link for the repo, and then click on it to go to the STA721-F24 organization in Github.\nFollow the instructions in the README in your repo and in the hw1.Rnw file to complete the assignment and push to GitHub. If you encounter issues with Gradescope submission, it is critical that you have pushed your final assignment to GitHub by the due date to validate timestamps."
  },
  {
    "objectID": "hw-01.html#gradescope-submission",
    "href": "hw-01.html#gradescope-submission",
    "title": "Homework 1",
    "section": "Gradescope Submission",
    "text": "Gradescope Submission\nYou MUST submit both your final .qmd .pdf files to the repo on Github and upload the pdf to Gradescope here: https://www.gradescope.com/courses/843802/assignments\nMake sure to Render to pdf; ask the TA about knitting to pdf if you cannot figure it out. Be sure to submit under the right assignment entry by the due date!"
  },
  {
    "objectID": "hw-01.html#grading",
    "href": "hw-01.html#grading",
    "title": "Homework 1",
    "section": "Grading",
    "text": "Grading\nTotal: 20 points."
  },
  {
    "objectID": "resources/slides/01-introduction.html#introduction-to-sta721",
    "href": "resources/slides/01-introduction.html#introduction-to-sta721",
    "title": "Introduction to STA721",
    "section": "Introduction to STA721",
    "text": "Introduction to STA721\n\nCourse: Theory and Application of linear models from both a frequentist (classical) and Bayesian perspective\nPrerequisites: linear algebra and a mathematical statistics course covering likelihoods and distribution theory (normal, t, F, chi-square, gamma distributions)\nIntroduce R programming as needed in the lab\nIntroduce Bayesian methods, but assume that you are co-registered in 702 or have taken it previously\nmore info on Course website https://sta721-F24.github.io/website/\n\nschedule and slides, HW, etc\ncritical dates (Midterms and Finals)\noffice hours\n\nCanvas for grades, email, announcements\n\n\nPlease let me know if there are broken links for slides, etc!"
  },
  {
    "objectID": "resources/slides/01-introduction.html#notation",
    "href": "resources/slides/01-introduction.html#notation",
    "title": "Introduction to STA721",
    "section": "Notation",
    "text": "Notation\n\nscalors are \\(a\\) (italics or math italics)\nvectors are in bold lower case, \\(\\mathbf{a}\\), with the exception of random variables\nall vectors are column vectors \\[\\mathbf{a}= \\left[\\begin{array}{c}\n    a_1 \\\\\n    a_2 \\\\\n    \\vdots \\\\\n    a_n\n     \\end{array} \\right]\n\\]\n\n\\(\\mathbf{1}_n\\) is a \\(n \\times 1\\) vector of all ones\n\ninner product \\(\\langle    \\mathbf{a}, \\mathbf{a}\\rangle = \\mathbf{a}^T\\mathbf{a}= \\|\\mathbf{a}\\|^2 = \\sum_{i=1}^n a_i^2\\); \\(\\langle    \\mathbf{a}, \\mathbf{b}\\rangle = \\mathbf{a}^T\\mathbf{b}\\)\nlength or norm of \\(\\mathbf{a}\\) is \\(\\|\\mathbf{a}\\|\\)"
  },
  {
    "objectID": "resources/slides/01-introduction.html#matrices",
    "href": "resources/slides/01-introduction.html#matrices",
    "title": "Introduction to STA721",
    "section": "Matrices",
    "text": "Matrices\n\nMatrices are represented in bold \\(\\mathbf{A}= (a_{ij})\\) \\[\\mathbf{A}= \\left[\\begin{array}{cccc}\n    a_{11} & a_{12} & \\cdots & a_{1m}  \\\\\n    a_{21} & a_{22} & \\cdots & a_{2m} \\\\\n    \\vdots & \\vdots & \\vdots & \\vdots\\\\\n    a_{n1} & a_{n2} & \\cdots & a_{nm}\n     \\end{array} \\right]\n\\]\n\nidentity matrix \\(\\mathbf{I}_n\\) square matrix with diagonal elements 1 and off diagonal 0\ntrace: if \\(\\mathbf{A}\\) is \\(n \\times m\\) \\(\\textsf{tr}(\\mathbf{A}) = \\sum_i^{\\max n,m } a_{ii}\\)\ndeterminant: for \\(\\mathbf{A}\\) is \\(n \\times n\\) then the determinant is \\(\\det(A)\\)\ninverse: if \\(\\mathbf{A}\\) is nonsingular \\(\\mathbf{A}&gt; 0\\), then its inverse is \\(\\mathbf{A}^{-1}\\)"
  },
  {
    "objectID": "resources/slides/01-introduction.html#statistical-models",
    "href": "resources/slides/01-introduction.html#statistical-models",
    "title": "Introduction to STA721",
    "section": "Statistical Models",
    "text": "Statistical Models\nOhm’s Law: \\(Y\\) is voltage across a resistor of \\(r\\) ohms and \\(X\\) is the amperes of the current through the resistor (in theory) \\[Y = rX\\]\n\nSimple linear regression for observational data \\[Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\text{  for  } i = 1,\n\\ldots, n\\]\nRewrite in vectors: \\[\\begin{eqnarray*}\n\\left[\n\\begin{array}{c}  y_1 \\\\ \\vdots \\\\  y_n \\end{array}\n\\right]   =  &\n\\left[ \\begin{array}{c}  1 \\\\ \\vdots \\\\ 1 \\end{array}  \\right]   \\beta_0 +\n\\left[ \\begin{array}{c}  x_1 \\\\ \\vdots \\\\  x_n \\end{array}\n\\right] \\beta_1 +\n\\left[ \\begin{array}{c}  \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n  \\end{array}\n\\right]\n=  &\n\\left[ \\begin{array}{cc}  1 &  x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_n\\end{array}  \\right]\n\\left[ \\begin{array}{c}  \\beta_0  \\\\  \\beta_1 \\end{array}\n\\right] +\n\\left[ \\begin{array}{c}  \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n  \\end{array}\n\\right] \\\\\n\\\\\n\\mathbf{Y}= & \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "resources/slides/01-introduction.html#nonlinear-models",
    "href": "resources/slides/01-introduction.html#nonlinear-models",
    "title": "Introduction to STA721",
    "section": "Nonlinear Models",
    "text": "Nonlinear Models\nGravitational Law: \\(F = \\alpha/d^\\beta\\) where \\(d\\) is distance between 2 objects and \\(F\\) is the force of gravity between them\n\nlog transformations \\[\\log(F) = \\log(\\alpha) - \\beta \\log(d)\\]\ncompare to noisy experimental data \\(Y_i =\\log(F_i)\\) observed at \\(x_i = \\log(d_i)\\)\nwrite \\(\\mathbf{X}= [\\mathbf{1}_n \\, \\mathbf{x}]\\)\n\\(\\boldsymbol{\\beta}= (\\log(\\alpha), -\\beta)^T\\)\nmodel with additive error on log scale \\(\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\mathbf{e}\\)\ntest if \\(\\beta = 2\\)\nerror assumptions?"
  },
  {
    "objectID": "resources/slides/01-introduction.html#intrinsically-nonlinear-models",
    "href": "resources/slides/01-introduction.html#intrinsically-nonlinear-models",
    "title": "Introduction to STA721",
    "section": "Intrinsically Nonlinear Models",
    "text": "Intrinsically Nonlinear Models\nRegression function may be an intrinsically nonlinear function of \\(t_i\\) (time) and parameters \\(\\boldsymbol{\\theta}\\) \\[Y_i = f(t_i, \\boldsymbol{\\theta}) + \\epsilon_i\\]"
  },
  {
    "objectID": "resources/slides/01-introduction.html#quadratic-linear-regression",
    "href": "resources/slides/01-introduction.html#quadratic-linear-regression",
    "title": "Introduction to STA721",
    "section": "Quadratic Linear Regression",
    "text": "Quadratic Linear Regression\nTaylor’s Theorem: \\[f(t_i, \\boldsymbol{\\theta}) = f(t_0, \\boldsymbol{\\theta}) + (t_i - t_0) f'(t_0, \\boldsymbol{\\theta}) + (t_i - t_0)^2\n\\frac{f^{''}(t_0, \\boldsymbol{\\theta})}{2}  + R(t_i, \\boldsymbol{\\theta})\\]\n\n\\[Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i \\text{  for  } i = 1, \\ldots, n\\]\n\n\nRewrite in vectors: \\[\\begin{eqnarray*}\n\\left[\n\\begin{array}{c}  y_1 \\\\ \\vdots \\\\  y_n \\end{array}\n  \\right]   =  &\n\\left[ \\begin{array}{ccc}  1 &  x_1 & x_1^2 \\\\ \\vdots & \\vdots \\\\ 1 &\n     x_n &  x_n^2\\end{array}  \\right]\n\\left[ \\begin{array}{c}  \\beta_0  \\\\  \\beta_1 \\\\ \\beta_2 \\end{array}\n\\right] +\n\\left[ \\begin{array}{c}  \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n  \\end{array}\n\\right] \\\\\n& \\\\\n\\mathbf{Y}= & \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\n\\end{eqnarray*}\\]\n\n\nQuadratic in \\(x\\), but linear in \\(\\beta\\)’s - how do we know this model is adequate?"
  },
  {
    "objectID": "resources/slides/01-introduction.html#kernel-regression-nonparametric",
    "href": "resources/slides/01-introduction.html#kernel-regression-nonparametric",
    "title": "Introduction to STA721",
    "section": "Kernel Regression (NonParametric)",
    "text": "Kernel Regression (NonParametric)\n\\[y_i =  \\beta_0 + \\sum_{j = 1}^J \\beta_j e^{-\\lambda (x_i - k_j)^d} + \\epsilon_i \\text{  for  } i = 1, \\ldots, n\\] where \\(k_j\\) are kernel locations and \\(\\lambda\\) is a smoothing parameter \\[\\begin{eqnarray*}\n\\left[\n\\begin{array}{c}  y_1 \\\\ \\vdots \\\\  y_n \\end{array}\n  \\right]   =  &\n\\left[ \\begin{array}{cccc}  1 &  e^{-\\lambda (x_1 - k_1)^d} &\n     \\ldots &  e^{-\\lambda (x_1 - k_J)^d}  \\\\\n     \\vdots & \\vdots & & \\vdots \\\\ 1 & e^{-\\lambda (x_n - k_1)^d} &  \\ldots & e^{-\\lambda (x_n - k_J)^d} \\end{array}  \\right]\n\\left[ \\begin{array}{c}  \\beta_0  \\\\  \\beta_1 \\\\\\vdots \\\\ \\beta_J \\end{array}\n\\right] +\n\\left[ \\begin{array}{c}  \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n  \\end{array}\n\\right] \\\\\n& \\\\\n\\mathbf{Y}= & \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\n\\end{eqnarray*}\\]\n\nLinear in \\(\\beta\\) given \\(\\lambda\\) and \\(k_1, \\ldots k_J\\)\nLearn \\(\\lambda\\), \\(k_1, \\ldots k_J\\) and \\(J\\)"
  },
  {
    "objectID": "resources/slides/01-introduction.html#hierarchical-models",
    "href": "resources/slides/01-introduction.html#hierarchical-models",
    "title": "Introduction to STA721",
    "section": "Hierarchical Models",
    "text": "Hierarchical Models\n\n\n\n\n\n\n\n\n\n\n\n\n\neach line represent individual sample trajectories\ncorrelation between an individual’s measurements\nsimilarities within groups\ndifferences among groups?\nallow individual regressions for each individual ?\nadd more structure?"
  },
  {
    "objectID": "resources/slides/01-introduction.html#linear-regression-models",
    "href": "resources/slides/01-introduction.html#linear-regression-models",
    "title": "Introduction to STA721",
    "section": "Linear Regression Models",
    "text": "Linear Regression Models\nResponse \\(Y_i\\) and \\(p\\) predictors \\(x_{i1}, x_{i2}, \\dots x_ip\\) \\[Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\ldots \\beta_{p}\n  x_{ip} + \\epsilon_i\\]\n\nDesign matrix \\[\\mathbf{X}=\n\\left[\\begin{array}{cccc}\n1 & x_{11} & \\ldots & x_{1p} \\\\\n1 & x_{21}  & \\ldots & x_{2p} \\\\\n\\vdots & \\vdots  & \\vdots & \\vdots \\\\\n1 & x_{n1} & \\ldots & x_{np} \\\\\n\\end{array} \\right] = \\left[ \\begin{array}{cc}\n1 & \\mathbf{x}_1^T  \\\\\n\\vdots & \\vdots \\\\\n1 & \\mathbf{x}_n^T\n\\end{array} \\right] =\n\\left[\\begin{array}{cccc}\n\\mathbf{1}_n & \\mathbf{X}_1 & \\mathbf{X}_2 \\cdots \\mathbf{X}_p\n\\end{array} \\right]\n\\]\nmatrix version \\[\\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}+ \\epsilon\\] what should go into \\(\\mathbf{X}\\) and do we need all columns of \\(\\mathbf{X}\\) for inference about \\(\\mathbf{Y}\\)?"
  },
  {
    "objectID": "resources/slides/01-introduction.html#linear-model",
    "href": "resources/slides/01-introduction.html#linear-model",
    "title": "Introduction to STA721",
    "section": "Linear Model",
    "text": "Linear Model\n\n\\(\\mathbf{Y}= \\mathbf{X}\\, \\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\)\n\\(\\mathbf{Y}\\) (\\(n \\times 1\\)) vector of random response (observe \\(\\mathbf{y}\\)); \\(\\mathbf{Y}, \\mathbf{y}\\in \\mathbb{R}^n\\)\n\\(\\mathbf{X}\\) (\\(n \\times p\\)) design matrix (observe)\n\\(\\boldsymbol{\\beta}\\) (\\(p \\times 1\\)) vector of coefficients (unknown)\n\\(\\boldsymbol{\\epsilon}\\) (\\(n \\times 1\\)) vector of “errors” (unobservable)\n\n\nGoals:\n\nWhat goes into \\(\\mathbf{X}\\)? (model building, model selection - post-selection inference?)\nWhat if multiple models are “good”? (model averaging or ensembles) \nWhat about the future? (Prediction)\nUncertainty Quantification - assumptions about \\(\\boldsymbol{\\epsilon}\\)\n\n\n\nAll models are wrong, but some may be useful (George Box)"
  },
  {
    "objectID": "resources/slides/01-introduction.html#ordinary-least-squares",
    "href": "resources/slides/01-introduction.html#ordinary-least-squares",
    "title": "Introduction to STA721",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\nGoal: Find the best fitting “line” or “hyper-plane” that minimizes \\[\\sum_i  (Y_i - \\mathbf{x}_i^T \\boldsymbol{\\beta})^2 = (\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}) = \\| \\mathbf{Y}-\n\\mathbf{X}\\boldsymbol{\\beta}\\|^2 \\]\n\nOptimization problem - seek \\(\\boldsymbol{\\beta}\\ni \\mathbf{X}\\boldsymbol{\\beta}\\) is close to \\(\\mathbf{Y}\\) in squared error\nMay over-fit \\(\\Rightarrow\\) add other criteria that provide a penalty Penalized Least Squares\nRobustness to extreme points \\(\\Rightarrow\\) replace quadratic loss with other functions\n\nno notion of uncertainty of estimates\n\nno structure of problem (repeated measures on individual, randomization restrictions, etc)\n\n\nNeed Distribution Assumptions of \\(\\mathbf{Y}\\) (or \\(\\boldsymbol{\\epsilon}\\)) for testing and uncertainty measures \\(\\Rightarrow\\) Likelihood and Bayesian inference"
  },
  {
    "objectID": "resources/slides/01-introduction.html#random-vectors",
    "href": "resources/slides/01-introduction.html#random-vectors",
    "title": "Introduction to STA721",
    "section": "Random Vectors",
    "text": "Random Vectors\n\nLet \\(Y_1, \\ldots Y_n\\) be random variables in \\(\\mathbb{R}\\) Then \\[\\mathbf{Y}\\equiv\n\\left[ \\begin{array}{c}\nY_1 \\\\\n\\vdots \\\\\nY_n\n\\end{array} \\right]\\] is a random vector in \\(\\mathbb{R}^n\\)\nExpectations of random vectors are defined element-wise: \\[\\textsf{E}[\\mathbf{Y}] \\equiv\n\\textsf{E}\\left[ \\begin{array}{c}\nY_1 \\\\\n\\vdots \\\\\nY_n\n\\end{array} \\right] \\equiv\n\\left[ \\begin{array}{c}\n\\textsf{E}[Y_1] \\\\\n\\vdots \\\\\n\\textsf{E}[Y_n]\n\\end{array} \\right] =\n\\left[ \\begin{array}{c}\n\\mu_1 \\\\\n\\vdots \\\\\n\\mu_n\n\\end{array} \\right]\n\\equiv \\boldsymbol{\\mu}\\in \\mathbb{R}^n\n\\] where mean or expected value \\(\\textsf{E}[Y_i] = \\mu_i\\)"
  },
  {
    "objectID": "resources/slides/01-introduction.html#model-space",
    "href": "resources/slides/01-introduction.html#model-space",
    "title": "Introduction to STA721",
    "section": "Model Space",
    "text": "Model Space\nWe will work with inner product spaces: a vector spaces, say \\(\\mathbb{R}^n\\) equipped with an inner product \\(\\langle \\mathbf{x},\\mathbf{y}\\rangle \\equiv \\mathbf{x}^T\\mathbf{y}, \\quad \\mathbf{x}, \\mathbf{y}\\in \\mathbb{R}^n\\)\n\n\nDefinition: SubspaceA set \\(\\boldsymbol{{\\cal M}}\\) is a subspace of \\(\\mathbb{R}^n\\) if is a subset of \\(\\mathbb{R}^n\\) and also a vector space.\nThat is, if \\(\\mathbf{x}_1 \\in \\boldsymbol{{\\cal M}}\\) and \\(\\mathbf{x}_2 \\in \\boldsymbol{{\\cal M}}\\), then \\(b_1\\mathbf{x}_1 + b_2 \\mathbf{x}_2 \\in \\boldsymbol{{\\cal M}}\\) for all \\(b_1, b_2 \\in \\mathbb{R}\\)\n\n\n\n\n\nDefinition: Column SpaceThe column space of \\(\\mathbf{X}\\) is \\(C(\\mathbf{X}) = \\mathbf{X}\\boldsymbol{\\beta}\\) for \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\)\n\n\n\n\nIf \\(\\mathbf{X}\\) is full column rank, then the columns of \\(\\mathbf{X}\\) form a basis for \\(C(\\mathbf{X})\\) and \\(C(\\mathbf{X})\\) is a p-dimensional subspace of \\(\\mathbb{R}^n\\)\n\n\nIf we have just a single model matrix \\(\\mathbf{X}\\), then the subspace \\(\\boldsymbol{{\\cal M}}\\) is the model space."
  },
  {
    "objectID": "resources/slides/01-introduction.html#philosophy",
    "href": "resources/slides/01-introduction.html#philosophy",
    "title": "Introduction to STA721",
    "section": "Philosophy",
    "text": "Philosophy\n\nfor many problems frequentist and Bayesian methods will give similar answers (more a matter of taste in interpretation)\n\nFor small problems, Bayesian methods allow us to incorporate prior information which provides better calibrated answers\n\nfor problems with complex designs and/or missing data Bayesian methods are often easier to implement (do not need to rely on asymptotics) \n\nFor problems involving hypothesis testing or model selection frequentist and Bayesian methods can be strikingly different.\n\nFrequentist methods often faster (particularly with “big data”) so great for exploratory analysis and for building a “data-sense”\n\nBayesian methods sit on top of Frequentist Likelihood\n\nGoemetric perspective important in both!\n\n\nImportant to understand advantages and problems of each perspective!\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/list-of-mathstuff.html",
    "href": "resources/slides/list-of-mathstuff.html",
    "title": "STA721-F24: Linear Models",
    "section": "",
    "text": "Definition: Projection\nDefinition: Orthogonal Complement\nDefinition: Null Space\nDefinition: Orthogonal Projections"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "labs/lab-01.html",
    "href": "labs/lab-01.html",
    "title": "lab-01",
    "section": "",
    "text": "Please see the instructions for HW-01 and be preapred to ask questions in lab (using R, theory, etc)"
  },
  {
    "objectID": "resources/slides/02-mles.html#outline",
    "href": "resources/slides/02-mles.html#outline",
    "title": "Maximum Likelihood Estimation",
    "section": "Outline",
    "text": "Outline\n\nLikelihood Function\nProjections\nMaximum Likelihood Estimates\n\n\nReadings: Christensen Chapter 1-2, Appendix A, and Appendix B"
  },
  {
    "objectID": "resources/slides/02-mles.html#normal-model",
    "href": "resources/slides/02-mles.html#normal-model",
    "title": "Maximum Likelihood Estimation",
    "section": "Normal Model",
    "text": "Normal Model\nTake an random vector \\(\\mathbf{Y}\\in \\mathbb{R}^n\\) which is observable and decompose\n\\[ \\mathbf{Y}= \\boldsymbol{\\mu}+ \\boldsymbol{\\epsilon}\\]\n\n\\(\\boldsymbol{\\mu}\\in \\mathbb{R}^n\\) (unknown, fixed)\n\n\\(\\boldsymbol{\\epsilon}\\in \\mathbb{R}^n\\) unobservable error vector (random)\n\n\nUsual assumptions?\n\n\\(E[\\epsilon_i] = 0 \\ \\forall i \\Leftrightarrow \\textsf{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}\\) \\(\\quad \\Rightarrow \\textsf{E}[\\mathbf{Y}] = \\boldsymbol{\\mu}\\) (mean vector)\n\\(\\epsilon_i\\) independent with \\(\\textsf{Var}(\\epsilon_i) = \\sigma^2\\) and \\(\\textsf{Cov}(\\epsilon_i, \\epsilon_j) = 0\\)\nMatrix version \\(\\textsf{Cov}[\\boldsymbol{\\epsilon}] \\equiv \\left[ (\\textsf{E}\\left[(\\epsilon_i -\\textsf{E}[\\epsilon_i])(\\epsilon_j - \\textsf{E}[\\epsilon_j])\\right]\\right]_{ij} = \\sigma^2 \\mathbf{I}_n\n\\quad \\Rightarrow \\textsf{Cov}[\\mathbf{Y}] = \\sigma^2 \\mathbf{I}_n\\) (errors are uncorrelated)\n\\(\\boldsymbol{\\epsilon}_i \\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}\\textsf{N}(0, \\sigma^2)\\) implies that \\(Y_i \\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}\\textsf{N}(\\mu_i, \\sigma^2)\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#likelihood-function",
    "href": "resources/slides/02-mles.html#likelihood-function",
    "title": "Maximum Likelihood Estimation",
    "section": "Likelihood Function",
    "text": "Likelihood Function\nThe likelihood function for \\(\\boldsymbol{\\mu}, \\sigma^2\\) is proportional to the sampling distribution of the data\n\\[\\begin{eqnarray*}\n{\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2) & \\propto & \\prod_{i = 1}^n \\frac{1}{\\sqrt{(2 \\pi\n                                 \\sigma^2)}} \\exp{- \\frac{1}{2}\n                                 \\left\\{ \\frac{( Y_i\n                                 - \\mu_i)^2}{\\sigma^2} \\right\\}}\n                                 \\\\\n& \\propto & ({2 \\pi} \\sigma^2)^{-n/2}\n\\exp{\\left\\{ - \\frac 1 2  \\frac{ \\sum_i(Y_i - \\mu_i)^2 )}{\\sigma^2}\n\\right\\}}   \\\\\n   & \\propto & (\\sigma^2)^{-n/2}\n\\exp{\\left\\{ - \\frac 1 2 \\frac{\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2}{\\sigma^2}\n\\right\\}} \\\\\n  & \\propto &  (2 \\pi)^{-n/2}\n| \\mathbf{I}_n\\sigma^2|^{-1/2}\n\\exp{\\left\\{ - \\frac 1 2 \\frac{\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2}{\\sigma^2}\n\\right\\}}  \n\\end{eqnarray*}\\]\n\nLast line is the density of \\(\\mathbf{Y}\\sim \\textsf{N}_n\\left(\\boldsymbol{\\mu}, \\sigma^2 \\mathbf{I}_n\\right)\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#mles",
    "href": "resources/slides/02-mles.html#mles",
    "title": "Maximum Likelihood Estimation",
    "section": "MLEs",
    "text": "MLEs\nFind values of \\(\\hat{\\boldsymbol{\\mu}}\\) and \\({\\hat{\\sigma}}^2\\) that maximize the likelihood \\({\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2)\\) for \\(\\boldsymbol{\\mu}\\in \\mathbb{R}^n\\) and \\(\\sigma^2 \\in \\mathbb{R}^+\\) \\[\\begin{eqnarray*}\n{\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2)\n    & \\propto & (\\sigma^2)^{-n/2}\n\\exp{\\left\\{ - \\frac 1 2 \\frac{\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2}{\\sigma^2}\n\\right\\}} \\\\\n\\log({\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2) )\n   & \\propto & -\\frac{n}{2} \\log(\\sigma^2)  - \\frac 1 2 \\frac{\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2}{\\sigma^2}\n\\\\\n\\end{eqnarray*}\\] or equivalently the log likelihood\n\nClearly, \\(\\hat{\\boldsymbol{\\mu}}= \\mathbf{Y}\\) but \\({\\hat{\\sigma}}^2= 0\\) is outside the parameter space\nIf \\(\\boldsymbol{\\mu}= \\mathbf{X}\\boldsymbol{\\beta}\\), can show that \\(\\hat{\\boldsymbol{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) is the MLE/OLS estimator of \\(\\boldsymbol{\\beta}\\) and \\(\\hat{\\boldsymbol{\\mu}}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\) if \\(\\mathbf{X}\\) is full column rank.\nshow via projections"
  },
  {
    "objectID": "resources/slides/02-mles.html#projections",
    "href": "resources/slides/02-mles.html#projections",
    "title": "Maximum Likelihood Estimation",
    "section": "Projections",
    "text": "Projections\ntake any point \\(\\mathbf{y}\\in \\mathbb{R}^n\\) and “project” it onto \\(C(\\mathbf{X}) = \\boldsymbol{{\\cal M}}\\)\n\nany point already in \\(\\boldsymbol{{\\cal M}}\\) stays the same\nso if \\(\\mathbf{P}_\\mathbf{X}\\) is a projection onto the column space of \\(\\mathbf{X}\\) then for \\(\\mathbf{m}\\in C(\\mathbf{X})\\) \\(\\mathbf{P}_\\mathbf{X}\\mathbf{m}= \\mathbf{m}\\)\n\\(\\mathbf{P}_\\mathbf{X}\\) is a linear transformation from \\(\\mathbb{R}^n \\to \\mathbb{R}^n\\)\nmaps vectors in \\(\\mathbb{R}^n\\) into \\(C(\\mathbf{X})\\)\nif \\(\\mathbf{z}\\in \\mathbb{R}^n\\) then \\(\\mathbf{P}_\\mathbf{X}\\mathbf{z}= \\mathbf{X}\\mathbf{a}\\in C(\\mathbf{X})\\) for some \\(\\mathbf{a}\\in \\mathbb{R}^p\\)\n\n\n\n\n\n\n\n\nExample\n\n\nFor \\(\\mathbf{X}\\in \\mathbb{R}^{n \\times p}\\), rank \\(p\\), \\(\\mathbf{P}_\\mathbf{X}= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}\\) is a projection onto the \\(p\\) dimensional subspace \\(\\boldsymbol{{\\cal M}}= C(\\mathbf{X})\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#idempotent-matrix",
    "href": "resources/slides/02-mles.html#idempotent-matrix",
    "title": "Maximum Likelihood Estimation",
    "section": "Idempotent Matrix",
    "text": "Idempotent Matrix\nWhat if we project a projection?\n\n\\(\\mathbf{P}_\\mathbf{X}\\mathbf{z}= \\mathbf{X}\\mathbf{a}\\in C(\\mathbf{X})\\)\n\\(\\mathbf{P}_\\mathbf{X}\\mathbf{X}\\mathbf{a}= \\mathbf{X}\\mathbf{a}\\)\nsince \\(\\mathbf{P}_\\mathbf{X}^2 \\mathbf{z}=  \\mathbf{P}_\\mathbf{X}\\mathbf{z}\\) for all \\(\\mathbf{z}\\in \\mathbb{R}^n\\) we have \\(\\mathbf{P}_\\mathbf{X}^2 = \\mathbf{P}_\\mathbf{X}\\)\n\n\n\nDefinition: ProjectionFor a matrix \\(\\mathbf{P}\\) in \\(\\mathbb{R}^{n \\times n}\\) is a projection matrix if \\(\\mathbf{P}^2 = \\mathbf{P}\\). That is all projections \\(\\mathbf{P}\\) are idempotent matrix.\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\nFor \\(\\mathbf{X}\\in \\mathbb{R}^{n \\times p}\\), rank \\(p\\), if \\(\\mathbf{P}_\\mathbf{X}= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}\\) use the definition to show that it is a projection onto the \\(p\\) dimensional subspace \\(\\boldsymbol{{\\cal M}}= C(\\mathbf{X})\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#null-space",
    "href": "resources/slides/02-mles.html#null-space",
    "title": "Maximum Likelihood Estimation",
    "section": "Null Space",
    "text": "Null Space\n\nDefinition: Orthogonal ComplementThe set of all vectors that are orthogonal to a given subspace \\(\\boldsymbol{{\\cal M}}\\) is called the orthogonal complement of the subspace denoted as \\(\\boldsymbol{{\\cal M}}^\\perp\\). Under the usual inner product, \\(\\boldsymbol{{\\cal M}}^\\perp \\equiv \\{\\mathbf{n}\\in \\mathbb{R}^n \\ni \\mathbf{m}^T\\mathbf{n}= 0 {\\text{ for }} \\mathbf{m}\\in \\boldsymbol{{\\cal M}}\\}\\)\n\n\n\n\nDefinition: Null SpaceFor a matrix \\(\\mathbf{A}\\), the null space of \\(\\mathbf{A}\\) is defined as \\(N(\\mathbf{A}) = \\{\\mathbf{n}\\ni \\mathbf{A}\\mathbf{n}= \\mathbf{0}\\}\\)\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\nShow that \\(C(\\mathbf{X})^\\perp\\) (the orthogonal complement of \\(C(\\mathbf{X})\\)) is the null space of \\(\\mathbf{X}^T\\), \\(\\, N(\\mathbf{X}^T)\\)."
  },
  {
    "objectID": "resources/slides/02-mles.html#orthogonal-projection",
    "href": "resources/slides/02-mles.html#orthogonal-projection",
    "title": "Maximum Likelihood Estimation",
    "section": "Orthogonal Projection",
    "text": "Orthogonal Projection\n\nDefinition: Orthogonal ProjectionsFor a vector space \\({\\cal V}\\) with an inner product \\(\\langle \\mathbf{x}, \\mathbf{y}\\rangle\\) for \\(\\mathbf{x}, \\mathbf{y}\\in {\\cal V}\\), \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are orthogonal if \\(\\langle \\mathbf{x}, \\mathbf{y}\\rangle = 0\\). A projection \\(\\mathbf{P}\\) is an orthogonal projection onto a subspace \\(\\boldsymbol{{\\cal M}}\\) of \\({\\cal V}\\) if for any \\(\\mathbf{m}\\in {\\cal V}\\), \\(\\mathbf{P}\\mathbf{m}= \\mathbf{m}\\) and for any \\(\\mathbf{n}\\in \\boldsymbol{{\\cal M}}^\\perp\\), \\(\\mathbf{P}\\mathbf{n}= \\mathbf{0}\\).\nThe null space of \\(\\mathbf{P}\\) is the orthogonal complement of \\(\\boldsymbol{{\\cal M}}\\)\n\n\n\nFor \\(\\mathbb{R}^N\\) with the inner product, \\(\\langle \\mathbf{x}, \\mathbf{y}\\rangle = \\mathbf{x}^T\\mathbf{y}\\), \\(\\mathbf{P}\\) is an orthogonal projection onto \\(\\boldsymbol{{\\cal M}}\\) if \\(\\mathbf{P}\\) is a projection (\\(\\mathbf{P}^2 = \\mathbf{P}\\)) and it is symmetric (\\(\\mathbf{P}= \\mathbf{P}^T\\))\n\n\n\n\n\n\n\n\nExercise\n\n\nShow that \\(\\mathbf{P}_\\mathbf{X}\\) is an orthogonal projection on \\(C(\\mathbf{X})\\)."
  },
  {
    "objectID": "resources/slides/02-mles.html#decompsition",
    "href": "resources/slides/02-mles.html#decompsition",
    "title": "Maximum Likelihood Estimation",
    "section": "Decompsition",
    "text": "Decompsition\n\nFor any \\(\\mathbf{y}\\in \\mathbb{R}^n\\), we can write it uniquely as a vector \\[ \\mathbf{y}= \\mathbf{m}+ \\mathbf{n}, \\quad \\mathbf{m}\\in \\boldsymbol{{\\cal M}}\\quad \\mathbf{n}\\in \\boldsymbol{{\\cal M}}^\\perp\\]\nwrite \\(\\mathbf{y}= \\mathbf{P}\\mathbf{y}+ (\\mathbf{y}- \\mathbf{P}\\mathbf{y}) = \\mathbf{P}\\mathbf{y}+ (\\mathbf{I}- \\mathbf{P})\\mathbf{y}\\)\nclaim that if \\(\\mathbf{P}\\) is an orthogonal projection, \\((\\mathbf{I}- \\mathbf{P})\\) is an orthogonal projection onto \\(\\boldsymbol{{\\cal M}}^\\perp\\)\nif \\(\\mathbf{n}\\in \\boldsymbol{{\\cal M}}^\\perp\\), then \\((\\mathbf{I}- \\mathbf{P})\\mathbf{n}= \\mathbf{n}- \\mathbf{P}\\mathbf{n}= \\mathbf{n}\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#back-to-mles",
    "href": "resources/slides/02-mles.html#back-to-mles",
    "title": "Maximum Likelihood Estimation",
    "section": "Back to MLEs",
    "text": "Back to MLEs\n\n\\(\\mathbf{Y}\\sim \\textsf{N}(\\boldsymbol{\\mu}, \\sigma^2 \\mathbf{I}_n)\\) with \\(\\boldsymbol{\\mu}= \\mathbf{X}\\boldsymbol{\\beta}\\) and \\(\\mathbf{X}\\) full column rank\nClaim: Maximum Likelihood Estimator (MLE) of \\(\\boldsymbol{\\mu}\\) is \\(\\mathbf{P}_\\mathbf{X}\\mathbf{Y}\\)\nLog Likelihood: \\[ \\log {\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2) =\n-\\frac{n}{2} \\log(\\sigma^2)\n- \\frac 1 2 \\frac{\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2}{\\sigma^2}\n\\]\nDecompose \\(\\mathbf{Y}= \\mathbf{P}_\\mathbf{X}\\mathbf{Y}+ (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\)\n\nUse \\(\\mathbf{P}_\\mathbf{X}\\boldsymbol{\\mu}= \\boldsymbol{\\mu}\\)\n\nSimplify \\(\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#expand",
    "href": "resources/slides/02-mles.html#expand",
    "title": "Maximum Likelihood Estimation",
    "section": "Expand",
    "text": "Expand\n\\[\\begin{eqnarray*}\n    \\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2 & = & \\|  { (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}+ \\mathbf{P}_x \\mathbf{Y}} -\n    \\boldsymbol{\\mu}\\|^2  \\\\\n  & = & \\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}+ \\mathbf{P}_x \\mathbf{Y}- {\\mathbf{P}_\\mathbf{X}}\\boldsymbol{\\mu}\\|^2  \\\\\n  & = & {\\|(\\mathbf{I}-\\mathbf{P}_\\mathbf{x})}\\mathbf{Y}+  {\\mathbf{P}_\\mathbf{X}}(\\mathbf{Y}- \\boldsymbol{\\mu})\n  \\|^2 \\\\\n& = & {\\|(\\mathbf{I}-\\mathbf{P}_\\mathbf{x})\\mathbf{Y}\\|^2} +  {\\|\n   {\\mathbf{P}_\\mathbf{X}}(\\mathbf{Y}- \\boldsymbol{\\mu}) \\|^2}  + {\\small{2 (\\mathbf{Y}-\n\\boldsymbol{\\mu})^T \\mathbf{P}_\\mathbf{X}^T (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}}}\\\\\n& = & \\|(\\mathbf{I}-\\mathbf{P}_\\mathbf{x})\\mathbf{Y}\\|^2 +  \\| {\\mathbf{P}_\\mathbf{X}}(\\mathbf{Y}- \\boldsymbol{\\mu}) \\|^2 + {0}\n\\\\\n& = & \\|(\\mathbf{I}-\\mathbf{P}_\\mathbf{x})\\mathbf{Y}\\|^2 +  \\| {\\mathbf{P}_\\mathbf{X}}\\mathbf{Y}- \\boldsymbol{\\mu}\\|^2\n  \\end{eqnarray*}\\]\n\nCrossproduct term is zero: \\[\\begin{eqnarray*}\n  \\mathbf{P}_\\mathbf{X}^T (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) & = &  \\mathbf{P}_\\mathbf{X}(\\mathbf{I}- \\mathbf{P}_\\mathbf{X})  \\\\\n  & = &  \\mathbf{P}_\\mathbf{X}- \\mathbf{P}_\\mathbf{X}\\mathbf{P}_\\mathbf{X}\\\\\n& = &  \\mathbf{P}_\\mathbf{X}- \\mathbf{P}_\\mathbf{X}\\\\\n& = & \\mathbf{0}\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "resources/slides/02-mles.html#log-likelihood",
    "href": "resources/slides/02-mles.html#log-likelihood",
    "title": "Maximum Likelihood Estimation",
    "section": "Log Likelihood",
    "text": "Log Likelihood\nSubstitute decomposition into log likelihood \\[\\begin{eqnarray*}\n\\log {\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2)  & = &\n-\\frac{n}{2} \\log(\\sigma^2) - \\frac 1 2 \\frac{\\| \\mathbf{Y}- \\boldsymbol{\\mu}\\|^2}{\\sigma^2}  \\\\\n  & = & -\\frac{n}{2} \\log(\\sigma^2)  - \\frac 1 2 \\left( \\frac{\\|(\\mathbf{I}- \\mathbf{P}_\\mathbf{X})\n  \\mathbf{Y}\\|^2}{\\sigma^2} + \\frac{\\| \\mathbf{P}_\\mathbf{X}\\mathbf{Y}- \\boldsymbol{\\mu}\\|^2 } {\\sigma^2} \\right)   \\\\\n& = &  \\underbrace { -\\frac{n}{2} \\log(\\sigma^2)  - \\frac 1 2  \\frac{\\|(\\mathbf{I}- \\mathbf{P}_\\mathbf{X})\n  \\mathbf{Y}\\|^2}{\\sigma^2} }  +  \\underbrace{- \\frac 1 2  \\frac{\\| \\mathbf{P}_\\mathbf{X}\\mathbf{Y}-\n  \\boldsymbol{\\mu}\\|^2 } {\\sigma^2}}   \\\\\n& = &  \\text{ constant with respect to } \\boldsymbol{\\mu}\\qquad  \\leq 0\n\\end{eqnarray*}\\]\n\nMaximize with respect to \\(\\boldsymbol{\\mu}\\) for each \\(\\sigma^2\\)\nRHS is largest when \\(\\boldsymbol{\\mu}= \\mathbf{P}_\\mathbf{X}\\mathbf{Y}\\) for any choice of \\(\\sigma^2\\) \\[\\therefore \\quad \\hat{\\boldsymbol{\\mu}}= \\mathbf{P}_\\mathbf{X}\\mathbf{Y}\\] is the MLE of \\(\\boldsymbol{\\mu}\\) (fitted values \\(\\hat{\\mathbf{Y}}= \\mathbf{P}_\\mathbf{X}\\mathbf{Y}\\))"
  },
  {
    "objectID": "resources/slides/02-mles.html#mle-of-boldsymbolbeta",
    "href": "resources/slides/02-mles.html#mle-of-boldsymbolbeta",
    "title": "Maximum Likelihood Estimation",
    "section": "MLE of \\(\\boldsymbol{\\beta}\\)",
    "text": "MLE of \\(\\boldsymbol{\\beta}\\)\n\\[{\\cal{L}}(\\boldsymbol{\\mu}, \\sigma^2)   =  -\\frac{n}{2} \\log(\\sigma^2)  - \\frac 1 2 \\left( \\frac{\\|(\\mathbf{I}- \\mathbf{P}_\\mathbf{X})\n  \\mathbf{Y}\\|^2}{\\sigma^2} + \\frac{\\| \\mathbf{P}_\\mathbf{X}\\mathbf{Y}- \\boldsymbol{\\mu}\\|^2 } {\\sigma^2} \\right)\\]\n\nRewrite as likeloood function for \\(\\boldsymbol{\\beta}, \\sigma^2\\): \\[{\\cal{L}}(\\boldsymbol{\\beta}, \\sigma^2 )  =  -\\frac{n}{2} \\log(\\sigma^2)  - \\frac 1 2 \\left( \\frac{\\|(\\mathbf{I}- \\mathbf{P}_\\mathbf{X})\n  \\mathbf{Y}\\|^2}{\\sigma^2} + \\frac{\\| \\mathbf{P}_\\mathbf{X}\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}\\|^2 } {\\sigma^2}\n\\right)\\]\n\n\n\nSimilar argument to show that RHS is maximized by minimizing \\[\\| \\mathbf{P}_\\mathbf{X}\n\\mathbf{Y}- \\mathbf{X}\\boldsymbol{\\beta}\\|^2\\] \nTherefore \\(\\hat{\\boldsymbol{\\beta}}\\) is a MLE of \\(\\boldsymbol{\\beta}\\) if and only if satisfies \\[\\mathbf{P}_\\mathbf{X}\\mathbf{Y}= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\]\nIf \\(\\mathbf{X}^T\\mathbf{X}\\) is full rank, the MLE of \\(\\boldsymbol{\\beta}\\) is \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}= \\hat{\\boldsymbol{\\beta}}\\)"
  },
  {
    "objectID": "resources/slides/02-mles.html#mle-of-sigma2",
    "href": "resources/slides/02-mles.html#mle-of-sigma2",
    "title": "Maximum Likelihood Estimation",
    "section": "MLE of \\(\\sigma^2\\)",
    "text": "MLE of \\(\\sigma^2\\)\n\nPlug-in MLE of \\(\\hat{\\boldsymbol{\\mu}}\\) for \\(\\boldsymbol{\\mu}\\) \\[ \\log {\\cal{L}}(\\hat{\\boldsymbol{\\mu}}, \\sigma^2)  =   -\\frac{n}{2} \\log \\sigma^2 - \\frac 1 2\n\\frac{\\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\|^2  }{\\sigma^2}\\]\nDifferentiate with respect to \\(\\sigma^2\\) \\[\\frac{\\partial \\, \\log {\\cal{L}}(\\hat{\\boldsymbol{\\mu}}, \\sigma^2)}{\\partial \\, \\sigma^2} =  -\\frac{n}{2} \\frac{1}{\\sigma^2}  +  \\frac 1 2\n\\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\|^2 \\left(\\frac{1}{\\sigma^2}\\right)^2 \\]\nSet derivative to zero and solve for MLE \\[\\begin{eqnarray*}\n0 & = &  -\\frac{n}{2} \\frac{1}{{\\hat{\\sigma}}^2}  +  \\frac 1 2\n\\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\|^2 \\left(\\frac{1}{{\\hat{\\sigma}}^2}\\right)^2  \\\\\n\\frac{n}{2} {\\hat{\\sigma}}^2& = & \\frac 1 2\n\\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\|^2 \\\\\n{\\hat{\\sigma}}^2& = & \\frac{\\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\|^2}{n}\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "resources/slides/02-mles.html#mle-estimate-of-sigma2",
    "href": "resources/slides/02-mles.html#mle-estimate-of-sigma2",
    "title": "Maximum Likelihood Estimation",
    "section": "MLE Estimate of \\(\\sigma^2\\)",
    "text": "MLE Estimate of \\(\\sigma^2\\)\nMaximum Likelihood Estimate of \\(\\sigma^2\\) \\[\\begin{eqnarray*}\n    {\\hat{\\sigma}}^2& = & \\frac{\\| (\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}\\|^2}{n} \\\\\n      & = & \\frac{\\mathbf{Y}^T(\\mathbf{I}- \\mathbf{P}_\\mathbf{X})^T(\\mathbf{I}-\\mathbf{P}_\\mathbf{X}) \\mathbf{Y}}{n} \\\\\n& = & \\frac{ \\mathbf{Y}^T(\\mathbf{I}- \\mathbf{P}_\\mathbf{X}) \\mathbf{Y}}{n} \\\\\n& = & \\frac{\\mathbf{e}^T\\mathbf{e}} {n}\n  \\end{eqnarray*}\\] where \\(\\mathbf{e}= (\\mathbf{I}- \\mathbf{P}_\\mathbf{X})\\mathbf{Y}\\) are the residuals from the regression of \\(\\mathbf{Y}\\) on \\(\\mathbf{X}\\)\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "resources/slides/template.html#outline",
    "href": "resources/slides/template.html#outline",
    "title": "template",
    "section": "Outline",
    "text": "Outline\n\n\n\n\nhttps://sta721-F24.github.io/website/"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": " Syllabus",
    "section": "",
    "text": "When in doubt about anything at all, ask questions!!!\n\nPrerequisites\nALL students are expected to be familiar with all the topics covered within the required prerequisites to be in this course. That is - mathematical statistics and probability, linear algebra, and multivariate calculus. Students are also expected to be familiar with R and are encouraged to learn LaTeX during the course.\n\n\nWorkload\nWork hours will include time spent going through the preassigned readings, attending lectures and lab sessions, and doing all graded work.\n\n\nGraded Work\nGraded work for the course will consist of homework assignments, lab exercises, two midterms and a final exam. Regrade requests for problem sets and lab exercises must be done via Gradescope AT MOST 24 hours after grades are released! Regrade requests for quizzes, midterm, and final exams must be done via Gradescope AT MOST 12 hours after grades are released! Always write in complete sentences and show your steps.\nStudents’ final grades will be determined as shown below:\n\nComponent Percentage\n\n\nComponent\nPercentage\n\n\n\n\nHomework\n20%\n\n\nMidterm\n25%\n\n\nMidterm II\n25%\n\n\nParticipation\n5%\n\n\nFinal Exam\n25%\n\n\n\nThere are no make-ups for any of the graded work except for medical or familial emergencies or for reasons approved by the instructor BEFORE the due date. See the instructor in advance of relevant due dates to discuss possible alternatives.\nGrades may be curved at the end of the semester. Cumulative averages of 90% – 100% are guaranteed at least an A-, 80% – 89% at least a B-, and 70% – 79% at least a C-, however the exact ranges for letter grades will be determined at the end of the course.\n\n\nDescriptions of graded work\n\nProblem sets\nHomework will be handed out on a weekly basis. They will be based on both the lectures and labs and will be announced every Thursday or Friday – be sure to check the website regularly! Also, please note that any work that is not legible by the instructor or TAs will not be graded (given a score of 0). Every write-up must be clearly written in full sentences and clear English. Any assignment that is completely unclear to the instructors and/or TAs, may result in a grade of a 0. For programming exercises, we will be using R/knitr with \\(\\LaTeX\\) for preparing assignments using github classroom for data analysis.\nEach student MUST write up and turn in her or his own answers. You are encouraged to talk to each other regarding homework problems or to the instructor/TA. However, the write-up, solution, and code must be entirely your own work. No sharing of solutions or code! The assignments must be submitted on Gradescope under Assignments. Note that you will not be able to make online submissions after the due date, so be sure to submit before or by the Gradescope-specified deadline. You may resubmit, so when in doubt submit work early. In certain situations if there are issues with submissions, the TA may review your GitHub repository prior to the due date.\nSolutions will be curated from student solutions with proper attribution. Every week the TAs will select a representative correct solution for the assigned problems and put them together into one solutions set with each answer being attributed to the student who wrote it. If you would like to OPT OUT of having your homework solutions used for the class solutions, please let the Instructor and TAs know in advance.\nFinally, your lowest homework score will be dropped!\n\n\nLab exercises\nThe objective of the lab assignments is to give you more hands-on experience with Bayesian data analysis. Attend the lab session and learn a concept or two and some R from the TA, and then work on the computational part of the problem sets. Each lab assignment should be submitted in timely fashion. You are REQUIRED to use R/knitr (or R/Rmarkdown in some cases).\n\n\nMidterm Exams\nThere will be two inclass midterm exams. Detailed instructions on the midterm will be made available later but please check dates on the calendar well in advance!\n\n\nFinal Exam\nThere will be a final exam after the reading week. If you miss any quiz or the midterm, your grade will depend more on the final exam score since there are no make-up exams. You cannot miss the final exam! Please check the important dates on the homepage for the date and time of the final before making plans to return home at the end of the semester. Detailed instructions on the final will be made available later.\n\n\n\nLate Submission Policy\n\nno late submission of homework or lab assignments, however we will drop the lowest score in each.\n\n\n\nCourse Topics\nFor a detailed day-by-day list of topics, please refer to the Course Schedule\n\n\nAcademic integrity\nDuke University is a community dedicated to scholarship, leadership, and service and to the principles of honesty, fairness, respect, and accountability. Citizens of this community commit to reflect upon and uphold these principles in all academic and nonacademic endeavors, and to protect and promote a culture of integrity.\nRemember the Duke Community Standard that you have agreed to abide by:\n\nTo uphold the Duke Community Standard:\n\n\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\nCheating or plagiarism on any graded assessments, lying about an illness or absence and other forms of academic dishonesty are a breach of trust with classmates and faculty, violate the Duke Community Standard, and will not be tolerated. Such incidences will result in a 0 grade for all parties involved. Additionally, there may be penalties to your final class grade along with being reported to the Office of Student Conduct. Review the academic dishonesty policies at https://studentaffairs.duke.edu/conduct/z-policies/academic-dishonesty.\n\n\nDiversity & Inclusiveness\nThis course is designed so that students from all backgrounds and perspectives all feel welcome both in and out of class. Please feel free to talk to me (in person or via email) if you do not feel well-served by any aspect of this class, or if some aspect of class is not welcoming or accessible to you. My goal is for you to succeed in this course, therefore, let me know immediately if you feel you are struggling with any part of the course more than you know how to manage. Doing so will not affect your grades, but it will allow me to provide the resources to help you succeed in the course.\n\n\nDisability Statement\nStudents with disabilities who believe that they may need accommodations in the class are encouraged to contact the Student Disabilities Access Office at 919-668-1267 or disabilities@aas.duke.edu as soon as possible to better ensure that such accommodations are implemented in a timely fashion.\n\n\nOther Information\nIt can be a lot more pleasant oftentimes to get one-on-one answers and help. Make use of the teaching team’s office hours, we’re here to help! Do not hesitate to talk to me during office hours or by appointment to discuss a problem set or any aspect of the course. Questions related to course assignments and honesty policy should be directed to me. When the teaching team has announcements for you we will send an email to your Duke email address. Be sure to check your email daily.\nMost of the course components will be held in person, but occasionally may need to be held online using Zoom meetings. If you have any concerns, issues or challenges, let the instructor know as soon as possible. Also, all students are strongly encouraged to rely on the forums in Sakai, for interacting among yourself and asking other students questions. You can also ask the instructor or the TAs questions on there and we will try to respond as soon as possible. If you experience any technical issues with joining or using the forums, let the instructor know.\n\n\nProfessionalism\nTry as much as possible to refrain from texting or using your computer for anything other than coursework during class and labs. Again, the more engaged you are, the quicker you will be able to get through the materials. You are responsible for everything covered in the lecture videos, lecture notes/slides, and in the assigned readings."
  },
  {
    "objectID": "HW/hw-01.html",
    "href": "HW/hw-01.html",
    "title": "Homework 1",
    "section": "",
    "text": "See Gradescope for any updates on due dates."
  },
  {
    "objectID": "HW/hw-01.html#due-1100pm-thurs-sept-5",
    "href": "HW/hw-01.html#due-1100pm-thurs-sept-5",
    "title": "Homework 1",
    "section": "",
    "text": "See Gradescope for any updates on due dates."
  },
  {
    "objectID": "HW/hw-01.html#rstudio",
    "href": "HW/hw-01.html#rstudio",
    "title": "Homework 1",
    "section": "RStudio",
    "text": "RStudio\nYou all should have R and RStudio installed on your computers by now or access to the Department RStudio Server https://rstudio.stat.duke.edu. If you need to setup your local version, first install the latest version of R here: https://cran.rstudio.com - remember to select the right installer for your operating system). Next, install the latest version of RStudio here: https://www.rstudio.com/products/rstudio/download/. Scroll down to the “Installers for Supported Platforms” section and find the right installer for your operating system."
  },
  {
    "objectID": "HW/hw-01.html#r-quarto",
    "href": "HW/hw-01.html#r-quarto",
    "title": "Homework 1",
    "section": "R & Quarto",
    "text": "R & Quarto\nYou are required to use the .qmd format to type up this report report. To get started see technical writing with Quarto. Feel free to include images of technical derivations if you are short of time to typeset in LaTeX. The macros.tex file should help with shortcuts."
  },
  {
    "objectID": "HW/hw-01.html#getting-started-with-github-classroom",
    "href": "HW/hw-01.html#getting-started-with-github-classroom",
    "title": "Homework 1",
    "section": "Getting started with Github Classroom",
    "text": "Getting started with Github Classroom\nGo to Github classroom to accept the invitation to create a repository for this assignment at HW1\nThis will create a private repo in the STA721-F24 organization on Github with your github user name. Note: You may receive an email invitation to join STA702-F24 on your behalf.\nHit refresh, to see the link for the repo, and then click on it to go to the STA721-F24 organization in Github.\nFollow the instructions in the README in your repo and in the hw1.qmd file to complete the assignment and push to GitHub. If you encounter issues with Gradescope submission, it is critical that you have pushed your final assignment to GitHub by the due date to validate timestamps."
  },
  {
    "objectID": "HW/hw-01.html#gradescope-submission",
    "href": "HW/hw-01.html#gradescope-submission",
    "title": "Homework 1",
    "section": "Gradescope Submission",
    "text": "Gradescope Submission\nYou MUST submit both your final .qmd .pdf files to the repo on Github and upload the pdf to Gradescope here: https://www.gradescope.com/courses/843802/assignments\nMake sure to Render to pdf; ask the TA about knitting to pdf if you cannot figure it out. Be sure to submit under the right assignment entry by the due date!"
  },
  {
    "objectID": "HW/hw-01.html#grading",
    "href": "HW/hw-01.html#grading",
    "title": "Homework 1",
    "section": "Grading",
    "text": "Grading\nTotal: 20 points."
  },
  {
    "objectID": "reading/02-mles.html",
    "href": "reading/02-mles.html",
    "title": "MLEs & Projections",
    "section": "",
    "text": "Readings:\n\nChristensen Chapter 1-2, Appendix A, and Appendix B\nSeber & Lee Chapter 3, Appendix B"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "STA721: Linear Models (Fall 2024)",
    "section": "Course Description",
    "text": "Course Description\nThis course introduces students to linear models and extensions for model building from the frequentist (OLS/MLE and penalized likelihoods) and Bayesian paradigms, with an emphasis on a geometric perspective. Course topics include optimal estimation and prediction, distributional assumptions and model checking, hypothesis testing and model selection including Bayes factors and intrinsic Bayes factors, and Bayesian Model Averaging. Students should have a strong background in matrix algebra and distribution theory. Co-requisite: STA 602L, 702L or equivalent."
  },
  {
    "objectID": "index.html#course-info",
    "href": "index.html#course-info",
    "title": "STA721: Linear Models (Fall 2024)",
    "section": "Course Info",
    "text": "Course Info\n\nTextbooks\n\n\n\nTextbook\nOrdering Information\n\n\n\n\n\nPlane Answers to Complex Questions Ronald Christensen (2011) 4th Edition Springer-Verlag, NY. The textbook is freely available as an eBook thru the Duke Library. You’re welcomed to read on screen or print it out. If you prefer a paperback version you can buy it at the cost of printing from Springer or purchase a hardback version at your favorite vendor.\n\n\n\nLinear Regression Analysis, George A.F Seber and Alan J. Lee (2003) 2nd Edition, Wiley eBook in Duke Library. Duke Library is aware the link to the ebook is broken See the\n\n\n\nCanvas site for a pdf version until the links are fixed.\n\n\nLecture\n   Tuesday and Thursday\n   1:25pm - 2:40pm\n   Old Chemistry 123 \n\n\nLabs\n   Fridays\n  10:05am - 11:20pm\n   LINK 088 (Clasroom 4) \n\n\nFinal Exam\n   December 15\n   9:00am - 12:00pm\n   Old Chemistry 123 \n\n\nInstructional Team and Office Hours\n\n\n\nRole\nName\nEmail\nOffice Hours\nLocation\n\n\n\n\nInstructor\nDr Merlise Clyde\n\nTues 2:45 - 3:45  or by appointment \n223E Old Chem\n\n\nTA\nBongjung Sung\n\nMon 9:30-11:30am\n203B Old Chem"
  },
  {
    "objectID": "index.html#course-topics",
    "href": "index.html#course-topics",
    "title": "STA721: Linear Models (Fall 2024)",
    "section": "Course Topics",
    "text": "Course Topics\nCourse topics will be drawn (but subject to change) from\n\nMotivation for Studying Linear Models as Foundation\nRandom Vectors and Matrices\nMultivariate Normal Distribution Theory\nConditional Normal Distribution Theory\nLinear Models via Coordinate free representations (examples)\nMaximum Likelihood Estimation & Projections\nInterval Estimation: Distribution of Quadratic Forms\nGauss-Markov Theorem & Optimality of OLS/GLS\nFormulation of Bayesian Inference\nSubjective and Default Priors\nRelated Shrinkage Methods and Penalized Likelihoods (Ridge regression, lasso)\nModel Selection (comparison of classical and Bayesian approaches)\nBayes Factors\nBayesian Model Averaging\nModel Checking: Residual Analysis & Diagnostics\nRobust Methods for Outliers\nGeneralized Linear Model\nHierarchical Models\n\nPlease check the website for updates, slides and current readings."
  }
]