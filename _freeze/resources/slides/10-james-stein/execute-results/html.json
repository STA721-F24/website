{
  "hash": "d6b0d8f44497f8faa360f582b8f74116",
  "result": {
    "engine": "knitr",
    "markdown": "---\nsubtitle: \"STA 721: Lecture 10\"\ntitle: \"James-Stein Estimation and Shrinkage\"\nauthor: \"Merlise Clyde (clyde@duke.edu)\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    smaller: true\n    logo: ../../img/icon.png\n    footer: <https://sta721-F24.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"   \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\nnumber-sections: false\nfilters:\n  - custom-numbered-blocks  \ncustom-numbered-blocks:\n  groups:\n    thmlike:\n      colors: [948bde, 584eab]\n      boxstyle: foldbox.simple\n      collapse: false\n      listin: [mathstuff]\n    todos: default  \n  classes:\n    Theorem:\n      group: thmlike\n      numbered: false\n    Lemma:\n      group: thmlike\n      numbered: false\n    Corollary:\n      group: thmlike\n      numbered: false\n    Proposition:\n      group: thmlike\n      numbered: false      \n    Proof:\n      group: thmlike\n      numbered: false\n      collapse: false   \n    Exercise:\n      group: thmlike\n      numbered: false\n    Definition:\n      group: thmlike\n      numbered: false\n      colors: [d999d3, a01793]\n    Feature: \n       numbered: false\n    TODO:\n      label: \"To do\"\n      colors: [e7b1b4, 8c3236]\n      group: todos\n      listin: [stilltodo]\n    DONE:\n      label: \"Done\"\n      colors: [cce7b1, 86b754]  \n      group: todos  \n---\n\n\n\n\n\n## Outline\n\n\\usepackage{xcolor}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator{\\sgn}{sgn}\n\\newcommand{\\e}{\\mathbf{e}}\n\\newcommand{\\Mb}{\\mathbf{M}}\n\\renewcommand{\\P}{\\mathbf{P}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\R}{\\textsf{R}}\n\\newcommand{\\mat}[1] {\\mathbf{#1}}\n\\newcommand{\\E}{\\textsf{E}}\n\\newcommand{\\SE}{\\textsf{SE}}\n\\newcommand{\\SSE}{\\textsf{SSE}}\n\\newcommand{\\RSS}{\\textsf{RSS}}\n\\newcommand{\\FSS}{\\textsf{FSS}}\n\\renewcommand{\\SS}{\\textsf{SS}}\n\\newcommand{\\MSE}{\\textsf{MSE}}\n\\newcommand{\\SSR}{\\textsf{SSR}}\n\\newcommand{\\Be}{\\textsf{Beta}}\n\\newcommand{\\St}{\\textsf{St}}\n\\newcommand{\\Ca}{\\textsf{C}}\n\\newcommand{\\Lv}{\\textsf{LÃ©vy}}\n\\newcommand{\\Exp}{\\textsf{Exp}}\n\\newcommand{\\GDP}{\\textsf{GDP}}\n\\newcommand{\\NcSt}{\\textsf{NcSt}}\n\\newcommand{\\Bin}{\\textsf{Bin}}\n\\newcommand{\\NB}{\\textsf{NegBin}}\n\\newcommand{\\Mult}{\\textsf{MultNom}}\n\\renewcommand{\\NG}{\\textsf{NG}}\n\\newcommand{\\N}{\\textsf{N}}\n\\newcommand{\\Ber}{\\textsf{Ber}}\n\\newcommand{\\Poi}{\\textsf{Poi}}\n\\newcommand{\\Gam}{\\textsf{Gamma}}\n\\newcommand{\\BB}{\\textsf{BB}}\n\\newcommand{\\BF}{\\textsf{BF}}\n\\newcommand{\\Gm}{\\textsf{G}}\n\\newcommand{\\Un}{\\textsf{Unif}}\n\\newcommand{\\Ex}{\\textsf{Exp}}\n\\newcommand{\\DE}{\\textsf{DE}}\n\\newcommand{\\tr}{\\textsf{tr}}\n\\newcommand{\\cF}{{\\cal{F}}}\n\\newcommand{\\cL}{{\\cal{L}}}\n\\newcommand{\\cI}{{\\cal{I}}}\n\\newcommand{\\cB}{{\\cal{B}}}\n\\newcommand{\\cP}{{\\cal{P}}}\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\pperp}{\\mathrel{{\\rlap{$\\,\\perp$}\\perp\\,\\,}}}\n\\newcommand{\\OFP}{(\\Omega,\\cF, \\P)}\n\\newcommand{\\eps}{\\boldsymbol{\\epsilon}}\n\\def\\ehat{\\hat{\\boldsymbol{\\epsilon}}}\n\\newcommand{\\Psib}{\\boldsymbol{\\Psi}}\n\\newcommand{\\Omegab}{\\boldsymbol{\\Omega}}\n\\newcommand{\\1}{\\mathbf{1}_n}\n\\newcommand{\\gap}{\\vspace{8mm}}\n\\newcommand{\\ind}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}}\n\\newcommand{\\iid}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}}\n\\newcommand{\\simiid}{\\ensuremath{\\mathrel{\\mathop{\\sim}\\limits^{\\rm\niid}}}}\n\\newcommand{\\eqindis}{\\mathrel{\\mathop{=}\\limits^{\\rm D}}}\n\\newcommand{\\SSZ}{S_{zz}}\n\\newcommand{\\SZW}{S_{zw}}\n\\newcommand{\\Var}{\\textsf{Var}}\n\\newcommand{\\corr}{\\textsf{corr}}\n\\newcommand{\\cov}{\\textsf{cov}}\n\\newcommand{\\diag}{\\textsf{diag}}\n\\newcommand{\\var}{\\textsf{var}}\n\\newcommand{\\Cov}{\\textsf{Cov}}\n\\newcommand{\\Sam}{{\\cal S}}\n\\newcommand{\\VS}{{\\cal V}}\n\\def\\H{\\mathbf{H}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\tY}{\\tilde{\\mathbf{Y}}}\n\\newcommand{\\Yhat}{\\hat{\\mathbf{Y}}}\n\\newcommand{\\yhat}{\\hat{\\mathbf{y}}}\n\\newcommand{\\Yobs}{\\mathbf{Y}_{{\\cal S}}}\n\\newcommand{\\barYobs}{\\bar{Y}_{{\\cal S}}}\n\\newcommand{\\barYmiss}{\\bar{Y}_{{\\cal S}^c}}\n\\def\\bv{\\mathbf{b}}\n\\def\\X{\\mathbf{X}}\n\\def\\XtX{\\X^T\\X}\n\\def\\tX{\\tilde{\\mathbf{X}}}\n\\def\\x{\\mathbf{x}}\n\\def\\xbar{\\bar{\\mathbf{x}}}\n\\def\\Xbar{\\bar{\\mathbf{X}}}\n\\def\\Xg{\\mathbf{X}_{\\boldsymbol{\\gamma}}}\n\\def\\Ybar{\\bar{\\Y}}\n\\def\\ybar{\\bar{y}}\n\\def\\y{\\mathbf{y}}\n\\def\\Yf{\\mathbf{Y_f}}\n\\def\\W{\\mathbf{W}}\n\\def\\L{\\mathbf{L}}\n\\def\\m{\\mathbf{m}}\n\\def\\n{\\mathbf{n}}\n\\def\\w{\\mathbf{w}}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\z{\\mathbf{z}}\n\\def\\v{\\mathbf{v}}\n\\def\\u{\\mathbf{u}}\n\n\\def\\zero{\\mathbf{0}}\n\\def\\one{\\mathbf{1}}\n\\newcommand{\\taub}{\\boldsymbol{\\tau}}\n\\newcommand{\\betav}{\\boldsymbol{\\beta}}\n\\newcommand{\\alphav}{\\boldsymbol{\\alpha}}\n\\newcommand{\\bfomega}{\\boldsymbol{\\omega}}\n\\newcommand{\\bfchi}{\\boldsymbol{\\chi}}\n\\newcommand{\\bfLambda}{\\boldsymbol{\\Lambda}}\n\\newcommand{\\Lmea}{{\\cal L}} \n\\newcommand{\\scale}{\\bflambda} \n\\newcommand{\\Scale}{\\bfLambda} \n\\newcommand{\\bfscale}{\\bflambda} \n\\newcommand{\\mean}{\\bfchi}\n\\newcommand{\\loc}{\\bfchi}\n\\newcommand{\\bfmean}{\\bfchi}\n\\newcommand{\\bfx}{\\x}\n\\renewcommand{\\k}{g}\n\\newcommand{\\Gen}{{\\cal G}}\n\\newcommand{\\Levy}{L{\\'e}vy}\n\\def\\bfChi    {\\boldsymbol{\\Chi}}\n\\def\\bfOmega  {\\boldsymbol{\\Omega}}\n\\newcommand{\\Po}{\\mbox{\\sf P}}\n\\newcommand{\\A}{\\mathbf{A}}\n\\def\\a{\\mathbf{a}}\n\\def\\K{\\mathbf{K}}\n\\newcommand{\\B}{\\mathbf{B}}\n\\def\\b{\\boldsymbol{\\beta}}\n\\def\\bhat{\\hat{\\boldsymbol{\\beta}}}\n\\def\\btilde{\\tilde{\\boldsymbol{\\beta}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\bg{\\boldsymbol{\\beta_\\gamma}}\n\\def\\bgnot{\\boldsymbol{\\beta_{(-\\gamma)}}}\n\\def\\mub{\\boldsymbol{\\mu}}\n\\def\\tmub{\\tilde{\\boldsymbol{\\mu}}}\n\\def\\muhat{\\hat{\\boldsymbol{\\mu}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\tk{\\boldsymbol{\\theta}_k}\n\\def\\tj{\\boldsymbol{\\theta}_j}\n\\def\\Mk{\\boldsymbol{{\\cal M}}_k}\n\\def\\M{\\boldsymbol{{\\cal M}}}\n\\def\\Mj{\\boldsymbol{{\\cal M}}_j}\n\\def\\Mi{\\boldsymbol{{\\cal M}}_i}\n\\def\\Mg{{\\boldsymbol{{\\cal M}_\\gamma}}}\n\\def\\Mnull{\\boldsymbol{{\\cal M}}_{N}}\n\\def\\gMPM{\\boldsymbol{\\gamma}_{\\text{MPM}}}\n\\def\\gHPM{\\boldsymbol{\\gamma}_{\\text{HPM}}}\n\\def\\Mfull{\\boldsymbol{{\\cal M}}_{F}}\n\\def\\NS{\\boldsymbol{{\\cal N}}}\n\\def\\tg{\\boldsymbol{\\theta}_{\\boldsymbol{\\gamma}}}\n\\def\\g{\\boldsymbol{\\gamma}}\n\\def\\eg{\\boldsymbol{\\eta}_{\\boldsymbol{\\gamma}}}\n\\def\\G{\\mathbf{G}}\n\\def\\cM{\\cal M}\n\\def\\D{\\boldsymbol{\\Delta}}\n\\def\\Dbf{\\mathbf{D}}\n\\def \\shat{{\\hat{\\sigma}}^2}\n\\def\\uv{\\mathbf{u}}\n\\def\\l {\\lambda}\n\\def\\d{\\delta}\n\\def\\deltab{\\mathbf{\\delta}}\n\\def\\Sigmab{\\boldsymbol{\\Sigma}}\n\\def\\Phib{\\boldsymbol{\\Phi}}\n\\def\\Lambdab{\\boldsymbol{\\Lambda}}\n\\def\\lambdab{\\boldsymbol{\\lambda}}\n\\def\\Mg{{\\cal M}_\\gamma}\n\\def\\S{{\\cal{S}}}\n\\def\\Sbf{{\\mathbf{S}}}\n\\def\\qg{p_{\\boldsymbol{\\gamma}}}\n\\def\\pg{p_{\\boldsymbol{\\gamma}}}\n\\def\\T{\\boldsymbol{\\Theta}}\n\\def\\Tb{\\boldsymbol{\\Theta}}\n\\def\\t{\\mathbf{t}}\n\n\n\n- Frequentist Risk in Orthogonal Regression\n- James-Stein Estimation\n\n\n. . .\n\nReadings:\n\n  -   Seber & Lee Chapter  Chapter 12\n\n## Orthogonal Regression\n\n-   Consider the model $\\Y = \\X \\b + \\e$ where $\\X$ is $n \\times p$ with\n    $n > p$ and $\\X^T\\X = \\I_p$.\n    \n- If $\\X$ has orthogonal columns, then $\\bhat = \\X^T\\Y$ is the OLS estimator of\n    $\\b$.\n    \n-   The OLS estimator is unbiased and has minimum variance among all\n\n-   The MSE for estimating $\\b$ is $\\E_\\Y[( \\b - \\bhat)^T(\\b -\\bhat)] = \\sigma^2\n  \\tr[(\\X^T\\X)^{-1}] = p\\sigma^2$\n  \n- Can always take a general regression problem and transform design so that the model matrix has orthogonal columns \n$$\\X \\b = \\U \\D \\V^T \\b = \\U \\alphav$$\nwhere new parameters are $\\alphav = \\D \\V^T \\b$ and $\\U^T\\U = \\I_p$.\n\n- Orthogonal polynomials, Fourier bases and wavelet regression are other examples. \n\n- $\\hat{\\alphav} = \\U^T\\Y$ and MSE of $\\hat{\\alphav}$ is $p\\sigma^2$\n\n- so WLOG we will assume that $\\X$ has orthogonal columns\n\n## Shrinkage Estimators\n\n- the $g$-prior and Ridge prior are equivalent in the orthogonal case\n$$\\b \\sim \\N(\\zero_p, \\sigma^2 \\I_p/\\kappa)$$\nusing the ridge parameterization of the prior $\\kappa = 1/g$\n- Bayes estimator in this case is\n$$\\bhat_\\kappa = \\frac{1}{1+\\kappa} \\bhat$$\n- MSE of $\\bhat_\\kappa$ is\n$$\\MSE(\\bhat_\\kappa) = \\frac{1}{(1+\\kappa)^2} \\sigma^2 p + \\frac{\\kappa^2}{(1 + \\kappa)^2} \\sum_{j=1}^{p} \\beta_j^2$$\n- squared bias term grows with $\\kappa$ and variance term decreases with $\\kappa$\n\n## Shrinkage\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n- in principle, with the right choice of $\\kappa$ we can get a better estimator and reduce the MSE\n\n- while not unbiased, what we pay for bias we can make up for with a reduction in variance\n\n- the variance-bias decomposition of MSE based on the plot suggests there is an optimal value of $\\kappa$ the improves over OLS in terms of MSE\n\n- \"optimal\" $\\kappa$\n$$\\kappa = \\frac{p \\sigma^2}{\\|\\b^*\\|^2}$$\nwhere $\\b^*$ is the true value of $\\b$\n\n- but never know that in practice!\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-james-stein_files/figure-revealjs/shrinkage-1.png){fig-align='center' width=5in height=5in}\n:::\n:::\n\n\n:::\n::::\n\n## Estimating $\\kappa$\n\n- if we use the optimal $\\kappa$, the shrinkage estimator can be written as\n$$\\btilde = \\frac{\\|\\b\\|^2}{p \\sigma^2 + \\|\\b\\|^2} \\bhat$$\nor \n$$\\btilde = \\left(1 - \\frac{p\\sigma^2}{p\\sigma^2 + \\|\\b\\|^2} \\right) \\bhat$$\n- but note that $\\E\\|\\bhat\\|^2 = p \\sigma^2 + \\|\\b\\|^2$ (the denominator)\n\n- plugging in $\\|\\bhat\\|^2$ for the denominator leads to an estimator that we can compute!\n$$\\btilde = \\left(1 - \\frac{p\\sigma^2}{\\|\\bhat\\|^2} \\right) \\bhat$$\n\n\n\n## James-Stein Estimators\n\nin James and Stein (1961) proposed a shrinkage estimator that dominated the MLE for the mean of a multivariate normal distribution \n$$\\btilde_{JS} = \\left(1 - \\frac{(p-2)\\sigma^2}{\\|\\bhat\\|^2} \\right) \\bhat$$\n(equivalent to our orthogonal regression case; just multiply everything by $\\X^T$ to show)\n\n-  they showed that this is  the best (in terms of smallest MSE) of all estimators of the form \n$\\left(1- \\frac{b}{\\|\\bhat\\|^2} \\right) \\bhat$\n\n- it is possible to show that the MSE of the James-Stein estimator is \n$$\\MSE(\\btilde_{JS}) = 2\\sigma^2$$\nwhich is less than the MSE of the OLS estimator if $p > 2$!  (more on this in STA732)\n\n::: footer\n:::\n\n## Negative Shrinkage?\n\n:::: {.columns}\n::: {.column width=\"65%\"}\n- one potential problem with the James-Stein estimator\n$$\\btilde_{JS} = \\left(1 - \\frac{(p-2)\\sigma^2}{\\|\\bhat\\|^2} \\right) \\bhat$$\nis that the term in the parentheses can be negative if $\\|\\bhat\\|^2 < (p-2)\\sigma^2$\n\n- How likely is this to happen?\n\n- Suppose that each of the parameters $\\beta_j$ are actually zero, then $\\bhat \\sim \\N(\\zero_p, \\sigma^2 \\I_p)$  then $\\|\\bhat\\|^2 /\\sigma^2 \\sim  \\chi^2_p$\n\n- compute the probability that $\\chi^2_p < (p-2)$\n\n- so if the model is full of small effects, the James-Stein can lead to negative shrinkage!\n\n:::\n\n\n\n::: {.column width=\"35%\"}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-james-stein_files/figure-revealjs/unnamed-chunk-1-1.png){fig-align='center' width=8in height=5in}\n:::\n:::\n\n\n\n\n:::\n\n::::\n\n\n## Positive Part James-Stein Estimator\n\n- any shrinkage estimator of the form $\\btilde = (1 - b) \\bhat$ is inadmissible if the shrinkage factor is negative or greater than one\n(there is a better estimator)\n\n- Baranchik (1964) proposed the positive part James-Stein estimator\n$$\\btilde_{PPJS} = \\left(1 - \\frac{(p-2)\\sigma^2}{\\|\\bhat\\|^2} \\right)^+ \\bhat$$ where $(x)^+ = \\max(x, 0)$\n\n- This is the same as the James-Stein estimator if the shrinkage factor is positive and zero otherwise. (related to testing the null hypothesis that all the $\\beta_j$ are zero)\n\n- it turns out this is also inadmissible! ie there is another estimator that has smaller MSE!\n\n- if we care about admissibility, we may want to stick to Bayes rules that do not depend on the data!\n\n- more on admissibility & minimaxity in STA732!\n\n::: footer\n:::\n\n## Bayes and Admissibilty\n\n- Bayes rules based on proper priors are generally always admissible \n(see Christian Robert (2007) The Bayesian Choice for more details)\n\n- unique Bayes rules are admissible\n\n- Generalized Bayes rules based on improper priors may not be inadmissible, but this will depend on the loss function and the prior\n\n- under regularity conditions, limits of Bayes rules will be admissible\n\n- the Positive-Part James-Stein estimator fails to be admissible under squared error loss as Bayes risk is  not continuous\n\n\n## Positive Part James-Stein Estimator and Testimators\n\n- the positive part James-Stein estimator can be shown to be related to  **testimators** where if we fail to reject the hypothesis that all the $\\beta_j$ are zero at some level,  we set all coefficients to zero, and otherwise we \nshrink the coefficients by an amount that depends on how large the test statistic ($\\|\\bhat\\|^2$) is.\n\n- note this can shrink all the coefficients to zero if the majority are small so increased bias for large coefficients that are not zero!\n\n- this is a form of **model selection** where we are selecting the model that has all the coefficients zero!\n\n\n## LASSO Estimator\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n- an alternative estimator that allows for shrinkage and selection is the LASSO (Least Absolute Shrinkage and Selection Operator).\n\n- the LASSO replaces the penalty term in the ridge regression with an $L_1$ penalty term\n$$\\bhat_{LASSO} = \\argmin_{\\b} \\left\\{ \\|\\Y - \\X \\b\\|^2 + \\lambda \\|\\b\\|_1 \\right\\}$$\n- the LASSO can also be shown to be the posterior mode of a Bayesian model with independent Laplace or double exponential prior distributions on the coefficients.\n\n- as the double exponential prior is a \"scale\" mixture of normals, this provides a generalization of the ridge regression.\n\n:::\n\n::: {.column width=\"40%\"}\n\n![](img/lasso.png)\n\nfrom [Machine Learning with R](https://livebook.manning.com/book/machine-learning-with-r-the-tidyverse-and-mlr/chapter-11/)\n:::\n\n:::\n\n\n\n\n\n\n",
    "supporting": [
      "10-james-stein_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}