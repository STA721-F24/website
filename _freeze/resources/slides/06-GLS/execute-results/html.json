{
  "hash": "927ac0e35bedc3af08d7743c68e71000",
  "result": {
    "engine": "knitr",
    "markdown": "---\nsubtitle: \"STA 721: Lecture 6\"\ntitle: \"Generalized Least Squares, BLUES &  BUES\"\nauthor: \"Merlise Clyde (clyde@duke.edu)\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    smaller: true\n    logo: ../../img/icon.png\n    footer: <https://sta721-F24.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"   \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\nnumber-sections: false\nfilters:\n  - custom-numbered-blocks  \ncustom-numbered-blocks:\n  groups:\n    thmlike:\n      colors: [948bde, 584eab]\n      boxstyle: foldbox.simple\n      collapse: false\n      listin: [mathstuff]\n    todos: default  \n  classes:\n    Theorem:\n      group: thmlike\n      numbered: false\n    Lemma:\n      group: thmlike\n      numbered: false\n    Corollary:\n      group: thmlike\n      numbered: false\n    Proposition:\n      group: thmlike\n      numbered: false      \n    Proof:\n      group: thmlike\n      numbered: false\n      collapse: true  \n    Exercise:\n      group: thmlike\n      numbered: false\n    Definition:\n      group: thmlike\n      numbered: false\n      colors: [d999d3, a01793]\n    Feature: \n       numbered: false\n    TODO:\n      label: \"To do\"\n      colors: [e7b1b4, 8c3236]\n      group: todos\n      listin: [stilltodo]\n    DONE:\n      label: \"Done\"\n      colors: [cce7b1, 86b754]  \n      group: todos  \n---\n\n\n\n\n\n## Outline\n\n\\usepackage{xcolor}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator{\\sgn}{sgn}\n\\newcommand{\\e}{\\mathbf{e}}\n\\newcommand{\\Mb}{\\mathbf{M}}\n\\renewcommand{\\P}{\\mathbf{P}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\R}{\\textsf{R}}\n\\newcommand{\\mat}[1] {\\mathbf{#1}}\n\\newcommand{\\E}{\\textsf{E}}\n\\newcommand{\\SE}{\\textsf{SE}}\n\\newcommand{\\SSE}{\\textsf{SSE}}\n\\newcommand{\\RSS}{\\textsf{RSS}}\n\\newcommand{\\FSS}{\\textsf{FSS}}\n\\renewcommand{\\SS}{\\textsf{SS}}\n\\newcommand{\\MSE}{\\textsf{MSE}}\n\\newcommand{\\SSR}{\\textsf{SSR}}\n\\newcommand{\\Be}{\\textsf{Beta}}\n\\newcommand{\\St}{\\textsf{St}}\n\\newcommand{\\Ca}{\\textsf{C}}\n\\newcommand{\\Lv}{\\textsf{Lévy}}\n\\newcommand{\\Exp}{\\textsf{Exp}}\n\\newcommand{\\GDP}{\\textsf{GDP}}\n\\newcommand{\\NcSt}{\\textsf{NcSt}}\n\\newcommand{\\Bin}{\\textsf{Bin}}\n\\newcommand{\\NB}{\\textsf{NegBin}}\n\\newcommand{\\Mult}{\\textsf{MultNom}}\n\\renewcommand{\\NG}{\\textsf{NG}}\n\\newcommand{\\N}{\\textsf{N}}\n\\newcommand{\\Ber}{\\textsf{Ber}}\n\\newcommand{\\Poi}{\\textsf{Poi}}\n\\newcommand{\\Gam}{\\textsf{Gamma}}\n\\newcommand{\\BB}{\\textsf{BB}}\n\\newcommand{\\BF}{\\textsf{BF}}\n\\newcommand{\\Gm}{\\textsf{G}}\n\\newcommand{\\Un}{\\textsf{Unif}}\n\\newcommand{\\Ex}{\\textsf{Exp}}\n\\newcommand{\\DE}{\\textsf{DE}}\n\\newcommand{\\tr}{\\textsf{tr}}\n\\newcommand{\\cF}{{\\cal{F}}}\n\\newcommand{\\cL}{{\\cal{L}}}\n\\newcommand{\\cI}{{\\cal{I}}}\n\\newcommand{\\cB}{{\\cal{B}}}\n\\newcommand{\\cP}{{\\cal{P}}}\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\pperp}{\\mathrel{{\\rlap{$\\,\\perp$}\\perp\\,\\,}}}\n\\newcommand{\\OFP}{(\\Omega,\\cF, \\P)}\n\\newcommand{\\eps}{\\boldsymbol{\\epsilon}}\n\\newcommand{\\Psib}{\\boldsymbol{\\Psi}}\n\\newcommand{\\Omegab}{\\boldsymbol{\\Omega}}\n\\newcommand{\\1}{\\mathbf{1}_n}\n\\newcommand{\\gap}{\\vspace{8mm}}\n\\newcommand{\\ind}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}}\n\\newcommand{\\iid}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}}\n\\newcommand{\\simiid}{\\ensuremath{\\mathrel{\\mathop{\\sim}\\limits^{\\rm\niid}}}}\n\\newcommand{\\eqindis}{\\mathrel{\\mathop{=}\\limits^{\\rm D}}}\n\\newcommand{\\SSZ}{S_{zz}}\n\\newcommand{\\SZW}{S_{zw}}\n\\newcommand{\\Var}{\\textsf{Var}}\n\\newcommand{\\corr}{\\textsf{corr}}\n\\newcommand{\\cov}{\\textsf{cov}}\n\\newcommand{\\diag}{\\textsf{diag}}\n\\newcommand{\\var}{\\textsf{var}}\n\\newcommand{\\Cov}{\\textsf{Cov}}\n\\newcommand{\\Sam}{{\\cal S}}\n\\newcommand{\\VS}{{\\cal V}}\n\\def\\H{\\mathbf{H}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\tY}{\\tilde{\\mathbf{Y}}}\n\\newcommand{\\Yhat}{\\hat{\\mathbf{Y}}}\n\\newcommand{\\yhat}{\\hat{\\mathbf{y}}}\n\\newcommand{\\Yobs}{\\mathbf{Y}_{{\\cal S}}}\n\\newcommand{\\barYobs}{\\bar{Y}_{{\\cal S}}}\n\\newcommand{\\barYmiss}{\\bar{Y}_{{\\cal S}^c}}\n\\def\\bv{\\mathbf{b}}\n\\def\\X{\\mathbf{X}}\n\\def\\XtX{\\X^T\\X}\n\\def\\tX{\\tilde{\\mathbf{X}}}\n\\def\\x{\\mathbf{x}}\n\\def\\xbar{\\bar{\\mathbf{x}}}\n\\def\\Xbar{\\bar{\\mathbf{X}}}\n\\def\\Xg{\\mathbf{X}_{\\boldsymbol{\\gamma}}}\n\\def\\Ybar{\\bar{\\Y}}\n\\def\\ybar{\\bar{y}}\n\\def\\y{\\mathbf{y}}\n\\def\\Yf{\\mathbf{Y_f}}\n\\def\\W{\\mathbf{W}}\n\\def\\L{\\mathbf{L}}\n\\def\\m{\\mathbf{m}}\n\\def\\n{\\mathbf{n}}\n\\def\\w{\\mathbf{w}}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\z{\\mathbf{z}}\n\\def\\v{\\mathbf{v}}\n\\def\\u{\\mathbf{u}}\n\n\\def\\zero{\\mathbf{0}}\n\\def\\one{\\mathbf{1}}\n\\newcommand{\\taub}{\\boldsymbol{\\tau}}\n\\newcommand{\\betav}{\\boldsymbol{\\beta}}\n\\newcommand{\\alphav}{\\boldsymbol{\\alpha}}\n\\newcommand{\\bfomega}{\\boldsymbol{\\omega}}\n\\newcommand{\\bfchi}{\\boldsymbol{\\chi}}\n\\newcommand{\\bfLambda}{\\boldsymbol{\\Lambda}}\n\\newcommand{\\Lmea}{{\\cal L}} \n\\newcommand{\\scale}{\\bflambda} \n\\newcommand{\\Scale}{\\bfLambda} \n\\newcommand{\\bfscale}{\\bflambda} \n\\newcommand{\\mean}{\\bfchi}\n\\newcommand{\\loc}{\\bfchi}\n\\newcommand{\\bfmean}{\\bfchi}\n\\newcommand{\\bfx}{\\x}\n\\renewcommand{\\k}{g}\n\\newcommand{\\Gen}{{\\cal G}}\n\\newcommand{\\Levy}{L{\\'e}vy}\n\\def\\bfChi    {\\boldsymbol{\\Chi}}\n\\def\\bfOmega  {\\boldsymbol{\\Omega}}\n\\newcommand{\\Po}{\\mbox{\\sf P}}\n\\newcommand{\\A}{\\mathbf{A}}\n\\def\\a{\\mathbf{a}}\n\\def\\K{\\mathbf{K}}\n\\newcommand{\\B}{\\mathbf{B}}\n\\def\\b{\\boldsymbol{\\beta}}\n\\def\\bhat{\\hat{\\boldsymbol{\\beta}}}\n\\def\\btilde{\\tilde{\\boldsymbol{\\beta}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\bg{\\boldsymbol{\\beta_\\gamma}}\n\\def\\bgnot{\\boldsymbol{\\beta_{(-\\gamma)}}}\n\\def\\mub{\\boldsymbol{\\mu}}\n\\def\\tmub{\\tilde{\\boldsymbol{\\mu}}}\n\\def\\muhat{\\hat{\\boldsymbol{\\mu}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\tk{\\boldsymbol{\\theta}_k}\n\\def\\tj{\\boldsymbol{\\theta}_j}\n\\def\\Mk{\\boldsymbol{{\\cal M}}_k}\n\\def\\M{\\boldsymbol{{\\cal M}}}\n\\def\\Mj{\\boldsymbol{{\\cal M}}_j}\n\\def\\Mi{\\boldsymbol{{\\cal M}}_i}\n\\def\\Mg{{\\boldsymbol{{\\cal M}_\\gamma}}}\n\\def\\Mnull{\\boldsymbol{{\\cal M}}_{N}}\n\\def\\gMPM{\\boldsymbol{\\gamma}_{\\text{MPM}}}\n\\def\\gHPM{\\boldsymbol{\\gamma}_{\\text{HPM}}}\n\\def\\Mfull{\\boldsymbol{{\\cal M}}_{F}}\n\\def\\NS{\\boldsymbol{{\\cal N}}}\n\\def\\tg{\\boldsymbol{\\theta}_{\\boldsymbol{\\gamma}}}\n\\def\\g{\\boldsymbol{\\gamma}}\n\\def\\eg{\\boldsymbol{\\eta}_{\\boldsymbol{\\gamma}}}\n\\def\\G{\\mathbf{G}}\n\\def\\cM{\\cal M}\n\\def\\D{\\boldsymbol{\\Delta}}\n\\def\\Dbf{\\mathbf{D}}\n\\def \\shat{{\\hat{\\sigma}}^2}\n\\def\\uv{\\mathbf{u}}\n\\def\\l {\\lambda}\n\\def\\d{\\delta}\n\\def\\deltab{\\mathbf{\\delta}}\n\\def\\Sigmab{\\boldsymbol{\\Sigma}}\n\\def\\Phib{\\boldsymbol{\\Phi}}\n\\def\\Lambdab{\\boldsymbol{\\Lambda}}\n\\def\\lambdab{\\boldsymbol{\\lambda}}\n\\def\\Mg{{\\cal M}_\\gamma}\n\\def\\S{{\\cal{S}}}\n\\def\\Sbf{{\\mathbf{S}}}\n\\def\\qg{p_{\\boldsymbol{\\gamma}}}\n\\def\\pg{p_{\\boldsymbol{\\gamma}}}\n\\def\\T{\\boldsymbol{\\Theta}}\n\\def\\Tb{\\boldsymbol{\\Theta}}\n\\def\\t{\\mathbf{t}}\n\n\n\n- General Least Squares and MLEs\n- Gauss-Markov Theorem & BLUEs\n- MVUE\n\n\n\n. . .\n\nReadings: \n\n- Christensen Chapter 2 and 10 (Appendix B as needed)\n \n- Seber & Lee Chapter 3\n \n## Other Error Distributions\n\nModel:  \n\\begin{align} \\Y & = \\X\\b + \\eps  \\quad\n              \\E[\\eps]  = \\zero_n \\\\\n              \\Cov[\\eps] & = \\sigma^2 \\V\n\\end{align}\nwhere $\\sigma^2$ is a scalar and $\\V$ is a $n \\times n$ symmetric matrix\n\n. . .\n\nExamples:\n\n- Heteroscedasticity: $\\V$ is a diagonal matrix with $[\\V]_{ii} =  v_i$\n  - $v_{i} = 1/n_i$ if $y_i$ is the mean of $n_i$ observations\n  - survey weights or propogation of  measurement errors in physics models\n  \n  \n- Correlated data:\n  - time series; first order auto-regressive model with equally spaced data $\\Cov[\\eps] = \\sigma^2 \\V$, where $v_{ij} = \\rho^{|i−j|}$.\n  \n- Hierarchical models with random effects\n\n## OLS under a General Covariance\n\n- Is it still unbiased? What's its variance?  Is it still the BLUE?\n\n- **Unbiasedness of** $\\bhat$\n\\begin{align}\n\\E[\\bhat] & = \\E[(\\XtX)^{-1} \\X^T \\Y] \\\\\n          & = (\\XtX)^{-1} \\X^T \\E[\\Y] =  (\\XtX)^{-1} \\X^T \\E[\\X\\b + \\eps] \\\\\n          & = \\b + \\zero_p = \\b\n\\end{align}\n\n-  **Covariance of** $\\bhat$\n\\begin{align}\n\\Cov[\\bhat] & = \\Cov[(\\XtX)^{-1} \\X^T \\Y] \\\\\n          & = (\\XtX)^{-1} \\X^T \\Cov[\\Y]  \\X (\\XtX)^{-1} \\\\\n          & = \\sigma^2 (\\XtX)^{-1} \\X^T  \\V \\X (\\XtX)^{-1}\n\\end{align}\n\n- Not necessarily $\\sigma^2 (\\XtX)^{-1}$ unless $\\V$ has a special form\n\n## GLS via Whitening\n\nTransform the data and reduce problem to one we have solved!\n\n- For $\\V > 0$ use the Spectral Decomposition\n$$\\V = \\U \\Lambdab \\U^T = \\U \\Lambdab^{1/2} \\Lambdab^{1/2} \\U^T$$\n\n- define the symmetric square root of $\\V$ as\n$$\\V^{1/2} \\equiv \\U \\Lambdab^{1/2} \\U^T$$\n- transform model:\n\\begin{align*}\n\\V^{-1/2} \\Y & = \\V^{-1/2} \\X\\b + \\V^{-1/2}\\eps \\\\\n\\tilde{\\Y} & = \\tilde{\\X} \\b + \\tilde{\\eps}\n\\end{align*}\n\n- Since $\\Cov[\\tilde{\\eps}] = \\sigma^2\\V^{-1/2} \\V \\V^{-1/2} = \\sigma^2 \\I_n$,\nwe know that $\\bhat_\\V \\equiv (\\tX^T\\tX)^{-1} \\tX^T\\tY$ is the BLUE for $\\b$ based on $\\tY$ ($\\X$ full rank)\n\n## GLS\n\n- If $\\V$ is known, then $\\tY$ and $\\Y$ are known linear transformations of each other\n\n- any estimator of $\\b$ that is linear in $\\Y$ is linear in $\\tY$ and vice versa from previous results\n\n- $\\bhat_\\V$ is the BLUE of $\\b$ based on either $\\tY$ or  $\\Y$!\n\n- Substituting back, we have \n\\begin{align}\n\\bhat_\\V & =  (\\tX^T\\tX)^{-1} \\tX^T\\tY \\\\\n         & = (\\X^T \\V^{-1/2}\\V^{-1/2} \\X)^{-1} \\X^T\\V^{-1/2}\\V^{-1/2}\\Y \\\\\n         & = (\\X^T \\V^{-1} \\X)^{-1} \\X^T\\V^{-1}\\Y \n\\end{align}\nwhich is the **Generalized Least Squares Estimator** of $\\b$\n\n. . .\n\n::: {.Exercise}\n## Weighted Regression\nConsider the model $\\Y = \\beta \\x + \\eps$ where $\\Cov[\\eps]$ is a  known diagonal matrix $\\V$.   Write out the GLS estimator in terms of sums and interpret.\n:::\n\n## GLS of $\\mub$ (Full Rank Case)$^{\\dagger}$\n\n- the OLS/MLE of $\\mub \\in C(\\X)$ with transformed variables is \n\\begin{align*}\n\\P_{\\tX} \\tY & = \\tX \\bhat_\\V \\\\\n\\tX \\left(\\tX^T\\tX\\right)^{-1}\\tX^T \\tY & = \\tX \\bhat_\\V \\\\\n\\V^{-1/2} \\X \\left(\\X^T\\V^{-1}\\X\\right)^{-1}\\X^T \\V^{-1} \\Y  & = \\V^{-1/2} \\X \\bhat_\\V \\end{align*}\n\n- since $\\V$ is positive definite, multiple thru by $\\V^{1/2}$, to show that $\\bhat_\\V$ is a GLS/MLE estimator of $\\b$ iff\n$$\\X \\left(\\X^T\\V^{-1}\\X\\right)^{-1}\\X^T \\V^{-1} \\Y = \\X \\bhat_\\V$$\n - Is $\\P_\\V \\equiv \\X \\left(\\X^T\\V^{-1}\\X\\right)^{-1}\\X^T \\V^{-1}$ a projection onto $C(\\X)$?  Is it an orthogonal projection onto $C(\\X)$?\n\n::: footer\n$\\dagger$ if $\\X$ is not full rank replace $\\left(\\X^T\\V^{-1}\\X\\right)^{-1}$ with $\\left(\\X^T\\V^{-1}\\X\\right)^{-}$\n:::\n## Projections \n\nWe want to show that $\\P_\\V \\equiv \\X \\left(\\X^T\\V^{-1}\\X\\right)^{-1}\\X^T \\V^{-1}$ is a projection onto $C(\\X)$\n\n- from the definition of $\\P_\\V$ it follows that $\\m \\in C(\\P_\\v)$ implies that\n$\\m = \\P_\\V \\m = \\X\\left(\\X^T\\V^{-1}\\X\\right)^{-1}\\X^T\\m$ so $C(\\P_\\V) \\subset C(\\X)$\n\n- since $\\P_\\tX$ is a projection onto $C(\\tX)$ we have \n\\begin{align*}\n\\P_{\\tX} \\tX & = \\tX \\\\\n\\tX \\left(\\tX^T\\tX\\right)^{-1}\\tX^T \\tX & = \\tX  \\\\\n\\V^{-1/2} \\X \\left(\\X^T\\V^{-1}\\X\\right)^{-1}\\X^T \\V^{-1} \\X & = \\V^{-1/2} \\X \\\\\n\\V^{-1/2} \\P_\\V \\X & = \\V^{-1/2} \\X \n\\end{align*}\n\n- We can multiply both sides by $\\V^{1/2} > 0$, so that\n$\\P_\\V \\X = \\X$\n\n- for $\\m \\in C(\\X)$, $\\P_\\V \\m = \\m$ and $C(\\X) \\subset C(\\P_\\V)$\n\n- $\\quad \\quad \\therefore C(\\P_\\V) = C(\\X)$ so that $\\P_\\V$ is a projection onto $C(\\X)$ \n\n\n::: footer\n:::\n\n## Oblique Projections \n::: {.Proposition} \n## Projection\nThe $n \\times n$ matrix $\\P_\\V \\equiv \\X \\left(\\X^T\\V^{-1}\\X\\right)^{-1}\\X^T \\V^{-1}$ is a projection onto the $C(\\X)$\n:::\n\n- Show that $\\P_\\V^2 = \\P_\\V$ (idempotent)\n\n- every vector $\\y \\in \\bbR^n$ may be written as $\\y = \\m + \\n$ where\n$\\P_\\v \\y = \\m$ and $(\\I_n - \\P_\\v )\\y = \\n$ where $\\m \\in C(\\P_\\V)$ and \n$\\u \\in N(\\P_\\V)$\n\n\n\n- Is $\\P_\\V$ an orthogonal projection onto $C(\\X)$ for the inner product space $(\\bbR^n, \\langle \\v, \\u \\rangle = \\v^T\\u)$?\n\n. . .\n\n::: {.Definition}\n## Oblique Projection\nFor the inner product space $(\\bbR^n, \\langle \\v, \\u \\rangle = \\v^T\\u)$, a projection $\\P$  that is not an orthogonal projection is called an _oblique projection_\n:::\n\n\n\n## Loss Function\n\nThe GLS estimator minimizes the following generalized squared error loss:\n\\begin{align}\n\\| \\tY - \\tX\\b \\|^2 & = (\\tY - \\tX\\b)^T(\\tY - \\tX\\b) \\\\\n                    & = (\\Y - \\X\\b)^T \\V^{-1/2}\\V^{-1/2}(\\Y - \\X\\b) \\\\\n                    & = (\\Y - \\X\\b)^T \\V^{-1}(\\Y - \\X\\b)  \\\\\n                    & = \\| \\Y - \\X\\b\\|^2_{\\V^{-1}}\n\\end{align}\nwhere we can change the inner product to be \n$$\\langle \\u, \\v \\rangle_{\\V^{-1}} \\equiv \\u^T\\V^{-1} \\v$$\n\n## Orthogonality in an Inner Product Space\n\n::: {.Definition}\n## Orthogonal Projecton\nFor an inner product space,  ($\\bbR^n, \\langle , \\rangle$). The projection $\\P$ is an orthogonal projection if for every vector $\\x$ and $\\y$ in $\\bbR^n$, \n$$\n  \\langle \\P\\x, (\\I_n -\\P)\\y \\rangle = \\langle (\\I_n - \\P) \\x ,\\P \\y \\rangle = 0\n$$\nEquivalently:\n$$\n  \\langle \\x ,\\P\\y \\rangle = \\langle \\P\\x , \\P\\y \\rangle =\\langle \\P\\x,\\y \\rangle\n$$\n\n:::\n  \n::: {.Exercise}\nShow that $\\P_\\V$ is an orthogonal projection under the inner product $\\langle \\x, \\y \\rangle_{\\V^{-1}} \\equiv \\x^T\\V^{-1} \\y$\n:::\n\n\n\n  \n\n## Variance of GLS\n\n- Variance of the GLS estimator $\\bhat_\\V =  (\\X^T\\V^{−1}\\X)^{−1}\\X^T\\V^{−1}\\Y$ is much simpler\n\\begin{align}\n\\Cov[\\bhat_\\V] & = (\\X^T\\V^{−1}\\X)^{−1}\\X^T\\V^{−1}\\Cov[\\Y]\\V^{−1}\\X(\\X^T\\V^{−1}\\X)^{−1} \\\\ \n& = (\\X^T\\V^{−1}\\X)^{−1}\\X^T\\V^{−1}\\V\\V^{−1}\\X(\\X^T\\V^{−1}\\X)^{−1} \\\\\n& = \\sigma^2(\\X^T\\V^{−1}\\X)^{−1}(\\X^T\\V^{−1}\\X)(\\X^T\\V^{−1}\\X)^{−1} \\\\\n& = \\sigma^2(\\X^T\\V^{−1}\\X)^{−1}\n\\end{align}\n\n. . .\n\n::: {.Theorem}\n## Gauss-Markov-Aitkin\nLet $\\btilde$ be a  linear unbiased estimator of $\\b$ and $\\bhat_\\V$ be the GLS estimator of $\\b$ in the linear model\n$\\Y = \\X\\b + \\eps$ with $\\E[\\eps] = \\zero_n$ and $\\Cov[\\eps] = \\sigma^2 \\V$ with $\\X$ and $\\V >0$ known.  Then  $\\bhat_\\V$ is the BLUE where\n$$\\Cov[\\btilde] \\ge  \\sigma^2 (\\X^T\\V^{-1} \\X)^{-1} = \\Cov[\\bhat_\\V] $$  \n:::\n\n\n\n\n## When will OLS and GLS be Equal?\n- For what covariance matrices $\\V$ will the OLS and GLS estimators be the same? \n\n\n\n- Figuring this out can help us understand why the GLS estimator has a lower variance in general.\n\n. . .\n\n::: {.Theorem}\nThe estimators $\\bhat$ (OLS) and $\\bhat_\\V$ (GLS) are the same for all $\\Y \\in \\bbR^n$ iff \n$$\\V = \\X \\Psib \\X^T + \\H \\Phib \\H^T$$ for some positive definite matrices $\\Psib$ and $\\Phib$ and a matrix $\\H$ such that $\\H^T \\X = \\zero$.\n\n:::\n\n\n## Outline of Proof\n\nWe need to show that $\\bhat$ and $\\bhat_\\V$ are the same for all $\\Y$. Since both $\\P$ and  $\\P_\\V$ are projections onto $C(\\X)$, $\\bhat$ and $\\bhat_\\V$ will be the same iff $\\P_\\V$ is an orthogonal projection onto $C(\\X)$ so that $\\P_\\V \\n = 0$ for $\\n \\in C(\\X)^\\perp$  (they have the same null spaces)\n\n1. Show that $C(\\X) = C(\\V\\X)$ iff $\\V$ can be written as \n$$\\V = \\X \\Psib \\X^T + \\H \\Phib \\H^T$$\n(Show $C(\\V \\X) \\subset C( \\X)$ iff $\\V$ has the above form and since the two subspaces have the same rank $C(\\X) = C(\\V\\X)$\n\n2. Show that $C(\\X) = C(\\V^{-1} \\X)$ iff $C(\\X) = C(\\V\\X)$\n3. Show that $C(\\X)^\\perp = C(\\V^{-1} \\X)^\\perp$ iff $C(\\X) = C(\\V^{-1} \\X)$\n4. Show that $\\n \\in C(\\X)^\\perp$ iff $\\n \\in C(\\V^{-1}\\X)^\\perp$ so $\\P_\\V \\n = 0$\n\n\n. . .\n\nSee Proposition 2.7.5 and Proof in Christensen \n\n\n\n## Some Intuition \n\nFor the linear model $\\Y = \\X\\b + \\eps$ with $\\E[\\eps] = \\zero_n$ and $\\Cov[\\eps] = \\sigma^2 \\V$, we can always write\n\n\\begin{align} \\eps & = \\P \\eps + (\\I - \\P)\\eps \\\\\n                   & = \\eps_\\X + \\eps_N   \n\\end{align} \n\n- we can recover $\\eps_N$ from the data $\\Y$ but not $\\eps_\\X$:\n\\begin{align} \\P \\Y  & = \\P( \\X\\b + \\eps_\\X + \\eps_n )\\\\\n                     & =  \\X\\b + \\eps_\\X  = \\X\\bhat \\\\\n      (\\I_n - \\P) \\Y  & =  \\eps_N = \\hat{\\eps} = \\e     \n\\end{align} \n\n- Can $\\eps_\\N$ help us estimate $\\X\\b$? What if $\\eps_N$ could tell us something about $\\eps_X$?\n\n- Yes if they were highly correlated!  But if they were independent or uncorrelated then knowing $\\eps_\\N$ doesn't help us!\n\n## Intuition Continued\n\n- For what matrices are $\\eps_\\X$ and $\\eps_N$ uncorrelated?\n\n- Under $\\V = \\I_n$:\n\\begin{align}\n\\E[\\eps_X \\eps_N] & = \\P \\E[\\eps \\eps^T](\\I-\\P) \\\\\n                  & = \\sigma^2 \\P(\\I- \\P) = \\zero\n\\end{align}\nso they are uncorrelated\n\n- For the $\\V$ in the theorem, introduce\n  - $\\Z_\\X$ where $\\E[\\Z_\\X]= \\zero_n$ and $\\Cov[\\Z_\\X] = \\Psib$\n  - $\\Z_\\N$ where $\\E[\\Z_\\N]= \\zero_n$ and $\\Cov[\\Z_\\N] = \\Phib$\n  - $\\Z_\\X$ and $\\Z_\\N$ are uncorrelated, $\\E[\\Z_\\X \\Z_\\N] = \\zero$\n  - $\\eps = \\X \\Z_\\X + \\H \\Z_\\N$ so that $\\eps$ has the desired mean and covariance $\\V$ in the theorem\n  \n## Intuition Continued  \n\nAs a consequence we have  \n\n- $\\eps_\\X = \\P\\eps = \\X\\Z_\\X$\n- $\\eps_\\N = (\\I_n - \\P)\\eps = \\H \\Z_\\N$\n- $\\eps_\\X$ and $\\eps_\\N$ are uncorrelated\n\\begin{align}\n\\E[\\eps_\\X \\eps_\\N] & = \\E[\\X \\Z_\\X \\Z_\\N^T \\H^T] \\\\\n                    & = \\X \\zero \\H^T \\\\\n                    & = \\zero \n\\end{align}\n\n- so that $\\eps_\\X$ and $\\eps_\\N$ are uncorrelated with $\\V = \\X \\Psib \\X^T + \\H\\Phib \\H$ ^T$ \n\n- Alternative Statement of Theorem: $\\bhat = \\bhat_\\V$ for all $\\Y$ under $\\Cov[\\Y] = \\sigma^2 \\V$ iff $\\P\\Y$ and $(\\I - \\P)\\Y$ are uncorrelated\n\n## Equivalence of GLS estimators\n\nThe following corollary to the theorem establishes when two GLS estimators for different $\\Cov[\\eps]$ are equivalent :\n\n::: {.Corollary}\nSuppose $\\V = \\X \\Psib \\X ^ T + \\Phib\\H \\Omegab \\H^T \\Phib$.  Then $\\bhat_\\V = \\bhat_\\Phib$\n:::\n\n\n- Can you construct an equivalent representation based on zero correlation of $\\P_\\Phib \\Y$ and $(\\I_n - \\P_\\Phib)\\Y$ when $\\Cov[\\eps] = \\sigma^2 \\V?$",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}